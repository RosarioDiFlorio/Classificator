In information theory, Fano's inequality (also known as the Fano converse and the Fano lemma) relates the average information lost in a noisy channel to the probability of the categorization error. It was derived by Robert Fano in the early 1950s while teaching a Ph.D. seminar in information theory at MIT, and later recorded in his 1961 textbook.
It is used to find a lower bound on the error probability of any decoder as well as the lower bounds for minimax risks in density estimation.
Let the random variables X and Y represent input and output messages with a joint probability 
  
    
      
        P
        (
        x
        ,
        y
        )
      
    
    {\displaystyle P(x,y)}
  . Let e represent an occurrence of error; i.e., that 
  
    
      
        X
        ≠
        
          
            
              X
              ~
            
          
        
      
    
    {\displaystyle X\neq {\tilde {X}}}
  , with 
  
    
      
        
          
            
              X
              ~
            
          
        
        =
        f
        (
        Y
        )
      
    
    {\displaystyle {\tilde {X}}=f(Y)}
   being an approximate version of 
  
    
      
        X
      
    
    {\displaystyle X}
  . Fano's inequality is

  
    
      
        H
        (
        X
        
          |
        
        Y
        )
        ≤
        H
        (
        e
        )
        +
        P
        (
        e
        )
        log
        ⁡
        (
        
          |
        
        
          
            X
          
        
        
          |
        
        −
        1
        )
        ,
      
    
    {\displaystyle H(X|Y)\leq H(e)+P(e)\log(|{\mathcal {X}}|-1),}
  
where 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
   denotes the support of X,

  
    
      
        H
        
          (
          X
          
            |
          
          Y
          )
        
        =
        −
        
          ∑
          
            i
            ,
            j
          
        
        P
        (
        
          x
          
            i
          
        
        ,
        
          y
          
            j
          
        
        )
        log
        ⁡
        P
        
          (
          
            x
            
              i
            
          
          
            |
          
          
            y
            
              j
            
          
          )
        
      
    
    {\displaystyle H\left(X|Y\right)=-\sum _{i,j}P(x_{i},y_{j})\log P\left(x_{i}|y_{j}\right)}
  
is the conditional entropy,

  
    
      
        P
        (
        e
        )
        =
        P
        (
        X
        ≠
        
          
            
              X
              ~
            
          
        
        )
      
    
    {\displaystyle P(e)=P(X\neq {\tilde {X}})}
  
is the probability of the communication error, and

  
    
      
        H
        (
        e
        )
        =
        −
        P
        (
        e
        )
        log
        ⁡
        P
        (
        e
        )
        −
        (
        1
        −
        P
        (
        e
        )
        )
        log
        ⁡
        (
        1
        −
        P
        (
        e
        )
        )
      
    
    {\displaystyle H(e)=-P(e)\log P(e)-(1-P(e))\log(1-P(e))}
  
is the corresponding binary entropy.


