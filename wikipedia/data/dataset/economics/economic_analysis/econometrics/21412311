In information theory and statistics, Kullback's inequality is a lower bound on the Kullback–Leibler divergence expressed in terms of the large deviations rate function. If P and Q are probability distributions on the real line, such that P is absolutely continuous with respect to Q, i.e. P<<Q, and whose first moments exist, then

  
    
      
        
          D
          
            K
            L
          
        
        (
        P
        ∥
        Q
        )
        ≥
        
          Ψ
          
            Q
          
          
            ∗
          
        
        (
        
          μ
          
            1
          
          ′
        
        (
        P
        )
        )
        ,
      
    
    {\displaystyle D_{KL}(P\|Q)\geq \Psi _{Q}^{*}(\mu '_{1}(P)),}
  
where 
  
    
      
        
          Ψ
          
            Q
          
          
            ∗
          
        
      
    
    {\displaystyle \Psi _{Q}^{*}}
   is the rate function, i.e. the convex conjugate of the cumulant-generating function, of 
  
    
      
        Q
      
    
    {\displaystyle Q}
  , and 
  
    
      
        
          μ
          
            1
          
          ′
        
        (
        P
        )
      
    
    {\displaystyle \mu '_{1}(P)}
   is the first moment of 
  
    
      
        P
        .
      
    
    {\displaystyle P.}
  
The Cramér–Rao bound is a corollary of this result.
