In econometrics, the Frisch–Waugh–Lovell (FWL) theorem is named after the econometricians Ragnar Frisch, Frederick V. Waugh, and Michael C. Lovell.
The Frisch–Waugh–Lovell theorem states that if the regression we are concerned with is:

  
    
      
        Y
        =
        
          X
          
            1
          
        
        
          β
          
            1
          
        
        +
        
          X
          
            2
          
        
        
          β
          
            2
          
        
        +
        u
      
    
    {\displaystyle Y=X_{1}\beta _{1}+X_{2}\beta _{2}+u}
  
where 
  
    
      
        
          X
          
            1
          
        
      
    
    {\displaystyle X_{1}}
   and 
  
    
      
        
          X
          
            2
          
        
      
    
    {\displaystyle X_{2}}
   are 
  
    
      
        n
        ×
        
          k
          
            1
          
        
      
    
    {\displaystyle n\times k_{1}}
   and 
  
    
      
        n
        ×
        
          k
          
            2
          
        
      
    
    {\displaystyle n\times k_{2}}
   matrices respectively and where 
  
    
      
        
          β
          
            1
          
        
      
    
    {\displaystyle \beta _{1}}
   and 
  
    
      
        
          β
          
            2
          
        
      
    
    {\displaystyle \beta _{2}}
   are conformable, then the estimate of 
  
    
      
        
          β
          
            2
          
        
      
    
    {\displaystyle \beta _{2}}
   will be the same as the estimate of it from a modified regression of the form:

  
    
      
        
          M
          
            
              X
              
                1
              
            
          
        
        Y
        =
        
          M
          
            
              X
              
                1
              
            
          
        
        
          X
          
            2
          
        
        
          β
          
            2
          
        
        +
        
          M
          
            
              X
              
                1
              
            
          
        
        u
        
        ,
      
    
    {\displaystyle M_{X_{1}}Y=M_{X_{1}}X_{2}\beta _{2}+M_{X_{1}}u\!,}
  
where 
  
    
      
        
          M
          
            
              X
              
                1
              
            
          
        
      
    
    {\displaystyle M_{X_{1}}}
   projects onto the orthogonal complement of the image of the projection matrix 
  
    
      
        
          X
          
            1
          
        
        (
        
          X
          
            1
          
          
            
              T
            
          
        
        
          X
          
            1
          
        
        
          )
          
            −
            1
          
        
        
          X
          
            1
          
          
            
              T
            
          
        
      
    
    {\displaystyle X_{1}(X_{1}^{\mathsf {T}}X_{1})^{-1}X_{1}^{\mathsf {T}}}
  . Equivalently, MX1 projects onto the orthogonal complement of the column space of X1. Specifically,

  
    
      
        
          M
          
            
              X
              
                1
              
            
          
        
        =
        I
        −
        
          X
          
            1
          
        
        (
        
          X
          
            1
          
          
            
              T
            
          
        
        
          X
          
            1
          
        
        
          )
          
            −
            1
          
        
        
          X
          
            1
          
          
            
              T
            
          
        
        .
      
    
    {\displaystyle M_{X_{1}}=I-X_{1}(X_{1}^{\mathsf {T}}X_{1})^{-1}X_{1}^{\mathsf {T}}.}
  
known as the annihilator matrix, or orthogonal projection matrix. This result implies that all these secondary regressions are unnecessary: using projection matrices to make the explanatory variables orthogonal to each other will lead to the same results as running the regression with all non-orthogonal explanators included.
