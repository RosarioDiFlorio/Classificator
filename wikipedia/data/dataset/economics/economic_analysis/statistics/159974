In mathematical optimization, the method of Lagrange multipliers (named after Joseph-Louis Lagrange) is a strategy for finding the local maxima and minima of a function subject to equality constraints.
For the case of only one constraint and only two choice variables (as exemplified in Figure 1), consider the optimization problem
maximize f(x, y)
subject to g(x, y) = 0.
We assume that both f and g have continuous first partial derivatives. We introduce a new variable (λ) called a Lagrange multiplier and study the Lagrange function (or Lagrangian or Lagrangian expression) defined by

  
    
      
        
          
            L
          
        
        (
        x
        ,
        y
        ,
        λ
        )
        =
        f
        (
        x
        ,
        y
        )
        −
        λ
        ⋅
        g
        (
        x
        ,
        y
        )
        ,
      
    
    {\displaystyle {\mathcal {L}}(x,y,\lambda )=f(x,y)-\lambda \cdot g(x,y),}
  
where the λ term may be either added or subtracted. If f(x0, y0) is a maximum of f(x, y) for the original constrained problem, then there exists λ0 such that (x0, y0, λ0) is a stationary point for the Lagrange function (stationary points are those points where the partial derivatives of 
  
    
      
        
          
            L
          
        
      
    
    {\displaystyle {\mathcal {L}}}
   are zero). However, not all stationary points yield a solution of the original problem. Thus, the method of Lagrange multipliers yields a necessary condition for optimality in constrained problems. Sufficient conditions for a minimum or maximum also exist.
For the general case of an arbitrary number n of choice variables and an arbitrary number M of constraints, the Lagrangian takes the form

  
    
      
        
          
            L
          
        
        
          (
          
            x
            
              1
            
          
          ,
          …
          ,
          
            x
            
              n
            
          
          ,
          
            λ
            
              1
            
          
          ,
          …
          ,
          
            λ
            
              M
            
          
          )
        
        =
        f
        
          (
          
            x
            
              1
            
          
          ,
          …
          ,
          
            x
            
              n
            
          
          )
        
        −
        
          ∑
          
            k
            =
            1
          
          
            M
          
        
        
          
            λ
            
              k
            
          
          
            g
            
              k
            
          
          
            (
            
              x
              
                1
              
            
            ,
            …
            ,
            
              x
              
                n
              
            
            )
          
        
        ,
      
    
    {\displaystyle {\mathcal {L}}\left(x_{1},\ldots ,x_{n},\lambda _{1},\ldots ,\lambda _{M}\right)=f\left(x_{1},\ldots ,x_{n}\right)-\sum \limits _{k=1}^{M}{\lambda _{k}g_{k}\left(x_{1},\ldots ,x_{n}\right)},}
  
again the constrained optimum of f coincides with a stationary point of 
  
    
      
        
          
            L
          
        
        .
      
    
    {\displaystyle {\mathcal {L}}.}
