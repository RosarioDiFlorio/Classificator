Partial (pooled) likelihood estimation for panel data assumes that density of yit given xit is correctly specified for each time period but it allows for misspecification in the conditional density of yi≔(yi1,…,yiT) given xi≔(xi1,…,xiT). Concretely, partial likelihood estimation uses the product of conditional densities as the density of the joint conditional distribution. This generality facilitates maximum likelihood methods in panel data setting because fully specifying conditional distribution of yi can be computationally demanding. On the other hand, allowing for misspecification generally results in violation of information equality and thus requires robust standard error estimator for inference.
In the following exposition, we follow the treatment in Wooldridge. Particularly, the asymptotic derivation is done under fixed-T, growing-N setting.
Writing the conditional density of yit given xit as ft (yit | xit;θ), the partial maximum likelihood estimator solves:

  
    
      
        
          
            max
            
              θ
              ∈
              Θ
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        
          ∑
          
            t
            =
            1
          
          
            T
          
        
        log
        ⁡
        
          f
          
            t
          
        
        (
        
          y
          
            i
            t
          
        
        ∣
        
          x
          
            i
            t
          
        
        ;
        θ
        )
      
    
    {\displaystyle {\underset {\theta \in \Theta }{\operatorname {max} }}\sum _{i=1}^{N}\sum _{t=1}^{T}\log f_{t}(y_{it}\mid x_{it};\theta )}
  
In this formulation, the joint conditional density of yi given xi is modeled as Πt ft (yit | xit ; θ). We assume that ft (yit |xit ; θ) is correctly specified for each t = 1,…,T and that there exists θ0 ∈ Θ that uniquely maximizes E[ft (yit│xit ; θ)]. But, it is not assumed that the joint conditional density is correctly specified. Under some regularity conditions, partial MLE is consistent and asymptotically normal.
By the usual argument for M-estimator (details in Wooldridge ), the asymptotic variance of √N (θMLE- θ0) is A−1 BA−1 where A−1 = E[ ∑t∇2θ logft (yit│xit ; θ)]−1 and B=E[( ∑t∇θ logft (yit│xit ; θ) ) ( ∑t∇θ logft (yit│xit; θ ) )T]. If the joint conditional density of yi given xi is correctly specified, the above formula for asymptotic variance simplifies because information equality says B=A. Yet, except for special circumstances, the joint density modeled by partial MLE is not correct. Therefore, for valid inference, the above formula for asymptotic variance should be used. For information equality to hold, one sufficient condition is that scores of the densities for each time period are uncorrelated. In dynamically complete models, the condition holds and thus simplified asymptotic variance is valid.
