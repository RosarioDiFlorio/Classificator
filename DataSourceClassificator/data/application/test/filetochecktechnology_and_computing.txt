10129817_178 - 0.946293613673 - technology_and_computing
[system, land, acquisition, scenario, list, practice, information, current]

Information systems for land acquisitions : wish lists, current practices and possible scenarios
Bibliography: leaves 87-89.
-------------------------------------

10134418_183 - 0.989660077746 - technology_and_computing
[model, urban, method, wind, dispersion, area, aermod, boundary, layer, speed]

Investigation of Dispersion and Micrometeorology Under Spatially Inhomogeneous Conditions
This dissertation summarizes the results from a study to develop and evaluate models to estimate dispersion of pollutants in boundary layers whose properties change with downwind distance.  Such boundary layers occur at the interface between rural and urban areas, and water and land bodies.  I first developed a method to estimate the meteorological inputs required to apply the current generation of dispersion models, such as AERMOD, to urban areas.  This method uses measurements made at a single level on a tower located in an urban area, and is based on the assumption that similarity methods applicable to spatially homogeneous conditions are locally valid even in an inhomogeneous urban area.  I show that under unstable conditions, measurements of temperature fluctuations improve upon commonly used energy balance methods to estimate heat flux.  Also, any bias in heat flux estimates has a minor effect on the prediction of surface friction velocity and turbulent velocities.  I examined a method to estimate urban micrometeorology using measurements made on a tower located in an upwind suburban area.  I applied an internal boundary layer model to trace the evolution of the boundary layer as it traveled from the suburban measurement location to the urban location of interest.  Estimates from the model were observations made during a field study conducted in Riverside, CA.    Estimates of friction velocity compare well with urban measurements only when the variation of friction velocity with height within the urban canopy was accounted for.  I examined the performance of two steady-state dispersion models in explaining concentrations measured during field studies designed to study low wind speed conditions typical of urban areas.  One model is based on the numerical solution of the two-dimensional mass conservation equation and the other is AERMOD, which accounts for low wind speeds by including wind meandering.  The numerical method performs better than AERMOD through a justifiable description of the interaction between dispersion and the gradient of the wind speed near the surface.  Including wind meandering, which occurs under low wind speeds, improves the performance of the numerical model.  As part of this study, I developed a method to improve estimates of surface friction velocity during low wind speeds. The last model is applicable to elevated sources, such as power plants, situated close to shorelines.  It is designed to be compatible with AERMOD (Cimorelli et al., 2005), the USEPA's regulatory model that is currently designed for spatially homogenous conditions.  The semi-empirical shoreline dispersion model accounts for plume entrainment by the thermal internal boundary Layer (TIBL), whose height increases with distance from the shoreline.  I show that AERMOD can be modified to account for two-dimensional shoreline effects, and this modified model performs as well in explaining observations as dispersion models specifically designed for shoreline sources.  I also developed a method to generate meteorological inputs that are compatible with the current structure of AERMOD's inputs.
-------------------------------------

10133633_183 - 0.997852668684 - technology_and_computing
[parameter, model, law, ehv, unknown, system]

Modeling and online parameter identification methods for Electrohydraulic Valvetrain Systems
We consider an Electrohydraulic Valve System (EHVS) model with uncertain parameters that may possibly vary with time. This is a nonlinear third order system consisting of two clearly separated subsystems, one for piston position and the other for the chamber pressure. The nonlinearities involved are flow-pressure characteristics of the solenoid valves, the pressure dynamics of the chamber due to varying volume, and a variable damping nonlinearity. We develop a parametric model that is linear in the unknown parameters of the system using filtering. We deal with a nonlinear parameterization in the variable damping term using the Taylor approximation. We design two parameter identifiers which employs either a continuous-time unnormalized least-squares update law with a forgetting factor or a gradient update law. These update laws exponentially converges to the true parameters under a persistence of excitation condition, which is satisfied due to the periodic regime of operation of EHVS. We present simulation results that show good following of unknown parameters even with the presence of sensor noise. We also create a hybrid model of the EHVS and apply the identifiers. In the presence of the unmodelled dynamics we find there remains good following of the unknown parameters
-------------------------------------

10133536_183 - 0.999982066283 - technology_and_computing
[disk, datum, performance, policy, workload, reliability, layout]

Improving disk array reliability and performance
In this work we present a new data layout and associated scheduling policies to improve RAID reliability and performance. Our implementation uses multiple mirrors, utilizing n disks in a redundancy group thus providing fault tolerance for n -1 disk failures. As this is an extended form of RAID 1, we refer to this as RAID1nr, where n is the number of mirrors and r indicates the position of the data is rotated on each mirror. The rotated layout is such that a different 1/n of the data is located on the outer edge of each disk. The redundancy scheme is simple mirroring thus there is no added complexity introduced such as parity or other redundant encoding techniques. We then provide several scheduling policies for reads and writes that take advantage of the data layout. These policies can be set by the administrator for the desired level of performance and reliability. For example, all read requests for a particular block range may be serviced from the same disk. Writes may be scheduled to a subset of disks in order to improve performance using our immediate and eventual consistency policies. We also present load balancing policies for skewed workloads. While the RAID1nr system supports up to n -1 mirrors, we show that even adding a single extra mirror provides the increased reliability offered by an extra copy of the data, as well as a significant performance increase for read workloads, write workloads, mixed workloads, workloads with skewed distributions, and during degraded mode operation
-------------------------------------

10134612_183 - 0.999997636063 - technology_and_computing
[adc, design, flash, part, resolution, speed]

High Speed ADC Design Methodology
Analog-to-digital converter (ADC) is a very fundamental and key part to nearly all kinds of electronics. The applications cover a wide range requiring different resolution to different sampling rate, including UWB systems, radar detection, wide band radio receivers, optical communication links, CCD imaging, ultrasonic medical imaging, digital receivers, base stations, digital video (for example, HDTV), xDSL, cable modems, and fast Ethernet. Among them, lower resolution very high speed ADC is a critical part for building UWB system, disk drive read channels and optical communication.This thesis consists of two parts. The first part focuses on the design of a high speed low resolution flash ADC in 90nm technology. Capacitive interpolation technique was used in this flash ADC in order to reduce the hardware requirement and input capacitance. No sample-and-hold (S/H) circuit is needed since the distributed capacitors (including capacitors in the very front end and the interpolated capacitors) serve to  sample and hold the signals. Offset cancellation and averaging techniques are also implemented to reduce the offsets and the non-linearity. The ADC design achieves a sampling speed of 2.3GSps with 4 bits resolution in 90nm CMOS technology.The second part describes a new comprehensive ADC design methodology for capacitive interpolated flash ADCs, aiming to provide a quantitative, yet handy design guideline for circuit designers to conduct practical ADC designs. This new ADC design methodology provides a quantitative and comprehensive mapping between ADC chip level performance specs and various design parameters at different levels, such as, interpolation factor, number of stages, pre-amplifier bandwidth, loading effects, transistor sizes, technology parameters and etc. It serves to allow IC designers to conduct quick and quantitative flash ADC designs for well-balanced overall chip performance in practices. A dynamic power consumption analysis technique for capacitive interpolated flash ADCs is also discussed.
-------------------------------------

10138233_183 - 0.999664570879 - technology_and_computing
[datum, reservation, algorithm, advance]

An Online Scheduling Algorithm with Advance Reservation for Large-Scale Data Transfers
Scientific applications and experimental facilities generate massive data sets that need to be transferred to remote collaborating sites for sharing, processing, and long term storage. In order to support increasingly data-intensive science, next generation research networks have been deployed to provide high-speed on-demand data access between collaborating institutions. In this paper, we present a practical model for online data scheduling in which data movement
operations are scheduled in advance for end-to-end high performance transfers. In our model, data scheduler interacts with reservation managers and data transfer nodes in order to reserve available bandwidth to guarantee completion of jobs that are accepted and confirmed to satisfy preferred time constraint given by the user. Our methodology improves current systems by allowing researchers and higher level meta-schedulers to use data placement as a service where they
can plan ahead and reserve the scheduler time in advance for their data movement operations. We have implemented our algorithm and examined possible techniques for incorporation into current reservation frameworks. Performance measurements confirm that the proposed algorithm is efficient and scalable.
-------------------------------------

10135170_183 - 0.992865748136 - technology_and_computing
[beamformer, multi-core, source, functional, technique, brain, activation, meg]

Multi-core beamformer for spatio-temporal MEG source activity reconstruction
Beamformer adaptive spatial filters have been used extensively in the field of magnetoencephalography (MEG) as tools to reconstruct functional activation of the brain. Conventional single beamformer techniques suffer from distortion in the presence of coherent activation of the cortex or are difficult to use due to the need of a priori information. These qualities present a major disadvantage to analyzing human brain responses, as coordinated functional responses require a degree of synchronous activation in different parts of the active cortex. In this dissertation, a novel beamformer technique, the multi-core beamformer, is developed that is robust to source correlation and does not require the use of a priori information. This novel approach is tested in both simulated and real experiments, including auditory and median-nerve stimulation, which provide well-studied systems to gauge the effectiveness of our new technique. Simulations show that the multi-core beamformer can successfully determine source time-courses, source powers, and source locations while minimizing or eliminating the distortion present in other methods. Results from real- life experiments show that the multi-core beamformer produces physiologically meaningful solutions that agree with previous functional imaging and neurophysiology studies. The use of the multi-core beamformer is expected to greatly contribute to the analysis of MEG recordings and, in general, improve our understanding of functional brain activity
-------------------------------------

10132992_183 - 0.998731433819 - technology_and_computing
[power, link, study, sensor, variable, network, wireless, control]

Experimental Study of the Effects of Tx Power Control and Blacklisting in Wireless Sensor Networks
We experimentally investigate the impact of variable transmission power on link quality, and propose variable power link quality control techniques to enhance the performance of data delivery in wireless sensor networks. This study extends the state of the art in two key respects: first, while there are a number of previous results on power control techniques for wireless ad hoc and sensor networks, to our knowledge nearly all of them have been simulation or analytical studies that assume idealized link conditions; second, while there are several recent experimental studies that have shown the prevalence of non-ideal unreliable communication links in sensor networks, these have not thoroughly investigated the impact of variable transmission power.
-------------------------------------

10138039_183 - 0.743389469173 - technology_and_computing
[number, copy, datum, dna, variation, multiple, gfl]

Reconstructing DNA copy number by joint segmentation of multiple sequences
Abstract
				
				
					
						Background
					Variations in DNA copy number carry information on the modalities of genome evolution and mis-regulation of DNA replication in cancer cells. Their study can help localize tumor suppressor genes, distinguish different populations of cancerous cells, and identify genomic variations responsible for disease phenotypes. A number of different high throughput technologies can be used to identify copy number variable sites, and the literature documents multiple effective algorithms. We focus here on the specific problem of detecting regions where variation in copy number is relatively common in the sample at hand. This problem encompasses the cases of copy number polymorphisms, related samples, technical replicates, and cancerous sub-populations from the same individual.
				
				
					
						Results
					We present a segmentation method named generalized fused lasso (GFL) to reconstruct copy number variant regions. GFL is based on penalized estimation and is capable of processing multiple signals jointly. Our approach is computationally very attractive and leads to sensitivity and specificity levels comparable to those of state-of-the-art specialized methodologies. We illustrate its applicability with simulated and real data sets.
				
				
					
						Conclusions
					The flexibility of our framework makes it applicable to data obtained with a wide range of technology. Its versatility and speed make GFL particularly useful in the initial screening stages of large data sets.
-------------------------------------

10134941_183 - 0.999946358473 - technology_and_computing
[system, model, cdf, flow, state, uncertainty, function, equation, density, probabilistic]

Uncertainty quantication in environmental flow and transport models
This dissertation is a work on the development of mathematical tools for uncertainty quantification in environmental flow and transport models. In hydrology, data scarcity and insufficient site characterization are the two ubiquitous factors that render modeling of physical processes uncertain. Spatio-temporal variability (heterogeneity) poses significantly impact on predictions of system states. Standard practices are to compute (analytically or numerically) the first two statistical moments of system states, using their ensemble means as predictors of a system's behavior and variances (or standard deviations) as a measure of predictive uncertainty. However, such approaches become inadequate for risk assessment where one is typically interested in the probability of rare events. In other words, full statistical descriptions of system states in terms of probabilistic density functions (PDFs) or cumulative density functions (CDFs), must be sought. This is challenging because not only parameters, forcings and initial and boundary conditions are uncertain, but the governing equations are also highly nonlinear. One way to circumvent these problems is to develop simple but realistic models that are easier to analyze. In chapter 3, we introduce such reduced-complexity approaches, based on Green-Ampt and Parlange infiltration models, to provide probabilistic forecasts of infiltration into heterogeneous media with uncertain hydraulic parameters. Another approach is to derive deterministic equations for the statistics of random system states. A general framework to obtain the cumulative density function (CDF) of channel- flow rate from a kinematic-wave equation is developed in the third part of this work. Superior to conventional probabilistic density function (PDF) procedure, the new CDFs method removes ambiguity in formulations of boundary conditions for the CDF equation. Having developed tools for uncertainty quantification of both subsurface and surface flows, we apply those results in final part of this dissertation to perform probabilistic forecasting of algae growth in an enclosed aquatic system
-------------------------------------

10130367_178 - 0.978794960797 - technology_and_computing
[context, design, environment, spatial, interactive, personal]

Spatial contexts : an interactive environment for personal design
Bibliography: leaves 133-140.
-------------------------------------

10135393_183 - 0.994577904435 - technology_and_computing
[bayesian, mapping, algorithm, qtl, model, statistics, trait, mcmc, chapter, test]

Bayesian Statistics and Its Application to Quantitative Trait Loci Mapping
Quantitative trait loci (QTL) mapping is one of the applications of statistics in genetics.This dissertation focuses two problems on QTL mapping which include a newpermutation method used to find the thresholds for the shrinkage Bayesian estimation ofquantitative trait loci parameters and three algorithms of handling the missing genotypeproblems in multiple QTL mapping under the generalized linear mixed model framework.In addition, this dissertation includes a review on Bayesian statistics and somedata analyses using Markov chain Monte Carlo (MCMC).      Chapter 2 is a review of the Bayesian statistics and some data analyses usingMCMC. It includes almost all the aspects of Bayesian statistics such as Bayes' theorem, prior and posterior distributions, Bayesian inference, and Markov chain Monte Carlo (MCMC) algorithms.       In Chapter 3, a new way to conduct the permutation test under the Shrinkage Bayesian method is developed. Permutation test is the most frequently used method for statistical test for QTL mapping. And it was applied on the QTL mapping based on the Bayesian approach. While using the traditional permutation test to get the thresholds for QTL mapping from the MCMC algorithms in the Bayesian models isquite time-consuming, a new way to permute the samples from the MCMC algorithmsis performed in Chapter 3. Empirical power analysis is done to test the method through the simulations.       Generalized linear mixed model has been applied to analyze the discrete traits. Research on handling the missing genotype problems in multiple QTL mapping under the generalized linear mixed model framework is presented in Chapter 4. Three algorithms were proposed: (1) expectation algorithm, (2) overdispersion model algorithm and (3)mixture model algorithm.
-------------------------------------

10135490_183 - 0.735480707879 - technology_and_computing
[work, memory, episodic, computer, tool, activity, digital, life, context, case]

Studying episodic access to personal digital activity : activity trails prototype
It was just a generation ago that computers entered the workplace. Back then, they only represented the work we did, nothing else. But today, some sort of computing device is involved in how we play, how we communicate, how we get our news and of course, how we work. What this means is that today almost all aspects of our lives are represented in some digital form. The rapid pace of change in technology and the dramatic shift in the use of computers has a cost associated with it. The legacy design of early computer systems is still prevalent in modern devices and goes unnoticed because of our familiarity with it. The desktop metaphor with its file and folder system, and the application paradigm with its document-centric view of information, both carry the legacy of a design that has far surpassed what it was originally designed to do. Digital representations should mediate what we do in the physical world, and since we do much more now than just work through computers, we need new representations that leverage our cognitive abilities in everyday life; in particular, present day computing devices do not facilitate the use of a powerful skill we use in our personal experiences, known as episodic memory. Episodic memory is how we remember our lives through stories. The human perceptual system samples the world continuously in order for the brain to store information, organize it and later recall it efficiently. At least this is the classic view of memory. However, people also leave a physical trace behind each and every one of their actions simply as the byproduct of their interaction with the environment. Because memory is finely tuned to reconstruct the past, our perceptual skills help us make meaning out of these traces. Time, proximity and familiar surroundings provide cues that naturally trigger our recollection of the past. Episodic memory is a human skill that taps into these cues by encoding the context surrounding events therefore allowing us to re-experience the past by recalling specific instances and the context in which they were experienced. Computers, in contrast to humans, only record the consequences of our actions and in doing so, they reduce the type and quantity of the memory cues available. Things have indeed changed dramatically in the last generation. Together with the avalanche of new digital gadgets and the general overwhelming presence of computers in our lives come new opportunities for research. One new and exciting ground for research is the use of activity recordings. The main research question of my work is understanding how to design an interface using these recordings to facilitate the use of episodic memory. There is a growing interest in the field of Human-Computer Interaction in research of life-logging technologies to assist with memory and reflection. But we are only at the very onset of understanding the impact of these technologies, and more importantly, how they might fit into the fabric of our daily lives. The work I present here was motivated by a year-long ethnographic study conducted at a law office. In this study, I used desktop activity recordings as a novel methodology to learn about the nature and details of work. I learned that what is usually considered multitasking behavior in the literature, is in fact the norm in this setting. Multitasking here is not "crunch mode'' type of behavior, but is a self- selected and all together different kind of work style. This style is engendered by both the nature of the legal work and the new digital tools available, in particular communication tools such as instant messaging and email. These tools have had an impact in how paralegals and attorneys interact with clients and with one another. My ethnographic data reveals that with the increasing frequencies and flexibility of the daily interactions comes an increased fragmentation of the context of each work thread. The lack of episodic support in these tools creates a heavy load for workers. Paralegals and attorneys have to put effort to bring together the history of a case from the many separate pieces of the past (email, instant messages, database entries, and so on). In other words, workers have to build context for a case before communicating with clients and this context consists of putting together a timeline representation about the history for a case, containing a chronology of events with the client and a view of any upcoming deadlines or pending issues for the case. My argument is that the tools available presently do not support this context-building process, so in addition to supporting for multitasking and interruptions we need to design support for this process. The second portion of the work in this thesis brings my findings to bear directly on a software design problem for Human-Computer Interaction. It describes the design and implementation of a software prototype tool called Activity Trails with the goal of supporting episodic memory. The thesis ends with a study conducted with researchers at UCSD evaluating the benefits of episodic access for everyday activity through detailed case studies of usage
-------------------------------------

10133313_183 - 0.854109534005 - technology_and_computing
[matching, map, travel]

Catchline International Journal of Geographical Information System, Vol. X, No. X, Month 2006, xxx–xxx Running heads (verso) J. Zhou and R. Golledge (recto) A three-step map matching method in the GIS environment A Three-step General Map Matching Method in the GIS Environment: Travel/Transportation Study Perspective
Despite its application in many fields, map matching in studies of travel/transport geography is unique in two aspects: 1) The correct road links traversed by the traveler need to be unambiguously identified; 2) All the identified links should form a meaningful travel route. This paper discusses the application of map matching methodologies in the context of deriving people’s travel behavior from GPS-traced multi-modal trip data. In recognition of the disadvantages associated with the existing algorithms, this research proposed and implemented a heterogeneous map matching approach suitable for travel/activity research needs which is uniquely characterized by: 1) data preprocessing with point cluster reduction and density leverage; 2) offering the candidate solution within a pool of “the best”; 3) the balancing of matching results from multiple matching factors with rank aggregation; 4) Utilizing the network constraint attributes to increase the matching accuracy; and 5) Use of the Dempster belief test to discern the noise and off-road travel.
-------------------------------------

10136099_183 - 0.992696140924 - technology_and_computing
[technique, digital, pipelined, dac, adc, amplifier, residue, hdc, calibration, dem]

A generalized tree-structured DEM DAC and enhanced harmonic distortion correction in pipelined ADCs
The first chapter of this dissertation discusses the tree- structured dynamic element matching (DEM) technique for unity-weighted, multi-bit digital to analog converters (DACs). In general, mismatches in nominally identical components of a unity weighted multi-bit DAC introduce non -linear distortion at the output of the DAC. For this reason, a DEM encoder is usually employed in the implementation of unity weighted multi-bit DACs, which objective is to permute the inputs to the nominally identical components of the DAC, so that the non-linear distortion at the output is avoided or at least minimized. The best among such DEM techniques is the tree structured DEM technique. However, prior to the work presented in Chapter 1, this technique could not be applied to unity- weighted multi-bit DACs with a number of components that is not a power of two. The second chapter of this dissertation focuses on the topic of digital calibration of residue amplifiers in pipelined analog to digital converters (ADCs). Pipelined ADCs provide a high resolution digital representation of analog input signals. In recent years, many digital calibration techniques have been developed that enable the design of pipelined ADCs with low-power analog components that behave nonideally. One such digital calibration technique is Harmonic Distortion Correction (HDC) which compensates for the non- ideal residue amplifier behavior. This technique is the best digital calibration technique known to the authors that addresses the problem of residue amplifiers. Nevertheless, the HDC technique, when implemented in the pipelined ADC, cannot accurately eliminate errors introduced by residue amplifiers under all pipelined ADC's input conditions. In particular, the problem in the implementation of the HDC technique arises due to the leakage of quantization error from the stages subsequent to the calibration stage. Chapter 2 presents an analysis of this problem and an all-digital solution which enables the HDC technique to properly operate regardless of the input signal level. The third chapter of this dissertation analyzes the number of samples that need to be averaged by the HDC algorithm in order to reliably estimate, and therefore eliminate, the residue amplifier errors from the pipelined ADC's output
-------------------------------------

10134659_183 - 0.972469107972 - technology_and_computing
[sensor, chapter, porous, silicon, vapor, filter, rugate, modulation]

Design and manipulation of 1-D rugate photonic crystals of porous silicon for chemical sensing applications
Porous silicon rugate photonic crystals are an attractive optical sensor material due to their high surface area, naked eye response, and controllable optical, morphological, and chemical characteristics. This thesis presents new ways to improve the selectivity, reversibility, and stability against interferents of remotely interrogated porous silicon sensors. After a brief introduction to rugate porous silicon, Chapters 2-5 present methods of directly sensing the interaction of organic vapors with the porous layer by chemical and physical sensor modulation. A 0.3 mm² fiber optic-coupled porous silicon sensor is constructed in Chapter 2 and implanted in a bed of activated carbon, demonstrating carbon bed end-of-service-life sensing. Chapter 3 furthers this concept by incorporating chemically modified sensor surfaces, with selectivity between water vapor, isopropanol, and heptane vapors demonstrated using acetylated and oxidized sensor chemistries. Chapter 4 introduces physical modulation of the porous silicon sensor, with thermal modulation of the photonic crystal to 160°C employed to rapidly and repeatedly desorb methyl salicylate and octanol vapors that foul the sensor response. Thermal modulation is applied to discrimination of pure heptane, cyclohexane, and isopropanol vapors in Chapter 5 by rapidly cycling a rugate sensor between 25- 80°C while exposed to partial pressures of organic vapors up to 7.5 Torr. Sensor responses to the thermally modulated sorption equilibrium allow discrimination of these pure analyte vapors. The final three chapters describe using porous silicon as tailored interference filters that increase the specificity of standoff optical detection. In Chapter 6, the stop bands of rugate filters are tuned to match mid-infrared molecular absorbance bands, including the 1250 cm⁻¹ P=O bond stretch. Standoff gas sensing is demonstrated with filters matched to the 2350cm⁻¹ stretch of CO₂. In Chapter 7, selective 2-D imaging of target compounds is demonstrated by matching filters to visible emission peaks of photoluminescent dyes that bind to dipicolinic acid, found in anthrax spores. Finally, wavelength separated, ratiometric referencing is demonstrated in Chapter 8, where a pH-responsive NH₃(g) sensor based on a dye-infused rugate filter with two stop bands encoded into the porous layer is shown to compensate for large fluctuations in probe light intensity and increase signal to noise
-------------------------------------

10136363_183 - 0.975774310657 - technology_and_computing
[memory, theatre]

Live in the wake
Live in the Wake is an evening-length dance theatre performance that took place at the Mandell Weiss Theatre on February 29-March 3, 2012. Utilizing movement, spoken text, video projections, and sound, scenic, and light design, the work examines the intersection of memory, imagination, and the human body. Live in the Wake creates an immersive world populated with the performer's real memories that are transformed and revealed by their imaginative blurring of the border between fact and fiction
-------------------------------------

10138688_183 - 0.990194456027 - technology_and_computing
[communication, approach, classification, performance, computational, hpc, parallel, computation, computing]

Multiclass Classification of Distributed Memory Parallel Computations
High Performance Computing (HPC) is a field concerned with solving large-scale problems in science and engineering. However, the computational infrastructure of HPC systems can also be misused as demonstrated by the recent commoditization of cloud computing resources on the black market. As a first step towards addressing this, we introduce a machine learning approach for classifying distributed parallel computations based on communication patterns between compute nodes. We first provide relevant background on message passing and computational equivalence classes called dwarfs and describe our exploratory data analysis using self organizing maps. We then present our classification results across 29 scientific codes using Bayesian networks and compare their performance against Random Forest classifiers. These models, trained with hundreds of gigabytes of communication logs collected at Lawrence Berkeley National Laboratory, perform well without any a priori information and address several shortcomings of previous approaches.
-------------------------------------

10135949_183 - 0.9819311893 - technology_and_computing
[bmp, signaling, i-smad, development, cartilage, tgf, role, smad]

Roles of Inhibitory Smads in Endochondral Bone Formation
Endochondral ossification involves a highly coordinated program of chondrocyte differentiation, proliferation, maturation, and hypertrophy. The bone morphogenetic protein (BMP) and transforming growth factor beta (TGF&beta;)/activin pathways are important regulators of these processes. The importance of extracellular antagonists as regulators of the duration, intensity and extent of BMP and TGF&beta;/activin signaling has been defined. For example, mice lacking the BMP receptor antagonist Noggin exhibit cartilage overgrowth concurrent with excessBMP activity. However, very little is known about the roles of intracellular inhibitors of BMP and TGF&beta;/activin pathways, such as the inhibitory Smad (I-Smad) proteins, Smad6 and Smad7. In vitro studies reveal that I-Smads 6 and 7 can regulate BMP- and TGF&beta;-mediated effects on chondrocytes. Although in vivo studies in which I-Smads were overexpressed in cartilage have shown that I-Smads have the potential to limit BMP signaling in vivo, the physiological relevance of I-Smad activity in skeletal tissues remains unknown. Furthermore, whether I-Smads impact TGF&beta; signaling in cartilage during development is unclear.This thesis includes two reviews (both have been published) and two original articles (one of which has been published). First, the known mechanisms by which BMP signaling regulate chondrogenesis, osteogenesis, and adipogenesis will be highlighted Chapter One, "BMP signaling in Skeletogenesis." Then, the roles of Smad proteins, which are the mediators of the canonical BMP/TGF&beta; pathways, in regulating skeletal development will be highlighted in Chapter Two, "Smad signaling in skeletal development and regeneration." Finally, the roles of Smad6 and Smad7 in endochondral bone formation will be described in Chapters Three ("Smad6 is essential to limit BMP signaling in cartilage development") and Four ("Smad7 regulates terminal maturation of chondrocytes during cartilage development in mice"). The last chapter will discuss the overall conclusions and future directions of my thesis project.
-------------------------------------

10129932_178 - 0.994235401152 - technology_and_computing
[master, equation, system, transcriptional, perturbation, retroactivity, singular, interconnection]

Stochastic Analysis of Retroactivity in Transcriptional Networks through Singular Perturbation
The input/output dynamic behavior of a biomolecular system is affected by interconnection to other downstream systems through impedance-like effects called retroactivity. In this paper, we study the effects of retroactivity at the interconnection between two transcriptional modules, focusing on stochastic behavior. In particular, we describe the system through the Master equation and develop a singular perturbation theory to obtain a reduced Master equation. We prove that the solution of the original Master equation converges fast to an e neighbor of the solution of the reduced Master equation, in which e is the singular perturbation parameter. Our analysis shows that the upstream system and the downstream one are statistically independent at the steady state. However, the interconnection slows down the dynamics of both the expectation and the variance of the output of the upstream transcriptional module.
-------------------------------------

10139550_183 - 0.999987887151 - technology_and_computing
[porous, material, code, gpu, algorithm, monte, carlo, structure, energy, adsorption]

High-throughput Characterization of Porous Materials Using Graphics Processing Units
We have developed a high-throughput graphics processing units (GPU) code that can characterize a large database of crystalline porous materials. In our algorithm, the GPU is utilized to accelerate energy grid calculations where the grid values represent interactions (i.e., Lennard-Jones + Coulomb potentials) between gas molecules (i.e., CH$_{4}$ and CO$_{2}$) and material's framework atoms. Using a parallel flood fill CPU algorithm, inaccessible regions inside the framework structures are identified and blocked based on their energy profiles. Finally, we compute the Henry coefficients and heats of adsorption through statistical Widom insertion Monte Carlo moves in the domain restricted to the accessible space. The code offers significant speedup over a single core CPU code and allows us to characterize a set of porous materials at least an order of magnitude larger than ones considered in earlier studies. For structures selected from such a prescreening algorithm, full adsorption isotherms can be calculated by conducting multiple grand canonical Monte Carlo simulations concurrently within the GPU.
-------------------------------------

10132005_183 - 0.962703885891 - technology_and_computing
[use, design, land, travel]

Cars and Drivers in the New Suburbs: Linking Access to Travel in Neotraditional Planning
Various 'new suburb' land use designs have recently been proposed to address a number of social and environmental problems, including the dominance of automobile travel. Transportation benefits are to be accomplished by reducing the surface street distance between locations, mixing land uses, and promoting walking, bicycling and transit via redesigned streets and street-scapes. That auto travel will fall is a largely unchallenged premise of these designs, though what little evidence exists is either weak or contrary. This paper presents a simple behavioral model to explain why. Generally speaking, driving is both discouraged and facilitated in the new suburbs, with the net effect being an empirical matter. In particular, both the number of automobile trips and vehicle-miles traveled can actually increase with an increase in access, such as a move to a more grid-like land use pattern. Whatever the merits of neotraditional and transit-oriented designs, and there are many, their transportation benefits have thus been oversold. Each development must be evaluated on a case by case basis to determine whether its net impact on auto use is positive or negative. An analytical framework for doing so is suggested.
-------------------------------------

10133624_183 - 0.999996498238 - technology_and_computing
[network, power, scheduling, multi-hop, wireless, o-sam, link]

Power Scheduling for Multi-Hop Wireless Networks
Multi-hop wireless networks remain an important research frontier ofwireless communications. Multi-hop wireless networks are rapidlydeployable to extend the coverage of the Internet, which can be aneconomical alternative to building new base stations. Multi-hopwireless networks are particularly useful for first responders fordisaster relief, and military operations in battlefields. In thisthesis, we study power scheduling issues for multi-hop wirelessnetworks. Power scheduling, also known as medium access controlconsisting of link scheduling, power control and source beamforming,fundamentally governs the capacity of multi-hop wireless networks.In the first part of this thesis, the achievable network throughputof large-scale multi-hop wireless networks is evaluated under apower scheduling scheme called opportunistic synchronous arraymethod (O-SAM). Under O-SAM, a large network is partitioned intomany small subnets, and within each subnet, the link with bestchannel gain is scheduled for transmission. We examine the impact oftraffic load, network topology and multiple antennas on theachievable network throughput. Compared with slotted ALOHA, thethroughput of O-SAM is significantly higher. In addition to O-SAM, adistributed synchronous array method (D-SAM) is proposed, and itsperformance is also evaluated.In the second part of this thesis, we focus on the power schedulingproblem for multi-input multi-output (MIMO) relay networks. Ageneralized water filling (GWF) theorem is established for link ratemaximization with multiple power constraints. The corresponding GWFalgorithm is a fast solution to an important class of convexoptimization problems.  The GWF algorithm is a useful building blockfor joint source and relay optimization for a multiuser MIMO relaynetwork. We study the power scheduling problems for both uplink anddown- link cases of the multiuser MIMO relay network. A number ofcomputational strategies are proposed to maximize the sum ratesubject to power constraints or to minimize the sum power subject torate constraints.
-------------------------------------

10133795_183 - 0.999571986459 - technology_and_computing
[device, spin, transport, injection, electron, semiconductor, surface]

Spin injection and transport in semiconductor and metal nanostructures
In this thesis we investigate spin injection and transport in semiconductor and metal nanostructures. To overcome the limitation imposed by the low efficiency of spin injection and extraction and strict requirements for retention of spin polarization within the semiconductor, novel device structures with additional logic functionality and optimized device performance have been developed. Weak localization/antilocalization measurements and analysis are used to assess the influence of surface treatments on elastic, inelastic and spin-orbit scatterings during the electron transport within the two-dimensional electron layer at the InAs surface. Furthermore, we have used spin- valve and scanned probe microscopy measurements to investigate the influence of sulfur-based surface treatments and electrically insulating barrier layers on spin injection into, and spin transport within, the two- dimensional electron layer at the surface of p-type InAs. We also demonstrate and analyze a three-terminal, all- electrical spintronic switching device, combining charge current cancellation by appropriate device biasing and ballistic electron transport. The device yields a robust, electrically amplified spin-dependent current signal despite modest efficiency in electrical injection of spin- polarized electrons. Detailed analyses provide insight into the advantages of ballistic, as opposed to diffusive, transport in device operation, as well as scalability to smaller dimensions, and allow us to eliminate the possibility of phenomena unrelated to spin transport contributing to the observed device functionality. The influence of the device geometry on magnetoresistance of nanoscale spin-valve structures is also demonstrated and discussed. Shortcomings of the simplified one-dimensional spin diffusion model for spin valve are elucidated, with comparison of the thickness and the spin diffusion length in the nonmagnetic channel as the criterion for validity of the 1D model. Our work contributes directly to the realization of spin valve and spin transistor devices based on III-V semiconductors, and offers new opportunities to engineer the behavior of spintronic devices at the nanoscale
-------------------------------------

10136714_183 - 0.997344562337 - technology_and_computing
[pdn, analysis, simulation, design, power, flow]

Analysis of microelectronic power distribution networks and exploration of 3D ICs
As the semiconductor process nodes advance to 28nm and below and three-dimensional (3D) silicon integration technology is emerging, power delivery is becoming an ever -greater challenge in VLSI design. Detailed analyses of power distribution networks (PDN) must be performed in order to ensure robust power delivery. Different aspects of analysis problems with current and future VLSI designs and 3D ICs are addressed in this dissertation. We review the traditional frequency-domain PDN analysis methodology and investigate the relationship between the frequency- domain PDN impedance and time-domain PDN noise. We demonstrate that the traditional frequency-domain analysis methodology has limitations. Due to the limitation of frequency-domain analysis methodology, we point out the importance of time-domain analysis which lead to our research work worst-case PDN noise prediction. We also propose a simulation flow for large-scale PDN using Discrete Fourier Transform (DFT). This flow utilizes the characterizations of PDNs and adaptively choose the simulation points based on different frequency ranges of PDNs. With such an adaptive sampling technique, we are able to simulate large-scale PDN models and achieve fairly results in a reasonable simulation time. Our simulation flow also naturally fits into parallel computation techniques, which further enhances the simulation efficiency. Finally, with our analysis and simulation tools, we explore the PDN characteristics of 3D ICs. We propose lumped and distributed models for 3D PDNs, and analyze 3D PDNs systematically in both frequency domain and time domain. Unique noise behaviors of 3D PDNs are discovered with our modeling analysis flow. Different design space of 3D PDNs are also studied and design guidelines based on the studies are provided
-------------------------------------

10139115_183 - 0.999842145285 - technology_and_computing
[downloading, secret, lunenfeld, machine, peter, war, tale, culture, computer, review]

Review: The Secret War Between Downloading and Uploading: Tales of the Computer as a Culture Machine by Peter Lunenfeld
An evaluative review of The Secret War Between Downloading and Uploading: Tales of the Computer as a Culture Machine, by Peter Lunenfeld.
-------------------------------------

10175328_189 - 0.989548634351 - technology_and_computing
[channel, gramicidin, binding]

Thallium ion distribution in the gramicidin ion conducting channel determined by x-ray diffraction
Gramicidin is the best characterized transmembrane ion channel. An opulence of biochemical and physiological data exists and it is the only channel for which a relatively well-defined structure is known. Thus, it has been proven to be an ideal model for actual physiological channels by providing valuable insight into a channel's structural/functional properties. Although much is known about the gramicidin channel, there is still no solid experimental data on the channel molecular dimensions. Therefore, this thesis supplies a very crucial piece of experimental structural data; namely, the direct location of two symmetric cation binding sites within the channel by x-ray diffraction. The binding sites are located at 9.4 $\pm$ 0.4 A about the midpoint of the channel. A previous $\sp{13}$C NMR study (Urry et al., 1982a,b; Urry, 1983) pinpointed cation binding between Trp-11 and Trp-13, and, based on rigid molecular models, this puts the binding sites at 11.0-11.5 A, which is at least 1.2 A larger than the value reported here. This suggests a considerable plasticity of the gramicidin channel consistent with conclusions drawn from recent molecular dynamic simulations (Roux and Karplus, 1988). Also, the smaller value leads to an elegant qualitative picture in which the flexible ends of the channel can conveniently provide the gating mechanism that transmits cations and blocks anions.
-------------------------------------

10136275_183 - 0.979576980731 - technology_and_computing
[energy, system]

The Spectroscopic-Assisted Investigation into Energy Migration through Disordered Systems
Harvesting solar energy is hindered by the financial difficulties in the creation of perfect crystal silicon-based photovoltaics.  The study of energy flow through disordered systems is therefore in interest as a potential way to manufacture low-cost amorphous systems.  It is shown through a series of time-resolved photon-counting fluorescence sensitization experiments that in a static system with disorder, anomalous diffusion results in that measured values for the exaction diffusion constants fall short of what is predicted for crystals based on traditional steady-state measurements for Forster parameters such as the Forster radius.  Various strategies are discussed to successfully tune the degree of energy migration through disordered systems.  The extent to which steady-state theory can be used as a simple tool in guiding the organic synthesis of potential active materials is brought to light.
-------------------------------------

10136683_183 - 0.99946746079 - technology_and_computing
[capacity, outage, work, channel, method, probability, satellite, ergodic, antenna, part]

Analysis and Mitigation of Tropospheric Effects on Ka Band Satellite Signals and Estimation of Ergodic Capacity and Outage Probability for Terrestrial Links
The first part of this work covers the effect of the troposphere onKa band (20-30 GHz) satellite signals.  The second part deals withthe estimation of the capacity and outage probability forterrestrial links when constrained to quadrature amplitudemodulations.The desire for higher data rates and the need for availablebandwidth has pushed satellite communications into the Ka band(20-30 GHz). At these higher carrier frequencies the effects ofscintillation and rain attenuation are increased.  In regards to theeffects of scintillation, the first part of this work quantifies,through the use of a multiple phase screen simulation model, thebenefits of using two receive antennas to mitigatetropospheric-induced scintillation on Ka band satellite downlinks.Two representative turbulence profiles are considered, andcumulative distribution curves for scintillation-induced attenuationare generated for selection and maximal ratio combining schemes andcompared to those for a single antenna.  The results indicate thatthere can be significant diversity gains achieved by combining twoantennas separated by only a short distance. Also, a comparison ofsimulation results with the results predicted by the \emph{basicRytov} approximation shows that at elevation angles greater than 10degrees, Rytov theory can accurately predict performance benefits ofantenna combining, but at elevation angles less than 10 degrees itis better to use multiple phase screen simulations to makeperformance predictions.  In addition, the effects ofscintillation-induced phase perturbations on the output power oflarge aperture antennas is examined. It is found that the outputpower degradation due to scintillation-induced phase perturbationsis generally negligible and can be countered by the simple means ofantenna tracking if necessary.In regards to rain attenuation, this work developed simple methodsfor estimating the outage probability and outage capacity andergodic capacity of satellite links due to rain fades.  Therain-induced fades of a satellite link are often modeled with alog-log-normal distribution. Researchers have determined methods forcalculating the outage probability for Shannon capacity forlog-log-normal channels. However, in practical communicationssystems, the input signal is constrained to a discrete signallingset such as finite-size quadrature amplitude modulations. Underthese conditions the outage probability with regards to theconstrained capacity is a more accurate measure. A method isdetailed in this work for tightly estimating the outage probabilityand outage capacity of satellite links with quadrature amplitudemodulations. In addition this work derives a lower bound for theergodic constrained capacity of log-log-normal channels. To date, noother method for calculating the outage probability, outagecapacity, or a lower bound for the ergodic capacity for alog-log-normal channel with a finite-size quadrature amplitudemodulation has been published.  Also, this portion of the workquantifies the benefit of using receive diversity to mitigate rainfades, providing the gains in outage capacity due to the use ofdiversity for a tropical region and a fairly dry region under theconstraint that practical constellations are transmitted. The aboveinformation and analysis methods provide useful tools for satellitesystem planners.The second part of this work examines terrestrial communicationlinks, which can suffer greatly from channel fading or shadowing.Two common statistical models for channels are the Rayleighdistribution and the log-normal distribution.  The goal of thissecond part of the work was to develop a simple method for tightlyestimating the ergodic capacity and outage probability of these twochannel types when used with quadrature amplitude modulatedsignalling sets. Specifically an innovative method was developed forestimating the ergodic constrained capacity for Rayleigh andlog-normal channels with and without antenna combining. Theexpressions facilitate straightforward computation of outageprobability as well. Researchers have determined methods forcalculating the ergodic Shannon capacity for log-normal and Rayleighchannels for single and multiple receive antenna systems. However,in practical communications systems, the input signal is constrainedto a discrete signalling set such as finite-size quadratureamplitude modulation constellations.  Under these conditions theergodic constrained capacity is a more accurate measure.  The methoddetailed in this work provides a uniform expression for computingthe ergodic capacity, both Shannon and constrained, of Rayleigh andlog-normal channels with and without antenna combining. Theexpressions facilitate straightforward computation of outageprobability as well.  Both the noise-limited andinterference-limited cases are studied.  To date, no other methodfor estimating the outage probabilities for the constrained capacityof Rayleigh or log-normal channels has been published for either thenoise-limited case or interference-limited case.  Also, no methodfor estimating the ergodic constrained capacity of a log-normalchannel or of an interference-limited Rayleigh channel has appearedin the literature. The analysis methods and information forterrestrial links developed in the second part of this work provideuseful tools for the designers of wireless communication systems ingeneral and have particular application to cellular mobile andultra-wideband systems.
-------------------------------------

10136498_183 - 0.988322935464 - technology_and_computing
[performance, raid, parity, storage, drive, ssd, power, higher, device, workload]

Incorporating solid state drives into distributed storage systems
Big data stores are becoming increasingly important in a variety of domains including scientific computing, internet applications, and business applications. For price and performance reasons, such storage is comprised of magnetic hard drives. To achieve the necessary degree of performance and reliability, the drives are configured into storage subsystems based on RAID (Redundant Array of Independent Disks). Because of their mechanical nature, hard drives are relatively power-hungry and slow compared to most other computer components. Big data centers spend tremendous amounts on power, including cooling, adding significantly to their overall costs. Additionally, drives are orders of magnitude slower than electrical computer components, resulting in significant performance challenges any time disk I/O is required. Recently, SSDs (solid state drives) have emerged based on flash memory technology. Although too expensive to replace magnetic disks altogether, SSDs use less power and are significantly faster for certain operations.This dissertation examines several new architectures that use a limited amount of faster hardware to decrease the power consumption and increase the performance or data redundancy of RAID storage subsystems.  We present RAID4S, which uses SSDs for parity storage in a disk-SSD hybrid RAID system. Because of itsbetter performance characteristics, SSD parity storage reduces thedisk overhead associated with parity storage and thereby significantly reduces the disk overheads caused by RAID. This decreases the power consumption and can be used to increase the performance of simple RAID schemes or increase redundancy by enabling the use of higher order RAID schemes with less performance penalty. Storing parity on SSDs can reduce the average number of I/Os serviced by the remaining disks by up to 25-50%. By replacing some hard drives with SSDs, we reduce power and improve performance. Our RAID4S-modthresh optimization improves performance in certain workloads.The other two architectures expose RAID's inability to handle heterogeneity in workloads and hardware. RAIDE is motivated by a workload imbalance we detected in stripe-unaligned workloads. By placing faster hardware to handle the higher workload at the edges of the array, we observed higher throughput. RAIDH places parity on faster devices based on device weights. Higher weighted devices are faster and thus store additional parity. The slowest devices may store no parity at all.  This technique can be used on any heterogeneous array to enable faster random write throughput.
-------------------------------------

10138242_183 - 0.973693959268 - technology_and_computing
[off-grid, lighting]

Market Trial: Selling Off-Grid Lighting Products in Rural Kenya
In this study, we performed a market trial of off-grid LED lighting products in Maai Mahiu, a
rural Kenyan town. Our goals were to assess consumer demand and consumer preferences with respect to off-grid lighting systems and to gain feedback from off-grid lighting users at the point of purchase and after they have used to products for some time.
-------------------------------------

10137508_183 - 0.99998754774 - technology_and_computing
[touch, system, air, technology]

A capacitive air touch sensor system in 65nm CMOS technology
In this thesis, the new methodology of an oscillator-based capacitive air touch sensor system is introduced for future touch screen technology. This high sensitivity and low power solution is based on oscillation frequency shifts for air touch screening position identification with improved touch sensitivity and extended reliability. This system includes two parts: first, analog circuits are used to sense signals from the antenna, and then digital circuits are used to determine and analyze the detected human's finger positions.The system is designed and implemented using a 65nm CMOS technology. It is capable of detecting up to 20fF capacitance difference, which is 50 times more sensitive than current projected-capacitive touch screen products. With fixed 1V supply voltage, the power of the entire system is only 1.3mW. The core size is 1mm by 1.2mm, which is compact enough to be adopted in any commercial products.
-------------------------------------

10134932_183 - 0.996431314864 - technology_and_computing
[electron, cluster, orbital, electronic, energy, transfer, structure]

Mixed valency and electronic structure in self-assembled monolayers, self-exchange, and hydrogen bonded assemblies
Mixed valency and electron transfer are explored in self- assembled monolayers, in intermolecular electron self- exchange reactions in solution, and in hydrogen bonded assemblies. Tetrathiafulvalene is derivatized for binding to gold in self-assembled monolayers, but the trinuclear ruthenium cluster Ru₃O(OAc)₆L₃ (where L is an ancillary ligand) is used as a building block for the majority of the work. While oxo-centered trinuclear hexaacetate clusters of many transition metals are known, the triruthenium cluster is particularly versatile because of the kinetically stable binding of a wide variety of ligands. The electronic structure can be depicted by molecular orbitals diagrams or more recently by computer generated combinations of atomic orbitals, and remains relatively unchanged for variously substituted clusters. The important differences with respect to getting an electron in or out of a cluster lie in electron delocalization onto ligand based orbitals. In combination with reorganization energies calculated from accumulated structural and vibrational data, the molecular orbital diagrams offer a great deal of explanatory power. When allowed by symmetry and energy matching, electrons in reduced clusters are delocalized onto pyridine pi* orbitals, greatly easing the transfer to an oxidized cluster in the face of a large reorganization energy. When electron delocalization is not allowed, electron self- exchange can be fast only if the reorganization energy is small. In hydrogen bonded assemblies of these ruthenium clusters, the electronic structure is still dominant in electron transfer behavior. In these cases the increase in delocalization upon dimerization appears to induce large changes in the orbital energies. This is consistent with the electronic absorptions and the thermal electron transfer behavior observed. The take-home message of this dissertation is that one must understand to electronic structure of a complex in order to understand its behavior in electron transfer reactions. This may seem obvious, but is often overlooked. With the knowledge of the electronic structure of reactants and products, one has a much greater chance of understanding the path between them. Molecular orbital diagrams seem cumbersome and outdated in this age of calculated chemistry, but many cases drawing them out is worth the investment in time. Who knows, you may even learn something
-------------------------------------

10134493_183 - 0.837887505826 - technology_and_computing
[datum, graph, ranking, keyword, node, objectrank, search, score]

Dynamic link-based ranking over large-scale graph- structured data
Information Retrieval techniques have been the primary means of keyword search in document collections. However, as the amount and the diversity of available semantic connections between objects increase, link-based ranking methods including ObjectRank have been proposed to provide high-recall semantic keyword search over graph-structured data. Since a wide variety of data sources can be modeled as data graphs, supporting keyword search over graph- structured data greatly improves the usability of such data sources. However, it is challenging in both online performance and result quality. We first address the performance issue of dynamic authority-based ranking methods such as personalized PageRank and ObjectRank. Since they dynamically rank nodes in a data graph using an expensive matrix-multiplication method, the online execution time rapidly increases as the size of data graph grows. Over the English Wikipedia dataset of 2007, ObjectRank spends 20-40 seconds to compute query-specific relevance scores, which is unacceptable. We introduce a novel approach, BinRank, that approximates dynamic link- based ranking scores efficiently. BinRank partitions a dictionary into bins of relevant keywords and then constructs materialized subgraphs(MSGs) per bin in preprocessing stage. In query time, to produce highly accurate top-K results efficiently, BinRank uses the MSG corresponding to the given keyword, instead of the original data graph. PageRank and ObjectRank calculate the global importance score and the query-specific authority score of each node respectively by exploiting the link structure of a given data graph. However, both measures favor nodes with high in-degree that may contain popular yet generic content, and thus those nodes are frequently included in top-K lists, regardless of given query. We propose a novel ranking measure, Inverse ObjectRank, which measures the content-specificity of each node by traversing the semantic links in the data graph in the reverse direction. Then, we allow users to adjust the importance of the three ranking measures (global importance, query-relevance, and content-specificity) to improve the quality of search results
-------------------------------------

10130171_178 - 0.99960396244 - technology_and_computing
[system, theorem, orientation, separation, buoyant, net, two-link]

Two-link swimming using buoyant orientation
The scallop theorem posits that a two-link system immersed in a fluid at low Reynolds number cannot achieve any net translation via cyclic changes in its hinge angle. Here, we propose an approach to “breaking” this theorem, based on a static separation between the centers of mass and buoyancy in a net neutrally buoyant system. This separation gives the system a natural equilibrium orientation, allowing it to passively reorient without changing shape.
-------------------------------------

10134348_183 - 0.999984652624 - technology_and_computing
[envelope, amplifier, power, efficiency, tracking, high, system]

Envelope amplifier design for wireless base-station power amplifiers
In order to deliver high data rates, modern wireless communication systems transmit complex modulated signals with high peak-to-average ratio, which demands wide bandwidth and stringent linearity performance for power amplifiers. To satisfy spectral mask regulations and achieve adequate error vector magnitude, power amplifiers typically operate at 6 to 10 dB back-off from the maximum output power, leading to low efficiency. To overcome the low efficiency problem, the envelope tracking power amplifier architecture has been proposed for this type of application due to its feature of high efficiency over a wide power range. The overall efficiency of an envelope tracking system relies not only on performance of the RF power amplifier but also on that of an envelope amplifier that provides a dynamically varying power supply voltage. This dissertation focuses on envelope amplifier design for efficiency enhancement of envelope tracking power amplifiers. First, the envelope tracking power amplifier architecture is analyzed, and the efficiency of a RF transistor in the envelope tracking technique is described. Then envelope amplifier behavior is investigated and a general purpose simulator is developed for analyzing and designing an envelope amplifier. Power loss and efficiency of the envelope amplifier is analyzed and compared with experimental results. The design of envelope amplifiers for high voltage (> 30 V) envelope tracking applications is described. A high voltage envelope amplifier is designed, implemented and verified. The overall envelope tracking system employing a GaN-HEMT RF transistor is demonstrated. Finally, a new architecture is developed for the efficiency enhancement of envelope amplifiers, using a digitally assisted controller design. Digital control is utilized to mitigate delay in the control loop inside the envelope amplifier, leading to lower overall power dissipation. A novel envelope amplifier architecture with dual-switcher stages based on the digitally-assisted control strategy is proposed, designed and implemented. The strategy is demonstrated to improve the efficiency of envelope amplifier as well as the system overall efficiency. The resulting performance of envelope tracking system employing a GaAs high voltage HBT with a single carrier W-CDMA input demonstrated state- of-the-art efficiency with good linearity performance
-------------------------------------

10131851_183 - 0.999605782731 - technology_and_computing
[driver, information, system, advanced, travel, behavior, traveler]

In-Laboratory Experiments to Investigate Driver Behavior under Advanced Traveler Information Systems (ATIS)
In-laboratory experimentation with interactive microcomputer simulation is a useful tool for studying the dynamics of driver behavior in response to advanced traveler information systems. Limited real-world implementation of these information systems has made it difficult to observe and study how drivers seek, acquire, process, and respond to real-time information. This paper describes the design and preliminary testing of an interactive microcomputer-based animated simulator, developed at the University of California, Irvine, to model pre-trip and enroute driver travel choices in the presence of advanced traveler information systems. The advantages of this simulator are realized in its versatility to model driver decision processing while presenting a realistic representation of the travel choice domain. Results from a case study revealed that increased driver familiarity with travel conditions and network layout reduces driver reliance on information systems and influences drivers diversion behavior.
-------------------------------------

10130446_178 - 0.713729753065 - technology_and_computing
[localization, particle, camera, rgb-d]

Efficient scene simulation for robust monte carlo localization using an RGB-D camera
This paper presents Kinect Monte Carlo Localization (KMCL), a new method for localization in three dimensional indoor environments using RGB-D cameras, such as the Microsoft Kinect. The approach makes use of a low fidelity a priori 3-D model of the area of operation composed of large planar segments, such as walls and ceilings, which are assumed to remain static. Using this map as input, the KMCL algorithm employs feature-based visual odometry as the particle propagation mechanism and utilizes the 3-D map and the underlying sensor image formation model to efficiently simulate RGB-D camera views at the location of particle poses, using a graphical processing unit (GPU). The generated 3D views of the scene are then used to evaluate the likelihood of the particle poses. This GPU implementation provides a factor of ten speedup over a pure distance-based method, yet provides comparable accuracy. Experimental results are presented for five different configurations, including: (1) a robotic wheelchair, (2) a sensor mounted on a person, (3) an Ascending Technologies quadrotor, (4) a Willow Garage PR2, and (5) an RWI B21 wheeled mobile robot platform. The results demonstrate that the system can perform robust localization with 3D information for motions as fast as 1.5 meters per second. The approach is designed to be applicable not just for robotics but other applications such as wearable computing.
-------------------------------------

10137644_183 - 0.960657082929 - technology_and_computing
[classification, system, activity, test, motion, upper, application, wolf, motor, function]

Recognition and Classification of the Wolf Motor Function Test Items using Multimode Sensor Fusion
Human motion monitoring and activity classification, specifically in the free-living environment, are becoming increasingly important as preventative, diagnostic and rehabilitative measures in health and wellness applications. In contrast to gait analysis, wearable sensor-based evaluation of upper body activities is not well studied. The work in this thesis tends to explore a novel system for upper limb activity monitoring and classification. The system focuses specifically on the application of motion classification to a complex task of automating rehabilitation evaluation, such as the Wolf Motor Function Test. The presented system consists of a novel wearable motion sensor platform that integrates accelerometers, gyroscopes and flex-sensors, and classification algorithms that convert motion data into an alphabet representation and form a string of primitives. String expressions are then derived for each test item and a regular expression based searching method is developed. We present results from the successful application of the proposed system to upper limb activity characterization in the context of the Wolf Motor Function Test.
-------------------------------------

10138970_183 - 0.769098634532 - technology_and_computing
[standard, provision, consent, article, undue, law]

What is an "Undue Burden"?  The Casey Standard as Applied to Informed Consent Provisions
The existing scholarship on abortion rights focuses mainly on what is wrong with the state of legislative or judicial provisions surrounding the right to choose.  In addition to highlighting a problem, this article proposes a solution.  It argues, in a call to action, that the undue burden standard should be consistently raised by advocates and addressed by courts.  Thus, it should run parallel to all other challenges.  This article provides a roadmap for advocates, both in text and in hypothetical application to a recent Fifth Circuit case, regarding how to raise the standard in full when dealing with informed consent provisions.  This article shows, by example, that a fuller application of the undue standard can and will change the outcome of cases dealing with pre-abortion ultrasound provisions and fetal pain laws.  The article is not overly optimistic about what this changed standard may mean for the sheer number of abortion laws proposed, but argues that more attention to all aspects of the standard is necessary for future success in challenging informed consent provisions.  My hope is that this piece will contribute to the conversation in the public interest field about taking risks in reproductive rights advocacy, and to the actions taken during the litigation of challenges to informed consent laws.
-------------------------------------

10130083_178 - 0.999827352956 - technology_and_computing
[reference, radio, transmission, bibliographical, efficient, frequency, phase, manipulation]

Phase manipulation for efficient radio frequency transmission
Includes bibliographical references (p. 109-112).
-------------------------------------

10133581_183 - 0.919145706598 - technology_and_computing
[radius, tip, mm, composite, impact, force, strain, panel, impactor, bumper]

Low velocity blunt impacts on composite aircraft structures
As composites are increasingly used for primary structures in commercial aircrafts, it is necessary to understand damage initiation for composites subject to low velocity impacts from service conditions, maintenance, and other ground equipment mishaps. In particular, collisions with ground vehicles can present a wide area, blunt impact. Therefore, the effects of bluntness of an impactor are of interest as this is related to both the external visual detectability of an impact event, as well as the development of internal damage in the laminate. The objective of this investigation is to determine the effect of impactor radius on the initiation of damage to composite panels. A pendulum impactor was used to strike 200 mm square woven glass/epoxy composite plates of 3.18mm and 6.35mm thicknesses. Hemispherical steel tips with radius 12.7mm to 152.4mm were mounted to a piezoelectric force sensor which measures the contact force history. Impacts were conducted with and without rubber bumpers to mimic the bumpers used on ground vehicles. Strain gauges were mounted to select panels. Experimental data show distinct threshold energy for the onset of delamination and fiber breakage. These energy levels increase with increasing tip radius. Strain increases with decreasing tip radius. Strains at the panel center are less affected by the presence of rubber bumpers with increasing tip radius. Finite element simulations match the experimentally measured contact force and strain data. Curved FE panels show peak contact forces independent of impactor radius and incident angle. In-plane compressive stresses decrease with increasing radius tips and incident angle
-------------------------------------

10132830_183 - 0.999468798625 - technology_and_computing
[traffic, signal]

Can we save energy used to power traffic signals without disrupting the flow of traffic?
About one quarter million intersections in this country had, as of 1996, been fitted with traffic control signals. That number is increasing annually as urban sprawl and increased crowding lead to more traffic and to the need for more vigorous traffic control capability. Each intersection has an average of 40 such signals using either 69 or 150 watt incandescent lamps. Half of these are on at a given time, and due to the existence of protected left turns , the red traffic signals are on the most, perhaps 55-60% of the time. This a large energy burden though of course, the sight seer gazing down on a clear evening at the San Francisco Bay Area from the surrounding hills, would not see these as a predominant source of lighting. Nonetheless, these are an important economic burden to the agencies that maintain them, for those agencies are governmental and a trend in the 1990's towards increasing economic pressure has require them to explore any avenue of cost savings that is available. Of course, seeking efficiency is often prescribed by governmental authorities and thus stands as a required and desirable product of day-to-day operations.
-------------------------------------

10135143_183 - 0.989510085845 - technology_and_computing
[protocol, localization, time-based, measurement, simultaneous, location, error]

On Secure Localization Without Simultaneous Challenges
Time-based secure localization protocols allow a group of mutually trusted entities called <italic>verifiers</italic> to cooperatively determine the location of an untrusted and possibly malicious stranger entity called the <italic>prover</italic>. Many applications associate certain privileges with the true physical location of an entity, therefore there is an incentive for a prover to claim a more "valuable" location, different from its true location. A well known threat to time-based localization protocols is <italic>distance fraud</italic> where a malicious prover misrepresents its location by intentionally changing its response time across a series of bilateral dialogues with individual verifiers. To address this threat, secure localization protocols must use a technique called "simultaneous multilateration". A recently introduced protocol used simultaneous challenges by multiple verifiers, over separate RF channels, to defend against distance fraud. The authors also claimed that simultaneous multilateration by using simultaneous challenges, is <italic>optimal in the sense of achieving the maximal security that can be provided by any time-based localization protocol</italic>. In the first part of this thesis,  we show that  the structure of this newly proposed protocol is unnecessary, and significantly more complex than existing protocols. We propose a new protocol named <bold><italic> Elliptical Multilateration (EM) </italic></bold> that does not use simultaneous challenges. Instead, our EM protocol achieves simultaneity in multilateration by using multiple passive receivers to observe the prover's response. Our EM protocol requires fewer resources, provides commensurate security against distance fraud, and is inherently more accurate.The second part of this thesis focuses on the issues related to practical implementation  of time-based localization protocols. Most existing works  focus on cryptographic aspects of time-based secure localization protocols. We found that the existing literature does not address the issues that arise when these time-based protocols are implemented on real systems. For example, many authors have designed protocols based on single-bit exchanges (which is non-conformant  with standard networking protocols and hardware, and extremely difficult to implement), ignore inevitable measurement errors etc. In this thesis, we show that the amount and magnitude of measurement errors depend largely on the structure of a protocol, and differ significantly across the known localization protocols. We investigate whether measurements can be made with sufficient accuracy to achieve localization on the order of a few meters. To the best of our knowledge, this thesis is the first work that attempts to analyze measurement error in practical realization of different localization protocols. The factors influencing the measurement errors, which are highlighted in this thesis, are significant and cannot be ignored. We show that taking into account the significance of message structure and the factors influencing the measurement error, can lead to new protocols that are no worse in terms of security, need fewer message exchanges, and achieve better accuracy in comparison to existing time-based localization protocols.
-------------------------------------

10132690_183 - 0.993458890228 - technology_and_computing
[design, approach, cost, approximation, model, continuous, network, strategy, delivery, numerical]

Design and Operation of Multimode, Multiservice Logistics Systems
This thesis introduces design strategies and operational planning techniques for multimode, multiservice networks for package delivery carriers where service levels are defined by the guaranteed delivery items of packages (e.g., overnight, two-day delivery, etc.). Large-scale transportation network design problems are typically challenging, due to the large number of interdependent decision variables and constraints. These problems are even more complex with multiple service levels. Conventional network design and routing models cannot sufficiently capture the complexity of multimode, multiservice networks. This thesis discusses two principal design and routing approaches employed in the literature, and shows how the two approaches can be integrated. One approach utilizes detailed mixed-integer programming formulations and numerical methods. The other employs less detailed models based on continuous approximations. While the first approach provides a much higher level of detail, the second is more revealing of “the big picture”. Therefore, numerical methods are well suited for operational control, while continuous approximation methods are particularly effective for strategic planning and design, especially under uncertainty. An approach based on the complementary use of analytical approximation models and numerical optimization is developed to design, test and evaluate integrated strategies. This is the first application of hybrid continuous approximation/numerical optimization models to large-scale integrated networks with shipment choice. As such, advancements in both continuous approximation and numerical optimization, and the integration of the two, are required. Continuous approximation cost functions are shown to be capable of realistically modeling complex distribution systems with multiple trans-shipments and peddling tours. This research also demonstrates the application of solution techniques to reduce complex cost models to a series of subproblems that can be solved with common spreadsheet technology. Cost components are shown to accurately model costs using independent cost validation. A variety of integration scenarios are analyzed and the advantages of integrated operations are presented. Qualitative conclusions suggest that benefits of integration are greater when deferred demand exceeds express demand. This insight helps to explain the different business strategies of package delivery firms in industry today. This research demonstrates how hybrid modeling approaches can be used to better understand and better plan operating strategies for distribution companies.
-------------------------------------

10133128_183 - 0.99999348037 - technology_and_computing
[circuit, method, simulation, linear, approach, transistor-level, numerical]

Efficient large-scale transistor-level transient analysis
With increasing design complexity, huge size of extracted interconnect data is pushing the capacity of transistor level simulation tools to the limits. Direct methods, such as LU decomposition used in Berkeley SPICE and its variations, are prohibitive due to the super linear complexity. In last decade, various numerical methodologies, such as circuit partitioning, fast linear solver, model order reduction, approximated device model, simplified numerical integration or linearization, piecewise linear waveform approximation and so on, have been introduced to improve the performance of simulation under the rising demand from advanced technologies. Although with significant runtime improvement, those methods usually trade off accuracy for speed. Inaccurate simulation results could lead to over-design that increases the product cost especially for nanometer high performance integrated circuit designs. Inspired by the increasing gap between the design complexity and post- layout transistor-level simulation tools, we propose two efficient transistor-level analysis approaches with spice accuracy for deep-submicron and nanometer VLSI circuits: (1) Efficient technique to solve linearized circuit equation: A novel two-stage Newton-Raphson approach is implemented to dynamically model the linear network and nonlinear devices interfaces. Coupled linear networks are solved by the adaptive algebraic multigrid method. The circuit latency and activity variations are well captured by adaptive strategies to greatly avoid the unnecessary repeated calculation. The proposed approach employs extra iterations between linear and nonlinear circuits inside the linearization process to ensure the global convergence. (2) New Numerical Integration Procedure: We propose a generalized operator splitting method for transistor-level transient analysis and demonstrate that the generalized method is unconditionally stable. Following the generalized approach, we partition the circuits and alternate the explicit and implicit numerical integrations between the partitions. The splitting algorithm is derived to significantly reduce the overhead during LU factorization. Thus the robust direct method still remains efficient for large-scale circuits. Unlike the existing fast transistor-level simulation methods, both approaches proposed in the dissertation offer guaranteed simulation accuracy as well as significant runtime advantage. They can be used in post-layout transistor-level analysis of large-scale digital and mix- signal circuits
-------------------------------------

10131391_183 - 0.943803312937 - technology_and_computing
[surface, transportation, seaport, infrastructure]

Surface Transportation Policy and Seaports
The nation is facing increasing international economic competition. Seaports and their supporting surface transportation system play an important role in helping the American economy remain strong and competitive.
      A critical link in the complex intermodal chain is on land -- primarily outside immediate port boundaries. There are indications that surface transportation is under stress. Surface transportation infrastructure may not be up to the demands of growing seaport cargo flows. In many locations, surface access is handicapped by aging and/or deteriorating infrastructure in need of better maintenance, rehabilitation or replacement. In others, altogether new infrastructure is necessary.
-------------------------------------

10139737_183 - 0.999819579361 - technology_and_computing
[application, computing, scientific, cloud, science]

Magellan: experiences from a Science Cloud
Cloud resources promise to be an avenue to address new categories of scientific applications including data-intensive science applications, on-demand/surge computing, and applications that require customized software environments.
However, there is a limited understanding on how to operate and use clouds for scientific applications. Magellan, a project funded through the Department of Energy?s (DOE) Advanced Scientific Computing Research (ASCR) program, is investigating the use of cloud computing for science at the Argonne Leadership Computing Facility (ALCF) and the National Energy Research Scientific Computing Facility (NERSC). In this paper, we detail the experiences to date at both sites and identify the gaps and open challenges from both a resource provider as well as application perspective.
-------------------------------------

10135287_183 - 0.999722112928 - technology_and_computing
[ghz, power, array, db, dbm, silicon, antenna, amplifier, on-chip, rf]

X- to W-band phased arrays and wafer-scale transmitters using silicon integrated circuits
The thesis presents X- to W-band arrays implemented in silicon technologies for different phased-array applications. An 8-20 GHz two-channel dual down-conversion receiver with selectable IF for interference mitigation is presented for digital beamforming applications. The receiver is fabricated using a 0.18mum SiGe BiCMOS process and results in a channel gain (I and Q paths) of 46-47 dB at 11-15 GHz and > 36 dB at 8-20 GHz with an instantaneous bandwidth of 150 MHz. The measured NF is < 4.1 dB (3.1 dB at 15-16 GHz). The measured OP1dB is -10 dBm and the input P1dB is -56 to -40 dBm at 15 GHz depending on the gain, which is sufficient for satellite applications. The on-chip channel-to-channel coupling is < -48 dB. The measured EVM is < 3% for a 1 Msps QPSK modulation at 8-20 GHz, and < 1.8% for a 0.1, 1 and 10 Msps QPSK, 16QAM and 64QAM modulations at 15 GHz. The chip has ESD protection on the RF and DC pads, consumes 70 mA per channel from a 3.0 V power supply and is 2.6x2.2 mm², including all pads. A 15 GHz 8-element phased array with a NF < 3.9 dB is also demonstrated with multiple simultaneous beam performance using digital beamforming. In another project, a silicon-based 8-element phased array based on an All-RF beamforming topology is integrated together with the antennas and digital control circuitry on a single Teflon board. The chip-on-board package, together with 8 X/Ku-band RF inputs and one RF output in a 2.2x2.5 mm² area, and the appropriate grounding and Vcc connections, are modeled using a 3-D EM solver. The design results in a low coupling between the different RF ports, and ensures stability even with a channel gain of 20 dB at 12 GHz. The measured patterns show a near-ideal performance up to a scan angle of 60° with an instantaneous scanning bandwidth of 11.4-12.6 GHz (limited by non true-time delay connections between the antennas and the chip). Temperature tests indicate that the silicon chip maintains excellent phase stability and rms phase error up to 100°C. Finally, the first mm-wave wafer-scale silicon power amplifier array is implemented using 0.13 um BiCMOS technology. The power combining is done in the free -space using the high efficiency on-chip antennas. First, a W-band SiGe power amplifier is designed and fabricated together with a high-efficiency on-chip microstrip antenna. The power amplifier consumes 120 mA from a 1.7 V supply and the antenna/amplifier results in an effective radiated power (EIRP=PtGt) > 10 dBm from 88 to 98 GHz, with a peak of 14.6 dBm at 92 GHz. Then, a 3x3 power amplifier array is demonstrated with an equivalent isotropic radiated power (EIRP) of 33-35 dBm at 90-98 GHz. This results in a total on-chip power of 21-23 dBm, and a total radiated power of 17.5-19.5 dBm. The our knowledge, this is the highest power (and EIRP) achieved from a single silicon chip at millimeter-waves. The measured patterns of the array show single-mode operation and 1̃00% free-space power-combining efficiency with a 3-dB beamwidth of 28° and a directivity of 15.5 dB (gain of 12 dB). The total power-combining efficiency including the antenna losses is 45±10%. It is shown that by using this technique high power (1-2 W) millimeter-wave transmitters with phased array capabilities can be realized in silicon technologies which will be compatible with best III-V solutions. The application areas are in millimeter-wave transmitters and wafer-scale phased arrays
-------------------------------------

10135656_183 - 0.825891302724 - technology_and_computing
[material, densification, capad, model, process, current, pressure, rate, experimental]

Modeling and Experimental Studies of Densification Rates in Current Activated Densification
Current Activated Pressure Assisted Densification (CAPAD) has emerged as one of the most promising methods of materials processing in recent years. This process involves the use of large currents to generate heat coupled with applied pressure in what has proven to be a very powerful powder consolidation technique. Despite intensive research, there is still much to learn about the intricacies of how this process works. In the first part of this work, three material systems, yttria stabilized zirconia, Si, and Al, were processed using the CAPAD technique in order to see how processing conditions affect the densification rates in the CAPAD process. It was found that all materials show a peak densification rate at a particular homologous temperature, which is discussed in terms of material bond type. Pressure was found to directly affect the magnitude of the maximum densification rate in all samples. In the second part of this study, a model was developed to estimate the density-temperature relationship in the CAPAD process. This model is dependent on two parameters that are linked to physical quantities. Experimental data for oxide ceramic materials was used to demonstrate the fitting capability of the model. Oxides were chosen because of the abundance of experimental data available in the literature. Estimating the dominant parameter can be done by finding a material system's activation energy for diffusion and comparing it to other known materials. Guidelines were given on how future CAPAD workers can use the model to save time and resources.
-------------------------------------

10136122_183 - 0.777633397015 - technology_and_computing
[detection, pitch, signal]

An Exploratory Study of the Effect of Demodulation on Pitch Detection
Pitch (F0) detection has been a very active research topic in recent years. Accurate F0 detection is very important in applications such as speech recognition and coding. This work examines the effects of demodulating a signal into its modulator and carrier parts, each bearing low and high frequency content of the signal respectively, on pitch detection accuracy. Since the carrier part is known to contain the high frequency pitch content of a speech signal, using it as an input to pitch detection algorithms proves to be beneficial in most of the tests carried out with different speakers, and under different noise levels.
-------------------------------------

10175394_189 - 0.999430706535 - technology_and_computing
[internet, smartphone, user, web, use, browser, pattern, native, application, technology]

An Empirical Analysis of Internet Use on Smartphones: Characterizing Visit Patterns and User Differences
The original vision of ubiquitous computing was for computers to assist humans by providing subtle and fitting technologies in every environment. The iPhone and similar smartphones have provided continuous access to the internet to this end. In the current thesis, my goal was to characterize how the internet is used on smartphones to better understand what users do with technology away from the desktop. Naturalistic and longitudinal data were collected from iPhone users in the wild and analyzed to develop this understanding. Since there are two general ways to access the internet on smartphones—via native applications and a web browser—I describe usage patterns through each along with the influence of experience, the nature of the task and physical locations where smartphones were used on these patterns. 
The results reveal differences between technologies (the PC and the smartphone), platforms (native applications and the mobile browser), and users in how the internet was accessed. Findings indicate that longitudinal use of web browsers decreased sharply with time in favor of native application use, web page revisitation through browsers occurred very infrequently (approximately 25% of URLs are revisited by each user), bookmarks were used sparingly to access web content, physical location visitation followed patterns similar to virtual visitation on the internet, and Zipf distributions characterize mobile internet use. The web browser was not as central to smartphone use compared to the PC, but afforded certain types of activities such as searching and ad hoc browsing. In addition, users systematically differed from each other in how they accessed the internet suggesting different ways to support a wider spectrum of smartphone users.
-------------------------------------

10132706_183 - 0.888044997808 - technology_and_computing
[activity, program, scheduling, datum, process, pilot, stop]

Experiments With A Computerized Self-Administrative Activity Survey
The process of activity scheduling is crucial to the understanding of travel behavior changes. In-depth research is urgently needed to unearth this process. To reveal this process, a new computer program, REACT!, has been developed to collect household activity scheduling data. The program is implemented as a stand-alone program with Internet connectivity for remote data transmission. It also contains a GIS for location identification and a special feature that traces the decisions in scheduling process. A pilot study was conducted in Irvine, California to evaluate the program performance. Experience from the pilot study validated the program's capability of guiding participants to complete data entry tasks on their own, thus the objective of reducing the cost and human resource of such a computerized survey is achieved. Other positive results regarding objectives of reducing instrumental biases and expanding program capabilities were also obtained. Areas for improvement were also identified.
      Based on the pilot data, activities with shorter duration were found more likely to be opportunistically filled in a schedule already anchored by their longer duration counterparts. In addition, the situations (e.g., location, involved person, and day of the week) under which an activity occurred were found related to its scheduling horizon. Analyses were also performed to validate that the above findings hold in the presence of a third factor (i.e., in-home vs. out-of-home, and work/school vs. non-work/school). Additionally, analysis of tour structure reveals that a certain portion of trip-chains was formed opportunistically. The proportion of opportunistic stops tends to increase as stop sequence increase. Travel time required to reach an activity is also positively related to scheduling horizon of the activity, with distant stop being planned earlier.
-------------------------------------

10134533_183 - 0.964937423142 - technology_and_computing
[material, energy, thermoelectric, application, efficiency, conversion, bulk, dimensional]

New nano structure approaches for bulk thermoelectric materials
A sustainable supply of environmentally clean energy is one of the most significant challenges facing the 21st century as fossil fuel supplies are decreasing and world energy demand keeps increasing. The field of thermoelectric (TE) materials and devices, for efficient solid state cooling and power generation, has expanded significantly in recent years partly due to the advent of nanotechnology and the promise of higher efficiencies of electrical energy versus thermal energy inter-conversion. Such solid-state refrigeration and power generation based on thermoelectric phenomenon offer significant promises to technical applications in the computer, energy conversion, and consumer market applications. While the area itself has been investigated since the 1950's, traditional bulk thermoelectric materials, such as Bi₂Te₃ generally had low efficiencies (228}06 %) for commercial energy conversion applications. However, a deeper utilization of nanoscience at the quantum mechanical level and fabrication of lower dimensional materials such as nanowires and quantum wells, has shown promise of greatly increasing the energy conversion efficiency. Lower dimensional materials, such as nanoscale thin films and nanowires, have been predicted to have large thermoelectric figures of merit and conversion efficiencies of waste heat to useful energy for practical application, and there are strong indications that this is possible. However, ultimately one needs large volumes of active thermoelectric material for an overall high efficiency. This is, at the present time, not feasible as the typical manufacturing of bulk volumes from low dimensional structures is time consuming and difficult. In this thesis, we lay out several practical schemes to incorporate nanoscale features in bulk- fabricated thermoelectric materials for easy fabrication and a high thermoelectric figure of merit, ZT. We propose to fabricate a novel nanostructure material to impart low dimensional confinement and nanoscale defects for a high figure of merit. Various unique architectures and associated process techniques as well as device applications are introduced. It is anticipated that these techniques will lead to many novel thermoelectric materials, innovative and useful materials processing technologies, and technical applications for efficient thermal management and energy conversion of sunlight and automobile waste heat
-------------------------------------

10133554_183 - 0.999947709632 - technology_and_computing
[network, interference, analysis, packet, transmission, system, single-hop, delay, performance, topic]

Analysis and design of interference-limited wireless systems
This thesis is concerned with the performance analysis and design of interference-limited wireless systems. Specifically, we consider wireless systems where transmissions from potential interferers are uncoordinated, i.e., there is no scheduling for the purpose of interference management. Special emphasis is placed on how the spatial distribution of the interferers affects the statistical characteristics of the interference power. The first topic we address is the performance analysis of the downlink of a cellular frequency-hopping multiple-access system, where coherent detection is employed. Channel estimation is performed with the help of hopping patterns which are broadcasted by the base-station and tracked by different users in the cell. In a time and frequency correlated channel, we identify which patterns result in the best performance in terms of the bit-error-probability at the receiver. We then consider the effect of the interference from the base-station of an adjacent cell and assess the importance of interference side information at the receiver on the bit-error-probability. The second topic we focus on is the analysis of random single-hop networks. The model we consider is that of a homogeneous Poisson point process of transmitters, each with a RX at a fixed distance. We evaluate the packet error probability and the network transmission capacity when the channel, consisting of fading and interference, is constant or varying during the packet transmission. The impact of coding, spatial diversity and multiple-stream transmission on the network capacity is identified with closed-form expressions in the small packet error probability regime. The third and final topic addressed is the evaluation of the mean end-to-end delay in random multi-hop networks. We extend the single-hop network model employed previously to accommodate routes, each consisting of a source, a number of relays and a final destination. Combining tools from the analysis of single-hop networks with queuing theory we derive a closed-form expression for the mean end-to-end delay over the typical route. The number of relays and their placement are determined such that the delay is minimized, for different packet arrival scenarios at the sources
-------------------------------------

10135055_183 - 0.975200404628 - technology_and_computing
[factor, transcription, network, transcriptional, species, datum, yeast, evolutionary, compensatory, mutation]

Evolution of transcriptional regulatory circuits in yeasts
A central challenge to post-genomic biology is to elucidate the cellular networks that underlie biological form and function. Such networks of transcriptional, post- transcriptional and post-translation regulation form an essential part of the cellular repertoire. The recent explosion of high-throughput genome-wide technologies has allowed us to begin to elucidate the structure and function of such networks. Additionally, these technologies also allow direct comparisons of networks to be made across species -- a subject that has received comparatively less attention. Simultaneous study of multiple species at appropriate evolutionary distances allows us to make more general statements about the robustness, evolvability, modularity and evolutionary redundancy of cellular networks than can otherwise be made when studying a single species in isolation. In this thesis, I describe the generation and analysis of genome- wide mRNA expression and transcription factor localization data across four diverse species of yeast separated by hundreds of millions of years of evolution. In Chapter 3, I generate genome-wide transcription factor localization data for the budding yeasts S. cerevisiae, and C. glabrata discovering a system of tightly coupled compensatory trans and cis mutations in the AP-1 transcriptional network. These compensatory mutations allow for conserved transcriptional regulation despite continued genetic change. Such systems of tightly coupled compensatory mutations might serve to counter the widespread divergence observed in transcriptional networks, and may constitute a general evolutionary mechanism maintaining the regulation of transcriptional networks. In Chapter 4, I generate genome-wide transcription factor localization data for several cell-cycle transcription factors, but in the fission yeast S. pombe. Similarly to previous studies, I find relatively poor conservation of binding between orthologous transcription factors. However, further analysis of our data along with that of previous studies suggests that transcription factors while not being particularly well conserved at the level of the binding of target genes show stronger conservation in other ways such as DNA binding motif, the functional enrichment of target genes, transcription factor expression and transcription factor activity
-------------------------------------

10132703_183 - 0.999699719823 - technology_and_computing
[transportation, technology, mobility, system, vehicle]

New Mobility: Using Technology and Partnerships to Create More Sustainable Transportation
Land development and vehicle travel continue to outpace population growth. Efforts to manage this growth and the adverse impacts associated with it have been mostly ineffective. Promising technology solutions include telecommunications (telecommuting, electronic commerce, teleconferencing, etc.), small personal vehicles (electric bikes and neighborhood vehicles); and new "smart" transport modes (car sharing, "smart" paratransit, etc.).
      These options have the potential to be environmentally and economically superior to today’s car-dominated system. Yet none have flourished. Why? One hypothesis is that many automobile substitutes and complements have been rejected because they have been introduced individually and incrementally - not as part of a coordinated transportation system
      New mobility is a fundamentally new approach to this problem, focusing on intermodal clustering of innovative technologies with existing transportation options to create a coordinated transportation system that could substitute for the traditional auto. The concept of new mobility was explored at a workshop hosted by the Institute of Transportation Studies at the University of California, Davis on November 2, 1998. This report describes the technologies of new mobility and suggests ways that they might be coupled together through public-private partnerships and experimentation at the local level.
-------------------------------------

101649_108 - 0.754887176019 - technology_and_computing
[sicilia, mediterranean, on-line, database, catania, mmm4, bibliography, proceedings, sep, suppletion]

Resources for suppletion: A typological database and a bibliography.
On-line proceedings of the 4th Mediterranean Morphology Meeting (MMM4), Catania, Sicilia 21-23 Sep 2003
-------------------------------------

10133011_183 - 0.999999823685 - technology_and_computing
[nims, systems, networked, infomechanical]

Reliable Actuation for Networked Infomechanical Systems
This poster explains Networked Infomechanical Systems (NIMS) and its applications. In addition, it details the problem of getting accurate location data for the NIMS node and the proposed solution using a separate encoder module.
-------------------------------------

10136266_183 - 0.999916111515 - technology_and_computing
[measurement, smart, distribution, meter, location]

Assessing the Usefulness of Distributed Measurements in the Smart Grid
The move to modernize power distribution, including the deployment of smart meters in the field, opens new possibilities for monitoring and control with the historically volatile distribution system. Given measurements from smart meters and other intelligent devices, location and characterization of the three basic fault types (line-to-ground, multi-line-to-ground, and line-to-line) is possible using voltage sag characteristics. In addition, distributed measurement locations result in greater fidelity in transient analysis, given waveforms captured closer to their source.
-------------------------------------

10131539_183 - 0.983646099094 - technology_and_computing
[suspension, simulation, pavement, type]

The Effect of Alternative Heavy Truck Suspensions on Flexible Pavement Response
In this paper, the dynamic effects of heavy vehicle suspensions are investigated by analyzing parametric studies performed using previously developed simulation tools. In particular, the truck simulation package, VESYM, and the flexible pavement simulation package. VESYS, are used to look at the influence of alternative suspension types, e.g., walking beam, leaf spring/short rocker, air spring and semi-active shock absorbers on pavement response.
      The intent of the study is to determine differences between suspension types as well as to look at the sensitivity to parameter changes and optimization within a particular suspension type.
      The simulation results indicate that substantial improvements in pavement longevity may be possible by careful suspension selection and optimization.
-------------------------------------

10136723_183 - 0.99997443198 - technology_and_computing
[device, performance, bascin, different, application, user, ranking, requirement, list, android]

BASCin: Benchmark for Android from Santa Cruz
With smartphones and tablets becoming too ubiquitous too quickly and a plethora of devices being available in the market, the dilemma of Which device is best for me? is quite natural, especially for a layman. Benchmarking each device's capability accurately would allow a user to decide which device is more suitable for him. However, existing benchmarking applications for Android devices provide rankings based purely on performance metrics without considering what a user's requirements are and hence fall short in providing meaningful recommendations. We propose a new benchmarking application called BASCin (Benchmark for Android from Santa Cruz) which provides rankings based not only on performance metrics, but also based on one's usage requirements for different types of mobile applications. Specifically BASCin provides four different types of ranking lists namely performance based, power-usage efficiency based, battery-usage based and performance & battery-usage based and thereby caters the needs of different categories of users. From our experiments we find that, different devices top the ranking list for different use-cases unlike a single list based on performance alone. Thus, by taking the usage requirements also into account, BASCin provides more meaningful and accurate recommendations. Additionally, BASCin can also identify the battery usage of each installed application and hence a user may decide to let go unwanted applications that drain his battery.
-------------------------------------

10135113_183 - 0.909246510766 - technology_and_computing
[thermal, media, magnetic, slider, height, control, bit, heat, recording, disk]

Investigation of bit patterned media, thermal flying height control sliders and heat assisted magnetic recording in hard disk drives
Many advanced technologies in the field of magnetic disk recording are being studied in order to achieve areal densities in excess of 1.6 gigabits per square millimeter (1 terabits per square inch). Bit patterned media (BPM) is one of these promising technologies. By using disks with physically separated magnetic patterns instead of conventional continuous media, bit patterned media avoid magnetic interference between adjacent bits and improve the thermal stability of the media. Currently, thermal flying height control (TFC) sliders are commonly used to compensate thermal effects during reading and writing and to maintain a stable and ultra-low head/disk spacing during drive operation. Heat assisted magnetic recording (HAMR) has been introduced in order to address difficulties in writing of information on magnetic media with high coercivity. By using a laser beam to locally heat the media above its Curie temperature, the magnetic material momentarily reduces its coercivity and permits writing of information on the disk. However, the method raises concerns about the stability of the lubricants on the disk. In this dissertation, we focus on the investigation of the head/disk interface for bit patterned media, the design of thermal flying height control sliders, and the implementation of heat assisted magnetic recording. In particular, we use a finite-element-based air bearing simulator to study the steady-state flying characteristics of sliders flying over bit patterned media. This air bearing simulator is then combined with a thermo-mechanical model of a slider in order to analyze thermal flying height control sliders featuring dual heater/insulator elements. Next, a finite element model of a thermal flying height control slider with an integrated heat assisted magnetic recording optical system is developed to study the effect of heat dissipation along the laser delivery path on the performance of the HAMR-TFC slider. The design parameters of the dual thermal flying height control heaters are optimized in order to minimize the dependence of the head/disk spacing on laser induced thermal effects. Finally, experimental techniques are developed to investigate the photo-thermo stability and tribological properties of HAMR-type lubricants which are designed to be resistant to the high temperatures experienced under laser exposure
-------------------------------------

10134979_183 - 0.999553454743 - technology_and_computing
[channel, gramicidin, functional]

Nanoscale sensing methodology via functional control of an ion channel-forming peptide, Gramicidin A
Gramicidin ion channels are formed by the association of two Beta-helix pentadecapeptides, each spanning half the lipid bilayer. A single dimerization event enables the flux of millions of cations across the bilayer per second. This signal amplification allows for the detection and monitoring of events on the single molecule level - a feat seldomly accomplished. Gramicidin is also remarkably simple to use as it both spontaneously self-incorporates into the membrane and forms channels under a broad range of conditions. Lastly, work primarily done by our lab has developed facile synthetic techniques for functional modifications of the native channel structure. This thesis represents a two-fold effort, both in the improved fundamental understanding of gramicidin A, as well as the exploration of this platform's utility as a tool for more applied problems. Through a comprehensive understanding of the influences which a permanent charge affixed to the channel entrance exert on both channel conductance and open channel lifetime, the ability to utilize the functional response to a wide variety of external stimuli has been realized
-------------------------------------

10136867_183 - 0.818200210186 - technology_and_computing
[technology, engineer, collectives, part, engineers, chris, world, engineering, collective, coop]

Engineering Collectives: Technology From the Coop
Engineers make the world, but not just as they please. Chris Csikszentmihályi recounts how engineers come to be part of one collective or another.
-------------------------------------

10139483_183 - 0.999443406676 - technology_and_computing
[performance, algorithm, memory]

Exploration of Optimization Options for Increasing Performance of a GPU Implementation of a Three-dimensional Bilateral Filter
This report explores using GPUs as a platform for performing high performance medical image data processing, specifically smoothing using a 3D bilateral filter, which performs anisotropic, edge-preserving smoothing. The algorithm consists of a running a specialized 3D convolution kernel over a source volume to produce an output volume. Overall, our objective is to understand what algorithmic design choices and configuration options lead to optimal performance of this algorithm on the GPU. We explore the performance impact of using different memory access patterns, of using different types of device/on-chip memories, of using strictly aligned and unaligned memory, and of varying the size/shape of thread blocks. Our results reveal optimal configuration parameters for our algorithm when executed sample 3D medical data set, and show performance gains ranging from 30x to over 200x as compared to a single-threaded CPU implementation.
-------------------------------------

10132132_183 - 0.923345734573 - technology_and_computing
[model, choice, constraint, binary, telecommuting]

Modeling the Choice of Telecommuting 3: Identifying the Choice Set and Estimating Binary Choice Models for Technology-Based Alternatives
Previous papers in this series have presented a conceptual model  of the individual decision  to telecommute  and explored relationships among  constraints, preference, and choice. A  related paper has developed a binary model of the preference for home-based  telecom-  muting. Noting that there is a wide gap between preferring to telecommute  (88%  of the  sample) and actually telecommuting  (13%), this paper develops binary logit models  of role-  commuting  adoption. Two  approaches to dealing with constraints are compared:  incorporating them  directly into the utility function, and using them  to define the choice set. Models  using the first approach  appear  to be statistically superior in this analysis, explaining  63-64%  of the information  in the data. Variables significant to choice include those relating to work  and travel drives, and awareness,  manager  support, job suitability, technology,  and discipline  constraints. The best model  was used to analyze the impact of relaxing three key constraints  on the 355 people in the sample for whom  telecommuting  was previously identified to be  a Preferred Impossible Alternative. When  unawareness, lack of manager  support, and job  unsuitability constraints are relaxed, 28%  of the people in the PIA  category would  be expected to adopt telecommuting.  The importance  of behavioral models  to accurately forecasting  telecommuting adoption is emphasized and is suggested to have wider implications for  predicting technology-based  activity changes.
-------------------------------------

10137293_183 - 0.999925735615 - technology_and_computing
[scheme, gaussian, channel, source, case, chapter]

Broadcasting Gaussian Sources Over Gaussian Channels
This dissertation studies several problems regarding the joint/separate source-channel coding in data transmission over networks. It has been known that separation based transmission schemes achieves optimality when there is only one sender and one receiver. However, other than this point-to-point case, separation is often suboptimal and it is generally unclear how to achieve optimality.In Chapter 2, several new hybrid digital/analog schemes are proposed for the problem of transmitting a Gaussian source over a Gaussian channel under different scenarios. Previously existing optimal schemes are combined in a way that the output of one encoder is taken as effective CSI at the other encoder. In each case, the optimum distortion-power tradeoff can be achieved by a continuum of auxiliary random variables for any power allocation between the two encoders.In Chapter 3, a new scheme is proposed for lossy transmission of a Gaussian source over a Gaussian broadcast channel with source side information available at each re- ceiver. The proposed scheme combines two schemes that were previously shown to achieve optimal point-to-point distortion/power tradeoff simultaneously at all receivers under two distinct conditions stated in terms of channel and side information quality parameters. For the two-receiver case, the combined scheme is shown to achieve the same kind of optimality for the entire region in the parameter space sandwiched between those two conditions. Crucial to this result is the new degree of freedom discovered in Chapter 2.In Chapter 4, the problem of broadcasting a pair of correlated Gaussian sources using optimal separate source and channel codes is studied. A universal upper bound of rate (bandwidth compression/expansion ratio) penalty is given and in particular, for a low-distortion scenario, separate coding can be shown to achieve optimum. For the case with a Gaussian broadcast channel, the power loss of separate coding is also dis- cussed. Although source-channel separation yields suboptimal performance in general, it is shown that the proposed scheme provides competitive performance comparing with best-known schemes.
-------------------------------------

10137641_183 - 0.999818323936 - technology_and_computing
[device, wave, spin, cmo]

Electric field induced spin wave generation for beyond CMOS magnonic logic devices
The semiconductor industry is fast approaching the fundamental scaling limit of conventional complementary metal-oxide-semiconductor (CMOS) devices. This has spurred numerous exploratory researches in the last decade, on technologies that can sustain Moore's law and eventually replace CMOS based devices. One such magnonics based, beyond CMOS device is called the spin wave device. A spin wave device is a low power magnetic logic device that uses a propagating wave of precessional spins, called a spin wave, to perform computations. Traditionally, spin waves have been generated using energetically inefficient, inductively coupled antennas. In order to attain atto-joule energy per operation, which is substantially lower than that of the current generation of CMOS devices, we propose and demonstrate a novel method of spin wave generation using the strain mediated magneto electric effect. We have conducted extensive experimental and numerical studies to design, engineer and fabricate such nano scale spin wave devices. These devices could in effect outperform and eventually replace the CMOS based logic devices.
-------------------------------------

10133729_183 - 0.993543139515 - technology_and_computing
[solution, method, problem, time, nonlinear, cod, complexity, dre, pde, dp]

Efficient idempotent methods for optimal control
Dynamic programming (DP) is a very powerful and robust tool for nonlinear optimization. Nevertheless, the applications have been limited to discrete / low dimensional systems due to the ubiquitous curse-of- dimensionality (CoD), which increases computation cost exponentially with the dimensionality of the problem. Application of DP to continuous-time and continuous-space systems gives rise to Hamilton-Jacobi-Bellman (HJB) PDEs, which are nonlinear and can have non-smooth solutions. Recently, a CoD-free method was developed to solve certain nonlinear semiconvex HJB PDEs. It is based on the linearity of the underlying semigroup on a suitable idempotent algebra. The CoD is avoided as it is grid-less and the solution is expressed as the maximum of quadratic functions. Moreover, the Hamiltonian is approximated by the maximum of M linear-quadratic Hamiltonians, where M is called the complexity. In process, the original problem is approximated by the optimal switching problem between M linear systems. Unfortunately, although the above method avoids the CoD, it suffers from a curse-of-complexity (CoC) as the number of quadratic bases used to approximate the value function, grow exponentially with the complexity. In this thesis, through the use of semidefinite programming based pruning techniques, this CoC has been partially abated. High dimensional, low complexity problems have been solved, pushing the envelope of the applicability of DP. This thesis also carries out the analysis of the error in the solution due to the PDE approximation and suboptimality of feedback control computed using it. This thesis also extends the original method for semiconcave Hamiltonians arising in cost minimization problems without nominal stability. As a generalization of a sub-problem within the CoD-free method, this thesis also develops the fundamental solution for the time-varying differential Riccati Equation (DRE). This is the counterpart of the state transition matrix in time- varying ODEs, and allows analytic computation of a general solution from a particular solution. It is also shown that the semiconvex duality transforms one DRE into another, and compatibility conditions are derived. In time- invariant special case, efficient doubling algorithms and analytic solutions are proposed. These show dramatic improvement over the time marching methods for long time horizon evolution of stiff DREs
-------------------------------------

10134143_183 - 0.958299720444 - technology_and_computing
[event, content-based, fulcrum, computation]

Implementation and empirical evaluation of a publish- subscribe framework employing content-based placement of computations
Content-based publish-subscribe (CBPS) framework acts as the basic element in context-aware applications due to its potential to provide an efficient event distribution, better separation of concerns and extensibility. This thesis attempts to evaluate a proximity relationship algorithm presented in the system Fulcrum BG05 in an actual publish-subscribe implementation called Ubibot VEG09. The Fulcrum makes CBPS subscriptions first-class by allowing the subscriptions to have computations associated with them and making those computations to be full-flown, including the ability to advertise their own events and subscribing to multiple events from multiple sources. The Ubibot is modified to support content-based routing and then implement the idea presented in Fulcrum. The solution is evaluated by using a buddy proximity scenario covering a wide range of behavioral possibilities, looking for anomalies. An overall performance of lg₂(distance) is achieved from the algorithm and lg2(Original Events) of reduced event traffic
-------------------------------------

10136468_183 - 0.998486978279 - technology_and_computing
[sensor, swnt, hybrid, sensitivity, high]

Carbon Nanostructure-Based Sensors
One-dimensional (1-D) nanostructure-based sensors provide better sensitivity as compared to conventional thin film-based sensors due to their comparable dimensions with respect to Debye length. Single-walled carbon nanotubes (SWNTs) are 1-D nanostructures having high electrical mobility, high mechanical strength and high specific surface area that facilitate building of low-power, ultrahigh density sensors within limited space. However, pristine SWNTs posses limited sensitivity and selectivity. This sensitivity issue could be resolved through surface modification of SWNTs with suitable recognition molecules. Choosing the right functional material, sensor performance, in terms of sensitivity, could be enhanced. The selectivity issue could be overcome by making an array of sensors using different functional material and combining them with suitable pattern recognition software and thus, developing an electronic nose. The overall objective of this dissertation is the development of a high density sensor array using single-walled carbon nanotube (SWNT) hybrid structures as   building blocks for the realization of highly sensitive and discriminative sensors for monitoring of pollutants in the environment. To conceive such desired sensor characteristics, SWNTs were functionalized with organic macromolecules like porphyrins and calixarene to target VOCs in the air, and with biomolecules like  polyT (a ssDNA) to target mercury ions in  water. Detailed analysis of SWNT hybrid formation through different routes such as solvent casting and electrochemical techniques using macromolecules was performed to investigate structure property relations and the effect of macronucleus towards sensor performance when exposed to various analytes. The SWNT-porphyrin hybrids exhibited a discriminating capability of VOCs at room temperature. However, electrochemically modified (as opposed to solvent casting) SWNT-poly(porphyrin) hybrid improved sensor sensitivity further, retaining their discriminating capabilities and providing a sub-ppm limit of detection towards VOCs. The SWNT-poly(metalloporphyrin) hybrids also showed a probable relationship between sensor response and  metal present within the porphyrin depending on the metal electronegativity. ssDNA functionalized SWNT hybrid exhibited highly selective and sensitive sensing response towards mercury ions in solvent phase.
-------------------------------------

10175482_189 - 0.991432806579 - technology_and_computing
[cell, input, spatial, model, place, memory]

The Neural Computations of Spatial Memory from Single Cells to Networks
Studies of spatial memory provide valuable insight into more general mnemonic functions, for by observing the activity of cells such as place cells, one can follow a subject’s dynamic representation of a changing environment. I investigate how place cells resolve conflicting neuronal input signals by developing computational models that integrate synaptic inputs on two scales. First, I construct reduced models of morphologically accurate neurons that preserve neuronal structure and the spatial
specificity of inputs. Second, I use a parallel implementation to examine the dynamics among a network of interconnected place cells. Both models elucidate possible roles for the inputs and mechanisms involved in spatial memory.
-------------------------------------

10130989_183 - 0.991098992038 - technology_and_computing
[use, file, world, notes1, note, cultures]

Notes1#1
This file contains notes on the use of World Cultures.
-------------------------------------

10139090_183 - 0.998060780744 - technology_and_computing
[online, memory, site]

Re-Imagining the Past in Transnational Online Communities
This essay examines transnational online communities as sites of identity, belonging and memory. It looks at the ways in which memories are circulated, shared and created within these virtual spaces. It explores, with the guidance of previous research on diasporic online sites, the ways in which notions of nostalgia, ordinariness and hybridity come together in different contexts of online remembering and how virtual memory sites shape the ways in which past is imagined, represented and experienced.
-------------------------------------

10130507_183 - 0.983065596738 - technology_and_computing
[field, light, vector, application, practical, natural]

Photopic field theory for natural hydroso1s
SIO Reference 58-66. The purpose of this note is to present an example of the application of the vector theory of the photic field ("light field") to an important class of scattering-absorbing optical media, namely the class of natural hydrosols consisting, e.g., of oceans, harbors, and lakes. The application is at the same time of practical value in that it yields explicit expressions for the depth-dependence of the light vector in terms of its components at the surface and certain of the optical properties of these media. Furthermore, the discussion presents particularly simple interpretations of the quasipotential and related functions. These interpretations emerge naturally from the geometry and physics of the present application. In this way we add to the evidence that the formalism of the photic field as developed by Moon, Spencer, and others  is of more than academic interest, and in fact provides an elegant tool for the study of the light vector in the practical settings encountered in the study of hydrological optics.While the practical context of the present discussion is limited specifically to that of natural hydrosols, the mathematical arsuments apply equally well to any arbitrary plane= parallel scattering-absorbing medium in which the light vector possesses a quasipotential.
-------------------------------------

10137465_183 - 0.999844071027 - technology_and_computing
[sensor, displacement, intensity-modulated, interferometric, performance, fiber]

Modeling and validation of performance limitations for the optimal design of interferometric and intensity-modulated fiber optic displacement sensors
Optical fiber sensors offer advantages over traditional electromechanical sensors, making them particularly well- suited for certain measurement applications. Generally speaking, optical fiber sensors respond to a desired measurand through modulation of an optical signal's intensity, phase, or wavelength. Practically, non- contacting fiber optic displacement sensors are limited to intensity-modulated and interferometric (or phase- modulated) methodologies. Intensity-modulated fiber optic displacement sensors relate target displacement to a power measurement. The simplest intensity-modulated sensor architectures are not robust to environmental and hardware fluctuations, since such variability may cause changes in the measured power level that falsely indicate target displacement. Differential intensity-modulated sensors have been implemented, offering robustness to such intensity fluctuations, and the speed of these sensors is limited only by the combined speed of the photodetection hardware and the data acquisition system (kHz-GHz). The primary disadvantages of intensity-modulated sensing are the relatively low accuracy (mum-mm for low-power sensors) and the lack of robustness, which consequently must be designed, often with great difficulty, into the sensor's architecture. White light interferometric displacement sensors, on the other hand, offer increased accuracy and robustness. Unlike their monochromatic- interferometer counterparts, white light interferometric sensors offer absolute, unambiguous displacement measurements over large displacement ranges (cm for low- power, 5 mW, sources), necessitating no initial calibration, and requiring no environmental or feedback control. The primary disadvantage of white light interferometric displacement sensors is that their utility in dynamic testing scenarios is limited, both by hardware bandwidth and by their inherent high-sensitivity to Doppler-effects. The decision of whether to use either an intensity-modulated interferometric sensor depends on an appropriate performance function (e.g., desired displacement range, accuracy, robustness, etc.). In this dissertation, the performance limitations of a bundled differential intensity-modulated displacement sensor are analyzed, where the bundling configuration has been designed to optimize performance. The performance limitations of a white light Fabry-Perót displacement sensor are also analyzed. Both these sensors are non- contacting, but they have access to different regions of the performance-space. Further, both these sensors have different degrees of sensitivity to experimental uncertainty. Made in conjunction with careful analysis, the decision of which sensor to deploy need not be an uninformed one
-------------------------------------

10130211_178 - 0.999316004151 - technology_and_computing
[technology]

A Simple Reactive Obstacle Avoidance Algorithm and Its Application in Singapore Harbor
Massachusetts Institute of Technology. Singapore-MIT Alliance in Research and Technology (SMART)
-------------------------------------

10139392_183 - 0.995847965721 - technology_and_computing
[array]

Array test - modified metadata again
testing array bug twice
-------------------------------------

10137720_183 - 0.999999678143 - technology_and_computing
[hardware, tool, software, fm-index, string, execution, datum, matching, genome, algorithm]

Hardware Implementation of a String Matching Algorithm Based on the FM-Index
String matching is the searching of patterns in a very long string called text. It is involved in DNA sequence mapping that matches millions of short patterns, called reads, on a reference genome. The length of the reads is in the range of 36 to 150 characters and a typical genome length is billions of characters. The processing massive amount of data led to the development of advanced algorithms. The FM-index, based on the Burrows-Wheeler Transform, is a recently developed data structure utilized by the fastest software tool to map millions of reads on a reference genome. Although the FM-index is a very sophisticated tool used for mapping, current software tools still need faster execution due to rapidly increasing data because of improved sequencing technologies.The focus of this research is improving the execution time of existing string matching algorithm based on the FM-index through hardware acceleration using FPGAs. We introduce FHAST (FPGA Hardware Accelerated Sequence-matching Tool) as an accelerator acting as a drop-in replacement for Bowtie, an industry-accepted software mapping tool. FHAST uses a multi-threaded architecture masking external memory latency by executing concurrent hardware threads. FHAST is implemented on a Convey HC-1 supercomputing system to take advantage of high memory bandwidth and shared memory space of hardware and software. We observe an actual speed up as high as 70x compared to Bowtie, which reduces execution runs from hours to minutes.
-------------------------------------

10133895_183 - 0.849998320591 - technology_and_computing
[nanowire, device, nwfet, effect, ndr, electron, conjugate, conduction]

Nanoscale Electronic Devices
Continuous downscaling in microelectronics has pushed conventional CMOS technology to its physical limits, while Moore's Law has correctly predicted the trend for decades, each step forward is accompanied with unprecedented technological difficulties and near-exponential increase in cost. At the same time, however, demands for low-power, low-cost and high-speed devices have never diminished, instead, even more stringent requirements have been imposed on device performances. It is therefore crucial to explore alternative materials and device architectures in order to alleviate the pressure caused by downscaling. To this end, we investigated two different approaches: (1) InSb nanowire based field effect transistors (NWFETs) and (2) single walled carbon nanotube (SWCNT) - peptide nucleic acid (PNA) -SWCNT conjugate. Two types of InSb nanowires were synthesized by template-assisted electrochemistry and chemical vapor deposition (CVD) respectively. In both cases, NWFETs were fabricated by electron beam lithography (EBL) and crystallinity was confirmed by transmission electron microscopy (TEM) and selected area diffraction (SAD) patterns. For electrochemistry nanowire, ambipolar conduction was observed with strong p-type conduction, the effect of thermal annealing on the conductivity was analyzed, a NWFET model that took into consideration the underlapped region in top-gated NWFET was proposed. Hole mobility in the channel was calculated to be 292.84 cm2V-1s-1 with a density of 1.5×1017/cm3. For CVD nanowire, the diameter was below 40nm with an average of 20nm. Vapor-liquid-solid (VLS) process was speculated to be the mechanism responsible for nanowire growth. The efficient gate control was manifested by high ION/IOFF ratio which was on the order of 106 and a small inverse subthreshold slope (<200 mV/decade). Scale analysis was used to successfully account for disparities observed among a number of sample devices. N-type conduction was found in all NWFETs with electron mobility between 110 cm2*V-1*s-1 and 169 cm2*V-1*s-1.In the approach of SWCNT-PNA-SWCNT conjugate, we chemically functionalized single walled carbon nanotubes to synthesize the conjugate and characterized its electrical properties. Negative differential resistance (NDR) was observed consistently at different temperatures and the mechanism was explained through the energy band diagram in which NDR effect was caused by misalignment between Fermi energy level at the source and resonance states in the potential well. The consistent NDR effect shows possible application for microelectronic devices.
-------------------------------------

10138702_183 - 0.99998521757 - technology_and_computing
[dof, user, interference, network, helper, cognitive]

Degrees of Freedom of the Interference Channel with a Cognitive Helper
Abstract—In this letter, we characterize the degrees of freedom (DoF) of theK≥3user Gaussian interference network with a cognitive helper where each node is equipped with only one antenna. Specifically, each user sends one independent message to its corresponding receiver through its own antenna and via the help of the cognitive helper. For this network, we show that the sum DoF value is outer bounded by(K + 1)/2whenKis odd andK2/(2(K + 1))whenKis even, respectively. The new DoF outer bounds are derived based on the fact that collaboration among users does not decrease the capacity region and increasing the number of users does not increase the capacity per user. In addition, we provide a new achievable scheme to achieve a total of(K + 1)/2DoF for anyK≥3. Thus, the exact DoF value of the network is characterized with the total DoF given as(K + 1)/2, wheneverKis odd. The new achievable scheme is based on interference neutralization and asymptotic interference alignment.
-------------------------------------

10134137_183 - 0.99999950431 - technology_and_computing
[network, scheduler, multicore, application, server, connection, cache, performance, core, hrw]

Multicore Scheduling for Network Applications Based on Highest Random Weight
The widening spectrum of network applications incurs increasing stress on physical resources for both the network infrastructure and the web servers. Meanwhile, the emergence of faster Ethernet has shifted the bottleneck of network performance to the processing capability of the web servers. This trend has driven the prevalence of Chip Multiprocessors (CMP, a.k.a. multicore). However, even running on the state of the art multicore web servers, the network performance still falls short of expectations.	In this study, we optimize multicore scheduling in both the OS kernel and userspace for three legacy network applications, i.e. Deep Packet Inspection (DPI), multimedia transcoding and SPECweb2005. In the OS kernel, we propose an interrupt affinity based scheduler to prevent starvation by separating interrupt handlers from userspace application. In the userspace, we first parallelize the network application and then propose an affinity based scheduler that affinitizes all the packets in the same connection to the same core. However, this scheduler is oblivious of load balancing, which can offset the cache benefits. We therefore propose several hash based schedulers to strike a balance between connection locality and load balancing. While the baseline Highest Random Weight (HRW) hash balances workload at the connection level, our Adjusted HRW (AHRW) achieves packet level load balancing by comparison of runqueue length of each core. In addition, we enable cache awareness of AHRW by means of a communication matrix in Cache-Aware AHRW (CA-HRW), and propose a hierarchical version, H-CAHRW, for different core/cache topologies. To incorporate QoS concerns, we also develop a Proportional Share HRW scheduler, PS-HRW, by allocating cores to each connection based on connection buffer size. We implement and verify all of our schedulers using real application measurements.	With the resurgent interest in system virtualization, we present a performance characterization of a virtualized multicore server under consolidated network workloads and show that L2 cache misses are the major bottleneck. We therefore optimize the virtual CPU migration policy to take advantage of the cache topology. Then, we port all our schedulers developed in the native system to a virtualized multicore server, and observe minimum performance degradation.
-------------------------------------

10136110_183 - 0.998056918137 - technology_and_computing
[model, linear, parametric, uncertainty, frequency, unstructured, parameter]

Mixed structured and unstructured uncertainty modeling method with application to Linear Tape-Open drives
Starting from multiple frequency domain measurements, this paper presents a procedure to formulate a dynamic model of a servo actuator that consists of a nominal model and an allowable model perturbation in the form of a parametric and unstructured uncertainty. A separation between parametric and unstructured uncertainty is achieved by first estimating low order linear parameter models via frequency domain curve fitting followed by a linear Principle Component Analysis (PCA) to bound the parametric variations on the estimated parameters. Remaining differences between the low order parametric models and the measured frequency responses are captured by a bounded unstructured uncertainty on a frequency dependent dual- Youla parameter that uses prior information on a stabilizing feedback controller. The resulting perturbation model is written in a standard Linear Fractional Transformation (LFT) form and the procedure is applied to experimental data obtained from several mechanically equivalent servo actuators in a Linear Tape- Open (LTO) drive
-------------------------------------

10133120_183 - 0.999926286123 - technology_and_computing
[network, event, datum, relationship, cbp, algorithm, scalability, implementation, context, internet-scale]

Open implementation approach to Internet-scale context awareness
Proliferation of hardware and software sensors and our desire to determine relationships between the near-real- time data from multiple publishers motivates our introduction of Internet-scale context-awareness (ISCA). Content-based publish/subscribe (CBPS) seems the most natural substrate for ISCA because it provides the right separation of concerns, efficient event distribution, extensibility, and scalability. However, our evolving information environment is different from that for which CBPS was designed. Attempting to use the black-box style transparency afforded by CBPS precludes efficiently detecting data relationships for publication as context- aware events and leads to information glut and device saturation. We overcome these problems by recognizing that any component-based system is an ecology for which we can achieve global efficiencies by providing top-down and bottom-up context and collaboration. We extend CBPS with an open implementation approach to enable subscribers to inject domain-specific knowledge into the network in the form of first-class publish/subscribe agents. Agents are distributed algorithms that observe and transform data, dynamically manage bounding region filters, and exchange data only on an as-needed basis to eliminate useless event traffic at the sensor-edges of the network. Filtering at the network edge reduces bottlenecks in the network core to increase the scalability of the system. Content-based routing mechanisms are leveraged to allow the user to control where code is deployed, to develop complex relationship hierarchies, and to construct one-to-one conversations by leveraging existing network knowledge without flooding the network with either advertisements or subscriptions. We are programming the network. We add dynamic contextual message filtering and distributed memoization to minimize re-computation at downstream nodes. Combining open implementation, distributed processing, content-addressability, and distributed memoization satisfies the required increases in expressiveness, efficiency, and scalability necessary to achieve our Internet-scale context-awareness vision. Our algorithm detecting the proximity of mobile buddies reduced event traffic from O (events) to an expectation of about ln (movement) event-hops. Complex traffic-route monitoring used eight times fewer events than basic CBPS and reduced aggregation enhanced CBPS load imbalances by distributing relationship computations over the event- entry edge-brokers. Our algorithms are scalable with increased reporting rates because they measure data movement
-------------------------------------

10131821_183 - 0.813726696681 - technology_and_computing
[corridor, use, transit, land]

Retrofit of Urban Corridors: Land Use Policies and Design Guidelines for Transit-Friendly Environments
This focus of this research is the urban commercial corridor and the relationship between vehicle transportation routes and surrounding land use and development patterns. Due to their arbitrary and haphazard development, underutilization, and poor connections to the surrounding residential neighborhoods commercial corridors often represent transit unfriendly environments. It is, however, hypothesized that retrofit, reinvestment and intensification could enhance the transit potential of corridors. The study examines three case study corridors in Los Angeles and investigates land use and policy frameworks, zoning regulations, and design guidelines that can better support existing or future transit.
-------------------------------------

10135783_183 - 0.993550718144 - technology_and_computing
[building, system, cbe, ufad]

Technical Report on California State Teachers Retirement System Building:  UFAD Performance and Blinds Study
For the week of July 19-23, 2010 members from CBE traveled to the California State Teachers Retirement System (CalSTRS) building in Sacramento with two main goals in mind: (1) help validate the CBE UFAD design tool with measured building data and (2) evaluate the performance of the CalSTRS UFAD system during different blinds setting scenarios (open, horizontal, and closed).  During certain times of year, CalSTRS building engineers have had trouble meeting setpoints because of solar loading.  Two spaces were monitored: (1) a conference room on the 3rd floor and (2) an open plan space on the 11th floor.
      This project was funded by CBE and the Public Interest Energy Research program.
-------------------------------------

10134002_183 - 0.999998638592 - technology_and_computing
[lidar, vehicle, system, sensor, tracking, approach, camera, different, object, fusion]

LIDAR, Camera and Inertial Sensors Based Navigation Techniques for Advanced Intelligent Transportation System Applications
During the past decade, numerous research has been carried out in in-vehicle navigation and positioning. All the approaches are trying to solve two problems: "where am I?", and "where are they?", i.e., the automatic vehicle positioning, and the surrounding vehicle detection and tracking.     Among a variety of sensor based systems, computer vision-based approaches have been one of the most popular and promising techniques, however it suffers from intensity variations, narrow fields of view, and low-accuracy depth information. Laser Ranging Sensor (LIDAR) is another attractive technology due to its high accuracy in ranging, its wide-area view, and low data-processing requirements. However, a major challenge for LIDAR-based systems is that their reliability depends on the distance and reflectivity of different objects. Moreover, LIDAR often suffers from noise issues, making it difficult to distinguish between different kinds of objects. In this dissertation, we address several fundamental problems in integrating LIDAR and camera systems for better navigation and positioning solutions.  As part of the research, we present a sensor fusion system to solve the "where are they" problem. The calibration of the sensor fusion system as well as the vehicle detection and tracking algorithms are proposed to determine the states of surrounding vehicles. The "where am I" solution focuses on the integration of LIDAR and inertial sensors for advanced vehicle positioning. Moreover, a vehicle tracking approach is presented for freeway traffic surveillance system.     Sensor fusion techniques have been used for years to combine sensory data from disparate sources. In this dissertation, a tightly coupled LIDAR/CV integrated system is introduced. LIDAR and camera calibration is the key component of sensor fusion system. A unique multi-planar LIDAR and computer vision calibration algorithm has been developed, which requires that the camera and LIDAR observe a planar pattern at different positions and orientations. Geometric constraints of the different 'views' of the LIDAR and camera images are resolved as the coordinate transformation and rotation coefficients.       The proposed sensor fusion system is utilized for mobile platform based vehicle detection and tracking. The LIDAR sensor estimates possible vehicle positions. Different Regions of Interest (ROIs) in the imagery are defined based on the LIDAR object hypotheses. An Adaboost object classifier is then utilized to detect the vehicle in ROIs. Finally, the vehicle's position and dimensions are derived from both the LIDAR and image data. Experimental results are presented to illustrate that this LIDAR/CV system is reliable.     In addition, an autonomous positioning solution for urban environment is provided in this dissertation. The positioning solution is derived by combining measurements from both LIDAR and inertial sensors, i.e., LIDAR, gyros and accelerometers. The inertial sensors provide the angular velocities as well as the accelerations of the vehicle, while LIDAR detects the landmark structures (posts and surfaces). In our implementation the positioning is performed in known environment, i.e., the map information is assumed to be a priori information. Extended Kalman Filter (EKF) is implemented in the positioning estimation.      This dissertation also presents a vehicle tracking approach used in a traffic surveillance system. One of the key challenges with freeway vehicle tracking is dealing with high density traffic, where occlusion often leads to foreground splitting and merging errors. We propose a real-time multi-vehicle tracking approach, which combines both local feature tracking and a global color probability model. In our approach, the corner features are tracked to provide position estimates of moving objects. Then a color probability is calculated in the occluded area to determine which object each pixel belongs to. This approach has been proved to be scalable to both stationary surveillance video and moving camera video.
-------------------------------------

10175413_189 - 0.994350294201 - technology_and_computing
[water, model, system, chapter, grain, current, technique, coarse]

Towards Adaptive Resolution Modeling of Biomolecular Systems in their Environment
Water plays a critical role in the function and structure of biological systems. Current techniques to study biologically relevant events that span many length and time scales are limited by the prohibitive computational cost of including accurate effects from the aqueous environment. The aim of this work is to expand the reach of current molecular dynamics techniques by reducing the computational cost for achieving an accurate description of water and its effects on biomolecular systems.
This work builds from the assumption that the “local” effect of water (e.g. the local orientational preferences and hydrogen bonding) can be effectively modelled considering only the atomistic detail in a very limited region. A recent adaptive resolution simulation technique (AdResS) has been developed to practically apply this idea; in this work it will be extended to systems of simple hydrophobic solutes to
determine a characteristic length for which thermodynamic, structural, and dynamic properties are preserved near the solute. This characteristic length can then be used for simulation of biomolecular systems, specifically those involving protein dynamics in water. Before this can be done, current coarse grain models must be adapted to couple with a coarse grain model of water.
This thesis is organized in to five chapters. The first will give an overview of water, and the current methodologies used to simulate water in biological systems. The second chapter will describe the AdResS technique and its application to simple test systems. The third chapter will show that this method can be used to accurately describe hydrophobic solutes in water. The fourth chapter describes the use of coarse grain models as a starting point for targeted search with all-atom models. The final chapter will describe attempts to couple a coarse grain model of a protein with a single-site model for water, and it’s implications for future multi-resolution studies.
-------------------------------------

10137758_183 - 0.999990895733 - technology_and_computing
[pattern, network, parallel, algorithm, computation]

Network-Theoretic Classification of Parallel Computation Patterns
Parallel computation in a high-performance computing environment can be characterized by the distributed memory access patterns of the underlying algorithm. During execution, networks of compute nodes exchange messages that indirectly exhibit these access patterns. Identifying the algorithm underlying these observable messages is the problem of latent class analysis over information flows in a computational network. Towards this end, our work applies methods from graph and network theory to classify parallel computations solely from network communication patterns. Pattern classification has applications to several areas including anomaly detection, performance analysis, and automated algorithm replacement. We discuss the difficulties encountered by previous efforts, introduce two new approximate matching techniques, and compare these approaches using massive datasets collected at Lawrence Berkeley National Laboratory.
-------------------------------------

10132642_183 - 0.792524706306 - technology_and_computing
[land, use, human, spatial, metropolitan]

Modeling Land Use Change in the Boston Metropolitan Region (Massachusetts)
Land use patterns and its change lie at the heart of modern theories of urban spatial structure, as land use deals essentially with the spatial aspects of all human activities on land and the way in which the earth's surface is adapted, or could be adapted, to serve human needs. People's print on land is most evident in metropolitan regions, which, in the US case, now house 75% of the whole population with only 1.5% of the country's land. In terms of land use, these are certainly the most complicated areas on earth, as the surface of this small but critical part of the earth is almost completely formed by human activities. As physical expression of human relationships with land, the spatial juxtaposition of different land uses also tells much about the relationships between people. The inquiry into the characteristics of the internal organization of metropolitan land use, however, is not merely an academic matter, observed more than half a decade ago by Homer Hoyt (1939). In the current context of metropolitan growth, the intense policy debates over zoning, growth management, infrastructure investment, environmental preservation, social justice and life quality improvement all depend upon the forces governing the spatial interrelationship of different types of areas and the past and prospective movements of different types of uses.
-------------------------------------

10129768_178 - 0.999577253107 - technology_and_computing
[emitter, system, reference, stability, bibliographical, thermal, selective, thermophotovoltaic, nano-structured]

Thermal stability of nano-structured selective emitters for thermophotovoltaic systems
Includes bibliographical references (p. 97-103).
-------------------------------------

10137284_183 - 0.999430363791 - technology_and_computing
[cmbf, stereo, aperture, prototype, dual, pair, image, imaging, system, filter]

Development of a single lens dual-aperture stereo imaging for  an application in stereo endoscope
Stereopsis is the impression of depth perceived from seeing two disparate images with our two eyes.  It is the most potent cue in personal space where our motor functions are often involved in dealing with an object.  Thus, stereo imaging makes the most profound impact on depth perception when it feeds a human with a view of personal space for remote operations such as minimally invasive surgery (MIS).  For successful implementation of stereo imaging in MIS, an imaging device or stereo endoscope requires two factors: (1) The two viewpoints be contained in an area corresponding to a diameter less than a few millimeters, which minimizes the surgical opening size necessary; and (2) high definition imaging for adequate visualization of the operation space.In this dissertation, a single lens having a dual aperture is demonstrated to fulfill the requirements of stereo endoscopy for MIS better than a typical dual lens system.  However, the left and right perspective images generated by the dual aperture overlap on the image plane unless a mechanism is incorporated to switch between the two apertures.  A novel method based on complementary multi-band bandpass filters (CMBFs) is presented here to address this key issue for the miniaturization of a dual-aperture imaging device.  The goal of this dissertation is to investigate if a single lens/dual aperture system employing CMBFs can be scaled down for use in a stereo endoscope suitable for MIS.  This goal is addressed with experimental studies as well as construction of prototype devices.The motivation for using the dual aperture scheme and CMBFs is presented.  The key role of stereopsis in human visuomotor coordination is discussed as well as how stereo imaging can improve visuomotor tasks for successful MISs.  The requirements of a stereo endoscope for MIS are reviewed in the context of past work, and an argument is made that a prototype based on the dual aperture and CMBF can best meet these requirements.The development of a prototype based on the dual aperture and CMBFs is described and an experiment to measure the stereo depth effect (SDE) of the prototype is presented.  A calculation predicted and an experiment confirmed that the SDE of the 3-mm diameter prototype is 4/7 of that for a hypothetical dual lens system of the same diameter.  Even at this reduced SDE, the prototype provides a SDE suitable for skull-base MIS, which is characterized by a viewing range of 1 to 2 cm.  Additional simulation results of the dual aperture prototype including the resolution, the F#, the magnification, and the focal distance also are presented.  It is made evident that a greater SDE could be enabled through a custom lens design.The concept of CMBFs is described in detail.  The transmission bands of each filter has the shape of a comb, which enables the two bandpass filters of a CMBF pair to be interwoven and which, when a filter is illuminated with matching spectral light bands, makes that filter open to that light while making the other filter closed to the same light.  The experiment shows the contrast between the open/close states of a CMBF pair as high as 28000:1.  In addition, because each CMBF in a pair passes a different half of the visible spectrum, another experiment is conducted to show capacity to render an RGB color image from a CMBF/dual aperture system.  However, these image pairs are composed of different color spectra due to the complementary nature of the CMBFs.  This is a concern because the difference can ultimately lead to color rivalry, in which the two different color images of a pair cannot be fused by the human visual perception system.  A simulation is built to show that each of the CMBFs in a pair should have 4 or greater transmission bands to yield an indiscernible color difference.The following changes to the prototype are recommended to improve the SDE and resolution of the CMBF/dual aperture system and to decrease the color difference of image pairs: (1) Design a custom lens system and detector array, (2) Develop complementary quadruple bandpass CMBFs, and (3) Consider using a dichroic mirror to avoid the need to match the light source to the CMBF.
-------------------------------------

10136412_183 - 0.749668339582 - technology_and_computing
[profile, design, engine, apex, method, seal, rotary, df]

New Rotary Engine Designs by Deviation Function Method
Conventional rotary engine designs are based on an epitrochoidal housing bore that is found by the path of the point at the rotor profile's apex. To seal the engine, the rotor apexes are replaced by spring-loaded apex seals that slide along the housing bore during rotation. The conventional designs are limited to the point-based epitrochoid housing profiles and cannot incorporate the profile of the apex seal. This dissertation presents the complete theory and algorithm of the deviation function (DF) method of rotary engine design. This method is based on conjugate pair design and generates new engine profiles from generating curves. The DF method of rotary engine design by apex seal profile is introduced and developed for generating new profile designs in which the housing profile conforms to the apex seal profile, for better sealing. The DF method of design by geometric parameters is developed to select profiles using the standard rotary engine geometry. Maximum theoretical compression ratio and swept area are two criteria that have a range of possible DF-designed profile solutions. For the apex seal design and selection process, a sealing index is defined and a multi-apex-sealing grid is developed to further improve apex sealing. The DF method of rotary engine design is extended to noncircular pitch curves, for generating more new profiles that incorporate a variable speed ratio between the rotor and main shaft. By using the DF method, a larger variety of engine profiles is available to meet multiple design criteria and allow more flexibility in the design process. Some example deviation functions are provided for process illustration and design development. Engine profile designs and methods using circular pitch curves are developed using both arc-based and nonarc-based apex seal profiles. Engine profile designs with noncircular pitch curves are developed using the arc-based seal profile.
-------------------------------------

10133422_183 - 0.746259171909 - technology_and_computing
[godai, traffic, host, management, datum, approach, enterprise, classification, end, hierarchical]

Challenges in Security and Traffic Management in Enterprise Networks
Management of enterprise networks is a challenging problem because of their continued growth in  size and functionality. We propose and evaluate a framework, <italic> Godai </italic>, which addresses the challenges in (i) setting thresholds in end host anomaly detectors,(ii) hierarchical summarization in data and (ii) application traffic classification. Godai enables IT operators to identify the end hosts that have been enslaved by an attacker to launch attacks and <italic> Godai </italic> achieves it by diversifying anomaly detector configuration. The general policies in the framework are holistic and achieve two goals: (a)balance the trade-offs between false alarm and mis-detection rates and (b) show that the benefits of full diversity can be attained at reduced complexity, by clustering the end hosts and treating a cluster homogeneously.The underlying principle of attack detection is to identify changes in data. <italic> Godai </italic>  generalizes the concept for data with hierarchical identifiers, e.g., IP prefixes, URLs. A parsimonious hierarchical summarization eases the burden on IT operators to interprete analysis reports. <italic> Godai </italic> proposes efficient and provable algorithms to produce parsimonious explanations from the output of any statistical model that provides predictions and confidence intervals, making it widely applicable. Finally, <italic> Godai </italic> takes a step towards associating applications to traffic flows. It critically re-visits the existing ad hoc techniques of traffic classification approaches based on transport  layer ports, host behavior and flow features and analyzes the effectiveness of different approaches. The results allow us to answer questions about the best available traffic classification approach, the conditions under which it performs well, and the strengths and limitations of each approach. The multifarious functionalities allow <italic> Godai </italic>  to be a viable solution in enterprise network management.
-------------------------------------

10136442_183 - 0.999618098283 - technology_and_computing
[spicec, parallel, parallelism, datum, memory, programming, thread, computation, system]

The SpiceC Parallel Programming System
As parallel systems become ubiquitous, exploiting parallelism becomes crucial for improving application performance. However, the complexities of developing parallel software are major challenges. Shared memory parallel programming models, such as OpenMP and Thread Building Blocks (TBBs), offer a single view of the memory thereby making parallel programming easier. However, theysupport limited forms of parallelism. Distributed memory programming models,such as the Message Passing Interface (MPI), support more parallelism types; however, their low level interfaces require great deal of programming effort.This dissertation presents the SpiceC system that simplifies the task of parallel programming while supporting different forms of parallelism and parallel computing platforms. SpiceC provides easy to use directives to express different forms of parallelism, including DOALL, DOACROSS, and pipelining parallelism. SpiceC is based upon an intuitive computation model in which each thread performs its computation in isolation from other threads using its private space and communicates with other threads via the shared space. Since, all data transfers between shared and private spaces are explicit, SpiceC naturally supports both shared and distributed memory platforms with ease.  SpiceC is designed to handle the complexities of real world applications.The effectiveness of SpiceC is demonstrated both in terms of delivered performance and the ease of parallelization for applications with the following characteristics.Applications that cannot be statically parallelized due to presence of dependences, often contain large amounts of input dependent and dynamic data level parallelism. SpiceC supports speculative parallelizationfor exploiting dynamic parallelism with minimal programming effort. Applications that operate on large data sets often make extensive use of pointer-based dynamic data structures. SpiceC provides support for partitioning dynamic data structures across threads and then distributing the computation among the threads in a partition sensitive fashion. Finally, due to large input sizes, many applications repeatedly performI/O operations that are interspersed with the computation. While traditionalapproach is to execute loops contain I/O operations serially, SpiceC introduces support for parallelizing computations in the presence of I/O operations. Finally, this dissertation demonstrates that SpiceC can handle the challengesposed by the memory architectures of modern parallel computing platforms.The memory architecture impacts the manner in which data transfers betweenprivate and shared spaces are implemented. SpiceC does not place thethe burden of data transfers on the programmer. Therefore portability of SpiceCto different platforms is achieved by simply modifying the handling of data transfers by the SpiceC compiler and runtime. First, it is shown how SpiceC can be targeted to shared memory architectures both with and without hardware support for cache coherence. Next it is shown how accelerators such as GPUs present in heterogeneous systems are exploited by SpiceC. Finally, the ability of SpiceC to exploit the scalability of a distributed-memory system, consisting of a cluster of multicore machines, is demonstrated.
-------------------------------------

10136805_183 - 0.999031696551 - technology_and_computing
[noise, image, datum, algorithm, outlier, gaussian, signal, pursuit, adaptive, non-gaussian]

Image and Signal Processing with Non-Gaussian Noise: EM-Type Algorithms and Adaptive Outlier Pursuit
Most of the studies of noise-induced phenomena assume that the noise source is Gaussian because of the possibility of obtaining some analytical results when working with Gaussian noises. The use of non-Gaussian noises is rare, mainly because of the difficulties in handling them. However, there is experimental evidence indicating that in many phenomena, the noise sources could be non-Gaussian, for example Poisson data and sparsely corrupted data. This thesis provides two classes of algorithms for dealing with some special types of non-Gaussian noise.Obtaining high quality images is very important in many areas of applied sciences, and the first part of this thesis is on expectation maximization (EM)-Type algorithms for image reconstruction with Poisson noise and weighted Gaussian noise. In these two chapters, we proposed general robust expectation maximization (EM)-Type algorithms for image reconstruction when the measured data is corrupted by Poisson noise and weighted Gaussian noise, without and with background emission. This method is separated into two steps: EM step and regularization step. In order to overcome the contrast reduction introduced by some regularizations, we suggested EM-Type algorithms with Bregman iteration by applying a sequence of modified EM-Type algorithms. One algorithm with total variation being the regularization is used for image reconstruction in computed tomography application.The second part of this thesis is on adaptive outlier pursuit method for sparsely corrupted data. In many real world applications, there are all kinds of errors in the measurements during data acquisition and transmission. Some errors will damage the data seriously and make the obtained data containing no information about the true signal, for example, sign flips in measurements for 1-bit compressive sensing and impulse noise in images. Adaptive outlier pursuit is used to detect the outlier and reconstruct the image or signal by iteratively reconstructing the image or signal and adaptively pursuing the outlier. Adaptive outlier pursuit method is used for robust 1-bit compressive sensing and impulse noise removal in chapters 4 and 5 respectively.
-------------------------------------

10136287_183 - 0.999969470581 - technology_and_computing
[layer, window, cuin, device, film, metal, performance, solution-processed, thin, chapter]

Fully Solution-Processed Copper Chalcopyrite Thin Film Solar Cells: Materials Chemistry, Processing, and Device Physics
Chalcopyrite solar cells have attracted a lot of attention due to their highest power conversion efficiency among all thin film solar cells. However, significant cost reductions as well as large scale production are necessary to compete with conventional electrical power generation. The development of new deposition technologies for the absorber layer as well as the conducting window layer that are compatible with atmospheric deposition on a manufacturing-scale are urgently required to significantly offset production costs. This dissertation demonstrates the development of fully solution-processed high performance CuIn(Se,S)2 photovoltaic devices based on a hydrazine processed absorber layer and metal nanowire composite window layer. Furthermore, the included studies present a deep understanding of the materials chemistry involved in the formation of the CuIn(Se,S)2 precursor molecules and thin films, as well as material design for metal nanowire composite window layers, and the charge transport mechanism in the fully solution-processed high performance CuIn(Se,S)2 photovoltaic devices. Chapter 2 presents the identification of the molecular precursor species present in hydrazine CuIn(Se,S)2 solutions, and precise control of energy band gap of CuIn(Se,S)2 by tailoring the bonding environment of the molecular species present in precursor solutions. Chapter 3 investigates secondary phase formation at the Mo/CuIn(Se,S)2 interface as well as a strategy for achieving a large grained CuIn(Se,S)2 film structure with demonstrated photovoltaic device performance using a sputtered metal oxide window layer. Chapter 4 focuses on the development of transparent conductors composed of solution processed silver nanowires composite window layers demonstrating better optoelectronic and mechanical properties than conventionally sputtered indium tin oxide films. Chapter 5 centers on the complete replacement of sputtered metal oxides by metal nanowires embedded in conductive nanoparticle window layer without any sacrifices in device performance, elucidate the role of each component of the window layers by probing spatially resolved carrier collection, and presents a detailed study of band alignment in fully solution-processed high performance CuIn(Se,S)2 photovoltaic devices by investigating current-voltage characteristics in the dark and under illumination from several controlled wavelength ranges. Thin film chalcopyrite solar cells employing solution-processed absorber layers combined with metal nanowire-metal oxide nanoparticle composite window layers are anticipated to effectively serve as a renewable energy source with reduced fabrication costs and competitive device performance.
-------------------------------------

10134615_183 - 0.97645068024 - technology_and_computing
[spad, photon, single, detector, process, avalanche, high, device, diode, cmos]

Development of shallow trench isolation bounded single- photon avalanche detectors for acousto-optic signal enhancement and frequency up-conversion
This dissertation describes the development of the first CMOS single photon avalanche diode (SPAD) fabricated in a deep-submicron commercial CMOS process. Single photon detection is often necessary for high-sensitivity, high dynamic range time-resolved optical measurements in diverse applications in medicine, biology, military, and optical communication links. Single photon avalanche diode (SPAD) detectors have become the device of choice and have made strong gains in recent years. They are unique in that they provide digital information of the arrival of an individual photon impinging on the detector, thus being a very powerful tool when time-of-arrival information and timing resolution are crucial. Timing precision of the detector will improve contrast in fluorescence lifetime imaging and resolution in laser ranging applications. Traditionally, single photon detectors have been fabricated using custom processes because of the conditions under which the devices operate under. Because of the need to sustain high currents and high electric- fields, special custom fabrication processes have been developed. These fabrication processes have great benefits such as low-noise, high detection efficiencies, low jitter, and tailored spectral responses towards longer wavelengths. However, these fabrication techniques are often undesirable due to increased capacitance from off- chip quenching, recharging, and processing circuitry, resulting in longer detector dead times and slower sampling rates. Furthermore, large-scale production and scalability to arrays is impractical. There has been a trend towards using Complementary Metal-Oxide- Semiconductor (CMOS) technology for constructing SPAD pixels and arrays with integrated circuitry to overcome these limitations. Though commercial CMOS technologies are by nature, generic, and are not designed for SPAD devices, they can still offer considerable advantages in certain areas where custom processes lack. This dissertation describes the modeling, simulation, and full characterization of a CMOS STI-bounded Single Photon Avalanche Diode (SPAD). State-of-the-art sampling rates, dead-time, and jitter performance are characterized, and the device is compared to traditional diffused-guard ring structures for solid-state SPADs. Further, novel applications of the CMOS SPAD for acousto-optic signal enhancement and frequency up-conversion of 1550nm are described. An optical scatter system for acoustic characterization of ultrasound responsive microbubbles and particles is designed and developed. Further, a novel method of fluorescence modulation using dye-loaded microbubbles is demonstrated
-------------------------------------

10175373_189 - 0.813755094753 - technology_and_computing
[center, datum, rural, building, architectural]

Rural Datascapes:  A Data Farm Network for Rural North Dakota
This thesis attempts to render architectural agency and aesthetics within the typological discussion of the data center in the rural American landscape.  The disciplinary question of the role of architecture and aesthetics in data center design is related to earlier examples of factories and warehouses during modernity.  The data center alters the traditional representative role of architecture; they are massive, horizontal buildings that are only conceivable from an aerial perspective, driven by logistics and efficiency.  This thesis engages these issues by focusing on the point at which the architectural and programmatic problems of the data center converge, the building form and envelope.  This thesis engages the building envelope as an expanded surface that considers not only logistical and environmental issues, but also engages the social and political architectural questions related to the identity of the data center in the rural landscape.
-------------------------------------

10133509_183 - 0.999998041907 - technology_and_computing
[algorithm, network, model, inference, social, ctbn, time, sampling, approximate, estimation]

Continuous Time Bayesian Network Approximate Inference and Social Network Applications
Many real world systems evolve asynchronously in continuous time, for examplecomputer networks, sensor networks, mobile robots, and cellular metabolisms.Continuous time Bayesian Networks (CTBNs) model such stochastic systems incontinuous time using graphs to represent conditional independencies amongdiscrete-valued processes. Exact inference in a CTBN is often intractable as thestate space of the dynamic system grows exponentially with the number ofvariables.In this dissertation, we first focus on approximate inference in CTBNs. Wepresent an approximate inference algorithm based on importance sampling. Unlikeother approximate inference algorithms for CTBNs, our importance samplingalgorithm does not depend on complex computations, since our sampling procedureonly requires sampling from regular exponential distributions which can be donein constant time. We then extend it to continuous-time particle filtering andsmoothing algorithms. We also develop a Metropolis-Hasting algorithm for CTBNsusing importance sampling. These algorithms can estimate the expectation of anyfunction of a trajectory, conditioned on any evidence set containing the valuesof subsets of the variables over subsets of the time line.We then apply our approximate inference algorithms to learning social networkdynamics. Existing sociology models for social network dynamics require directobservation of the social networks. Furthermore, existing parameter estimationtechnique for these models uses forward sampling without considering the givenobservations, which affects the estimation accuracy. In this dissertation, wedemonstrate that these models can be viewed as CTBNs. Our sampling-basedapproximate inference method for CTBNs can be used as the basis of anexpectation-maximization procedure that achieves better accuracy in estimatingthe parameters of the model than the standard learning algorithm from thesociology literature. We extend the existing social network models to allow forindirect and asynchronous observations of the links. A Markov chain Monte Carlosampling algorithm for this new model permits estimation and inference.Experiments on both synthetic data and real social network data show that ourapproach achieves higher estimation accuracy, and can be applied to varioustypes of social data.
-------------------------------------

10133703_183 - 0.999910102071 - technology_and_computing
[design, mpsoc, methodology, simulation, software, accurate, implementation, information, power]

MPSoC Simulation and Implementation of KPN Applications
Design of Multiprocessor System-on-a-Chips (MPSoC) currently suffers from poor tool support.  MPSoC is considered to be the next general design platform for embedded system designs. As complex designs such as multimedia and gaming processing become more common in handheld devices and traditional ASIC solutions are too slow and too expensive, MPSoC allows a fast software solution by running multiple low-cost, low-speed, low-power embedded processors in parallel and combining their processing power to solve more complex computation problems.  However, current design methodologies for MPSoC generally restrict the specification of the software for the convenience that it can be analyzed statically.  Such restrictions prevent MPSoC designs to reach their full potentials.In this thesis, I propose an MPSoC design methodology that does not impose unnecessary restrictions on the software.  Specifically, \emph{Kahn Process Network} (KPN) is used to model the applications such that each process in the KPN process networks can be expressed by the full power of high-level programming languages.  Unfortunately, allowing the full power of high-level programming languages prevents the software to be analyzed statically.  Therefore, similar to optimizing software for single-processor systems, a profile-based methodology is proposed to explore the vast design space of MPSoC for applications written in KPN.There are two main ingredients in the methodology.  1. The MPSoC simulation must be made both fast and accurate.  The speed of the simulation must allow designers to modify and experiment different design options in the limited design time allocated for system-level design exploration.  At the same time the simulation must be accurate enough for the exploration results to be meaningful.  A new MPSoC simulation framework that simulates in the speed close to behavioral simulation and generates performance results with less than 5\% error is shown.  2. An analysis from the simulation must provide accurate MPSoC-specific profiling information about the implementation for guiding the designers to make design decisions.  Execution characteristics of MPSoC make such profiling information very different from single-processor systems.  A new profiling technique specifically to determine performance-critical information for MPSoC is described.Three optimization techniques at various implementation levels that use the proposed methodology are shown and applied to an MPEG-2 Decoder design.  The experiments show that the optimization techniques using the methodology can efficiently optimize the implementations in term of performance, power and area.  The results show that the methodology allows designers to explore the MPSoC design space more efficiently with the accurate MPSoC profiling information.
-------------------------------------

10132225_183 - 0.999991370046 - technology_and_computing
[system, network, transportation, information, object-oriented, representation, travel]

Information Representation for Driver Decision Support Systems
The successful development of Intelligent Transportation Systems (ITS) depends on the capability of incorporating a vast amount of information about the location of facilities which generate travel as well as a realistic representation of elements of the transportation network in which travel occurs. An integral part of this system is an Advanced Traveler Information System (ATIS). Such a system can be based on an innovative and comprehensive Geographic Information System (GIS). Whereas current ITS primarily use simplified transportation networks as their basis, using an object-oriented GIS allows us to provide a more realistic representation of elements of the network and the ways that people perceive them. We can represent the network by defining roads or street hierarchies and by storing environmental data as layers which can be overlain, aggregated, or decomposed at will. Storing the transportation network as a hierarchy facilitates the calculation of different paths through the network and allows the introduction of different path selection criteria. A long-run aim of ITS is to develop a real time multi-strategy travel decision support system over a multi-modal network. We examine the advantages of an object-oriented system over the link-node system in pursing such a goal. We also identify, the shortcomings of link-node technology that are overcome by using an object-oriented data model. And finally, we discuss some of the theoretical and applied implications of our suggestions.
-------------------------------------

10132868_183 - 0.999993416195 - technology_and_computing
[design, vehicle, system, simulation, transit, scheme, hcppt, real-time, research]

High Coverage Point to Point Transit (HCPPT): A New Design Concept and Simulation-Evaluation of Operational Schemes
This dissertation research proposes the development and evaluation of a new concept for high coverage point-to-point transit systems (HCPPT). Overall, three major contributions can be identified as the core of this research: the proposed scheme design, the development of sophisticated routing rules that can be updated in real-time to implement and optimize the operation of such a design, and the implementation of a multi-purpose simulation platform in order to simulate and evaluate such a design under real network conditions.
      The design is based on Shuttle-style operations with a large number of deployed vehicles under a coordinated transit system that uses advanced information supply schemes with fast routing and optimization schemes. The system design is rather innovative and ensures that no more than one transfer is needed for the travelers, by using transfer hubs as well as reroutable and non-reroutable portions in the vehicles’ travel plans. It yields flexibility for demand-side benefits from options such as price incentives for time-bound “passenger-pooling” at the stops without destination constraints, by the users.
      A strict optimization formulation and solution for such a problem is computationally prohibitive in real-time. The design proposed in this dissertation is effectively geared towards a decomposed solution using detailed rules for achieving vehicle selection and route planning. If real-time update of probabilities based upon modeling the future dispatch decisions is included, then this scheme can be considered as a form of quasi-optimal predictive-adaptive control problem.
      Finally, a multi-purpose simulation platform is developed as part of this research in order to evaluate the performance of the system. The final simulations of HCPPT required point-to-point vehicle simulation, which is not possible with off-the-shelf simulators. The simulation framework uses a well-known microscopic traffic simulator that was significantly modified for demand-responsive vehicle movements and passenger tracking. A simulated case study in Orange County showed that with enough deployed vehicles, the system can be substantially better, even competitive with personal auto travel, compared to the often-unsuccessful traditional DRT systems and the existing fixed route public transit. Furthermore, HCPPT can be incrementally implemented by contracting out services to existing private operators.
-------------------------------------

10134841_183 - 0.999732546633 - technology_and_computing
[navigation, dissertation, ins, inertial, error, problem, localization, estimation, systems, sensor]

High Accuracy Sensor Aided Inertial Navigation Systems
Reliable and high accuracy (decimeter level) localization of a rover relative to a defined frame is an enabling technology for numerous Intelligent Transportation Systems (ITS) applications (e.g., automotive guidance, routing, lane departure warning). The goal of localization is to compute the navigation state of the rover in some defined frame of reference such that the expected errors in the estimate is within a given performance specification. Inertial navigation is a popular navigation technique since it provides full six Degree-Of-Freedom (DOF) navigation information. Further inertial sensors have been studied for decades and have well understood error models. This dissertation discusses the theoretical and implementation aspects of certain sensor aided Inertial Navigation Systems (INS). Though the presentation can be easily generalized to all forms of INS, the primary focus of this dissertation will be on automotive INS. This dissertation formulates the localization problem in a mathematically rigorous fashion and poses it as a nonlinear Bayesian estimation problem. The INS kinematic equations and linearized error state equations required by the Bayesian estimation solution are derived. Aiding techniques like  GPS, Vision and stationary aiding are described and mathematically formulated. Observability and performance analysis are presented for each of these aiding scenarios. The last part of the dissertation defines and formulates the Near Real Time estimation problem.
-------------------------------------

10131706_183 - 0.961713276668 - technology_and_computing
[spatial, ability]

Do People Understand Spatial Concepts: The Case of First-Order Primitives
The purpose of this paper is to examine whether people in general understand elementary spatial concepts, and to examine whether or not naive spatial knowledge includes the ability to understand important spatial primitives that are built into geographic theory, spatial databases and geographic information systems (GIS). The extent of such understanding is a partial measure of spatial ability. Accurate indicators or measures of spatial ability can be used to explain different types of spatial behavior. In this paper I first examine the relation between spatial ability and spatial behavior, then present experimental evidence of the ability of people to understand spatial concepts such as nearest neighbors (proximity), and spatial distributions. A final commentary is made about the possible difference between "common sense" and "expert" spatial knowledge, and the implications of such results for the comprehension of space at all scales.
-------------------------------------

10133765_183 - 0.998812919283 - technology_and_computing
[motion, image, hamiltonian, space, analysis, energy, hmc, recognition, video, framework]

A Physics-Based, Neurobiologically-Inspired Stochastic Framework for Activity Recognition
We present a multi-disciplinary framework for motion modeling and recognition in machine vision. Building upon the neurobiological model of motion recognition, we propose computational equivalents for the Motion Energy and Form Pathways. We derive the Hamiltonian Energy Signature (HES) from first-principles in physics as the basis for the Motion Energy Pathway. The Form Pathway is modeled using existing low-level feature descriptors based on shape, appearance, and gradients. We propose an Integration methodology to combine both pathways using a variant of the Feature Integration and Biased Competition neurobiological models, which we implement via statistical hypothesis testing using the bootstrap. We also show the extensibility of our physics-based approach by proposing a physically-significant, compact representation for the gait of a person called the Gait Action Image (GAI), which is based on core physics principles employed in the HES formulation. We then show the generalizability of our neurobiologically-inspired integration framework by casting the GAI within this infrastructure. Since Motion and image analysis are both important for activity recognition in video, we also develop a new approach that extends the Hamiltonian Monte Carlo (HMC) to allow us to simultaneously search over the combined motion and image space in a concerted manner using well-known Markov Chain Monte Carlo (MCMC) techniques. For motion analysis in video, we use tracks generated from the video to calculate the Hamiltonian equations of motion for the systems under study, thus utilizing analytical Hamiltonian dynamics to derive a physically significant HMC algorithm which can be used for activity analysis. We then use image analysis to help explore both the motion energy space and the image space by integrating the Hamiltonian energy-based approach with an image-based data-driven proposal to drive the HMC, thereby yielding a Data Driven HMC (DDHMC). We reduce the enormity of the search space by driving the Hamiltonian dynamics-based MCMC with image data in this DDHMC. We also develop the reverse algorithm, which uses motion energy proposals to search the image space. While HMC has been used in other contexts, this is possibly the first work that shows how it can be used for activity recognition in video, taking into account the image analysis results and using the physical motion information of the system. In addition, the DDHMC framework has potential application to other domains where statistical sampling techniques are useful, as we outline in the section on future work.Experimental validation of the theory is provided on the well-known KTH, Weizmann, and USF Gait datasets with very promising results.
-------------------------------------

10135134_183 - 0.999999229089 - technology_and_computing
[network, datum, tool, center]

Visualization software for data center switch network
Today's data centers may contain tens of thousands computers. Manually monitoring these networks is time consuming, so there is an emerging need to automate these tasks by implementing network management systems. It is important for network engineers to have a tool enabling them to monitor the network to spot problems before users are affected. This need is also critical during the network design since it will make the network debugging easier. A well structured management software will also enables the designers to demonstrate the network behavior before development. One important component of most network management tools is visualization tool. Data centers' network architecture typically consists of a tree of routing and switching elements with more specialized and expensive equipment moving up the network hierarchy. When deploying the highest-end IP switches/routers, resulting topologies may only support 50% of the aggregate bandwidth available at the edge of the network, while still incurring tremendous cost. Non uniform bandwidth among data center nodes complicates application design and limits overall system performance. UCSD data center switch research group has proposed a new technique on how to leverage largely commodity Ethernet switches to support the full aggregate bandwidth of clusters consisting of tens of thousands of elements. The group argues that appropriately architected and interconnected commodity switches will deliver more performance at less cost than higher-end solutions. In this project a visualization tool was developed for the proposed network. This tool uses data captured from the network and use graphic techniques to visualize the data
-------------------------------------

10133905_183 - 0.984780708948 - technology_and_computing
[wind, slp, model, speed, site]

Synoptic and local influences on boundary layer processes, with an application to California wind power
This dissertation examines atmospheric boundary-layer processes. The equatorial Pacific cold tongue region is examined, with observations showing that monthly anomalies in low-level cloud amount and near-surface atmospheric temperature advection are negatively correlated. In warm advection, soundings show the surface layer is stably stratified, inhibiting the upward mixing of moisture, while cold advection favors a more convective atmospheric boundary layer and greater cloud amount. Two global coupled climate models fail to simulate this, suggesting specific areas for possible improvement. Climatology and low-level wind variability near three California wind farms are then explored: San Gorgonio Pass and Tehachapi Pass in Southern California, and Solano County further north. Each site has a pronounced annual cycle with highest wind speeds in the warm months. While winter winds depend more on SLP, summertime winds are stronger, more diurnally dependent, and show more topographic influence on direction, though SLP variability is lesser. Self- organizing maps reveal that oceanic high SLP and continental low SLP synoptic patterns lead to higher wind speeds. SLP gradients at 100km separation are strongly correlated to cotemporaneous site wind speeds. Dynamically downscaled reanalysis data at 10km resolution reveals that a thermally driven flow at the northern site commences at the coast and propagates inland in a distinct packet. A statistical downscaling scheme is developed for relating GCM output to site winds. The multilinear regression model integrates weather type information and observational findings to reproduce wind speeds, showing skill for both monthly and daily data. Monthly mean wind speed changes over the 21st century are implied of up to 0.6 m/s in the summertime in three downscaled coupled climate models under greenhouse forcing. However, the discrepancies between models prevent consensus. Analysis of the discrepancies reveal that in one model the western North American surface heating coincides with a decrease in SLP, while others show increased continental SLP. The coupled models' representations of these regional patterns are discussed
-------------------------------------

10135979_183 - 0.734333732906 - technology_and_computing
[wc, sewersnort, monitoring, pipeline, infrastructure, sensor, wastewater, wcs, ghg, environment]

Mobile sensor network to monitor wastewater collection pipelines
The Wastewater Collection System (WCS) has long been recognized as one of the critical infrastructures in the urban fabric, along with fresh water and power system infrastructure. An aging Wastewater Collection System may jeopardize public and environmental health by contaminating the sources  of drinking water and by polluting the natural environment. Although the largest part of WCS failures are caused by aging infrastructure, careless dumping can lead to explosions and cause major catastrophes. Moreover, there is a growing consensus that greenhouse gas (GHG) generated from the wastewater infrastructure represents a significant fraction of GHG emissions. While a WCS failure can cause unimaginable hardship, systematic monitoring of the WCS has made little progress over the years due to several challenges that include invisibility, harsh environment, vast geographical span, and the requirement of minimizing service interruption. Thus far, various schemes have been designed and developed to monitor WCS (e.g., robots, Closed Circuit TV (CCTV)). However, due to the cost and the complications associated with these techniques, only a fraction of WCS areinspected each year.To address some of these issues we propose a novel WCS monitoring method based on mobile pipeline floating sensors named; SewerSnort together with wireless manhole beacons/base stations. SewerSnort is dropped upstream of the WCS and traverses a path in the WCS network. While traveling downstream, the sensor detects emergency conditions (leaks, spills, illegal dumps, and dangerous methane gas concentration) and notifies base stations along the way. Also, the data is carried to the destination (i.e., treatment plant) where it is analyzed to detect anomalies. In this work, we design a wireless mobile pipeline floating sensor unit SewerSnort which is a fully automated and end-to-end monitoring solution; develop a GPS-free radio-frequency based localization scheme inside pipelines; develop an algorithm to detect WCS functional deficiencies; develop a simulation tool to assist field deployment; and develop a radio propagation model to estimate the radio channel quality inside sewer environment. The simulator provides tools to analyze scenarios; to trace the path of SewerSnort; to develop a mobility model for a convoy of SewerSnorts; and to visualize SewerSnort movement in real-time and monitor the in-sewer incidents. Also, possible applications of SewerSnort to real-world problem such as illegal dumping, improving the accuracy of GHG emission e stimates, and performing preventive WCS monitoring, are presented to mitigate or to address our important environmental issues.
-------------------------------------

10130407_178 - 0.999317534146 - technology_and_computing
[signal, continuous, discrete, neural, ppc, cognitive, cortical, task, performance, prosthetic]

Cognitive signals for brain–machine interfaces in posterior parietal cortex include continuous 3D trajectory commands
Cortical neural prosthetics extract command signals from the brain with the goal to restore function in paralyzed or amputated patients. Continuous control signals can be extracted from the motor cortical areas, whereas neural activity from posterior parietal cortex (PPC) can be used to decode cognitive variables related to the goals of movement. Because typical activities of daily living comprise both continuous control tasks such as reaching, and tasks benefiting from discrete control such as typing on a keyboard, availability of both signals simultaneously would promise significant increases in performance and versatility. Here, we show that PPC can provide 3D hand trajectory information under natural conditions that would be encountered for prosthetic applications, thus allowing simultaneous extraction of continuous and discrete signals without requiring multisite surgical implants. We found that limb movements can be decoded robustly and with high accuracy from a small population of neural units under free gaze in a complex 3D point-to-point reaching task. Both animals’ brain-control performance improved rapidly with practice, resulting in faster target acquisition and increasing accuracy. These findings disprove the notion that the motor cortical areas are the only candidate areas for continuous prosthetic command signals and, rather, suggests that PPC can provide equally useful trajectory signals in addition to discrete, cognitive variables. Hybrid use of continuous and discrete signals from PPC may enable a new generation of neural prostheses providing superior performance and additional flexibility in addressing individual patient needs.
-------------------------------------

10135551_183 - 0.999998916666 - technology_and_computing
[relay, channel, multihop, network, node, power, estimation, phase, source, wireless]

Multihop Power Scheduling and MIMO Relay Channel Estimation
While single-hop wireless networks are commonly deployed today, multihop wireless networks are still in early stages of development. These networks have tremendous potential to be the technology of choice for providing ubiquitous Internet connectivity; minimizing the need for expensive wired infrastructure; and are relatively easy to deploy and maintain. However, there are still many fundamental challenges in wireless multihop relay networks. In this dissertation, we address two challenges that are of significant importance for wireless multihop relay networks.The first challenge we address is multihop transmission and power scheduling. In a multihop relay network, there are two types of nodes: terminal nodes and router nodes. Terminal nodes transmit directly to the local router node with a single hop. Router node collects the data from its local terminal nodes and is responsible for transmitting these data to the access point (AP). As the router nodes are more sophisticated than the terminal nodes, we assume they can support complex signal processing techniques such as Dirty Paper Coding (DPC) and can support multiple antennas communications as well. In this dissertation, our goal is to design a transmission scheme, which can balance the power consumption in each router node so as to prolong the network lifetime.We first propose a DPC based mutihop transmission scheme. An optimization problem of power scheduling and rate allocation to minimize a power related objective function or to maximize a rate related objective function is formulated. Then, a general gradient projection method is proposed to solve the optimization problem for networks where both single antennas and multiple antennas can be equipped in each node. Some useful properties are explored to realize fast computation. Furthermore, an alternative subgroup method is also provided to reach a tradeoff between performance and complexity when the network size becomes large. Numerical results show that our proposed method achieves better power saving and power balance performances compared with existing schemes.The second challenge we address is the channel estimation and training design for multihop relay channels. We consider a two-hop amplify and forward (AF) Multiple Input Multiple Output (MIMO) channel first. To overcome the ambiguity problem in channel estimates, we propose an innovative channel estimation scheme. This scheme has two phases. In the first phase, the source transmits no signal while the relay transmits and the destination receives. In the second phase, the source transmits, the relay amplifies and forwards, and the destination receives. At the destination, the data received in the first phase are used to estimate the relay-to-destination channel, and the data received in the second phase are used to estimate the source-to-relay channel. The linear minimum mean square error estimation (LMMSE) is used for channel estimation, which allows the use of prior knowledge of channel correlations. The algorithms for finding the optimal source training matrix used at the relay for the first phase, and the optimal source training matrix at the source and the optimal relay training matrix at the relay for the second phase, are developed. Power allocation along the diagonals of source and relay training matrices is solved by using an alternating algorithm with low complexity and fast convergence. The two-phase LMMSE based channel estimation method for two-hop AF MIMO relay channels can be extended for multihop AF MIMO relay channel estimation.In summary, we discuss and provide solutions to two critical challenges in wireless multihop relay networks: multihop transmission and power scheduling; MIMO relay channel estimation and training design. Our work advance the state-of-the-art in wireless multihop relay networks, and bring us closer to realizing the vision of ubiquitous multihop relay networks.
-------------------------------------

10129862_178 - 0.999979072353 - technology_and_computing
[quaternion, bingham, 3-d, distribution]

Tracking 3-D Rotations with the Quaternion Bingham Filter
A deterministic method for sequential estimation of 3-D rotations is presented. The Bingham distribution is used to represent uncertainty directly on the unit quaternion hypersphere. Quaternions avoid the degeneracies of other 3-D orientation representations, while the Bingham distribution allows tracking of large-error (high-entropy) rotational distributions. Experimental comparison to a leading EKF-based filtering approach on both synthetic signals and a ball-tracking dataset shows that the Quaternion Bingham Filter (QBF) has lower tracking error than the EKF, particularly when the state is highly dynamic. We present two versions of the QBF, suitable for tracking the state of first- and second-order rotating dynamical systems.
-------------------------------------

10139940_183 - 0.738059904108 - technology_and_computing
[system, design, firm, air, cost, reheat, vavr, energy, building]

VAV Reheat Versus Active Chilled Beams and DOAS
There have been a number of articles recently claiming that Dedicated Outdoor Air Systems (DOAS) plus active chilled beam (ACB) systems are superior to Variable Air Volume Reheat (VAVR) systems on energy efficiency, first cost, air quality, etc.see references 1-4.  The ASHRAE Golden Gate Chapter recently decided to hold a head-to-head competition to put these claims to the test.  Three mechanical engineering firms with offices in the Bay Area each provided a Design Development (DD) level design for a real office building currently in design, the U.C. Davis Medical Center Graduate Studies Building (GSB) in Davis, California.  One firm designed an ACB+DOAS system, another firm designed a VAVR system, and the third firm designed a hybrid combination of these two systems.  A fourth engineering firm then simulated each of the three designs using the EnergyPlus energy simulation program.  Finally, a major mechanical contractor provided a detailed HVAC construction cost estimate for each design.  The VAV reheat system had both the lowest first costs and the lowest energy costs of the three systems.  The analysis also showed that many of the other supposed advantages of ACB+DOAS relative to VAVR, such as improved indoor air quality and a lower floor/floor height, also turned out to be largely overstated.  Note that the results of this analysis are only strictly applicable to these three designs and this building and climate, but the conclusions may also apply more broadly. 
-------------------------------------

10130065_178 - 0.99999894182 - technology_and_computing
[wireless, network, reference, dynamic, bibliographical, index, computation]

Distributed computation in wireless and dynamic networks
Includes bibliographical references (p. 211-221) and index.
-------------------------------------

10137354_183 - 0.998524157297 - technology_and_computing
[power, path, circuit, critical, bias, delay, body]

A Power Reduction Technique Through Dynamic Runtime Algorithm For CMOS VLSI Circuits
All digital circuits have design margins for delay and power consumption. This thesis introduces an algorithm that exploits the design margin for delay to reduce power consumption instead, through the novel application of body bias to the transistors on the critical path. A runtime circuit monitors the activity of critical paths, and applies body bias to transistors on non-critical paths for specific input vectors where the value computed by the critical path is a don't care. In sub-100 nm CMOS devices, the application of adaptive body bias reduced leakage power while slightly increasing the signal propagation delay. When a portion of the circuit does not use up the whole clock cycle, the available slack can be used to reduce leakage power dissipation without compromising performance.
-------------------------------------

10133589_183 - 0.999820691223 - technology_and_computing
[navigation, route, area, nmus, driver, methodology, algorithm, energy, vehicle, emission]

Design and Development of Novel Routing Methodologies for Dynamic Roadway Navigation Systems
To date, traditional navigation systems have embedded algorithms that attempt to minimize trip distance and/or travel time. However, many drivers are now becoming increasingly concerned with fuel costs and vehicle emissions that are detrimental to the environment. Therefore, it is desirable to create new "environmentally-friendly" and "energy-friendly" navigation algorithms. Taking advantage of the latest navigation technology, in this dissertation, we have developed new navigation techniques that focus on minimizing energy consumption and vehicle emissions. These methods combine sophisticated mobile-source energy and emission models with route minimization algorithms that are used for navigational purposes. We know that road type play a great role in emissions and fuel consumption. We further have developed a high accurate and standalone road type classification methodology using only a short vehicle velocity trajectory, without external mapping system. Under chaotic traffic conditions (highly congested, road closures, natural disasters, etc.), a shortest-distance route might take an unreasonably longer travel time and can consume much more energy. On the other hand, under chaotic traffic conditions, a shortest-duration algorithm might frequently advise a driver to switch routes to avoid congested roadways and maintain a desirable travel time. The number of possible routes varies by roadway network topology and location within the network. It is useful to know how many possible routes exist. Therefore, we have developed and justified a navigational mobility index (NMI) with an initial focus on freeway networks. NMI can be based on the number of possible routes weighted by shared segments among routes from a source to a destination (referred to as a node-to-node NMI). Based on node-to-node NMI, node-NMI and area NMI are also defined and justified. Application of NMI includes: 1) measure the degree of freedom in which drivers can choose routes from a route choice perspective; 2) determine the potential effectiveness of navigation systems; 3) determine the overall connectivity level of an area; and 4) guide the movement of people during evacuation in a disaster event.Based on the proposed NMI concept, we further develop a new routing methodology that is based on maximizing the degree of freedom for re-routing while driving from a known location to a desired destination. Not only is this routing methodology beneficial for dealing with random incidents, it is also useful during major disaster situations when people in an affected area need to be quickly evacuated and relocated to safer areas. A variety of experiments have been carried out to determine the effectiveness of the proposed concept and routing methodology.The main contribution of this dissertation is summarized as follows. 1) We prove that a shortest-duration and a shortest-distance routes are not necessary energy efficient routes. We combine a state-of-art CMEM model with navigation technologies to develop an environmentally-friendly navigation methodology, which is unique; 2) Road type plays an important role in vehicle emission and energy consumption. We develop a high accuracy, low complexity, and stand-alone road-type classification algorithm based only on a short vehicle speed trajectory without external support such as a map system. 3) We originally propose and define a navigational mobility index (NMI) concept specifically for navigational purposes. Compared to other existing similar concepts, it has some desirable properties. NMI can be used to evaluate the potential effectiveness of a navigation system. 4) Node-NMI and area-NMI are further defined based on the definition of node-to-node NMI. Node-NMI and area-NMI can be used to assess the overall degree of freedom of an area. Area-NMI is comparable across different areas. 5) For emergency evacuation and navigation under chaotic traffic conditions (highly congested or road link closure), drivers frequently want to switch routes. Based on the defined NMI, we originally propose a navigation methodology giving routes in which drivers have the best degree of freedom for re-routing.  This is highly desirable under emergency evacuation scenarios, in which drivers are more likely to arrive to the safe area using NMI-based navigation than using the traditional shortest-distance or shortest-duration navigation.
-------------------------------------

10175543_189 - 0.792017509152 - technology_and_computing
[ceiling, corridor]

Jim West Mansion after renovation: corridor with arched ceiling
Color photograph of a long corridor with a groin-vaulted ceiling. Closed curtains, likely hiding windows or other openings are along the right side of the hall and pendant ceiling lamps are hung from the center of each arch.
-------------------------------------

10136459_183 - 0.99773182807 - technology_and_computing
[material, storage, different, energy, charge, phase, high]

Synthesis and characterization of nanostructured transition metal oxides for energy storage devices
Finding a promising material and constructing a new method to have both high energy and power are key issues for future energy storage systems. This dissertation addresses three different materials systems to resolve those issues. Pseudocapacitive materials such as RuO2 and MnO2 display high capacitance but Nb2O5, displays a different charge storage mechanism, one highly dependent on its crystal phase rather than its surface area. Various sol-gel techniques were used to synthesize the different phases of Nb2O5 and electrochemical testing was used to study their charge storage with some phases displaying comparable charge storage to MnO2. To overcome the electrical limitations of using an insulating material, the core-shell structure (Nb2O5/C) was also examined and the method could be generalized to improve other pseudocapacitors. Besides electronic conductivity, the diffusion of the electrolyte ions through the shell material is a critical factor for fast charging/discharging in the core-shell structure. This dissertation also involves another topic, a reconfigurable electrode, that displays both high energy and power density. By constructing a reconfigurable electrode which has different electrical properties (metallic or insulating state) depending on the amount of intercalated `guest' ions into `host' material, it can be used as a battery or electrochemical capacitor material in the insulating or metallic state respectively. Metal oxide bronzes having metal-insulator transition were investigated in this study.
-------------------------------------

10139112_183 - 0.989398084691 - technology_and_computing
[fiber, application, raman, sensor, optical]

Fiber sensors for molecular detection using Raman and surface enhanced Raman scattering
In this dissertation, highly sensitive optical fiber sensors based on Raman spectroscopy (RS) and surface-enhanced Raman scattering (SERS) are studied with focus on applications in various chemical and biological detections. In particular, two main categories of optical fibers have been used as the sensing platforms: one is the conventional multimode optical fiber and the other is the hollow core photonic crystal fiber (HCPCF). For the conventional multimode optical fiber, we've developed two types of probes using SERS techniques: the first is based on a double substrate "sandwich" structure with colloidal metal nanoparticles, and the second is based on interference lithography-defined nanopillar array structure on the fiber facet with the metal film deposition. For the HCPCF, the photonic bandgap guiding mechanism provides an ideal sensing platform because the confinement of both light and sample inside the fiber enables direct interaction between the propagating wave and the analyte. We demonstrate that by filling up the air channel(s) of the fiber with gas or liquid samples, it can significantly increase the sensitivity of the sensors in either regular Raman or SERS applications. For RS applications, these fiber sensors were tested with ambient gases, organic vapors, and biomedically important glucose molecule. For SERS application, these fiber sensors were evaluated with Rhodamine 6G, trans-1,2-bis(4-pyridyl)-ethylene, toluene vapor, 2,4-dinitrotoluene vapor, proteins and bacteria. We also demonstrate that these fiber sensors can be integrated with the portable Raman spectrometer in order to make it practical for out-of-laboratory applications. The techniques developed in this study are expected to have significant impact in chemical, biological, environmental, national security, and other applications.
-------------------------------------

10134269_183 - 0.999989858149 - technology_and_computing
[code, image, ldpc, algorithm, binary, decoding, belief, propagation, complexity, performance]

Optimizing and decoding LDPC codes with graph-based techniques
Low-density parity-check (LDPC) codes have been known for their outstanding error-correction capabilities. With low- complexity decoding algorithms and a near capacity performance, these codes are among the most promising forward error correction schemes. LDPC decoding algorithms are generally sub-optimal and their performance not only depends on the codes, but also on many other factors, such as the code representation. In particular, a given non- binary code can be associated with a number of different field or ring image codes. Additionally, each LDPC code can be described with many different Tanner graphs. Each of these different images and graphs can possibly lead to a different performance when used with iterative decoding algorithms. Consequently, in this dissertation we try to find better representations, i.e., graphs and images, for LDPC codes. We take the first step by analyzing LDPC codes over multiple-input single-output (MISO) channels. In an n_T by 1 MISO system with a modulation of alphabet size 2M̂, each group of n_T transmitted symbols are combined and produce one received symbol at the receiver. As a result, we consider the LDPC-coded MISO system as an LDPC code over a 2̂{M n_T}-ary alphabet. We introduce a modified Tanner graph to represent MISO-LDPC systems and merge the MISO symbol detection and binary LDPC decoding steps into a single message passing decoding algorithm. We present an efficient implementation for belief propagation decoding that significantly reduces the decoding complexity. With numerical simulations, we show that belief propagation decoding over modified graphs outperforms the conventional decoding algorithm for short length LDPC codes over unknown channels. Subsequently, we continue by studying images of non-binary LDPC codes. The high complexity of belief propagation decoding has been proven to be a detrimental factor for these codes. Thereby, we suggest employing lower complexity decoding algorithms over image codes instead. We introduce three classes of binary image codes for a given non-binary code, namely: basic, mixed, and extended binary image codes. We establish upper and lower bounds on the minimum distance of these binary image codes, and present two techniques to find binary image codes with better performance under belief propagation decoding algorithm. In particular, we present a greedy algorithm to find optimized binary image codes. We then proceed by investigation of the ring image codes. Specifically, we introduce matrix-ring-image codes for a given non-binary code. We derive a belief propagation decoding algorithm for these codes, and with numerical simulations, we demonstrate that the low-complexity belief propagation decoding of optimized image codes has a performance very close to the high complexity BP decoding of the original non-binary code. Finally, in a separate study, we investigate the performance of iterative decoders over binary erasure channels. In particular, we present a novel approach to evaluate the inherent unequal error protection properties of irregular LDPC codes over binary erasure channels. Exploiting the finite length scaling methodology, that has been used to study the average bit error rate of finite-length LDPC codes, we introduce a scaling approach to approximate the bit erasure rates in the waterfall region of variable nodes with different degrees. Comparing the bit erasure rates obtained from Monte Carlo simulation with the proposed scaling approximations, we demonstrate that the scaling approach provides a close approximation for a wide range of code lengths. In view of the complexity associated with the numerical evaluation of the scaling approximation, we also derive simpler upper and lower bounds and demonstrate through numerical simulations that these bounds are very close to the scaling approximation
-------------------------------------

10134282_183 - 0.999988282673 - technology_and_computing
[node, cooperation, network, ad, scheme]

An analytical and simulation study of wireless ad hoc networks with and without cooperation
Wireless ad hoc networks have gained popularity in recent years. A networks' ability to self-organize without a centralized base station and the ease of deployment in a rough terrain make ad hoc networks desirable. Furthermore, for small mobile devices, it is not practical to mount multiple antennas to exploit the benefits of spatial diversity for combating multipath fading. With ad hoc networks, it is possible to take advantage of spatial diversity as nodes may cooperatively act together as if they were multiple antennas. For this dissertation, the system model is first presented to obtain the information- theoretic upper bound for peer-to-peer ad hoc network with hop-by-hop routing. This bound is not physically achievable, but is useful as a yardstick against any node cooperation scheme studied. Using the same system model, lower bound with node cooperation is studied in depth for two cases: simple time-share bound and frequency reuse bound. It is found that having more than two nodes cooperatively transmit together as a relay node does not provide significant improvement over two nodes cooperatively transmitting. The results also indicate that a more elegant scheme must be used for the frequency reuse case before the lower bound with node cooperation could match the information-theoretic upper bound. The simple scheme used here does not employ interference avoidance strategies beyond selecting nodes which are simultaneously sending must meet certain interference criteria. The results of using this scheme indicate that at a high signal to noise ratio, the bound with cooperation approaches peer-to-peer bound with no cooperation
-------------------------------------

10129923_178 - 0.990160566877 - technology_and_computing
[clock]

Tuning an activator-repressor clock employing retroactivity
Activator-repressor systems have been shown to
be capable of oscillations and are therefore an important clock
motif in the field of Synthetic and Systems Biology. In this paper,
we propose a method to regulate oscillatory behavior in such
systems by the addition of DNA binding sites for the proteins
involved in the clock network. We show that the retroactivity
effect caused by this addition can effectively change the relative
timescales among the protein dynamics and impact the behavior
of the clock. We also employ root locus analysis to obtain a
graphical interpretation of the results.
-------------------------------------

10129808_178 - 0.89181942828 - technology_and_computing
[designing, communication, reference, bibliographical, complementary, system]

Designing complementary communication systems
Includes bibliographical references (p. 163-172).
-------------------------------------

10134484_183 - 0.999841455261 - technology_and_computing
[bsdf, measurement, material, datum]

BSDF acquisition and analysis of paper
This thesis details the acquisition and analysis of Bidirectional Scattering Distribution Function (BSDF) measurements for paper. Although paper is typically perceived to be a simple, diffuse material, it is instead very complex and consists of multiple properties. The BSDF measurements reveal that paper is a rough material that showcases specular reflection, retroreflection, and subsurface scattering, and these features can vary for different sides of the same sheet of paper. While these characteristics make paper one of the most interesting materials available, its structure has never been thoroughly investigated in appearance modeling research, nor has detailed BSDF data been supplied for it in existing measurement databases. This thesis provides in- depth documentation of various BSDF measurements for paper and supplies this data through the UCSD BSDF Database, which is publicly available for others to access and benefit from. These measurements also supplement a larger project involving the derivation of the Paper BSDF, a physically-based appearance model for paper-like materials, thus showing how this data is applicable for realistic image synthesis
-------------------------------------

10132768_183 - 0.99999152073 - technology_and_computing
[nonintegrable, system, quantum, simple, orbit, state, periodic, classical]

Spectra of Regular Quantum Graphs
We consider a class of simple quasi-one-dimensional classically nonintegrable systems that capture the essence of the periodic orbit structure of general hyperbolic nonintegrable dynamical systems. Their behavior is sufficiently simple to allow a detailed investigation of both classical and quantum regimes. Despite their classical chaoticity, these systems exhibit a “nonintegrable analogue” of the Einstein–Brillouin–Keller quantization formula that provides their spectra explicitly, state by state, by means of convergent periodic orbit expansions
-------------------------------------

10134097_183 - 0.845544521914 - technology_and_computing
[plane, color, algorithm, lens, fluid, edge, image]

Multiband deblurring for fluid lens cameras
Unique image processing challenges are produced by the Fluid Lens Camera System. Over traditional glass lens systems, better miniaturization potential and fixed length lens while zooming are unique abilities offered by the fluid lens. Non-uniform blurring for each color plane of the image is also caused by the fluid in the lens. A sharp green color plane and blurred blue and red color planes are also produced by this fluid. For natural and medical images, the edges in the green and blue color planes are similar. In this work, the sharpness of the blurred color planes is improved by sharing edge information between color planes. Avoiding shading artifacts while improving edge quality is the goal of this work. Several algorithms are discussed: a wavelet-based algorithm, a contourlet- based algorithm, a Support Vector Regression algorithm, and an Adaboost classification algorithm. In each algorithm, the strengths of the previous algorithm are built upon in order to improve results. A major advantage of these methods is that shading and edge information is combined without using a complicated point spread function. While the focus of this dissertation is on using the green color plane to improve the blue color plane, the same algorithms could be applied to the red color plane as well. Infrared imaging, medical image overlaying, satellite mapping, and remote sensing are all multiband system with high edge correlation where this work could be applied
-------------------------------------

10137224_183 - 0.999671081043 - technology_and_computing
[memory, mtl, task, information, retention, interval, capacity, damage]

Working memory, long-term memory, and medial temporal lobe function
Working memory has traditionally been viewed as independent of the hippocampus and related medial temporal lobe (MTL) structures. Yet, memory-impaired patients with MTL damage are sometimes impaired at remembering visual or relational information across delays as short as a few seconds. The challenge has been to understand the nature of these impairments. Discussion of the possible contribution of MTL structures to working memory has often focused on a distinction between tasks with short retention intervals (a few seconds) and tasks with longer retention intervals. Yet, questions about the possible role of the MTL in working memory do not turn on any particular retention interval. Instead, the important distinction is between tasks where the material to be learned and maintained is within the capacity of working memory and tasks where what is to be learned exceeds capacity. When capacity is exceeded, or when material must be retrieved following the redirection of attention, performance depends in part on long-term memory, even if the retention interval is brief. In three experiments, we assessed the ability of patients with MTL damage to retain relational information (object-location associations and object-in-scene information) or visual information (colored squares) across brief delays. In all experiments, patients performed as well as controls when working memory was sufficient to support performance and they were impaired only when the task requirements exceeded working memory capacity. This pattern of results supports the idea that maintenance of relational or visual information in working memory is intact after MTL damage and that damage to the MTL structures impairs performance only when the task depends, in part, on long-term memory
-------------------------------------

10135436_183 - 0.999885382751 - technology_and_computing
[device, nw, nm, current, fet, material, tfet, band-tail, cnt, capacitance]

Modeling, Design, and Analysis of III-V Nanowire Transistors and Tunneling Transistors
The aggressive downsizing of the transistor will continue for atleast another decade. The critical feature size (physical gatelength) of transistors will drop to 5 nm in 2020 (referred to as the11 nm technology node). In the sub-10 nm range, a variety oflow-dimensional materials and structures are being considered toincrease device and circuit performance. Examples are semiconductornanowires (NWs), carbon nanotubes (CNTs), and single-atomic layersof carbon called graphene.In order to investigate the performance, understand the physics,propose device design, and guide experiments of nanometer scalecomplementary metal-oxide semiconductor (CMOS) devices withone-dimensional (1-D) novel channel materials, such as III-V NWs, ageneralized quantum mechanical modeling and simulation approach isundertaken in this dissertation. We have developed models andsimulation tools, derived theory to understand and investigate III-VNW field-effect transistors (FETs) for next generation high-speed,low-power logic applications. These alternative materials andgeometries are being investigated for two different types oftransistors, (a) standard FETs, and (b) band-to-band tunneling FETs(TFETs).In the first part of the dissertation, we have investigated the keydevice metrics such as the quantum capacitance, the drive current,the charge, the power-delay product, the energy-delay product, andswitching frequency of NW FETs based on InSb, InAs, and InPmaterials. We have identified two operational regimes for thesenanoscale devices, namely, the quantum capacitance limit (QCL) andthe classical capacitance limit (CCL). It is shown that n-type NWFETs upto <=50 nm in core diameter operate in the QCL, and thecorresponding p-type devices operate in the CCL. Drive currents ata fixed gate overdrive for the n- and p-type devices are foundto be well-matched. Significant performance improvement in terms ofdevice metrics are predicted for devices operating in the QCL.In the second part of the dissertation, we have investigated III-VNW and CNT TFETs. A generalized approach to quickly determine thedrive current as a function of materials, diameter, and electricfield is developed. It is found that a CNT with the same bandgap asa NW can provide 10x drive current. We have developed ageneral non-equilibrium Green's function (NEGF) based approachwithin recursive Green's function (RGF) algorithm to investigate theeffects of `band-tails' on the subthreshold characteristics ofTFETs. Band-tails can result from heavy doping, impurities, andphonons. We show that band-tails resulting from necessary heavydoping of the source are not a show-stopper for TFETs.
-------------------------------------

10134082_183 - 0.99999603445 - technology_and_computing
[fp, spe, hardware, processor, unit, design, performance, area, strategy, asic]

Strategies for sharing a floating point unit between SPEs
Designing special-purpose processors and ASICs to execute computer programs requires a methodology that varies greatly from traditional general purpose software programming. The benefits of specialized processor designs and ASICs are : lower power consumption, and greater efficiency, as opposed to general purpose processors. Those benefits are the driven motivation in the Arsenal design that aims to incorporate 10s to 1000s of specialized processing elements (SPEs) into one system. Each one of the SPEs performs a well defined functionality that represents the variety of hardware designs, from general purpose processors to special-purpose processors and ASICs. Among those specialized hardware units, the Floating Point hardware infrastructure (FP) presents an important and interesting challenge reflected by its significant area and power requirements on the system. To one end, it makes the idea of having one FP unit per SPE prohibitively expensive. On the other hand, reducing the number of FP units could potentially create a bottleneck, and hence a negative impact on performance. Therefore, there is a significant trade-off between area, power, energy and performance aspects for sharing the FP hardware among Arsenal's SPEs. This thesis focus is designing and analyzing different strategies for sharing FP hardware for the SPEs across Arsenal. Therefore, the main goal is to find a proper balance between area, energy and performance for a set of FP sharing strategies over a sample set of FP applications. Our results show that a shared FP hardware per SPEs complex reduces area, energy and energy-delay with negligible performance degradation amongst all designs
-------------------------------------

10129841_178 - 0.996165659214 - technology_and_computing
[thermal, grant, foundation, simulation, national, polydimethylsiloxane, science, dynamics, molecular, pdm]

Molecular dynamics simulation of thermal energy transport in polydimethylsiloxane (PDMS)
National Science Foundation (U.S.) (Grant CBET-0755825)
-------------------------------------

10137505_183 - 0.992307470392 - technology_and_computing
[control, signal, neuronal]

Fast human movements and sparse optimal control policies
In this work we seek to find the underpinnings of neuronal control and movement. In doing so, we hope to gain insight into the function of the brain and also inspiration for the development of advanced robotics and neuronal prosthetics. In motor control, there is strong evidence that many different signals in the brain are combined. We propose that these various signals are different control policies and that these signals are sparse (reflecting the simplicity of neuronal computation). Additionally, neuronal time-integration plays a crucial role in motor control. Hence, we propose that biological motor control involves : 1. Sparse control signals. 2. Time-integration of control signals. 3. Combination of multiple control signals. We used these three simple ideas to outperforms previous models significantly better on human data (with p < 0.001)
-------------------------------------

10135475_183 - 0.882453421755 - technology_and_computing
[finger, backdrivable, robot, modular]

Design, fabrication and testing of ModBot, the biomimetic, backdrivable, modular finger robot
The work in this thesis introduces a new type of biomimetic fully backdrivable modular robotic finger that makes use of low friction joints actuated by cable drives to achieve given tasks such as reaching, grasping fragile objects and applying force to surfaces. The ModBot finger is a 3 degree of freedom finger that has the capabilities to process force and touch sensation at and along the tip of the finger while retaining all capable movement and degrees of freedom achieved by a human finger. Every piece of the ModBot finger has evolved many times in order to shed weight, lower inertia and achieve a backdrivable system. To achieve a true backdrivable system gears could not be used, due to the backlash, friction had to be reduced to its smallest value, weight was optimized for a high strength to weight ratio and part shapes were carefully engineered to achieve a low inertia, all of this allows for a smooth operating joint. This is all achieved with a minimal amount of parts used per finger. The contribution of this thesis is to create a modular finger robot that is fully backdrivable, biomimetic and that does not use gears. The second contribution is to optimize the design of the robot utilizing different materials, mechanisms and sensors to achieve near frictionless movement. The final contribution is to design the robot that can be modular, so that it can placed in various positions with multiple fingers to accommodate a given situation or test setup
-------------------------------------

10136864_183 - 0.999997926349 - technology_and_computing
[connectivity, functional, network, asd, structural, social, brain, met]

Imaging Genetics of Functional and Structural Connectivity in Children with Autism
Autism spectrum disorders (ASD) are heterogeneous yet highly heritable neurodevelopmental disorders characterized by atypical social behavior, delayed and/or abnormal verbal and nonverbal communication, as well as unusual repetitive behaviors and restricted interests. In vivo neuroimaging studies have consistently reported reductions in functional and structural connectivity of large-scale brain networks and recent genetic and neurobiological work suggests that ASD are related to altered synaptic and local-circuit connectivity. This dissertation seeks to provide insight into the neurobiological basis of ASD by using a network approach and by characterizing risk factors to ultimately aid in the development of more effective diagnostic tools and biologically-based treatments and interventions.In chapter 1, we examine functional connectivity of brain systems involved in social and emotional processing in ASD during an emotion-processing task. We use the amygdala and right inferior frontal gyrus, pars opercularis, as seeds in whole-brain functional connectivity analyses. We show that ASD is related to reduced integration within and segregation between distinct functional systems, which indicates that brain networks may partially reflect immature patterns of connectivity In chapter 2, we examine intrinsic functional connectivity with resting-state fMRI (rsfMRI) and structural connectivity with diffusion tensor imaging  (DTI) using a complex network approach. Using graph theoretical methods, we show that pairwise differences in functional connectivity are reflected in network level reductions in modularity and local efficiency, yet higher global efficiency. Structural networks displayed lower levels of white matter integrity and atypical age-related changes in global efficiency. By combining functional and structural network properties we further show that there is an age-related imbalance between structure and function in ASD. In chapter 3 we examine the neural correlates of an established autism risk polymorphism in Met receptor tyrosine kinase (MET). We show that this polymorphism is a potent modulator of key social brain circuitry in children and adolescents with and without ASD as MET risk genotype was associated with atypical fMRI activation and deactivation patterns to social stimuli (i.e., emotional faces), as well as reduced functional and structural connectivity in temporo-parietal regions known to have high MET expression, particularly within the DMN.
-------------------------------------

10131854_183 - 0.999286835608 - technology_and_computing
[knowledge, space, route]

Time and Space in Route Preference
It is only in the last few decades that geographers have become aware of their special skills and knowledge systems for understanding space in domains other than objective physical reality. As this awareness has grown, however, it has become more and more obvious that spatial knowledge is much more than the sensing or description of landmarks, routes and areas in an internal representation of environment.
-------------------------------------

10132833_183 - 0.984953426304 - technology_and_computing
[model, trucking, technology, information, industry, fleet, multivariate]

Trucking Industry Adoption of Information Technology: A Structural Multivariate Probit Model
The objective of this research is to understand the demand for information technology among trucking companies. A multivariate discrete choice model is estimated on data from a large-scale survey of the trucking industry in California. This model is designed to identify the influences of each of twenty operational characteristics on the propensity to adopt each of seven different information technologies, while simultaneously allowing the seven error terms to be freely correlated. Results showed that the distinction between for-hire and private fleets is paramount, as is size of the fleet and the provision of intermodal maritime and air services.
-------------------------------------

10132977_183 - 0.955256841487 - technology_and_computing
[environment, datum, physical, activity, link]

The Built Environment and Physical Activity: Empirical Methods and Data Resource
Does a person’s environment influence their physical activity? Intuition, theory, and preliminary evidence all suggest that there is an association between environment and physical activity, but questions of causality and magnitude remain poorly answered, in large part due to data challenges. If public health policy is to make meaningful links to the built environment, the literature will require careful tests of causal links and an understanding of the magnitude of those links. This paper reviews the data that are available for testing hypotheses about the built environment, physical activity, and health outcomes, both to educate the research community about existing data and current challenges and to illuminate data gaps that should be addressed as this research agenda moves forward.
-------------------------------------

10134113_183 - 0.919201943099 - technology_and_computing
[il-1beta, folding, il-1ra, protein, landscape, area, study, pro-il-1beta, region, frustration]

The expanding folding/functional landscape of the interleukin-1 family
Interleukin (IL)-1Beta is one of the "master" cytokines that initiates the innate immune response. IL-1Beta is a tightly regulated signaling molecule, and requires a delicate balance between its on/off states to maintain homeostasis in the organisms that produce it. Local and system-wide deleterious effects are seen when the actions of IL-1Beta are thrown off-balance. Therefore, there are multiple mechanisms that exist to keep this cytokine in check, from transcription to degradation. In this study, we characterize the native state of the inactive precursor form of IL-1Beta, pro-IL-1Beta. Little detailed structural information is known about pro-IL-1Beta, despite the extreme importance of IL-1Beta as an immune system modulator. We find that pro-IL-1Beta is an extended, loosely packed confirmation with respect to the well-folded mature protein. The presence of the N-terminal region in pro-IL-1Beta prevents the C-terminal region, the eventual mature protein, from functioning by binding IL-1RI. With a combination of proteolysis and deutermium exchange mass spectrometry, we show that while pro-IL- 1Beta is in a different conformation that the mature protein, an area important in the kinetic refolding pathway of the mature protein is protected from both proteolysis and deuterium incorporation into the amide backbone. Thus, pro-IL-1Beta is primed for quick- conversion into the mature form once the appropriate combination of signals is received by the cell producing it. IL-1Beta is also regulated extracellularly. It has co-evolved with a competitive inhibitor, IL-1 receptor antagonist (IL-1Ra). Both IL-1Beta and IL-1Ra bind IL- 1RI, but only IL-1Beta elicits a cell-signaling response. Despite sharing only 30% sequence identity, IL- 1Beta and IL-1Ra share the same tertiary structure, the Beta-trefoil fold. Since the structures between the agonist and antagonist are conserved, we would also like to know if their folding behavior and stability pathways are conserved as well. The dominant view of protein folding is the energy landscape theory, where protein sequences are designed well enough to fold on a "minimally -frustrated" funnel-shaped landscape. While minimizing frustration, or energetic traps on the landscape on-route to the native state leads robust and faster folding, areas within a structure that are functionally important have been shown to add frustration into a folding pathway. Therefore a balance must be struck between maintaining a functional protein that folds well. Recent minimalist C- alpha, or Go-model simulations of IL-1Beta determined that a region in IL-1Beta important for function (the Beta -bulge) added topological frustration into folding landscape for this protein, where contacts formed early in the folding trajectory must be unmade and remade again late in the native state formation. As predicted from these theoretical simulations, removal of this area abolishes most of the frustration on the folding landscape. Since IL-1Ra lacks the Beta-bulge, we undertook equilibrium and kinetic folding studies to determine if indeed the folding of IL-1Ra is faster than that of IL-1Beta. We find that IL-1Ra, while maintaining the similar thermodynamic stability, folds faster from the I to N transition, indicating a less-frustrated landscape. Go-model simulations of IL-1Ra folding revealed a region of frustration in the vicinity of the loop between strands 11 and 12 of the protein, not in IL-1Beta. This area is in close proximity the receptor binding sites of both IL- 1Ra and IL-1Beta. Folding and biological activity studies mutations made to this area in IL-1Ra, along with two other functionally important areas were undertaken. These studies revealed new insights into the functional landscape of IL-1Ra that could not be inferred by inspection and comparison of the structures of unbound/ bound IL-1Ra and IL-1Beta to IL-1RI alone. Finally, the newest member of the IL-1 family of proteins, IL-33, contains a C-terminal b-trefoil structural motif. Early in vivo functional studies indicate it has a unique functional landscape as compared to all other family members. We undertook a combined folding and structural study of the Beta-trefoil region of IL-33, and show it is slightly destabilized and faster folding as compared to wt IL-1Ra and IL-1Beta in the same conditions
-------------------------------------

10137336_183 - 0.999793642292 - technology_and_computing
[control, process, system, design, nonlinear, chemical, network, model, predictive]

Distributed Model Predictive Control of Nonlinear and Two-Time-Scale Process Networks
Large-scale chemical process systems are characterized by highly nonlinear behavior and the coupling of physico-chemical phenomena occurring at disparate time scales. Examples include fluidized catalytic crackers, distillation columns, biochemical reactors as well as chemical process networks in which the individual processes evolve in a fast time-scale and the network dynamics evolve in a slow time-scale.Traditionally, the design of advanced model-based control systems for chemical processes has followed the centralized paradigm in which one control system is used to compute the control actions of all manipulated inputs.  While the centralized paradigm to model-based process control has been successful, when the number of the process state variables, manipulated inputs and measurements in a chemical plant becomes large - a common occurrence in modern plants -, the computational time needed for the solution of the centralized control problem may increase significantly and may impede the ability of centralized control systems (particularly when nonlinear constrained optimization-based control systems like model predictive control-MPC are used), to carry out real-time calculations within the limits set by process dynamics and operating conditions. One feasible alternative to overcome this problem is to utilize cooperative, distributed control architectures in which the manipulated inputs are computed by solving more than one control (optimization) problems in separate processors in a coordinated fashion.Motivated by the above considerations, this dissertation presents rigorous, yet practical, methods for the design of distributed model predictive control systems for nonlinear and two-time-scale process networks. Beginning with a review of results on the subject, the first part of this dissertation presents the design of two, sequential and iterative, distributed MPC architectures via Lyapunov-based control techniques for general nonlinear process systems. Key practical issues like the feedback of asynchronous and delayed measurements as well as the utilization of cost functions that explicitly account for economic considerations are explicitly addressed in the formulation and design of the controllers and of their communication strategy. In the second part of the dissertation, we focus on the design of model predictive control systems for nonlinear two-times-scale process networks within the framework of singular perturbations. Both centralized and distributed MPC designs are presented. Throughout the thesis, the applicability, effectiveness and computational efficiencyof the control methods are evaluated via simulations using numerous, large-scale chemical process networks.
-------------------------------------

10133618_183 - 0.999995717628 - technology_and_computing
[algorithm, computation, tool, matrix, gusto, design, hardware, architecture, different, processing]

GUSTO : general architecture design utility and synthesis tool for optimization
Matrix computations lie at the heart of many scientific computational algorithms including signal processing, computer vision and financial computations. Since matrix computation algorithms are expensive computational tasks, hardware implementations of these algorithms requires substantial time and effort. There is an increasing demand for a domain specific tool for matrix computation algorithms which provides fast and highly efficient hardware production. This thesis presents GUSTO, a novel hardware design tool that provides a push-button transition from high level specification for matrix computation algorithms to hardware description language. GUSTO employs a novel top-to-bottom design methodology to generate correct-by-construction and cycle-accurate application specific architectures. The top-to-bottom design methodology provides simplicity (through the use of a simple tool chain and programming model), flexibility (through the use of different languages, e.g. C/MATLAB, as a high level specification and different parameterization options), scalability (through the ability to handle complex algorithms) and performance (through the use of our novel trimming optimization using a simulate & eliminate method providing results that are similar to these in commercial tools). Although matrix computations are inherently parallel, the algorithms and commercial software tools to exploit parallel processing are still in their infancy. Therefore, GUSTO also provides the ability to divide the given matrix computation algorithms into smaller processing elements providing architectures that are small in area and highly optimized for throughput. These processing elements are then instantiated with hierarchical datapaths in a multi-core fashion. The different design methods and parameterization options that are provided by GUSTO enable the user to study area and performance tradeoffs over a large number of different architectures and find the optimum architecture for the desired objective. GUSTO provides the ability to prototype hardware systems in minutes rather than days or weeks
-------------------------------------

10134593_183 - 0.99836323514 - technology_and_computing
[nmr, application, material, solid-state, molecular, study, analytical, science]

Theory and Applications of NMR to Problems in Material Science and Analytical Chemistry
ABSTRACT OF THE DISSERTATIONTheory and Applications of NMR to Problems in Material Science and Analytical ChemistrybyArun AgarwalDoctor of Philosophy, Graduate Program in ChemistryUniversity of California, Riverside, August, 2010Dr. Leonard J Mueller, Chairperson	 Recent developments in NMR spectroscopy have seen an increase in its application for the study of structure and dynamics in the field of material science. NMR can be used as a complementary technique to X-ray diffraction and other structure probing techniques such as AFM and STM in the study of complex systems. Due to its non-invasive nature, measurements can be made on samples without altering their structural or functional properties. In this thesis three different application of solid-state NMR will be highlighted.  First, the butyl-substituted spiro-biphenalenyl radical will be studied.  This molecule is a member of a new class of organic conductors which posses the highest conductivity of any neutral organic molecular solid via a magnetic phase transition. <super>13</super>C chemical shift measurements allow us to track the transition between the diamagnetic and paramagnetic states, which we find builds in as a gradual increase in the spin-density at the aromatic sites and a decrease in the electron spin-spin coupling between adjacent radicals. Our model suggests that the electron is not delocalized over the entire molecule, but is in fast exchange between the phenalenyl rings. Next, the photophysics of photomechanically responsive molecular crystal nanostructures based on anthracene esters is studied using solid-state NMR. A detailed study of these materials provides significant insight into the molecular-level dynamics that give rise to the micron-scale response of the nanorods. This work provides insight into how solid-state photochemistry within nanostructured molecular crystals can give rise to novel photomechanical behavior on micron lengthscales. Finally, the last chapter discusses several examples of analytical applications of NMR.  A combined approach of using solid as well as solution state NMR spectroscopy to study the stereo-chemical and functional properties of industrially useful compounds has been presented. The approach taken has the potential to be used for a wide variety of industrial applications.
-------------------------------------

10138820_183 - 0.999272498164 - technology_and_computing
[noise, window, building, operable, outdoor]

User perspectives on outdoor noise in open-plan offices with operable windows
Recent research suggests that buildings with operable windows in general, and mixed-mode (MM) buildings in particular can provide improved thermal comfort and control opportunities for users. Yet, there have been concerns about outdoor noise sources like traffic or construction noise when windows are opened. Concerns like these may hinder the installation of operable windows in buildings. This paper examines 23,000 office building occupants' perspectives on noise from both sealed and naturally ventilated/MM buildings. Results suggest thatoccupants near operable windows are more satisfied than those near sealed windows or those far from either window type. Among occupants dissatisfied with noise, complaints about indoor noise sources --like people talking-- are about 10 times more prevalent than outdoor noise complaints. 
-------------------------------------

10134586_183 - 0.999737310172 - technology_and_computing
[device, nanowire, high, silicon, measurement, gain, sensitivity]

Silicon nanowire phototransistor : designing, fabricating and characterizing a high responsibility, broadband photodetector
Nanowire photodetectors have been attracting increased attention due to their potential for very high sensitivity detection stemming from the unique properties of these quasi-one dimensional structures. Silicon photodetectors are of particular interest due to their low cost, ease of processing and ability for integration with conventional fabrication techniques. This work focuses on utilizing silicon nanowires towards creating a very high responsivity detector sensitive to a wide range of wavelengths from the ultraviolet to the near infrared spectrum. A physical understanding of the silicon nanowire phototransistors studied in this work is crucial for applying them towards high sensitivity imaging. The novel device concept is first presented qualitatively, illustrating how surface states in conjunction with geometrical effects of the nanowire create a large phototransistive gain. The device theory is then formalized mathematically, revealing the important physical quantities responsible for gain and the theoretical sensitivity achievable in such devices. Subsequently, simulations are performed to validate the concept and determine the parameters which govern device behavior. A top-down fabrication approach is utilized in creating these devices, allowing for precise control over geometry, traditional doping techniques, large area device formation, and compatibility with industrial fabrication lines. The devices are patterned through either traditional technology with e-beam lithography or maturing technology with nanoimprint lithography, and the nanowires formed through highly anisotropic dry etching. The finished devices are embedded in dielectric to support a top transparent contact. Characterization of these devices exhibits very high responsivity and phototransistive gain. Static measurements at room temperature show the initial demonstration of the device. Spectral measurements are then performed, showing absorption enhancement effects in vertical nanowire structures. Temperature dependent measurements demonstrate the capabilities of the device to detect illumination levels down to the sub-femtowatt at visible wavelengths, and picowatt at infrared wavelengths, unseen in bulk or thin-film devices. Finally, dynamic measurements determine the bandwidth of the device and a critical time constant in the kHz range. The characterization of these devices reveals both their potential and limitations. Future work on this device is proposed to engineer more reliable control over gain, lowered dark current for room temperature operation, and increased sensitivity
-------------------------------------

10136711_183 - 0.999853411112 - technology_and_computing
[control, system, fault, process, actuator, fault-tolerant]

Data-Based Monitoring and Fault-Tolerant Control of Nonlinear Processes
Fault-tolerant control is an essential component in modern process industries as abnormal situations account for over $20 billion in lost annual revenue in the US alone. Traditionally, control systems rely on centralized control architectures utilizing dedicated wired links to measurement sensors and control actuators to operate a plant at desired conditions and a separate monitoring system for detecting faults. While this paradigm to process operations and control has been successful, modern chemical plants that rely on highly automated processes to maintain  robust operations and efficient production are vulnerable to abnormal situations like, for example, actuator faults. Loss of control in a chemical process can lead to the waste of raw materials and energy resources, as well as downtime and production losses but most importantly it may lead to personnel injury or death and/or environmental hazard. This issue has prompted significant research efforts in the integration and application of fault-tolerant control to existing legacy control systems. This dissertation will present a paradigm shift to the existing approach of designing control systems and monitoring systems in that it proposes to design distributed control systems that are stabilizing, robust and optimal, and whose design leads to closed-loop system structures that facilitate fault isolation with the flexibility to not only avert disaster in the case of an abnormal situation but maintain optimal plant operation. To present our method of fault-tolerant control, we will focus on a broad class of non-linear process systems subject to disturbances and persistent control actuator faults. In general terms, the method includes the design of distributed model predictive control laws combined with a fault-detection and isolation approach based on process models and fault-free data that leads to successful detection and isolation of an actuator fault. After isolation of an actuator fault, the fault-tolerant control system estimates the fault magnitude, calculates a new optimal operating point, and ultimately reconfigures the distributed model predictive control system to maintain stability of the process in an optimal manner. Throughout the thesis, detailed examples of large-scale chemical process systems are used to demonstrate the approach.
-------------------------------------

10175419_189 - 0.999998128487 - technology_and_computing
[route, wireless, node, link, protocol, information, selection, network, packet, path]

Design and Evaluation of Primitives for Passive Link Assessment and Route Selection in Static Wireless Networks
Communication in wireless networks elementally comprises of packet exchanges over individual wireless links and routes formed by these links. To this end, two problems are fundamental: assessment of link quality and identification of the least-cost (optimal) routes. However, little is known about achieving these goals without incurring additional overhead to IEEE 802.11 networks. In this thesis, I design and experimentally evaluate two frameworks that enable individual 802.11 nodes to characterize their wireless links and routes by employing only local and passively collected information.

First, I enable 802.11 nodes to assess their links by characterizing packet delivery failures and failure causes. The key problem is that nodes cannot individually observe many factors that affect the packet delivery at both ends of their links and in both directions of 802.11 communication. To this end, instead of relying on the assistance of other nodes, I design the first practical framework that extrapolates the missing information locally from the nodes' overhearing, the observable causal relationships of 802.11 operation and characterization of the corrupted and undecodable packets. The proposed framework employs only packet-level information generally reported by commodity 802.11 wireless cards.

Next, I design and evaluate routing primitives that enable individual nodes to suppress their poor route selections. I refer to a route selection as poor whenever the employed routing protocol fails to establish the existing least-cost path according to an employed routing metric. This thesis shows that an entire family of the state-of-the art on-demand distance-vector routing protocols, including the standards-proposed protocol for IEEE 802.11s mesh networks, suffers from frequent and long-term poor selections having arbitrary path costs. Consequently, such selections generally induce severe throughput degradations for network users. To address this problem, I design mechanisms that identify optimal paths locally by employing only the information readily available to the affected nodes. The proposed mechanisms largely suppress occurrence of inferior routes. Even when such routes are selected their durations are reduced by several orders of magnitude, often to sub-second time scales.

My work has implications on several key areas of wireless networking: It removes systematic failures from wireless routing and serves as a source of information for a wide range of protocols including the protocols for network management and diagnostics.
-------------------------------------

10137755_183 - 0.995054808199 - technology_and_computing
[ground, motion, levee, tool]

Ground motion estimation for evaluation of levee performance in past earthquakes
Levees provide vital functions for water delivery and flood protection. However, they present unique challenges for seismic design because their great length makes engineering evaluation of stability at closely spaced regular intervals impractical. Accordingly, relatively broad, empirically-driven risk assessment tools have the potential to serve as effective screening tools. We are undertaking a large data collection and synthesis effort to support the development of such tools, with the initial focus being on levee performance from the 2007 Mw6.6 Niigata Chuetsu-oki earthquake in Japan. Naturally, ground shaking is a key variable in this process, so the reliable estimation of ground shaking hazards fromseismic networks is an essential element of the case history analysis. We postulate thatdirect application of Krigingtechniques can produce biased ground motion estimates due to variable site conditions. Accordingly, we apply Kriging to residuals of ground motion prediction equations (GMPEs), which remove the average site effect. The resulting maps of residuals can be readily applied with the GMPE to produce ground motion maps that properly reflect spatial variations of geologic conditions.The proposed procedure produces ground motions near levees that are lower in some areas than those produced bydirect Kriging.
-------------------------------------

10175370_189 - 0.99955458059 - technology_and_computing
[browser, mobile, speculative, loading]

Speeding Up Mobile Browsers without Infrastructure Support
Mobile browsers are known to be slow. We characterize the performance of mobile browsers and find out that resource loading is the bottleneck. Leveraging an unprecedented set of web usage data collected from 24 iPhone users continuously over one year, we examine the three fundamental, orthogonal approaches to improve resource loading without infrastructure support: caching, prefetching, and speculative loading, which is first proposed and studied in this work. Speculative loading predicts and speculatively loads the subresources needed to open a webpage once its URL is given. We show that while caching and prefetching are highly limited for mobile browsing, speculative loading can be significantly more effective. Empirically, we show that client-only solutions can improve the browser speed by 1.4 seconds on average. We also report the design, realization, and evaluation of speculative loading in a WebKit-based browser called Tempo. On average, Tempo can reduce browser delay by 1 second (~20%).
-------------------------------------

10139961_183 - 0.836188338483 - technology_and_computing
[use, boat]

Boats (Use of)
Ancient Egyptian boats are defined as river-going vessels (in contrast with sea-going ships). Their use from late Prehistory through the Ptolemaic and Roman Periods included general transportation and travel, military use, religious/ceremonial use, and fishing. Depending on size and function, boats were built from papyrus or wood. The oldest form of propulsion was paddling, although there is some evidence for towing as well. Sailing was probably introduced towards the end of the late-Predynastic Period.
      
       
-------------------------------------

10134917_183 - 0.999357903569 - technology_and_computing
[layer, design, neighbor, protocol, throughput, mac, uv, communication, transmission, different]

MAC Design for Optical Wireless Communications
This thesis mainly focuses on the higher layer protocol design for optical wireless communication (OWC) networks in two different optical bands: (1) medium access control (MAC) protocol design and neighbor discovery methods for deep ultraviolet (UV) outdoor communications (UVOC), and (2) configuration of indoor visible light communication (VLC) networks. For UVOC, solar blind and non-line-of-sight (NLOS) operations are attractive. Light beams from UV light-emitting diode (LED) arrays propagate through scattering media, creating spatially different communication links. This unique physical (PHY) layer characteristic was first captured experimentally based on a UV testbed, from which mathematical signal propagation models were developed and their impact on MAC design was realized, i.e., full duplexing and multi-rate transmission. Then we propose a novel contention-based MAC protocol (UVOC-MAC) that inherently accounts for the UV PHY layer and fully exploits multi-fold spatial reuse opportunities. Evaluations via simulation and analysis show that UVOC-MAC effectively mitigates collisions and achieves high throughput. We further develop efficient neighbor discovery protocols by accounting for the varying channel qualities along different scattering directions. Besides a list of neighbor nodes' identities, a ranked list of node pointing directions in terms of channel qualities was also included in the constructed table to facilitate the process. Utilizing neighbor feedback or alternating a leader node were proved to be able to alleviate the negative effects of random access based collisions and thus expedite neighbor discovery.VLC by lighting LEDs is gaining popularity, but there is very limited research on the higher layer protocol design. Our extensive channel measurements using a physical layer testbed suggest two effective means to increase data rates, shrinking the beam width and tuning the transmission beam to point towards a target receiver. We design a configuration framework called VICO, by leveraging above PHY features towards achieving the highest throughput while maintaining fairness. VICO tries to schedule transmissions while minimizing conflicts of links. It also opportunistically tunes the idle LEDs to reinforce existing transmissions to increase throughput to the extent possible. Under these proposed treatments, VICO can provide as much as 5-fold increase in throughput as compared to a simple scheduler that does not exploit the possible variations in beamwidth or beam-angle.
-------------------------------------

10136785_183 - 0.929492502984 - technology_and_computing
[support, measure, classifier]

Applications of Text Classification to Enterprise Support Documents
In the business world today there is a vast amount of information, and in order to process this information it must be structured.  I develop a set of classifiers and corresponding user interfaces to assign tags to data that correspond with a structured framework.  The classifiers are applied through a case study in the support area of a major networking company.  The three classifiers provide tags for technical support documents with an F measure of .585, customer service requests with an F measure of .706, and customer support forum messages with an F measure of .78.
-------------------------------------

10133724_183 - 0.807642294911 - technology_and_computing
[datum, integration, community, source, inconsistency, registration, resolution, task, member, model]

Community-oriented information integration
To allow their members to collaboratively maintain the community knowledge, modern online communities need to integrate their members' structured data into a single community database. Existing integration solutions employed by enterprises are not suited for communities, as they rely on a central authority to carry out the integration tasks and are therefore too costly and not scalable to large numbers of sources. To solve this problem, we propose the community-oriented integration (CII) paradigm, which removes the need for a central authority by delegating the integration tasks to the individual community members. In this dissertation, we describe how to decentralize two main integration tasks: The registration of sources in the integration system, which becomes the responsibility of each source owner and the resolution of inconsistencies in the collective data, which is delegated to the consumers of the integrated data. To facilitate this distribution, we introduce a novel architecture and two tools - RIDE and Ricolla - that assist the community members, who typically lack the sophistication of the central authority and an overview of the system, in carrying out the source registration and inconsistency resolution tasks, respectively, autonomously. RIDE models source registrations as sets of Global and Local As View (GLAV) mappings and assists the source owner in creating a registration that balances two competing requirements: Making her data visible to applications that run on top of the community database (by exporting more data) and minimizing the cleaning cost required for publication (by publishing less data). We model these trade-offs as different self-reliance levels, present decidability results and appropriate decision procedures (when existent) and describe an algorithm for interactively guiding the user towards a registration with a particular self-reliance level. On the inconsistency resolution front, Ricolla models inconsistent data as sets of possible worlds and displays them to the users through a novel data model that summarizes them in an easily understandable and compact form. It also offers a flexible architecture that enables different schemes for the inconsistency resolution, allowing among others users to resolve inconsistencies individually, according to their own opinions, or in collaboration with their peers
-------------------------------------

10135732_183 - 0.999994251952 - technology_and_computing
[model, system, demonstration, datum, barbara, demand, simulation, santa, travel]

Forecasting with Dynamic Microsimulation: Design, Implementation, and Demonstration
In this project we develop a new travel demand forecasting system that integrates demographic microsimulation with urban simulation and travel demand model systems. Our research objective is to identify the barriers in integrating complex simulation models and eliminate them by offering a demonstration of problems and solutions. The basic ingredients of this new model system are: a) a dynamic demographic simulator designed and tested with repeated observations of the same individuals in another context that will be transferred to a case study in Santa Barbara, CA; b) a modified version of the recently finalized Urbansim model that will also be calibrated with data from Santa Barbara, CA; and c) travel demand models that account for intrahousehold interactions and path based accessibility that were estimated with data from California. The model system is unique because it combines within a day and across years human behavior dynamics and it will push the frontier of modeling and simulation one step further. A demonstration of a pilot test is offered using data from Santa Barbara, CA.
-------------------------------------

10136312_183 - 0.999998970961 - technology_and_computing
[design, antenna, optimization, radar, use, system, reconfigurable, pso]

Nature-Inspired Optimization Techniques Applied to Antennas for Wireless Communications and Radar
In this work, two nature-inspired optimization techniques, namely Particle Swarm Optimization (PSO) and Covariance Matrix Adaptation Evolution Strategy (CMAES), are presented and compared. First, comparisons of each algorithm in resource limited problems are provided using mathematical functions as well as real-world antenna design problems. In particular, a dual polarized weather radar antenna array element is optimized for use in newly proposed weather radar systems. In the last half of this work, PSO is applied to two other antenna systems.  The first application investigates the use of a smoothed Sigmoid septum design in circular waveguide for possible use in high power microwave systems. PSO is also applied on two newly proposed reconfigurable E-shaped patch antenna designs, which include a polarization (RHCP/LHCP) reconfigurable and a frequency reconfigurable design. Both designs are optimized using a simple MEMS circuit model for fast optimization and measured, and possible bias network implementations are discussed.
-------------------------------------

10134568_183 - 0.788244511938 - technology_and_computing
[saw, sensor, mass, experimental, wave, result, measurement, film, surface, static]

Experimental Investigation of Mass Sensing With Surface Acoustic Wave Devices
We present an experimental study of mass sensitivity for a Surface Acoustic Wave (SAW) sensor. Chemical SAW sensors have been developed to detect the mass variation of the mass adsorbed into sensing film by showing resonant frequency shifts. Previous experimental results have focused on the static sensitivity of theoretical SAW sensors assuming that the film thickness is negligible compared to the central wavelength of surface waves. Most current research is focused on dynamic experimental measurement and analytical techniques that are application specific. We have studied a SAW sensor with relatively thick isotropic film layers sputtered on as a static measurement comparison. The results can be used in a detection algorithm to quantify mass on the SAW sensor. Our results show a set of nodal response wave form changes and frequency shifts due to a continuous mass distribution. The development of the static experimental technique was completed with duplicate SAW tests. The duplicate SAWs had 12 measurements with 10 independently sputtered layers of 100 <em>f</em>Ým SiO2, hermitically sealed, and hermitical seal removed using a HP8510A network analyzer. In addition, our results show worst case total error of 0.1% between replicates. Further testing and modeling is required to correlate macro to micro scales for quantifying mass detection with SAW sensors. This research discusses the issues of utilizing a SAW sensor that could be readily integrated into cell phones for distributed sensing as being requested by Homeland Security.
-------------------------------------

10136614_183 - 0.992423791337 - technology_and_computing
[model, item, dif, detection, bifactor, method]

Detection of Differential Item Functioning in the Generalized Full-Information Item Bifactor Analysis Model
In the field of psychometrics, there has been an increase in interest concerning the evaluation of fairness in standardized tests for all groups of participants.  One possible feature of standardized tests is a group of testlets that may or may not contain differential item functioning (DIF) favorable to one group of participants over another.  A testlet is a cluster of items that share a common stimulus.  In this dissertation, a DIF detection method useful for testlet based data was developed and tested for accuracy and efficiency.  The proposed model is an extension of the generalized full-information item bifactor analysis model.  Unlike other IRT-based DIF detection models, the proposed model is capable of evaluating locally dependent test items and their potential impact on the DIF estimates.  This assures the new capability of the bifactor DIF detection method that was not evident in previous methods.  Item parameters were estimated using a maximum likelihood estimation (MLE) method producing expected a posteriori (EAP) scores.  Using the restrictions of a bifactor model, the dimensionality of integration can be analytically reduced and the efficiency can be increased.  Following prior research regarding DIF on a PISA dataset, the proposed DIF model was applied to mathematics items of the Program for International Student Assessment (PISA) 2009 dataset to confirm the utility of the model.  After the meaning of results to the PISA research community is conveyed, a simulation study was conducted to provide concrete evidence of the model's utility.  Finally, limitations of this study from computational and practical standpoints were discussed, as well as directions for further research.
-------------------------------------

10136125_183 - 0.979619033004 - technology_and_computing
[placement, isq, protocol, value, failure, stability, implant, survival, analysis, sensitivity]

The Predictive Value of Resonance Frequency Analysis Measurements in the Surgical Placement and Loading of Endosseous Implants
Implant stability is an important factor guiding the selection of placement and loading protocols. An evaluation of the currently available techniques for measuring stability clearly demonstrates a need for a non-invasive, quantitative, repeatable, and reliable way to measure implant stability over time. A potential candidate for this purpose is resonance frequency analysis (RFA).A retrospective study was performed on implant patient data collected over a five-year period. Patients were categorized according to their placement protocol (one-stage vs. two-stage) and loading protocol (early vs. traditional). RFA measurements were recorded during placement and prior to loading. Survival or failure was determined after a minimum follow-up period of two years. Receiver operating characteristic (ROC) statistical analysis was used to determine ISQ cut-off points with respective sensitivity and specificity values for different placement and loading protocols.In predicting implant failure, sensitivity progressively increased and specificity decreased as ISQ cut-off values increased. All failures occurred at ISQ < 66 for the placement protocol and ISQ < 67 for the loading protocol. When ISQ values were less than 60, higher survival rates were observed when implants were placed utilizing a two-stage rather than a one-stage protocol. The area under the ROC curve for placement was 0.80 (p < 0.05) and the area under the ROC curve for loading was 0.89 (p < 0.05).RFA is a non-invasive technique used to measure the stability of implants and help guide placement and loading protocols. This study showed that increasing ISQ values correlated with increased sensitivity in detecting implant failure. Due to the high survival rates of dental implants, additional studies with an increased samples size and more implant failures are necessary to further elucidate the relationship between ISQ values and survival rates.
-------------------------------------

10133686_183 - 0.996023218057 - technology_and_computing
[signal, position, frequency, phase, train, estimation, estimate]

Frequency and phase estimation for application in non- optical position tracking of Maglev vehicles
This thesis considers how the position of a magnetic levitation train can be found non-optically using signal processing techniques. An overview of magnetic levitation trains is given, including an explanation of the necessity for non-optical position sensing. A least-squares with forgetting factor algorithm is derived, showing its convergence and stability. The input signal is manipulated so that the least-squares algorithm can be applied. Extremum seeking is introduced and utilized for phase estimation of the high frequency signal. When adding noise to the incoming signal, the frequency estimates remain accurate if the forgetting factor is allowed to vary proportional to the slope of the estimate. Additive noise has little effect on the extremum seeking phase estimate. The frequency and phase estimation is combined to reconstruct an estimate of the carrier signal to demodulate the signal and extract the position of the train
-------------------------------------

10133859_183 - 0.999999173877 - technology_and_computing
[retrieval, memory, network, task, default, study, activity, hippocampus, activation, recollection]

Imaging frontal and medial temporal lobe interaction during memory retrieval and disentangling the effects of the default network
Exploring how the medial temporal lobe interacts with different areas of the brain during memory tasks is an important component in understanding the dynamics of memory retrieval. To achieve this goal, functional magnetic resonance imaging (fMRI) was used to determine how the frontal and medial temporal lobes interact during memory retrieval and subsequent use of recollected information and how memory retrieval may integrate with or affect default network activity. The first study used blocked and event-related fMRI to examine hippocampal activity during long term memory recollection and post- retrieval processing of paired items. Subjects were asked to make living/nonliving judgments about items visually presented (classify) or about items retrieved from memory (recollect-classify). In addition, active (odd/even digit classification) and passive (fixation) baselines were used to differentiate task-related activity from default network activations. During the r̀ecollect-classify' task, activity in the anterior hippocampus was selectively reduced relative to 'classify' and baseline tasks (active and passive), and hippocampal activity was inversely correlated with bilateral DLPFC activity. The finding was that frontal and hippocampal activity are dissociated during memory retrieval and post-retrieval processing. The second study examined the effects of retrieval instruction on brain regions implicated in episodic memory retrieval. The finding was that the default network showed a greater decrease in activation for poorly-remembered than strongly -remembered responses regardless of whether subjects were instructed to suppress the cued material or to perform non -elaborative or elaborative retrieval. The hippocampus showed an increase in activation only with successful memory recall. Further examination revealed that the retrieval network, commonly identified by differences in activation during successful retrieval and baseline, seems to be modulated somewhat by task instruction, but not recall success. The hippocampus showed differential activity based on top-down modulation elicited by retrieval instructions while the default network did not. The third study examined recollection and familiarity with attention to reaction-time to explore its contribution to regional activations. The finding was that the hippocampus is functionally dissociated from other regions of the retrieval-network during recollection. Portions of the retrieval-network are generally influenced by reaction- time and show suppressed signal when subjects are task- engaged in either recollection or familiarity; suppression is greater for longer trials. The hippocampus, however, shows a positive response only for recollection trials, where activation is greater for longer recollection trials, but not longer familiarity trials. It is concluded from these experiments that the hippocampus is dissociated from the rest of the default network during tasks of memory retrieval. While many studies have examined how the hippocampus, pre-frontal cortex, retrieval network, and default network are related to memory retrieval, these studies take into account additional components of the memory retrieval tasks like reaction time and false alarm rate in order to disentangle memory retrieval itself from related components. Prior studies have shown functional connectivity of the hippocampus and the default network, but these studies, taken together, suggest that hippocampus is affected by task instruction as well as task success, and is dissociated from the default network during tasks involving memory retrieval. The summation of these studies exposes how the modulation of activity in non-memory related networks in the brain may affect activation attributed to memory retrieval, and why it is so important to take these confounds into account in memory recall studies
-------------------------------------

10139166_183 - 0.937229525257 - technology_and_computing
[battlefield, space, memory, preserved, century, nature, place]

"Something Terrible Happened Here": Memory and Battlefield Preservation in the Construction of Race, Place, and Nation
This dissertation considers the changing place of race at nationally preserved battlefields from the Civil War and Indian Wars. As sites of contestation and carnage, these preserved spaces serve as strong indicators of the power of place that dominated America's ethos and national identity during the nineteenth century. The battles themselves resulted from power struggles over land, property, economic expansionism, and race-based debates. These conflicts over place and people did not end with a defeat or victory on the battlefield. They transitioned into a second phase that continued to use battlefields as sites of negotiation over racial entitlement and disenfranchisement. This dissertation argues that preserved battlefields are landscapes expansive in nature, crossing time and space. They are not evidence of one year, one day, or one event. These battlefields actually reproduce historic environments to fit the needs of those preserving and viewing them. In other words, the spaces are cultural landscapes, representing constructed spaces reflecting the processes in which culture--and cultural conflict--manipulates, affects, and frames nature over the long twentieth century (1865-present). While contending with the challenges of contemporary America, preservationists and visitors used these battlefields to contribute to the larger national project intent on (re)defining who had the power and access to be included in the national narrative.Historically, the Anglo male dominated collective memory on the preserved battlefields. As such, it is presumed that these battlefields are purely masculine and Anglo in nature. This dissertation indicates that this is not the case. Sentimentalism was a key tool utilized by preservationists to control memories. Through sentimentalism, battlefields became semi-fictitious spaces based on selective and dramatic accounts of the past. They did not preserve a pure narrative of nineteenth century warfare; instead, they preserved desired interpretations of the past to better serve the present. Today, Native Americans and African Americans utilize preserved battlefields to present their own voices, inserting themselves into the nation's collective memory. It is the ongoing relationship between memory, nature, and nineteenth century warfare that is at the heart of this research.
-------------------------------------

10137113_183 - 0.999977718401 - technology_and_computing
[thermal, model, system, design]

Compact Modeling and Analysis for Electronic and Thermal Effects of Nanometer Integrated and Packaged Systems
Design and verification of today's nanometer very-large-scale integrated (VLSI) system remain a very challenging problem. For instance, the sub-90-nm technology has caused extremely large parasitic global interconnects and complicated models such as clock networks, power delivery networks and thermal models for packaged systems, which are difficult to be analyzed directly due to the limited computing resources. In addition, the high performance VLSI systems such as multi-core and emerging 3D stacked integrated systems, also lead to excessive high temperature on chip due to the elevated power densities. As a result, temperature should be explicitly managed both at design time through thermal-aware optimization and design techniques and at runtime through on-chip dynamic thermal management (DTM). Hence, accurate yet compact thermal models are required for thermal-aware design and optimization.In this dissertation, we focus on those challenging issues and have proposed three novel techniques to facilitate the verification of the electronic and thermal effects of the nanometer integrated systems. Specifically, first, we have introduced a wideband model order reduction algorithm (WBMOR) to provide a general solution for the large system analysis problems. With the novel imaginary axis sampling technique and adaptive sample point placement, WBMOR is able to generate a reduced system accurate within the specified frequency band. Second, we have proposed a com- posable thermal modeling technique (ThermComp) for compact thermal modeling. ThermComp builds compact thermal models for each basic module, and uses these models to assemble different multi-core architecture thermal models, which improves the thermal modeling and analysis efficiency at design time. Last but not least, a runtime thermal estimation and prediction method (FRETEP) framework has been proposed to enable fine-grained DTM. With a thermal sensor based error compen- sation method utilizing only limited number of thermal sensors, FRETEP is able to estimate and predict the full-chip thermal behavior accurately with even inaccurate power estimation. Furthermore, a power-driven thermal sensor placement algorithm has been developed for FRETEP to further enhance the thermal estimation accuracy.
-------------------------------------

10131908_183 - 0.999998778386 - technology_and_computing
[assignment, algorithm, network, traffic, computer]

A Faster Path-Based Algorithm for Traffic Assignment
This paper takes a fresh look at the arguments against path-enumeration algorithms for the traffic assignment problem and provides the results of a gradient projection method. The motivation behind the research is the orders of magnitude improvement in the availability of computer storage over the last decade. Faster assignment algorithms are necessary for real-time traffic assignment under several of the proposed Advanced Traffic Management System (ATMS) strategies, and path-based solutions are preferred. Our results show that gradient projection converges in 1/10 iterations than the conventional Frank-Wolfe algorithm. The computation time improvement is of the same order for small networks, but reduces as the network size increases. We discuss the computer implementation issues carefully, and provide schemes to achieve a 10-fold speed-up for larger networks also. We have used the algorithm for networks of up to 2000 nodes on a typical computer work station, and we discuss certain data structures to save storage and solve the assignment problem for even a 5000 node network.
-------------------------------------

10131690_183 - 0.999732067915 - technology_and_computing
[network, transportation, neo-traditional, type, trip, community]

A Comparative Assessment of Travel Characteristics for Neo-Traditional Developments
The primary intent of this research is to explore the claim that transportation benefits can be derived from neo-traditional neighborhood design. Conventional transportation planning models are used as tools to evaluate the performance differences of two hypothetical street networks designed to replicate a neo-traditional and a conventional suburban community. Relative transportation benefits are measured in terms of vehicle-miles travelled (VMT), average trip lengths, and congestion on links and at intersections. This comparison provides an assessment of how well the two networks in question deal with trips generated by the activities which they serve. All aspects of the modelled communities are held constant except for the actual configuration of the networks. This type of evaluation can indicate, for example, whether the same level of activity (defined by the land uses within the community) creates greater congestion on one type of network, or if trip lengths on one network are generally longer. The ultimate goal is to determine if one network type, because of the nature of its design, can result in a more efficient transportation system.
-------------------------------------

10137300_183 - 0.993785174673 - technology_and_computing
[park, linear, infrastructure]

Interventions on Pacoima Wash: Repurposing Linear Infrastructure into Park Spaces
There is a movement underway in Los Angeles led by community groups, non-profits, and local officials to combat environmental racism with the creation of new public parks and greenspaces.  This is a dramatic change in the city's land use priorities.  In this paper, I situate the current round of park development within the literature on environmental racism and the siting of industry activities and their attendant linear circulation infrastructure in predominately low-income and minority communities.  Utilizing Kevin Lynch's classifications of urban forms, this paper demonstrates three typologies for park interventions on linear infrastructure--parkways, nodal parks, and cap parks--and how they would operate upon Pacoima Wash.
-------------------------------------

10137920_183 - 0.959897983173 - technology_and_computing
[radio, captive]

Captive Radio
Captive Radio is a half-hour documentary about families of victims of kidnapping in Colombia who use a weekly radio program to communicate with their loved ones held hostage in the jungle. 
-------------------------------------

10137444_183 - 0.99999946629 - technology_and_computing
[code, subspace, network, algorithm, rank-metric, list-decoding]

List decoding of subspace codes and rank-metric codes
Subspace codes and rank-metric codes can be used to correct errors and erasures in networks with linear network coding. Both types of codes have been extensively studied in the past five years. We develop in this document list-decoding algorithms for subspace codes and rank-metric codes, thereby providing a better tradeoff between rate and error-correction capability than existing constructions. Randomized linear network coding, considered as the most practical approach to network coding, is a powerful tool for disseminating information in networks. Yet it is highly susceptible to transmission errors caused by noise or intentional jamming. Subspace codes were introduced by Koetter and Kschischang to correct errors and erasures in networks with a randomized protocol where the topology is unknown (the non-coherent case). The codewords of a subspace code are vector subspaces of a fixed ambient space; thus the codes are collections of such subspaces. We first develop a family of subspace codes, based upon the Koetter-Kschichang construction, which are efficiently list decodable. We show that, for a certain range of code rates, our list- decoding algorithm provides a better tradeoff between rate and decoding radius than the Koetter-Kschischang codes. We further improve these results by introducing multiple roots in the interpolation step of our list-decoding algorithm. To this end, we establish the notion of derivative and multiplicity in the ring of linearized polynomials. In order to achieve a better decoding radius, we take advantage of enforcing multiple roots for the interpolation polynomial. We are also able to list decode for a wider range of rates. Furthermore, we propose an alternative approach which leads to a linear-algebraic list-decoding algorithm. Rank-metric codes are suitable for error correction in the case where the network topology and the underlying network code are known (the coherent case). Gabidulin codes are a well-known class of algebraic rank-metric codes that meet the Singleton bound on the minimum rank-distance of a code. In this dissertation, we introduce a folded version of Gabidulin codes along with a list-decoding algorithm for such codes. Our list-decoding algorithm makes it possible to achieve the information theoretic bound on the decoding radius of a rank-metric code
-------------------------------------

10136321_183 - 0.999998924359 - technology_and_computing
[model, synchronization, error, datum, statistical, dynamical, neuron, optimization, chaotic, variable]

Synchronization and statistical methods for the data assimilation of HVc neuron models
Within the context of data assimilation, we describe the use of chaotic synchronization to overcome instabilities in the search space of the associated optimization problem and use numerical examples to demonstrate how the elimination of positive (conditional) Lyapunov exponents allows one to determine both the number and specific state variable choice of dynamical dimensions that must be measured in order to ensure feasibility of synchronization -based optimization techniques. We present a novel objective function based upon the chaotic synchronization- error metric that utilizes a strongly-coupled fiducial trajectory as a full dimensional surrogate of the measured data. This synchronization fiducial serves as the metric origin in the space of model dynamical rajectories, thus making the search over model parameters more informative and eliminating the need for collocation of the dynamical variables. The relationship between this (and other) proposed objective functions and the configuration functions of classical statistical physics are explored, including an additive-noise approximation to errors in the model expressed as a path integral over the joint probability distribution of the dynamics. From these considerations we create two statistical (derivative-free) numerical optimization algorithms on parallel processors that employ Monte Carlo techniques to evaluate the distribution of unknown state variables and model parameters; this is done on chaotic and electrophysiological twin-experiments, where we demonstrate how these methods are used to assist in the design of neuron models and stimulus (current injection) protocols. Finally, we report data-assimilation results, including model error estimates, of a model optimized to a current-clamp recording of a neuron from the High Vocal center (HVc) of the zebra finch birdsong neural pathway
-------------------------------------

10138139_183 - 0.999891285888 - technology_and_computing
[electron, gas, carrier, interface, oxide, density]

Modulation doping to control the high-density electron gas at a polar/non-polar oxide interface
A modulation-doping approach to control the carrier density of the high-density electron gas at a prototype polar/non-polar oxide interface is presented. It is shown that the carrier density of the electron gas at a GdTiO3/SrTiO3 interface can be reduced by up to 20% from its maximum value (~3 × 1014 cm−2) by alloying the GdTiO3 layer with Sr. The Seebeck coefficient of the two-dimensional electron gas increases concurrently with the decrease in its carrier density. The experimental results provide insight into the origin of charge carriers at oxide interfaces exhibiting a polar discontinuity.
-------------------------------------

10136296_183 - 0.999504418884 - technology_and_computing
[mimo, wireless, energy, rate, antenna, design, adaptation, speed, device, unique]

Towards Gigabit and Green 802.11 Wireless Networks
Wireless is an increasingly dominant communication medium.The continued quest for wireless connectivity in a multitude of mobile devices, along with the emerging bandwidth hungry applications, has resulted  in a huge growth of the wireless traffic.Multiple-Input Multiple-Output (MIMO) is considered the dominant technology to provide  gigabit wireless links, and to accommodate the increasing demand of speed over wireless.  By using multiple transmit and receive antennas, MIMO can support more reliable and fastercommunication.  But how efficient are the current MIMO systems? Our experiments with commodity MIMO 802.11n devices reveal that, the current MIMO wireless is low speedand energy hungry. The fundamental reason for MIMO devices' poor performance is the use of legacy 802.11a/b/g, single antenna designsover the multiple antenna, MIMO 802.11n setting. Specifically, the existing designs used over the new MIMO 802.11n devices, are oblivious to MIMO unique communication characteristics. They do not also consider that, MIMO speed comes at the cost of increased power consumption, proportional to the number of antennas.  In order to investigate solutions to these problems, this dissertation first experimentally studies the unique featuresof MIMO wireless and their impact on existing designs' performance.  Then, it revises the key mechanisms that control speed and energy over MIMO wireless, named Rate Adaptation, and MIMO Energy Save, and develops three systems.History-Aware Robust Rate Adaptation (HA-RRAA)  is our first step towards gigabit wireless.It opportunistically selects the best goodput PHY transmission rate  for legacy 802.11a/b/g networks by introducing novel mechanisms to capture short-term channel dynamics.Different from HA-RRAA, our MIMO Rate Adaptation (MiRA) proposal, seeks to identify the best goodput PHYtransmission rate in  MIMO 802.11n networks by considering the unique features of MIMO.Finally, MIMO  Energy Save seeks to select the optimal antenna setting at runtime to minimize energy consumption. Our proposals depart from existing designs in three fundamental ways.   They manage the unique MIMO communication modes in a distinct manner. They consider new metrics, to capture the tradeoffs between speed and power consumption.Our proposals also apply novel learning mechanisms  to capture the wireless channel dynamics. There are three main contributions in this dissertation. First, it builds a strong connection between wireless communication theory and wireless system design. Specifically, this dissertation provides the first experimental study of fundamental MIMO wireless communication tradeoffs (i.e. diversity vs. spatial multiplexing MIMO modes,  speed vs. number of antennas) using 802.11n standard-compliant commodity testbeds. Then, it uncovers their impact on existing designs' performance.   Second, it proposes novel and practical rate adaptation and energy save designs that consider MIMO unique characteristics, and are able deliverhigh performance gains. Third, this dissertation provides the first implementation and evaluation of MIMO rate adaptation and energy save using 802.11n standard-compliant commodity devices. The high performance gains in real world settings make our proposalsa significant step towards gigabit and green wireless networks.
-------------------------------------

10136911_183 - 0.985849836398 - technology_and_computing
[information, source, side, gaussian, quantization, scheme, wyner-ziv, analog, coding, correlation]

Extremely Low-Delay Coding of Gaussian Sources with Side Information at the Decoder
One of the innovations brought about by the emerging and thriving of wireless sensor networks is Wyner-Ziv (WZ) coding, in other words, lossy source coding with side information at the decoder. While previous work mostly focuses on using capacity-achieving codes to approach the Wyner-Ziv bound, which naturally introduces huge block lengths and huge delay, we study extremely low-delay Wyner-Ziv coding of Gaussian sources. Three related but distinct problems are considered. The first involves scalar quantization and scalar noiseless coding of the quantization indices when only decoder has access to side information. Under high-resolution assumptions and appropriately defined decodability constraints, the optimal quantization level density is conjectured to be periodic. The performance of variable-length coding with uniform quantization is also characterized. The results are then incorporated in predictive Wyner-Ziv coding for Gaussian sources with memory, and optimal prediction filters are numerically designed so as to strike a balance between maximally exploiting both temporal and spatial correlation and limiting the propagation of distortion due to occasional decoding errors. Finally, zero-delay schemes are also employed in transform coding with small block lengths, where the Gaussian source and side information are transformed separately with the premise that corresponding transform coefficient pairs exhibit good spatial correlation and minimal temporal correlation. For the specific source-side information pairs studied, it is shown that transform coding, even with a small block-length, outperforms predictive coding.     In the second part, we study the zero-delay joint source-channel coding (JSCC) problem of transmitting a Gaussian source over a Gaussian channel in Wyner-Ziv scenario. To achieve zero-delay, after applying scalar quantization to the source, the properly scaled analog information, namely the quantization error, is superimposed on the scaled digital information, i.e., the quantized source, and then transmitted. At the decoder, several decoding schemes are proposed. It is shown that all the schemes, when optimized over all related parameters, are superior to pure analog transmission for high enough correlation between source and side information. The robustness of one of the proposed Hybrid Digital Analog (HDA) schemes against varying channel and side information conditions is also compared with that of the purely analog scheme.         Since JSCC WZ problem is intimately related with JSCC without side information but with bandwidth expansion factor 2, in the third part we investigated the mapping method from 1-D source space to 2-D channel space which integrates HDA schemes into spiral mapping or pure analog mapping. The performance comparison with existing coding algorithms is also presented.
-------------------------------------

10136997_183 - 0.999996179542 - technology_and_computing
[esd, protection, ultrasonic, system, ic, imaging]

Electrostatic Discharge Protection and Circuits for Ultrasonic Imager-on-Chip
Electrostatic discharge (ESD) is one of the main reasons that cause integrated circuit (IC) reliability problem. Transient high voltage and current released by ESD in a very short time could produce latent damage or permanent breakdown in ICs, which may affect circuit performance, shorten product life time and increase manufacturing and assembling cost. Various ESD protection structures have been developed to protect ICs against ESD stress. However, with the advancement of IC technology and the increasing applications of mobile electronics, ESD protection design is facing severe challenges.  This dissertation presented two novel ESD protection structures. One is a very-low-triggering-voltage dual-direction silicon-controlled rectifier (VLTdSCR) with adjustable ESD critical parameters and dual-polarity high ESD protection ability, as well as insignificant ESD-induced parasitics. The other one is a new nano crossbar ESD protection device consisting of the SixOyNz composite, as dielectric, and two metal layers, as top and bottom electrodes. This device has insulator between electrodes and hence extremely-low leakage current, ideal for mobile applications where power consumption is a great concern. Among all the medical imaging technologies, pulse-echo ultrasonic imaging system features low risk, low cost and real time scan, etc. It interprets the reflected ultrasonic waves to build clear inner image of human body or tissues to aid doctor's diagnosis. In recent years, the portability of ultrasonic imaging system is attracting the attention of public, for its promising applications in harsh environments, such as rural villages or battlefields where stable power supply is usually not available. In this dissertation, the concept of ultrasonic imager-on-chip (UIC), integrating all the function modules in traditional ultrasonic imaging system onto one die, is proposed. It has the advantages of low cost, high power efficiency and great degree of mobility. UIC system architecture, design challenges and reliability issues, especially ESD protection, are discussed, too. A high-voltage pulse generator, as the driver of ultrasonic transducer and usually fabricated in HV process, is designed in a low-voltage (LV) silicon-on-insulator complementary-metal-oxide-semiconductor (SOI CMOS) technology based on an RC-biasing stacked LV device topology, demonstrating the idea of UIC system integration.
-------------------------------------

10129974_178 - 0.967181791126 - technology_and_computing
[flow, reference, control, bibliographical, information, database, decentralized]

Decentralized information flow control for databases
Includes bibliographical references (p. 177-194).
-------------------------------------

10139263_183 - 0.99986454241 - technology_and_computing
[algorithm, river, model, atmospheric, detection, event]

Detecting Atmospheric Rivers in Large Climate Datasets
Extreme precipitation events on the western coast of North America are often traced to an unusual weather phenomenon known as atmospheric rivers. Although these storms may provide a significant fraction of the total water to the highly managed western US hydrological system, the resulting intense weather poses severe risks to the human and natural infrastructure through severe flooding and wind damage. To aid the understanding of this phenomenon, we have developed an efficient detection algorithm suitable for analyzing large amounts of data. In addition to detecting actual events in the recent observed historical record, this detection algorithm can be applied to global climate model output providing a new model validation methodology. Comparing the statistical behavior of simulated atmospheric river events in models to observations will enhance confidence in projections of future extreme storms.

Our detection algorithm is based on a thresholding condition on the total column integrated water vapor established by Ralph et al. (2004) followed by a connected component labeling procedure to group the mesh points into connected regions in space. We develop an efficient parallel implementation of the algorithm and demonstrate good weak and strong scaling. We process a 30-year simulation output on 10,000 cores in under 3 seconds.
-------------------------------------

10137717_183 - 0.916755401366 - technology_and_computing
[crystal, chain, structure, solution, state]

The Spectroscopic-Assisted Studies on Photophysics and Mechanochemistry of Anthracene-Based Materials
A series of 2,6-dialkoxyanthracenes are synthesized and their photophysical properties in liquid solution and solid state are studied. Both spectral lineshapes and luminescence decay kinetics of these molecules in dilute solution or in a polymer matrix are identical.  We identify a conformational change in solution that leads to two emissive states that can interconvert.  In the solid state, the crystal structures of 2,6-dialkoxyanthracenes change significantly with lengthening alkoxy chains from methoxy to hexyloxy, evolving from herringbone structures to face-to-face two-dimensional sheets. The results from temperature-dependent experiments on single crystals indicate the intermolecular couplings between molecules with longer alkoxy chains are quite different from the J-type aggregates seen in crystalline anthracene. By tuning the side chain length and modifying the crystal structures of 2,6-dialkoxyanthracenes, we can change the luminescence properties of the Frenkel excitons that exist in the crystals.        The 9-tert-butylanthracene (TBA) can undergo a reversible photoisomerization to the Dewar isomer. The dependence of both the formation and reversion of the Dewar isomer on pressure was investigated using absorption and fluorescence spectroscopy with TBA doped polystyrene as a model system. It is demonstrated that both the photoisomerization and its reverse reaction rates are decreased with higher pressure.
-------------------------------------

10130413_178 - 0.999844336239 - technology_and_computing
[federal, force, nextgen, system, states, san, united, airport, francisco, decision]

Forecast-Based Decision Support for San Francisco International Airport: A NextGen Prototype System That Improves Operations during Summer Stratus Season
United States. Federal Aviation Administration (Air Force Contract FA8721-05-C-0002)
-------------------------------------

10139650_183 - 0.999833387022 - technology_and_computing
[dssoa, digital, surrogate, original]

The Digital-Surrogate Seal of Approval: a Consumer-oriented Standard
We propose the "Digital-Surrogate Seal of Approval" (DSSOA) as a simple way of describing digital objects created from printed books and other non-digital originals as surrogates for the analog original. The DSSOA denotes that a digitization accurately and completely replicates the content and presentation of the original. It can be used to express an intended goal during the planning stages of digitization and to guarantee the quality of existing digital surrogates. The DSSOA Criteria can be used to evaluate individual digital objects or entire completed collections. DSSOA is independent of production technologies and methodologies and focuses instead on the perspective of consumers — including libraries that rely on digital surrogates.
-------------------------------------

10134275_183 - 0.999839301247 - technology_and_computing
[power, amplifier, output, dbm, voltage, requirement, technology, cmo]

CMOS RF power amplifier design approaches for wireless communications
This dissertation focuses on the design of CMOS power amplifiers for modern wireless handsets, where stringent linearity requirements and high power efficiency are difficult to achieve simultaneously. CMOS technology has been an attractive technology for research in fully- integrated transceivers due to its low cost and high- integration capability, as well as its continuously improving high-frequency performance. Its advantages, however, come at the cost of continuously reduced breakdown voltages, low isolation and high power loss in the substrate. To address these limitations, a stacked-FET design technique is first developed to systematically divide the voltage stress among several transistors connected in series, allowing the use of a larger supply voltage. The voltage swing of each stacked device is added in phase to provide a larger output power to the load without the requirement of a large impedance transformation. To investigate this technique, a fully- integrated 20 dBm RF power amplifier is first implemented using 0.25-mum silicon-on-sapphire MOSFETs. By using triple-stacked FETs, the optimum load impedance for a 20 dBm power amplifier increases to 50 Omega so impedance transformation is not required at the output. Measurement of a single-stage linear power amplifier shows a small- signal gain of 17.1 dB and a saturated output power of 21.0 dBm with a power added efficiency (PAE) of 44.0% at 1.88 GHz. With an IS-95 code division multiple access (CDMA) modulated signal, the power amplifier shows average output power of 16.3 dBm and PAE of 18.7% with ACPR below -42 dBc. The concept is then further demonstrated at higher voltage and power level. A single-stage quadruple- stacked-FET linear power amplifier is presented using 0.28 -mum 2.5-V standard I/O FETs in a 0.13-&mu;m silicon-on- insulator (SOI) CMOS technology. The PA is designed to withstand up to 9 V of supply voltage before reaching its breakdown limit. The measured PA achieves a small-signal gain of 14.6 dB, a saturated output power of 32.4 dBm, and a PAE of 47% at 1.9 GHz with a 6.5-V supply. Using a reverse-link IS-95 CDMA modulated signal, the PA shows an average output power of up to 28.7 dBm with a PAE of 41.2% while meeting the adjacent channel power ratio requirement. The PA also shows an average output power of up to 29.4 dBm with a PAE of 41.4% while meeting the adjacent channel leakage ratio requirement of an uplink wideband code division multiple access (WCDMA) modulated signal. These performances are comparable to those of GaAs -based power amplifiers. To fully exploit the advantages of higher-speed CMOS technology and the availability of co -integrated digital circuitry, a digital-intensive transceiver architecture is explored as an alternative in the second part of the dissertation. A single-ended digitally-modulated power amplifier (DPA) is demonstrated in a 0.13-mum 1.2-V SOI CMOS technology, to be used in a multi-standard RF polar transmitter. The amplitude modulation is done by digitally controlling the number of activated unit amplifiers whose currents are summed at the output. The DPA is designed for multi-mode multi-band functionality by avoiding frequency-selective components, except for the final-stage output matching network. The measured DPA delivers a 24.9-dBm peak output power at 900 MHz with a maximum power efficiency of 62.7%. Similar high -efficiency performance is also exhibited at 1.92 GHz with a reconfigured matching network. By employing a digital pre-distortion technique, the DPA could meet linearity requirements for both the enhanced data rate for GSM evolution (EDGE) and WCDMA standards
-------------------------------------

10139208_183 - 0.918871746242 - technology_and_computing
[load, energy, project, plug, leed]

Plug Load Energy Analysis: The Role of Plug Loads in LEED Certification
Plug loads use 12% of site energy in U.S. office buildings. The relative importance of plug loads is rising and it is projected to increase more in years to come. We studied the predicted and simulated plug load energy consumption using data submitted to the U.S. Green Building Council for LEED certification. The study included 660 LEED for Commercial Interiors projects and 429 LEED for New Construction projects. This is the first study to analyze LEED submittal data related to plug load energy use. The submittal data from these projects was mined and statistically analyzed. The results show that 73% of the projects under LEED-CI that attempted the credit dedicated to plug loads earned 2 of 2 points available (90% or more of eligible equipment is ENERGY STAR rate). Additionally, we found that projects most frequently specify ENERGY STAR rated laptops, monitors, desktops and printers, whereas televisions, fax machines, refrigerators and dishwashers were less frequently specified. Under LEED-NC, the median peak plug load power intensity reported among the projects was 10.8 W/m2. Most of the projects complied with the LEED requirement of 25% process load energy use, with the median percentage being 25% and the 1st and 3rd quartiles ranging from 18% to 31%. 32% of the projects reported using eQUEST as an energy simulation tool. Only 5 of 429 LEED-NC projects reviewed attempted and were approved exceptional calculations for claiming energy savings on efficient plug loads or office equipment.
-------------------------------------

10136039_183 - 0.999958915636 - technology_and_computing
[channel, access, adaptation, wireless, rate, throughput, mac, multi-channel, multiple]

Multi-Channel and Multi-Rate Adaptation for High-Throughput Wireless Networks
As more mobile devices are becoming connected everyday, we face an unprecedented demand and growth for bandwidth in wireless networks.  The gradual shift from personal computers (PCs) to tablets and mobile phones brings new opportunities as well as challenges.  Users tend to be mobile and demand instant access to their contents.  Bandwidth usage ranges from simple browsing of web pages to audio and video streaming.  Due to the limited and scarce resources, care must be taken in order to utilize the spectrum more efficiently, to mediate access to a shared common medium, and, especially, to disseminate information. This dissertation focuses on designing a new channel access scheme as well as using its channel access time more efficiently through multiple channels and dynamic rate adaptation for high-throughput wireless networks.First, it provides a new multi-channel medium access control (MAC) for bandwidth exploitation.  Current IEEE 802.11 uses one common channel for both control and data packets because it is simple even though many other adjacent channels are available and left intact.  We can improve the throughput significantly by simply allowing other channels to be used.  The added complexity includes channel switching and additional control packets to ensure that the neighbors are aware of the channel selection.  Collision freedom is difficult in wireless environment, but I show that it can be achieved for a single transceiver without requiring temporal synchronization among nodes through an asynchronous split phase together with an observation phase as well as a unique handshake.Second, it provides an in-depth study and analysis of rate adaptation for single and multiple antenna systems, together with a new throughput enabled approach.  Throughput-based approach eliminates complexity of sender-initiated rate adaptation by incorporating errors and multiple access interference (MAI) implicitly.  Its main job is to simply find the best attainable throughput at the sender and keep using it.  My contributions include providing a collision-free multiple channels access MAC with no temporal synchronization and an efficient, robust, yet simple, throughput enabled rate adaptation for wireless networks.  Each proposed approach to multi-channel MAC and multi-rate adaptation is validated through analysis and extensive simulations and prototype implementations.  My work provides a better understanding on the current limitation of the wireless systems and new insights for further improvements, from the time node accessing the medium through multi-channel MAC to making the best use of its access time through an efficient and robust rate adaptation.
-------------------------------------

10135182_183 - 0.99999829785 - technology_and_computing
[network, server, processing, nic, performance, tcb, web, architecture, speed, study]

Accelerating I/O Processing in Server Architectures
Ethernet continues to be the most widely used network architecture today due to its low cost and backward compatibility with the existing Ethernet infrastructure. Driven by increasing networking demands of cloud workloads such as Internet search, web hosting etc, network speed rapidly migrates from 1Gbps to 10Gbps and beyond. High speed networks require general purpose servers to provide highly efficient network processing. However, traditional architectural designs have been focused on CPUs and often decoupled from I/O considerations, thus being inefficient for network processing.	In this study, we start with fine-grained driver and OS instrumentation to fully understand the network processing overhead over 10GbE on mainstream servers and make several new observations. Motivated by the studies, we propose a new server I/O architecture where DMA descriptor management is shifted from NICs to an on-chip network engine and descriptors are extended to address performance issues while processing packets. In addition, we also conduct extensive experiments on a real integrated NIC platform to understand the benefits of integrating NICs into CPU die.  Our studies reveal that simple NIC integration gains little help. We therefore propose an enhanced integrated NIC (EINIC) to address the performance issues of high speed networks. We also find that TCP Control Block (TCB) can pose a challenge in web servers with a large volume of concurrent sessions. Therefore, we also analyze challenges from a large number of concurrent web sessions on managing per-session TCB and propose a new TCB cache architecture to manage TCB data for web servers.   	As virtualization has gained resurgent interest and is becoming a key enabling technology in cloud infrastructures, understanding and improving virtualized network processing performance over high speed networks becomes critical. We conduct an experimental study of virtualized network performance on servers with 10GE networking to identify its performance bottlenecks. Then, we develop two VMM scheduler optimizations and design a simplified switch to reduce the network virtualization overhead. We also propose efficient architectural support by extending Direct Cache Access (DCA) to effectively avoid cache misses on packets in virtualized environment.
-------------------------------------

10135671_183 - 0.991365123723 - technology_and_computing
[channel, ion, abeta, structure, membrane, structural, flux]

Probing structural features of amyloid-beta Ion channels in membranes using A-beta mutants
A current hypothesis for the pathology of Alzheimer's disease (AD) proposes that amyloid-beta (Abeta) peptides induce uncontrolled, neurotoxic ion flux across cellular membranes. The resulting inability of neurons to regulate their intracellular concentration of ions, in particular calcium ions, has been associated with cell death and may thus contribute to cognitive impairment typical for AD. The mechanism of the ion flux is not fully understood since no experimentally based Abeta channel structures at atomic resolution are currently available, and few polymorphisms have been predicted by computational models. Structural models and experimental evidence suggest that Abeta channel is an assembly of loosely-associated mobile beta-sheet subunits. Using planar lipid bilayers, we present a study showing that amino acidic substitutions can be used to infer which residues are essential for channel structure and/or line the pore. We tested: Abeta42-F19P, Abeta42-F20C, Abeta42-A42C, and Abeta42-D1C. The substitution of F19P inhibited channel formation. All the cysteine mutants tested are capable of forming channels, but with different characteristics. This and other structural information on or in membrane are needed to aid the understanding of channel formation and structure. Additionally, this information should aid studies of drug design aiming to control unregulated Abeta ion fluxes
-------------------------------------

10130258_178 - 0.999768177283 - technology_and_computing
[space, system, design, development, stage, allocation, computer, early, architectural, 3-dimensional]

Development of a computer system for 3-dimensional space allocation in the early stages of architectural design.
Bibliography: leaves 94-97.
-------------------------------------

10136366_183 - 0.734012166855 - technology_and_computing
[transport, structure, coherent, larval, system, settlement, coastal, lc, upwelling, dynamics]

Coherent Structures and Larval Transport in the California Current System
In the California Current system (CCS), coherent structures such as jets and eddies strongly control the biological response to coastal upwelling. One of the outstanding problems in marine ecology is to understand the mechanisms of larval transport. Details of transport dynamics from nearshore to offshore, and subsequent delivery of coastally spawned propagules back to favorable settlement areas, strongly control marine population dynamics. Recent developments in applied dynamical systems allow the identification of coherent structure boundaries by numerical calculation of Lagrangian coherent structures (LCS), finite-time analogs of stable and unstable manifolds of hyperbolic fixed points. These material lines divide the flow into regions of disparate transport fate and illuminate the skeleton of filamentation and mixing. Using altimetric observations of the CCS, we show that LCS are less sensitive to noise and under-resolution than Eulerian metrics, and can map events undetectable by analysis of the frozen-time velocity field. However, small-scale dynamics such as lobes are not well resolved by altimetric observations. In an ocean circulation model of an idealized CCS, LCS are used to track filamentation and eddy-eddy interactions, important larval transport pathways from the shelf offshore. Filaments and packets caused by eddy-eddy interaction are stable to horizontal swimming perturbations, as predicted by the structural stability of hyperbolic manifolds. Retention in the upwelling jet is high and strongly patterns larval settlement. Settlement events are best correlated with integrated upwelling intensity and not upwelling relaxation events as commonly assumed. These studies demonstrate that coherent structures play a large role in coastal ecosystems, and that techniques and theorems from dynamical systems can provide insight into dominant transport pathways controlling coastal ecosystems.
-------------------------------------

10133325_183 - 0.997039500933 - technology_and_computing
[charmstone, cache, type, california, plantation]

The Plantation Cache  and a New Charmstone Type from Southern California
The discovery of the Plantation Cache has called attention to an unusual assemblage of archaeological remains. We have presented evidence that supports the hypothesis that they comprise the cached paraphernailia of a shaman. Ethnographic data from a variety of tribes indicate that the caching of ritual items was common, so it is not surprising that archaeologists might occasionally come across such a feature. The Plantation Cache contributes to the small sample of known caches of charmstones in northern California. We have also demonstrated—in light of the discovery of six nearly identical artifacts from northern California—that the Plantation Cache "football" variety of charmstone constitutes a distinctive type. We believe that the football-shaped charmstone represents a legitimate formal artifact type that has heuristic value for California archaeology. Criteria have been presented that identify the proposed football type of charmstone and discriminate the type from similar forms. We predict that additional examples will be identified, and hope that this paper contributes to that outcome.
-------------------------------------

10135070_183 - 0.95188694386 - technology_and_computing
[spectral, operator, riemann, fractal, lapidus, string, hypothesis, function]

Generalized Fractal Strings, Complex Dimensions and a Spectral Reformulation of the Riemann Hypothesis
The spectral operator was introduced for the first time by M. L. Lapidus and his collaborator M. van Frankenhuijsen in their theory of complex dimensions in fractal geometry cite,The corresponding inverse spectral problem was first considered by M. L. Lapidus and H. Maier in their work on a spectral reformulation of the Riemann hypothesis in connection with the question "Can One Hear The Shape of a Fractal String?". The spectral operator is defined on a suitable Hilbert space as the operator mapping the counting function of a generalized fractal string to the counting function of its associated spectral measure. It relates the spectrum of a fractal string with its geometry. The inverse spectral problem for vibrating fractal strings studied by M. L. Lapidus and H. Maier has a positive answer if and only if the Riemann zeta function has no zeros on Re(s)=D, where D is in (0,1) is the dimension of the fractal string. In this work, we provide a functional analytic framework allowing us to study the spectral operator. In particular, by determining the spectrum of the spectral operator, we give a necessary and sufficient condition providing its invertibility in the critical strip. We show that such a condition is  related to the location of the critical zeroes of the Riemann zeta function or equivalently that the spectral operator is invertible if and only if the Riemann hypothesis is true. As a result, the spectral operator is invertible for any D in (0,1)-{1/2} if and only if the Riemann hypothesis is true.The latter results provides a spectral reformulation of the Riemann hypotesis in terms of a rigorously defined map (the spectral operator). Hence, it sheds new light to the earlier work obtained by M. L. Lapidus and H. Maier and later revisited by M. L. Lapidus and M. van Frankenhuijsen.
-------------------------------------

10137705_183 - 0.999999117769 - technology_and_computing
[network, algorithm, wireless, diversity, mesh, delay, congestion, chapter]

Multi-hop routing for wireless mesh networks
Wireless Mesh networks have the potential to provide inexpensive and quick access to the internet for military communications, surveillance, education, healthcare and disaster management. This work caters to the growing high- bandwidth demands by providing low delay and high throughput by designing efficient, robust routing algorithms for wireless mesh networks. Chapters 2 and 3 of this dissertation describe adaptive routing algorithms that opportunistically route the packets in the absence of reliable knowledge about channel statistics and the network model. We design two adaptive routing algorithms, Distributed Opportunistic Routing (d-AdaptOR) and No Regret Routing (NRR), which minimize the expected number of transmissions and thus improving the throughput. The remainder of the dissertation concerns with the design routing algorithms to avoid congestion in the network. In Chapter 4, we describe a Distributed Opportunistic Routing algorithm with Congestion Diversity (ORCD) which employs receiver diversity and minimizes end-end delay. In Chapter 5, we present the Congestion Diversity Protocol (CDP), a distributed routing protocol for 802.11-based multi-hop wireless networks that combines important aspects of shortest-path and back-pressure routing to achieve improved end-end delay performance. This work reports on a practical (hardware and software) implementation of CDP in an indoor Wi-Fi testbed
-------------------------------------

10131192_183 - 0.999510072628 - technology_and_computing
[pavement, dynamic, load, truck, framework, research]

Truck Pavement Interactions: Requisite Research
A framework for consideration of the effects of dynamic loads on pavement performance is presented. The paper discusses requisite research which will permit both the pavement engineer and the truck designer to effectively utilize such a framework to arrive at optimal solutions which will result in overall savings to the agencies responsible for design, construction, maintenance, and rehabilitation of pavement facilities and to the users of the facilities as well.
      Included is a discussion of needed research to evaluate: the dynamic response of jointed portland cement concrete pavements to load, the influence of dynamic loads on the development of rutting in asphalt concrete pavements, and the development of new suspension concepts to reduce dynamic load variations with pavement roughness.
      Also included are recommendations for field measurement procedures to truly identify dynamic load spectra, methods to identify pavement profiles to reflect the effects of such profiles on truck suspension performance, and measurements to evaluate the methodology developed within the proposed framework.
-------------------------------------

10139520_183 - 0.726794166296 - technology_and_computing
[performance, result, memory, optimal, multi-core, many-core, platform, algorithm, optimization]

Multi-core and Many-core Shared-memory Parallel Raycasting Volume Rendering Optimization and Tuning
Given the computing industry trend of increasing processing capacity by adding more cores to a chip, the focus of this work is tuning the performance of a staple visualization algorithm, raycasting volume rendering, for shared-memory parallelism on multi-core CPUs and many-core GPUs. Our approach is to vary tunable algorithmic settings, along with known algorithmic optimizations and two different memory layouts, and measure performance in terms of absolute runtime and L2 memory cache misses. Our results indicate there is a wide variation in runtime performance on all platforms, as much as 254percent for the tunable parameters we test on multi-core CPUs and 265percent on many-core GPUs, and the optimal configurations vary across platforms, often in a non-obvious way. For example, our results indicate the optimal configurations on the GPU occur at a crossover point between those that maintain good cache utilization and those that saturate computational throughput. This result is likely to be extremely difficult to predict with an empirical performance model for this particular algorithm because it has an unstructured memory access pattern that varies locally for individual rays and globally for the selected viewpoint. Our results also show that optimal parameters on modern architectures are markedly different from those in previous studies run on older architectures. And, given the dramatic performance variation across platforms for both optimal algorithm settings and performance results, there is a clear benefit for production visualization and analysis codes to adopt a strategy for performance optimization through auto-tuning. These benefits will likely become more pronounced in the future as the number of cores per chip and the cost of moving data through the memory hierarchy both increase.
-------------------------------------

10129771_178 - 0.984575835371 - technology_and_computing
[system, design, affordable, desalination, humidification, small-scale, bibliographical, thermal, reference, dehumidification]

Thermal design of humidification dehumidification systems for affordable and small-scale desalination
Includes bibliographical references (p. 249-262).
-------------------------------------

10135700_183 - 0.995148240937 - technology_and_computing
[traffic, model, emission, efficient, mesoscopic, microscopic, tool, simulation]

Incorporating Vehicular Emissions into an Efficient Mesoscopic Traffic Model: An Application to the Alameda Corridor, CA
We couple EMFAC with a dynamic mesoscopic traffic model to create an efficient tool for generating information about traffic dynamics and emissions of various pollutants (CO2, PM10, NOX, and TOG) on large scale networks. Our traffic flow model is the multi-commodity discrete kinematic wave (MCDKW) model, which is rooted in the cell transmission model but allows variable cell sizes for more efficient computations. This approach allows us to estimate traffic emissions and characteristics with a precision similar to microscopic simulation but much faster. To assess the performance of this tool, we analyze traffic and emissions on a large freeway network located between the ports of Los Angeles/Long Beach and downtown Los Angeles. Comparisons of our mesoscopic simulation results with microscopic simulations generated by TransModeler under both congested and free flow conditions show that hourly emission estimates of our mesoscopic model are within 4 to 15 percent of microscopic results with a computation time divided by a factor of 6 or more. Our approach provides policymakers with a tool more efficient than microsimulation for analyzing the effectiveness of regional policies designed to reduce air pollution from motor vehicles.
-------------------------------------

10138381_183 - 0.997864300405 - technology_and_computing
[site, parameter, procedure, gmpe, ground]

Site effects in parametric ground motion models for the GEM-PEER Global GMPEs Project
We review site parameters used in ground motion prediction equations (GMPEs) for various tectonic regimes and describe procedures for estimation of site parameters in the absence of site-specific data. Most modern GMPEs take as the principal site parameter the average shear wave velocity in the upper 30 m of the site (Vs30) either directly or as the basis for site classification into categories. Three GMPEs developed for active regions also use basin depth parameters. We review estimation procedures for Vs30 that utilize surface geology, terrain-based site categories, ground slope, or combinations of these. We analyze the relative efficacy of those procedures using a profile data set from California assembled in a recent NGA project. The results indicate that no single procedure is most effective and that prediction dispersion is lower for young sediments than for stiff soils or rock.
-------------------------------------

10130129_178 - 0.973976004943 - technology_and_computing
[design, doped, measurement, reference, bibliographical, single, electron, silicon, phosphorus, transistor]

On the design of single electron transistors for the measurement of spins in phosphorus doped silicon
Includes bibliographical references (p. 66-67).
-------------------------------------

10139467_183 - 0.999505278812 - technology_and_computing
[datum, web, semantic, biohackathon, life, tool]

The 3rd DBCLS BioHackathon: improving life science data integration with Semantic Web technologies
Abstract
				
				
					
						Background
					BioHackathon 2010 was the third in a series of meetings hosted by the Database Center for Life Sciences (DBCLS) in Tokyo, Japan. The overall goal of the BioHackathon series is to improve the quality and accessibility of life science research data on the Web by bringing together representatives from public databases, analytical tool providers, and cyber-infrastructure researchers to jointly tackle important challenges in the area of in silico biological research.
				
				
					
						Results
					The theme of BioHackathon 2010 was the 'Semantic Web', and all attendees gathered with the shared goal of producing Semantic Web data from their respective resources, and/or consuming or interacting those data using their tools and interfaces. We discussed on topics including guidelines for designing semantic data and interoperability of resources. We consequently developed tools and clients for analysis and visualization.
				
				
					
						Conclusion
					We provide a meeting report from BioHackathon 2010, in which we describe the discussions, decisions, and breakthroughs made as we moved towards compliance with Semantic Web technologies - from source provider, through middleware, to the end-consumer.
-------------------------------------

10137142_183 - 0.99998682162 - technology_and_computing
[graphene, interface, mobility, carrier, tunnel, nernst, dirac, magnetic, high, lsmo]

Experimental Studies of Oxide Magnetic Tunnel Junctions and Graphene
The spin state of a ferromagnet (FM) on its surface can deviate significantly from that in the bulk. This effect could be strongly orientation-dependent in manganites due to the strong spin-orbit interaction. We have successfully fabricated high-quality (110)-oriented La0.7Sr0.3MnO3 (t) /SrTiO3 (3ML)n superlattices and systematically studied their crystal structure as well as interface magnetism. Compared to the (100)-oriented counterparts, LSMO has a thinner dead-layer and a higher interface moment at the (110)-orientated LSMO/STO interface. The magnetism of the manganite interface could be manipulated by taking advantage of the orientation-dependent nature of the exchange interactions.To engineer the interface between the tunnel barrier and LSMO, we have developed a high quality Al2O3 (HfO2) tunnel barrier in LSMO/A2O3/Co stacking structure with A2O3 (HfO2) grown <italic>ex-situ<italic> by the Atomic Layer Deposition as well as LSMO/STO/Co with LSMO and STO epitaxially grown by Pulsed Laser Deposition in both (100) and (110) orientations. Our specially designed shadow mask technique ensures a high yield and high performance of magnetic tunnel junction devices. The magneto-transport measurements show very interesting results.The thermoelectric properties of graphene have been extensively studied both experimentally and theoretically. The exotic band structure of graphene leads to unusual thermoelectric properties which are sensitive to the carrier mobility. However, all the previous experiments were based on graphene samples with fixed mobility and make comparisons between different samples. Recently, we showed that it is possible to tune the carrier mobility of the same graphene device over a wide range. We adopted this method and systematically studied the magneto- Seebeck and Nernst effects for different mobility values. The crossover behavior of the Seebeck signal reported before around Charge Neutral Point is related to the splitting of zeroth Landau Level. Moreover, we demonstrate that the Nernst peak linearly depends on the carrier mobility in graphene.Besides, we find that the empirical relation between the longitudinal and Hall resistivities and its counterpart between the Seebeck and Nernst coefficients hold surprisingly well for graphene in the quantum transport regime except near the Dirac point. The validity of the relations is cross-examined by independently varying the magnetic field and the carrier density in graphene. Our experimental results validate both derivative relations for massless Dirac fermions except near the Dirac point.
-------------------------------------

10139315_183 - 0.999907374182 - technology_and_computing
[datum, index, access, time]

Parallel In Situ Indexing for Data-intensive Computing
As computing power increases exponentially, vast amount of data is created by many scientific re- search activities. However, the bandwidth for storing the data to disks and reading the data from disks has been improving at a much slower pace. These two trends produce an ever-widening data access gap. Our work brings together two distinct technologies to address this data access issue: indexing and in situ processing. From decades of database research literature, we know that indexing is an effective way to address the data access issue, particularly for accessing relatively small fraction of data records. As data sets increase in sizes, more and more analysts need to use selective data access, which makes indexing an even more important for improving data access. The challenge is that most implementations of in- dexing technology are embedded in large database management systems (DBMS), but most scientific datasets are not managed by any DBMS. In this work, we choose to include indexes with the scientific data instead of requiring the data to be loaded into a DBMS. We use compressed bitmap indexes from the FastBit software which are known to be highly effective for query-intensive workloads common to scientific data analysis. To use the indexes, we need to build them first. The index building procedure needs to access the whole data set and may also require a significant amount of compute time. In this work, we adapt the in situ processing technology to generate the indexes, thus removing the need of read- ing data from disks and to build indexes in parallel. The in situ data processing system used is ADIOS, a middleware for high-performance I/O. Our experimental results show that the indexes can improve the data access time up to 200 times depending on the fraction of data selected, and using in situ data processing system can effectively reduce the time needed to create the indexes, up to 10 times with our in situ technique when using identical parallel settings.
-------------------------------------

10129695_178 - 0.996209900896 - technology_and_computing
[architecture]

Needs and goals in urban architecture.
Massachusetts Institute of Technology. Dept. of Architecture. Thesis. 1971. B.Arch.
-------------------------------------

10134216_183 - 0.94202207631 - technology_and_computing
[energy, transfer, frequency, wave, tide, semidiurnal, high, home, low, site]

Identifying two steps in the internal wave energy cascade
In addition to their own convective breaking, the energetic baroclinic tides at a generation site stimulate the weaker non-linear interactions that might comprise the more-typical open ocean energy cascade. Observations of two classes of nonlinear interactions are presented. The first transfers energy from the semidiurnal tide to near- diurnal subharmonics. The second transfers energy from low -frequency, high-shear waves to high frequencies. Evidence is shown for wave-wave interactions between the low-mode, semidiurnal tide and high-mode, near-inertial motions. Profiles of velocity and density were collected aboard the Research Platform FLIP, at two sites during the Hawaii Ocean Mixing Experiment (HOME). In HOME Nearfield, significant bicoherences are observed between the dominant low-mode semidiurnal tide and opposing pairs of near- diurnal waves of vertical scale O(100 m). Growth rates of diurnal waves during each fortnightly cycle agree with theoretical predictions for the Parametric Subharmonic Instability (PSI) mechanism. At the Farfield location, 430 km from the generation site, near-diurnal waves are also observed but are not significantly bicoherent with the semidiurnal tide. Triple correlations between low frequency vertical shears and high frequency Reynolds stresses, -<uiw dUi/dz>, are used at both HOME sites to estimate energy transfers from low frequencies, including both wind-generated motions and PSI subharmonics, to high frequencies. Energy bispectra show significant energy transfers to pairs of waves with nearly identical frequency. However, wavenumber resonances do not fit either the Induced Diffusion (ID) or eikonal models of interaction. Peak transfer rates in the Nearfield are of order 1 x 10⁻⁷ W kg⁻¹, while transfer rates in the Farfield are 3-4x smaller. Nearfield energy transfers are larger than local turbulence dissipation, but fall within an order of magnitude. Farfield energy transfers and turbulence dissipation agree within a factor of 3 throughout the measurement profile. The suggestion is that the HOME observations of energy transfer from the semidiurnal tide to near-diurnal waves via PSI, and subsequently from low to high frequency waves, represent a skeleton of the open-ocean energy cascade
-------------------------------------

10136082_183 - 0.969049272418 - technology_and_computing
[density, model, gang, method, spatial, information, region, street, estimation, rivalry]

Incorporating Spatial Information into Density Estimates and Street Gang Models
The spatial features within a region influence many processes in human activity.  Mountains, lakes, oceans, rivers, freeways, population densities, housing densities, and road networks are examples of geographical factors that impact spatial behaviors.  Separated into two parts, the work presented here incorporates this information into both density estimation methods and models of street gang rivalries and territories.Part I discusses methods for producing a probability density estimate given a set of discrete event data.  Common methods of density estimation, such as Kernel Density Estimation, do not incorporate geographical information.   Using these methods could result in non-negligible portions of the support of the density in unrealistic geographic locations.  For example, crime density estimation models that do not take geographic information into account may predict events in unlikely places such as oceans, mountains, etc.  To obtain more geographically accurate density estimates, a set of Maximum Penalized Likelihood Estimation methods based on Total Variation norm and H1 Sobolev semi-norm regularizers in conjunction with a priori high resolution spatial data is proposed.   These methods are applied to a residential burglary data set of the San Fernando Valley using geographic features obtained from satellite images of the region and housing density information.Part II addresses the behaviors and rivalries of street gangs and how the spatial characteristics of the region affect the dynamics of the system.  Gangs typically claim a specific territory as their own, and they tend to have a set space, a location they use as a center for their activities within the territory.   The spatial distribution of gangs influences the rivalries that develop within the area.  One stochastic model and one deterministic model are proposed, providing different types of outputs.  Both models incorporate important geographical features from the region that would inhibit movement, such as rivers and large highways.  In the stochastic method, an agent-based model simulates the creation of street gang rivalries.  The movement dynamics of agents are coupled to an evolving network of gang rivalries, which is determined by previous interactions among agents in the system.  Basic gang data, geographic information, and behavioral dynamics suggested by the criminology literature are integrated into the model. The deterministic method, derived from a stochastic approach, modifies a  system of partial differential equations from a model for  coyotes.  Territorial animals and street gangs often exhibit similar behavioral characteristics.  Both groups have a home base and mark their territories to distinguish claimed regions.  To analyze the two methods, the Hollenbeck policing division of the Los Angeles Police Department is used as a case study.
-------------------------------------

10135595_183 - 0.999999414433 - technology_and_computing
[network, function, linear, computing, target, class, message, capacity]

Network coding for function computation
In this dissertation, the following network computing problem is considered. Source nodes in a directed acyclic network generate independent messages and a single receiver node computes a target function f of the messages. The objective is to maximize the average number of times f can be computed per network usage, i.e., the "computing capacity". The network coding problem for a single-receiver network is a special case of the network computing problem in which all of the source messages must be reproduced at the receiver. For network coding with a single receiver, routing is known to achieve the capacity by achieving the network min-cut upper bound. First we extend the definition of min-cut to the network computing problem and show that the generalized min-cut is an upper bound on the maximum achievable rate and is tight for computing (using coding) any target function in multi-edge tree networks and for computing linear target functions in any network. We also study the bound's tightness for different classes of target functions. In particular, we give a lower bound on the computing capacity in terms of the Steiner tree packing number and a different bound for symmetric functions. We also show that for certain networks and target functions, the computing capacity can be less than an arbitrarily small fraction of the min-cut bound. Next, we study the use of linear codes for network computing in single-receiver networks with various classes of target functions of the source messages. Such classes include reducible, injective, semi-injective, and linear target functions over finite fields. Computing capacity bounds are given with respect to these target function classes for network codes that use routing, linear coding, or nonlinear coding. Lastly, we consider the scenario in which a set of sources generate messages in a network over a finite field alphabet and a receiver node demands an arbitrary linear function of these messages. We formulate an algebraic test to determine whether an arbitrary network can compute linear functions using linear codes. We identify a class of linear functions that can be computed using linear codes in every network that satisfies a natural cut-based condition. Conversely, for another class of linear functions, we show that the cut- based condition does not guarantee the existence of a linear coding solution. For linear functions over the binary field, the two classes are complements of each other
-------------------------------------

10129759_178 - 0.999987192813 - technology_and_computing
[pin, system, process, insertion, development, reference, control, circuit, bibliographical, board]

Development of control system to automate the printed circuit board pin insertion process
Includes bibliographical references (p. 150-154).
-------------------------------------

10129928_178 - 0.999978517749 - technology_and_computing
[cns-0642719, system, piece-wise, continuous, foundation, national, order, control, career, science]

Safety control of piece-wise continuous order preserving systems
National Science Foundation (U.S.) (NSF CAREER AWARD # CNS-0642719)
-------------------------------------

10136969_183 - 0.849629632224 - technology_and_computing
[power, current, constraint, source, grid, method, programming, voltage, optimization, noise]

Power network verication and optimization at planning stages
Power integrity has become a critical issue in nano-scale VLSI design. With technology scaling, the circuit integration density grows rapidly. However, the number of IO's dedicated for power does not scale up accordingly due to limited advancement in packaging technology. The increase of total current causes large voltage drops in the on-chip power grid, and the increase of clock frequencies results in large Ldi/dt noise due to the inductive effect of the power grid. Voltage drops degrade circuit timing performance while voltage bounces may cause reliability issues. On the other hand, the decrease in supply voltage leads to a smaller noise margin which makes the design of on-chip power grid an even more challenging task. As a result, full-chip power grid verification and optimization have become essential for reliable chip design. In the first part of this thesis, we propose novel methods of generating the worst-case noise for early power distribution system verification. These methods take into account the effect of the transition time of load currents, and thus allow a more realistic worst-case noise prediction. In the case of one current source, we introduce a dynamic programming algorithm on the time- domain impulse response of the power distribution system, and a modified Knuth-Yao Quadrangle Inequality Speedup is developed which reduces the time complexity of the algorithm to O(nmlog n), where n is the number of discretized current values and m is the number of zeros of the system impulse response. In the case of multiple current sources, the dynamic programming algorithm is extended to generate the worst-case noise subject to a set of hierarchical current constraints. We show that the hierarchical constraints can be generalized into submodular polyhedron constraint and our algorithms still work. Furthermore, for other general magnitude and slope constraints of current sources, we propose the solutions by network flow and submodular flow which are more efficient than direct linear programming solution. The second part of the thesis describes a power grid sizing method to minimize the worst voltage drop over all test locations and current source distributions. We reduce the original problem into a convex programming problem whose objective is to minimize the maximum effective resistance between the current entry node and all current exit nodes under the constraint of constant total wire area. In order to solve the convex programming problem efficiently, we adopt a Krylov space method to evaluate the effective resistances simultaneously and deduce a simple formula to update the derivative of effective resistance relative to perturbation of wire widths gradually. The proposed optimization method can also be applied to power grids in the real world, which are required to have small effective resistances among power stations for reducing the power losses during long distance transmission. Finally, we propose a method for dynamic power distribution network verification and optimization at early design stages. This approach predicts and minimizes the worst total voltage violation area for all outputs of a given on-chip power grid with multiple current sources. We assume duty-cycle constraints, group magnitude constraints and transition time constraints on current sources which makes the prediction more realistic. Simultaneously, we minimize the worst violation area through the allocation of decoupling capacitors (Decap) and controlled equivalent series resistors (ESR). Based on the simulation of adjoint network, the sensitivity of the violation area relative to current sources, Decap and ESR can be derived and the sequential quadratic programming method is adopted for optimization
-------------------------------------

10133860_183 - 0.999704259544 - technology_and_computing
[model, datum, transaction, modeling, series, time, frequency, stock, framework]

Statistical Modeling of Marked Point Processes and (Ultra-)High Frequency Data
The studies of stock transaction data, i.e., both the regularly-spaced high frequency data and the irregularly-spaced ultra-high frequency data, have been among the frontiers of modern financial data analysis. One of those data sets is the Trade and Quote (TAQ) data from the New York Stock Exchange (NYSE), which is a collection of all stock transaction information (e.g., the transaction date, time, prices and volumes, etc.) for every trading day. The analysis of the intraday transaction data still remains highly challenging today, especially on the statistical modeling aspects.In this research, two new statistical modeling frameworks, namely, the Multi-Logit Mixture Autoregressive (MLMAR) models and the multivariate Mixture Transition Distribution (MMTD) models, are proposed respectively to handle above two types of financial data. The models are the univariate and multivariate generation of the MTD-type time series models. The MLMAR time series model is a univariate time series model for the regularly-spaced intraday stock prices, which includes the exogenous information, such as the transaction volumes, the trading frequencies or any other market information, into the modeling framework. The MMTD model is a modeling framework for marked point processes in general, and ultra-high frequency transaction data in particular.In both modeling frameworks, we solve a series of problems, which include the model specification, parameter estimation, prediction methodology and their applications to the stock transaction data. To show the capacity and advantage of the new models over the existing models, we also compare the new models with those benchmark models and show the new models' advantages in terms of either describing the underlying data generating process or prediction performance. For each class of time series model, potential extensions and related modeling issues are also discussed thereafter.
-------------------------------------

10136073_183 - 0.998845286482 - technology_and_computing
[information, architecture, architectural, technology, building]

The Architecture of Information at Plateau Beaubourg
During the course of the 1960s, computers and information networks made their appearance in the public imagination. To architects on the cusp of architecture's postmodern turn, information technology offered new forms, metaphors, and techniques by which modern architecture's technological and utopian basis could be reasserted. Yet by the end of the 1970s, when computers and networks fully appeared in the workplace, schools, and even homes, architects had all but abandoned information technology as a source of architectural ideas, relegating computers to a supporting role in architectural practice where they performed only the most mundane of tasks, one from which they would emerge only two decades later.This dissertation argues that architecture in the 1970s did not in fact retreat from information technology but rather that the changing nature of information technology demanded new modes of architectural thinking that destabilized the traditional discursive function of the machine underpinning modern architecture. It examines various ways in which information technology influenced architectural thinking during this troubled period of transition through the historical treatment of a single case study, the Centre Georges Pompidou in Paris (or Beaubourg, as it was and is still known). It considers on the building's role in a more general program of social and cultural reorganization in the information society, from the original conception of the building as an enormous information processing machine to the reception of Piano and Rogers' building in the years following its completion. In chapters examining the informational ideas in the competition brief, the architectural responses to the competition, the sources for the winning scheme by Piano and Rogers and its relationship to technological utopianism in British architecture, the development of the final building and its challenge to the megastructure paradigm, and the privileging of the user in new techniques of architectural programming first deployed in a cultural building at Beaubourg, this dissertation tries to identify a broad spectrum of modes of engagement between architecture and information technology beyond the tool-based approaches prevalent today.
-------------------------------------

10133753_183 - 0.982384816483 - technology_and_computing
[optimization, problem, reconstruction, structure, global, algorithm, framework, dissertation, scene, geometry]

From pictures to 3D : global optimization for scene reconstruction
Reconstructing the three-dimensional structure of a scene using images is a fundamental problem in computer vision. The geometric aspects of 3D reconstruction have been well- understood for a decade, but the involved optimization problems are known to be highly non-convex and difficult to solve. Traditionally, these problems are tackled using heuristic initializations followed by local, gradient- based optimization algorithms, which are prone to being enmeshed in local minima. In contrast, this dissertation proposes powerful, global optimization methods to derive provably optimal, yet practical, algorithms for estimating 3D scene structure and camera motion. This dissertation develops a branch and bound framework to solve several well-established problems in multiview geometry to their global optima, with a certificate of optimality. The framework relies on the construction of efficient and tight relaxations to the involved non-convex problems, using modern convex optimization methods. The underlying geometry of the task is exploited to restrict the search space to a small, fixed number of dimensions, which alleviates the worst case exponential complexity of branch and bound in practice. The dissertation begins by deriving optimal solutions to triangulation and camera pose estimation for an arbitrary number of views and points, using extensions to the theory of fractional programming. Next, the framework is amplified to solve the conceptually important affine and metric reconstruction stages of stratified autocalibration to their global optima. Additionally, an algorithm for directly upgrading a projective reconstruction to a metric one is proposed, based on elegant real algebraic geometry methods for global optimization of polynomial systems. Further, large- scale bilinear programs that arise in diverse applications such as shape from exemplar models and non-rigid structure from motion, are globally optimized using a novel branching strategy that exploits problem structure typical to 3D reconstruction. The final part of the dissertation develops a complete pipeline for real-time 3D reconstruction using stereo images and straight line features. The core structure from motion problem constitutes efficient optimization of an overdetermined system of polynomials that is fast enough to be used in a robust hypothesize-and-test framework. The algorithm has already found application in the autonomous navigation system for the well-known humanoid robot, ASIMO
-------------------------------------

10135372_183 - 0.9254393343 - technology_and_computing
[object, recognition, image, contextual, feature, scene, appearance]

Beyond appearance features : contextual modeling for object recognition
The goal of object recognition is to locate and identify instances of an object within an image. Examples of this task include recognition of faces, logos, scenes and landmarks. The use of this technology can be advantageous in guiding a blind user to recognize objects in real time and augmenting the ability of search engines to permit searches based on image content. Traditional approaches to object recognition use appearance features - e.g., color, edge responses, texture and shape cues - as the only source of information for recognizing objects in images. These features are often unable to fully capture variability in object classes, since objects may vary in scale, position, and viewpoint when presented in real world scenes. Moreover, they may introduce noisy signals when objects are occluded and surrounded by other objects in the scene, and obscured by poor image quality. As appearance features are insufficient to accurately discriminate objects in images, an object's identity can be disambiguated by modeling features obtained from other object properties, such as the surroundings and the composition of objects in real world scenes. Context, obtained from the object's nearby image data, image annotations and the presence and location of other objects, can help to disambiguate appearance inputs in recognition tasks. Recent context-based models have successfully improved recognition performance, however there exist several unanswered questions with respect to modeling contextual interactions at different levels of detail, integrating multiple contextual cues efficiently into a unified model and understanding the explicit contributions of contextual relationships. Motivated by these issues, this dissertation proposes novel approaches for investigating new types of contextual features and integrating this knowledge into appearance based object recognition models. We analyze the contributions and trade -offs of integrating context and investigate contextual interactions between pixels, regions and objects in the scene. Furthermore, we study context as (i) part of recognizing objects in images and (ii) as an advocate for label agreement to disambiguate object identity in recognition systems. Finally, we harness these discoveries to address other challenges in object recognition, such as discovering object categories in weakly labeled data
-------------------------------------

10135097_183 - 0.908441250139 - technology_and_computing
[flow, ray, model, interaction, fin, fish, flexible, structure, motion]

Flow interaction with highly flexible structures
Studying the interaction between fluid and structure is an essential step towards the understanding of many engineering and physical problems, from the flow instability of structures to the biolocomotion of insects, birds and fishes. The simulation of such problems is computationally challenging. This justifies the attempts to develop more sophisticated and more efficient numerical models of fluid-solid interactions. In this dissertation, we proposed numerical models both in potential flow and fully viscous flow for the interaction of immersed structure with a strongly unsteady flow. In particular we have developed efficient approaches to study two groups of problems, the flow interaction with skeleton-reinforced fish fins and flow interaction with highly flexible bluff bodies. Fins of bony fishes are characterized by a skeleton-reinforced membrane structure consisting of a soft collagen membrane strengthened by embedded flexible rays. Morphologically, each ray is connected to a group of muscles so that the fish can control the rotational motion of each ray individually, enabling multi-degree of freedom control over the fin motion and deformation. We have developed fluid-structure interaction models to simulate the kinematics and dynamic performance of a structurally idealized fin. The first method includes a boundary- element model of the fluid motion and a fully-nonlinear Euler-Bernoulli beam model of the embedded rays. In the second method, we use an improved immersed boundary approach. Using these models, we study thrust generation and propulsion efficiency of the fin at different combinations of parameters at both high-Re and intermediate-Re flow. Effects of kinematic as well as structural properties are examined. It has been illustrated that the fish's capacity to control the motion of each individual ray, as well as the anisotropic deformability of the fin determined by distribution of the rays (especially the detailed distribution of ray stiffness), is essential to high propulsion performance. We also note that this structural design is a recurring motif in nature. To understand flow-induced vibrations of deformable bluff objects, we study dynamics of a pressurized elastic ring within a uniform flow by using an immersed-boundary algorithm. The vibration of the flexible ring is decomposed into a pitching and flexible bending modes. Across the resonance region, nonlinear behavior of the ring is studied and the hydrodynamic loads are recorded. It is observed that within the resonance region, the lift force demonstrates a beating phenomenon reminiscent of findings from reduced models and low-degree -of-freedom systems
-------------------------------------

10130255_178 - 0.993801353991 - technology_and_computing
[space, design, leaf, reference, housing, role, bibliographical, conceptualization]

Open space in housing : a conceptualization of the role of design
Includes bibliographical references (leaves 139-141).
-------------------------------------

101650_108 - 0.999505829197 - technology_and_computing
[system, x-ray, breast]

A Novel Detection System Consisting of a Large Area Sensor and a Multi-Cell Si-Pad Array Operated in Spectroscopic Mode for X-Ray Breast Imaging
<p>The ability of coherent x-ray scatter to provide the molecular structure of breast tissues could add a new dimension in x-ray breast imaging capable of tracking the molecular structural changes during disease progression and (if improving the sensitivity to low-contrast lesions without increasing the radiation dose. Work is under way to build a laboratory prototype dual-sensor breast-imaging scanning system, which combines the diagnostic information from both the transmitted primary and the forward scattered x-rays. This required the design and development of a coherent x-ray scatter detection system based on a high-resistivity multi-element 2D Si-pad array, a multi-channel low-noise pulse processing front-end electronics chip, the XA1.3, and a new DAQ system. Results on the characterization and optimization of the detector-readout electronics-DAQ system and its performance to measure diffraction signatures are presented.</p>
-------------------------------------

10137160_183 - 0.999799697193 - technology_and_computing
[network, nanowire, electrical, thermoelectric, transport, property, device, surface]

Growth, Characterization, Modeling and Device Applications of Semiconductor Nanowire Networks
Semiconducting nanowire networks composed specifically of indium phosphide or silicon are developed with the goal of understanding their electrical, thermal and optoelectronic properties while developing scalable, manufacturable solutions to a number of problems of contemporary interest to society, with particular emphasis on direct conversion of heat to electricity. Nanowire networks are grown by metal organic chemical vapor deposition on non-single crystalline surfaces leading to highly interconnected networks of nanowires capable of long-range three-dimensional transport while retaining many of the unique properties of highly conned nanowire structures and displayingadvantageous and unique properties such as mechanical flexibility. Growth of semiconducting nanowire networks is discussed in depth, especially relating to the role of the non-single crystalline surfaces from which they grow and morphological changes associated with doping. Finite element simulations suggest that the physical intersectionspresent within a nanowire network are found to play a complex and potentially useful role in thermal transport and in electrical transport through experiment, demonstrating quantized conductance for the first time at room temperature. Electrical transport over distances far in excess of the dimensions of the individual nanowires is also studied experimentally by applying surface photovoltage techniques for the first time to nanowire networks. The theoretical model developed to analyze data from this, first of its type, experiment reveals insights that can aid in developing improved thermoelectric devices. Such thermoelectric devices were fabricated using a highly scalable and very low cost approach. Thermoelectric testing displays large series electrical resistance but Seebeck voltages comparable to its bulk counterpart. The preliminary results clearly indicate that if series electrical resistance can be decreased, nanowire networks will be an excellent candidate for thermoelectric energy conversion materials.
-------------------------------------

10134472_183 - 0.989034344155 - technology_and_computing
[method, datum, approach, cost-sensitive, cost, classier, problem, research, misclassification, classification]

A comparison of methods for learning cost-sensitive classiers
There is a significant body of research in machine learning addressing techniques for performing classification problems where the sole objective is to minimize the error rate (i.e., the costs of misclassification are assumed to be symmetric). More recent research has proposed a variety of approaches to attacking classification problem domains where the costs of misclassification are not uniform. Many of these approaches make algorithm-specific modifications to algorithms that previously focused only on minimizing the error rate. Other approaches have resulted in general methods that transform an arbitrary error-rate focused classier into a cost-sensitive classier. While the research has demonstrated the success of many of these general approaches in improving the performance of arbitrary algorithms compared to their cost-insensitive contemporaries, there has been relatively little examination of how well they perform relative to one another. We describe and categorize three general methods of converting a cost-sensitive method into the cost- insensitive problem domain. Each method is capable of example-based cost-sensitive classification. We then present an empirical comparison of their performance when applied to the KDD98 and DMEF2 data sets. We present results showing that costing, a technique that uses the misclassification cost of individual examples to create re -weighted training data subsets, appears to outperform alternative methods when applied to DMEF2 data using increased number of re-sampled subsets. However, the performance of all methods is not statistically differentiable across either data set
-------------------------------------

10130102_178 - 0.982197442901 - technology_and_computing
[bibliographical, reference, mode, zero, correspondence, sound]

Zero sound modes in the AdS/CFT correspondence
Includes bibliographical references (p. 41-42).
-------------------------------------

10135418_183 - 0.99999830336 - technology_and_computing
[tile, migration, coprocessor, architecture, dark]

Efficient cache-coherent migration for heterogeneous coprocessors in dark silicon limited technology
Current trends in processor manufacturing indicate that in order to meet power budgets, chips will have to power-gate an increasing fraction of transistors. This so called dark silicon will play an important role in motivating future architectures. Recent research proposes to allocate these large dark regions to power-efficient specialized coprocessors. GreenDroid is a tiled architecture where each tile includes a set of coprocessors, generated at design time, that target the anticipated workload. A central challenge to retaining the efficiency of this system is providing low overhead migration mechanisms so that tasks can move around the chip to exploit different coprocessors. This thesis presents a prototype migration system for GreenDroid and other tiled coprocessor-based architectures. The results show that for single-threaded irregular workloads, migration between c-cores spread across a four-tile configuration can provide power savings from 4.8x to 6.2x compared to an all software approach on a single tile. When compared against an ideal case where all c-cores fit on a single tile, the energy overhead of migrating amongst tiles ranges from 2.6% to 24.1%
-------------------------------------

10135926_183 - 0.99998563709 - technology_and_computing
[sensor, capacitance, mobile, ambient, frequency, system, technology]

Oscillator-based Touch Sensor for Mobile Applications
In this thesis, an oscillator-based touch sensor system is presented. Existing technologies for touchscreen sensors include resistive, capacitive, infrared, and optical imaging; however, none of them is suited for remote sensing in mobile applications. The goal of this thesis is to explore the implementation of a remote sensor system targeted for mobile applications.The proposed design uses an input-loaded oscillator that senses the ambient capacitance. The variation in ambient capacitance alters the operating frequency of the oscillator. A digital processing unit analyzes the change in frequency, and makes detection based on certain threshold.The design is achieved by 65nm technology, having a core area of 335um x 230um, power consumption of 2.5mW, and runs on a clock frequency of 16MHz. The system's sensitivity is one order of magnitude higher than current touchscreen technologies, requiring a minimum ambient capacitance variation of 50fF.
-------------------------------------

10135131_183 - 0.997811957486 - technology_and_computing
[optical, object, microscopic, joystick, trapping, user, interface, scissors, order, cell]

Development of a dual joystick-controlled optical trapping and cutting system for optical micro-manipulation of cells
In many situations there is a need to physically manipulate microscopic objects with as much dexterity as our own hands provide in the macroscopic world. An example would be applying stresses onto cells in order to determine their mechanical properties. Our existing microscope-laser experimental system is capable of manipulating microscopic objects using two optical tweezers and one optical scissors to move and cut these objects, respectively. Despite these capabilities, however, the point-and-click user interface for optical trapping and cutting was cumbersome and hard to use, limiting the system's potential for micro-manipulation. In order to resolve this limitation, a new, more intuitive and hands- on user interface using two joysticks was designed and developed from the ground up in order to provide responsive, real-time control of the optical tweezers and scissors for microscopic manipulation. This new joystick user interface was then used to verify whether or not forces other than those due to microtubule dynamics act upon chromosomes during mitosis, tested in mitotic PtK2 and Indian Muntjac cells by 1) depolymerizing microtubules using nocodazole, and 2) disrupting microtubules using laser ablation with optical scissors, and subsequently attempting to freely manipulate chromosomes using joystick -controlled optical trapping
-------------------------------------

10137679_183 - 0.995312760552 - technology_and_computing
[programming, model, stable, psc, algorithm, extension, answer, program, h-asp, computational]

Extensions of Answer Set Programming
This work discusses two new extensions of Answer Set Programming (ASP) and a new computational method for solving the following two problems : (1) given a finite propositional logic program P which has a stable model, find a stable model M of P, and (2) given a finite propositional logic program P which has no stable model, find a maximal program P' which is a subset of P which has a stable model and find a stable model M' of P'. The first extension called Preference Set Constraint (PSC) programming extends the Set Constraint programming introduced by Marek and Remmel to allow reasoning with preferences. The generality of PSC programming is demonstrated by showing that PSC programming can be used to express optimal stable models of Answer Set Optimization (ASO) of Brewka, Niemela and Truszczynski; general preferences of Son and Pontelli. An extension of PSC programming can be used to express preferred answer sets and weakly preferred answer sets of Brewka and Eiter. It is proven that under mild assumptions the problem of determining whether M is a preferred PSC stable model of a PSC program P is CoNP-complete. The second extension called Hybrid ASP (H-ASP) allows to combine logical reasoning and numerical algorithms. One of the goals of H- ASP is to allow users to reason about dyamical systems that exhibit both continuous and discrete aspects. In this work it is shown how H-ASP can be used to compute a finite horizon optimal strategy for an agent acting in a dynamic domain. The discussion leads to an introduction of a new programming language H-ASP#. It is proven that the computational complexity of H-ASP# program W# is EXP- complete in the length of W#. The new computational method for solving the above mentioned two problems called the Metropolized Forward Chaining (MFC) algorithm is based on combining the Metropolis algorithm and the Forward Chaining algorithm of Marek, Nerode and Remmel. The use of Stochastic Approximation Monte Carlo (SAMC) algorithm of in the MFC instead of the Metropolis algorithm is discussed. The results of the computational experiments conducted with the two versions of MFC are reported. Values for some of the parameters to be used with MFC are suggested. An asymptotic result for certain choices of parameters is proven
-------------------------------------

10135776_183 - 0.996673494001 - technology_and_computing
[code, neural, stimulus]

Understanding Perception Through Neural 'Codes'
A major challenge for cognitive scientists is to deduce and explain the neural mechanisms of the rapid transposition between stimulus energy and recalled memory-between the specific (sensation) and the generic (perception)-in both material and mental aspects. Researchers are attempting three explanations in terms of neural codes. The microscopic code: cellular neurobiologists correlate stimulus properties with the rates and frequencies of trains of action potentials induced by stimuli and carried by topologically organized axons. The mesoscopic code: cognitive scientists formulate symbolic codes in trains of action potentials from feature-detector neurons of phonemes, lines, odorants, vibrations, faces, etc., that object-detector neurons bind into representations of stimuli. The macroscopic code: neurodynamicists extract neural correlates of stimuli and associated behaviors in spatial patterns of oscillatory fields of dendritic activity, which self-organize and evolve on trajectories through high-dimensional brain state space. This multivariate code is expressed in landscapes of chaotic attractors. Unlike other scientific codes, such as DNA and the periodic table, these neural codes have no alphabet or syntax. They are epistemological metaphors that experimentalists need to measure neural activity and engineers need to model brain functions. My aim is to describe the main properties of the macroscopic code and the grand challenge it poses: how do very large patterns of textured synchronized oscillations form in cortex so quickly?
-------------------------------------

10134466_183 - 0.83434803152 - technology_and_computing
[response, model, error, different, system, composite, scale]

Model error estimation in composite impact response prediction using hierarchical Bayes networks
Predicting the failure response of complex systems often requires computational models that can capture the nonlinear response of the material and structure across multiple scales. Typically, the output response is a direct result of the complex interactions of different phenomena at different scales of the hierarchical system. Therefore, computed model errors correspond to accumulated model errors that have been propagated across several levels of the system. The objective of the current work is to identify and quantify the errors introduced by computer analytical models at different scales in the ballistic impact response simulation of a composite laminate. To that end, a Bayesian network based framework was implemented to systematically estimate the model contribution of uncertainty to the response prediction at each sub-scale of the composite problem. The developed method can be used for optimal allocation of validation resources by determining the type and number of experimental tests needed to reduce uncertainty at different subsystems levels of large engineering systems
-------------------------------------

10137206_183 - 0.914659879379 - technology_and_computing
[complex, plane, realization, triangulation, theorem, geometric, 3-polytope, grid, convex, embedding]

Embeddings of Polytopes and Polyhedral Complexes
When does a topological polyhedral complex (embedded in R<super>d</super>) admit a geometric realization (a rectilinear embedding in R<super>d</super>)?  What are the constraints on the possible realizations?  Two classic results concerning such questions are Fary's theorem, which states that every planar graph can be drawn in the plane such that each edge is a straight line segment, and Tutte's theorem, which provides necessary and sufficient conditions for embedding a planar graph such that all faces are convex.  The present work is motivated largely by the question of whether these types of results generalize to higher dimensions.We begin by constructing an irrational polytopal complex consisting of 1278 convex 3-polytopes in R<super>3</super>.  The methods of this construction may also be used to produce small topological complexes with no geometric realization, as well as geometric complexes which encode arbitrary point and line configurations.  This allows us to prove universality theorems for 3-dimensional polytopal complexes and arrangements.  We also investigate geometric realizations of plane triangulations, and describe an explicit algorithm that embeds a plane triangulation with n vertices in a 4n<super>3</super> x 8n<super>5</super> integer grid, in such a way that at each step of the algorithm, the resulting region of the plane is convex.  This embedding, by nature of its sequential convexity, may be lifted vertically to a 3-polytope.  This process gives a new proof of Steinitz's Theorem for triangulations, and provides a new upper bound on the size of the integer grid necessary to embed the vertices of simplicial 3-polytopes.  For certain classes of triangulations, this grid size is subexponential.
-------------------------------------

10135303_183 - 0.962903797006 - technology_and_computing
[map, retinocollicular, retinotopic, rgc, activity, mapping, mouse, receptor, mutant, development]

Molecular and cellular mechanisms of retinotopic mapping in the superior colliculus
Retinal ganglion cells (RGCs) project axons from their cell bodies in the eye to targets in the superior colliculus (SC) of the midbrain. The wiring of RGC axons to their synaptic targets creates an ordered representation, or ̀map', of retinal space within the brain. The mechanisms of development of this map are the primary topic of this dissertation. The dissertation begins with an overview of the architecture and function of the murine visual system with a focus on RGCs. The paradigms of retinotopic map development have been divided into those involving guidance molecules and those involving correlated activity. I describe a set of ratiometric 'Relative Signaling' (RS) rules that quantitatively predict how a composite gradient of EphA receptors expressed by RGCs is translated into topographic order in the SC. I describe the analysis of the retinotopic maps of novel compound mutant mice, which establish the general utility of the RS rules for predicting retinocollicular topography, including the equivalence of different EphA receptor gene products. I describe how spontaneous, patterned activity in the retina during the time of retinocollicular mapping could be instructive to the development of retinotopy. I analyze the retinocollicular maps of novel compound mutant mice lacking the beta2 subunit of the nicotinic acetylcholine receptor, which are thought to lack spontaneous, patterned activity in the retina. The analysis of retinocollicular maps in these compound mutant mice was ambiguous, but suggested that correlated activity does not play a role in retinocollicular mapping. Finally, I speculate on how different dynamics of retinotopic mapping in amniotes and anamniotes may arise because of the differing degree of encephalization between the two groups. I also comment on how increased encephalization allows for a greater behavioral repertoire but comes at the costs of an inability to regenerate and a greater time to develop
-------------------------------------

10171_7 - 0.98100577025 - technology_and_computing
[technique]

Modelling wind direction from satellite scatterometer data
Most of the common techniques for estimating conditional probability densities are inappropriate for applications involving periodic variables. In this paper we apply two novel techniques to the problem of extracting the distribution of wind vector directions from radar scatterometer data gathered by a remote-sensing satellite
-------------------------------------

10134187_183 - 0.948363247138 - technology_and_computing
[poc, algorithm, ocean]

Development and application of ocean color algorithms for estimating particulate organic carbon in the Southern Ocean from satellite observations
Empirical algorithms have been developed for estimating surface concentration of particulate organic carbon (POC) from remotely-sensed ocean color in the Southern Ocean using field data POC, spectral remote-sensing reflectance, R/rslambda, and the inherent optical properties (IOPs) of seawater. Several algorithm formulations have been considered. The best algorithm performance was obtained for the power function fit POC (mg m⁻³) = 189.29 R/ rs(443)/R/rs(555)⁻⁰·⁸⁷ with mean bias of 3%, normalized mean square error 7%, and determination coefficient 0.93. Analysis of match-up comparisons between satellite-derived and in situ POC support application of this algorithm in the Southern Ocean. The bio-optical relationships on which the POC algorithms are based exhibit significant variability mainly due to differing particulate assemblages. To quantify the sources of this variability, Mie scattering modeling and empirical data were used to calculate IOPs, POC, and chlorophyll-a content for 21 representative classes of particles. These classes represent colloids, organic detritus, minerals, and various plankton species. By using this reductionist approach, 38 different bulk models of seawater were constructed and analyzed. The utility of this approach in advancing an understanding of variability in the POC algorithms is shown; for example, the relationship between POC and particulate backscattering is investigated. The POC retrieval algorithm based on the reflectance band ratio was applied to SeaWiFS satellite data to demonstrate seasonal and interannual variability in POC in the Southern Ocean (south of 35°S) from 1997 through 2007. Typically the surface POC concentrations range from 30 to 120 mg mg m⁻³ while the monthly means range from 70-80 mg m⁻³. The seasonal maximum stock of POC (0.6 Pg) integrated within the top 100 m of the ocean occurs in December. The seasonal range of area-normalized POC is 5.5 - 6.6 g m⁻². The region south of 55°S provides a dominant contribution to the accumulation of POC during the productive period of the season. During the austral spring, the area-normalized POC accumulates in these high-latitude waters at rates from about 0.2 to 0.7 g m⁻² month⁻¹. The comparison of these rates with large-scale satellite-based estimates of net primary production indicates that only a small fraction (<10%) of production accumulates as POC
-------------------------------------

10133994_183 - 0.994910974149 - technology_and_computing
[magnetic, recording, use, fept, film, thin]

L10-FePt Media for Next-Generation Data Storage Devices
L10-ordered FePt thin films are a leading candidate for next-generation magnetic recording, such as bit-patterned media (BPM), heat-assisted magnetic recording (HAMR), and multilevel three-dimensional (ML3D) magnetic recording, because of their excellent material properties. Its high magnetocrystalline anisotropy of ~ 10^8 ergs/cm^3 allows for grain sizes less than a few nanometers while maintaining good thermal stability. It also has been demonstrated that L10-FePt can achieve coercivity greater than 10 Tesla. However, achieving the L10-phase requires post-annealing and/or deposition at elevated substrate temperature. In this work, the magnetic and microstructural properties of FePt thin films were investigated and improved for the use in perpendicular magnetic recording. FePt thin films were then fabricated on pre-patterned substrates for the use in BPM, with a heat sink layer for the use in HAMR, and with two magnetic layers for the use in ML3D magnetic recording.
-------------------------------------

10134294_183 - 0.954912054962 - technology_and_computing
[technology, sorter, detection, mufac, flow, fluorescence, cell, microfluidic, waveguide, system]

Lab-on-a-chip flow cytometer and microfluidic fluorescence activated cell sorter (MuFACS) for biomedical applications
This dissertation details the development of a portable, low cost lab-on-a-chip flow cytometer and/or fluorescence- activated cell sorter MuFACS), in which microfluidics, micro-acoustics, on-chip optics and electronics are integrated into one tiny polydimethylsiloxane (PDMS) chip creating novel functionalities. This work demonstrates a compact, high-speed sorter, integrated optofluidic waveguides, and a novel color-space-time coding technique for low-cost fluorescence detection. The microfluidic fluorescence-activated cell sorter employs fast-response piezoelectric actuators in conjunction with a high-speed, low timing jitter closed loop control system ensures high purity sorting of targeted biological samples with single- cell manipulation capabilities. By deflecting the entire sub-nanoliter volume of fluid, the MuFACS can sort various biological samples regardless of their physical or chemical properties. A high enrichment factor (>230 fold) is demonstrated at a sorting throughput of greater than 1, 000 cells/sec. By integrating the entire sorting system onto the chip, this technology holds great promise to rapidly become competitive with commercial benchtop sorters. Teflon AF coated optofluidic waveguides demonstrated in this work enhance the coupling efficiency of photons to fluorescent samples, thus increasing sensitivity and permitting multi-spot illumination. The waveguides also enable the space-time coding technology, which works in conjunction with a specially designed spatial filter and the finite-impulse-response (FIR) match filter algorithm in order to further enhance detection sensitivity. The multi-color fluorescence detection technology with an on-chip color filter waveguide array, known as Color-Space-Time (COST) coding, enables the discrimination of up to 11 fluorescent wavelengths using a single photodetector. This novel technology holds great promise for compactness by fundamentally altering the scaling rule; that is the number of bulky optical components required for detection will no longer scale linearly with the number of detection parameters. In this way, the technique can significantly lower the cost and the volume of the whole system in addition to miniaturization of the device. Such a low-cost, compact, portable flow cytometer and MuFACS system can be readily afforded by individual clinics and research labs, providing point-of-care diagnosis and analysis. The development of the microfluidic flow cytometer/MuFACS can improve global quality of life; and while the technology is not fully ready today, great strides have been made towards achieving this goal, as demonstrated in this dissertation
-------------------------------------

10136649_183 - 0.999950068163 - technology_and_computing
[module, weyl, global, loop]

Global Weyl Modules for Twisted and Untwisted Loop Algebras
A family of modules called global Weyl modules has recently been defined over generalized loop algebras. Part I of this dissertation contains a characterization the homomorphisms between these global Weyl modules, under certain restrictions. The crucial tool in this section is the reconstruction of the fundamental global Weyl module from a local one. In Part II, global Weyl modules are defined for loop algebras which have been twisted by a graph automorphism of the Dynkin diagram. We analyze their relationship with the twisted local Weyl module and with the the untwisted global Weyl module.
-------------------------------------

10137685_183 - 0.996209900896 - technology_and_computing
[janco, architecture, zurich, bucharest, marcel, dada]

Between Dada and Architecture: Marcel Janco in Zurich and Bucharest, 1916-1939
Abstract not available.
-------------------------------------

10136762_183 - 0.996634862084 - technology_and_computing
[information, datum, credibility, graphical, instantiation, experiment, judgment, element]

Assessing the credibility of information found on the internet : responses to graphical instantiations of data
Seeking information is a part of life and evaluating the quality of information is a critical part of every information-seeking act. Information seeking on the Internet is unique because the vetting of information is not routine and alternative sources for information are numerous. Credibility has been shown to be a chief consideration in whether to use information found, but credibility cannot be directly observed but rather is assessed on the basis of both content and the presentation of the content, so called surface-level characteristics of the information. Frameworks describing credibility assessment posit different ways surface-level characteristics affect credibility judgments, distinguishing reflective and heuristics processes. Many surface-level characteristics of web pages have been shown to influence judgments of credibility; of particular interest for this research is the way in which data is presented. Across four experiments 477 subjects participated in simulated Internet information seeking acts in which the presence of certain graphical elements on web pages was manipulated. Some of these elements were instantiations of data, and both tabular and chart forms of data presentation were used. All data presented was already available in the text of the web page. Across the four experiments the context differed: experiments 1 and 2 involved medical information, experiment 3 involved general-purpose information, and experiment 4 involved information presented in an online shopping context. Overall, the effects of graphical instantiations of data were similar to non-data graphical elements when explicit judgments were being made about medical or general information; however, in the context of online shopping, when compared with non-data graphical elements the graphical instantiations of data demonstrated positive effects on credibility and related judgments. The pattern of results across all experiments most strongly supports models of credibility assessment implicating implicit judgment heuristics affecting evaluations of credibility through the aesthetics of the presentation and the associations that graphical instantiations of data have to credible material. These results suggest a particularly strong effect of instantiations of data that are highly aesthetic, suggesting that if one wants information to be believed one has to not just present data, but present data beautifully
-------------------------------------

10133930_183 - 0.999999738575 - technology_and_computing
[synchronization, communication, mimo, system, acquisition, snr, low, code, siso, performance]

Synchronization at low SNR in MIMO communications
A key requirement for the increased reliability, range and throughput of the wireless communications is the ability to synchronize in a low signal-to-noise ratio (SNR) environment. It is particularly important in multiple- input and multiple-output (MIMO) communications, where a separate synchronization needs to be performed for each transmit-receive antenna pair. Moreover, the SNR for synchronization in MIMO communications is generally lower than in the single-input and single-output (SISO) case, since the transmit power is distributed amongst the multiple transmit antennas for a fixed total transmit power. Thus, the synchronization is a potential bottleneck for performance improvements in future wireless communications. This dissertation presents a synchronization architecture for packet-based MIMO communications. Specifically, it describes a direct- sequence spread-spectrum (DSSS) based synchronization system for improving the synchronization performance at low SNR, utilizing a parallel code acquisition scheme. This dissertation presents the performance analysis for the packet-based SISO communications as well as for the pilot and packet-based MIMO communications. It proposes a staggered transmission strategy for the parallel code acquisition in systems with multiple transmitter antennas, and also presents the proof for its optimality. Furthermore, it describes an architecture for the parallel code acquisition and presents the implementation of the SISO acquisition system (which is a basic building block for the MIMO acquisition system) on a radio prototype. Finally, it reports the experimental results that confirm the reliable operation at low SNR. The parallel code acquisition forms the backbone of the proposed MIMO synchronization system. This dissertation presents the performance analysis for the SISO synchronization system (which is a basic building block for the MIMO synchronization system) and describes its implementation on a radio prototype. Digital and RF tests verify the accurate translation of the synchronization system into hardware. Calibrations in the lab and experiments conducted at outdoor test sites confirm the ability to synchronize at low SNR
-------------------------------------

10133645_183 - 0.996003416597 - technology_and_computing
[datum, exponential, component, family, framework, linear, distribution, model, parameter, mixed]

Generalized statistical methods for mixed exponential families
This dissertation considers the problem of learning the underlying statistical structure of complex data sets for fitting a generative model, and for both supervised and unsupervised data-driven decision making purposes. Using properties of exponential family distributions, a new unified theoretical model called Generalized Linear Statistics is established. The complexity of data is generally a consequence of the existence of a large number of components and the fact that the components are often of mixed data types (i.e., some components might be continuous, with different underlying distributions, while other components might be discrete, such as categorical, count or Boolean). Such complex data sets are typical in drug discovery, health care, or fraud detection. The proposed statistical modeling approach is a generalization and amalgamation of techniques from classical linear statistics placed into a unified framework referred to as Generalized Linear Statistics (GLS). This framework includes techniques drawn from latent variable analysis as well as from the theory of Generalized Linear Models (GLMs), and is based on the use of exponential family distributions to model the various mixed types (continuous and discrete) of complex data sets. The methodology exploits the connection between data space and parameter space present in exponential family distributions and solves a nonlinear problem by using classical linear statistical tools applied to data that have been mapped into parameter space. One key aspect of the GLS framework is that often the natural parameter of the exponential family distributions is assumed to be constrained to a lower dimensional latent variable subspace, modeling the belief that the intrinsic dimensionality of the data is smaller than the dimensionality of the observation space. The framework is equivalent to a computationally tractable, mixed data-type hierarchical Bayes graphical model assumption with latent variables constrained to a low- dimensional parameter subspace. We demonstrate that exponential family Principal Component Analysis, Semi- Parametric exponential family Principal Component Analysis, and Bregman soft clustering are not separate unrelated algorithms, but different manifestations of model assumptions and parameter choices taken within this common GLS framework. Because of this insight, these algorithms are readily extended to deal with the important mixed data -type case. This framework has the critical advantage of allowing one to transfer high-dimensional mixed-type data components to low-dimensional common-type latent variables, which are then, in turn, used to perform regression or classification in a much simpler manner using well-known continuous-parameter classical linear techniques. Classification results on synthetic data and data sets from the University of California, Irvine machine learning repository are presented
-------------------------------------

10137653_183 - 0.999742943861 - technology_and_computing
[performance, system, metric]

Design and Development of Performance Metrics for Elite Runners
Recent advancements in mobile health applications of human motion sensing systems have enabled the proliferation of low-cost solutions in health and athletics assessment.  As sensor data analysis and systems become more sophisticated the next innovation in this field is to provide user guidance towards improved outcomes.  To this end, physiology research in distance running provides the framework to compute physics-based metrics to characterize running performance.  This thesis presents a novel running gait cycle characterization system to monitor the race performance of elite runners based on several monotonic, computationally light metrics and an overall efficiency metric.  The presented real time race results demonstrate the ability of motion sensing algorithms and systems to monitor, guide and enhance runner performance.
-------------------------------------

10137025_183 - 0.999999128812 - technology_and_computing
[serializer, agc, low-power, signal, adc, solution, power, output, system]

A 2-bit 1Gsps ADC Array with 32:1 Serializer in 45nm CMOS SOI Technology
In this thesis, a SoC (system on chip) solution is proposed for the IF (intermediate frequency) band signal processing and transmission of a radiometer system. This highly integrated and low-power solution is designed for systems where low ADC resolution (< 2-bit) is needed but high-speed data transmission (> 10Gbps) and low-power operation (< 10mW/channel) is strongly desired. The presented solution includes an array of 2-bit ADCs with duty cycle controlled AGC (automatic gain control) function and a low-power 32:1 serializer. AGC function is included in the front end in order to cope with the wide range of input power (-10dBm ~ -20dBm). The AGC loop is controlled by digitized output duty cycle with an error of 2%. A current steering 5-level tree architecture serializer is designed to achieve a high serializing factor and low-power operation.The circuit is designed using a 45nm SOI CMOS technology. It is capable of digitizing the IF signals using a power of 5.4mW/channel and transmitting the signals at 32Gbps. The serializer has an output reflection (S22) of less than -10dB from DC to 32GHz with 400mV differential output swing. The serializer consumes less than 50mW of power (3.2mW/channel), and the AGC loop consumes 2.2mW/channel.
-------------------------------------

10129876_178 - 0.993531606828 - technology_and_computing
[tev, model, event, limit]

Search for contact interactions in μsuperscript +μsuperscript - events in pp collisions at √s=7  TeV
Results are reported from a search for the effects of contact interactions using events with a high-mass, oppositely charged muon pair. The events are collected in proton-proton collisions at √s=7  TeV using the Compact Muon Solenoid detector at the Large Hadron Collider. The data sample corresponds to an integrated luminosity of 5.3  fbsuperscript -1. The observed dimuon mass spectrum is consistent with that expected from the standard model. The data are interpreted in the context of a quark- and muon-compositeness model with a left-handed isoscalar current and an energy scale parameter Λ. The 95% confidence level lower limit on Λ is 9.5 TeV under the assumption of destructive interference between the standard model and contact-interaction amplitudes. For constructive interference, the limit is 13.1 TeV. These limits are comparable to the most stringent ones reported to date.
-------------------------------------

10134396_183 - 0.999983020628 - technology_and_computing
[planetlab, distribution, node]

A characterization of node lifetime distributions in the PlanetLab test bed
In this thesis, we identify patterns of node-lifetimes among nodes participating in PlanetLab. While it is common to assume that time-to-failure follows an exponential distribution, we show that this assumption does not hold true in trace data collected from the PlanetLab test bed. By applying clustering techniques to over a year's worth of time-to-failure data from PlanetLab, we identify a set of six node lifetime distributions that are well-suited to characterizing the nodes of this particular dataset. Because of the uniqueness of the distributions, the characterization provides the additional benefit of allowing us to infer anomalies such as common administrative policies, outages, and other such events
-------------------------------------

10135424_183 - 0.99356662114 - technology_and_computing
[rna, site, detection, chapter, small, part, binding, rca, capillary, method]

Capillary Electrophoresis-based Methodology Development for Biomolecule Analysis
Capillary electrophoresis (CE) is a separation tool with wide applications in biomolecule analysis. Fast and high-resolution separation requiring minute sample volumes is advantageous to study multiple components in biological samples. Flexible modes and methods can be developed. In this thesis, I focus on developing and applying novel CE methods to study multi-target nucleic acid sensing with high sensitivity (Part I) and interactions between multiple components, i.e. proteins, nanoparticles and drugs (Part II). In Part I (Chapter 2-4), rolling circle amplification (RCA) was combined with CE-laser induced fluorescence detection (LIF) for sensitive detection of DNA and small RNA. In Chapter 3, development of the RCA-CE method for DNA detection was systematically carried out. The obstacle of injecting long ssDNA to capillary was overcome by digesting them into identical short ssDNA fragments. Under optimized conditions, LOD was as low as 1.6 fmol. Dual-target sensing was also demonstrated with two padlock probes.  In Chapter 4, to further improve the sensitivity for small RNA sensing, the stand-alone RCA was studied.  Eliminating ligation and using the target small RNA as RCA primer simplified the assay procedure and lowered the detection limit to 200- or 35- amol with good specificity. Highly reproducible detection and accurate quantification of the target small RNA in plant total RNA extracts was achieved with the complete digestion of the background RNA molecules by RNase A. Two polymerases, Phi29 and Bst were compared on their performance. In Part II (Chapter 5-7), CE was first explored as a flexible platform to quantitatively measure the dissociation constant of the nanoparticle-protein interaction. Then, mechanism studies on driving force and binding sites identification were carried out. Surface ligand pyrolysis was revealed by mass spectrometry. Desolvation was considered as a driving force which could overcome electrostatic repulsion in our systems. Peptides associated with binding site were identified by cross-linking and mass spectrometry.  They were surface peptides within drug binding site 2. Ibuprofen and naproxen, drugs sharing exactly the same binding site, would suppress the binding between human serum albumin and the Fe3O4 NPs. Fusidic acid, binding HSA in a site remote to site 2, showed no effect.
-------------------------------------

10137290_183 - 0.948965552128 - technology_and_computing
[ordinary, modularity, 2-adic, real, place, representation, galois, field, dihedral]

Modularity of nearly ordinary 2-adic residually dihedral Galois representations
We prove modularity of some two dimensional 2-adic Galois representations over a totally real field that are nearly ordinary at all places above 2 and that are residually dihedral. We do this by employing the strategy of Skinner and Wiles using Hida families together with the 2-adic patching method of Khare and Wintenberger. As an application we deduce modularity of some elliptic curves over totally real fields that have good ordinary or multiplicative reduction at places above 2.
-------------------------------------

10136515_183 - 0.999982423012 - technology_and_computing
[system, thermal, thermoelectric, receiver, module, electrical, power]

Thermoelectrics Combined with Solar Concentration for Electrical and Thermal Cogeneration
A solar tracker and concentrator was designed and assembled for the purpose of cogeneration of thermal power and electrical power using thermoelectric technology. A BiTe thermoelectric module was placed between the concentrated sunlight and a water cooling system to produce electrical power from the temperature gradient. The system was tested with the intent of determining if thermoelectric devices could provide beneficial synergy to solar thermal systems. The system was able to transfer 78% of the incident 384 watts of thermal energy from the sun via the water cooling system, but was not able to produce a substantial amount of electrical power with the thermoelectric module. The TE module only performed at 15% of its expected output of 3 watts. Either the modules were not as powerful as advertised or the thermal impedances of the various layers within the receiver were not well matched for optimal heat transport. The closed loop water cooling system worked well to display how well the system can extract thermal energy from the receiver, but the system was not well designed for optimal thermal storage after extracting the heat. Additional studies were done for optimizing the system performance, such as the effect of concentrator misalignment on the receiver output, as well as the introduction of absorbing coatings on the receiver for increased output.  After preliminary testing, simulations and modifications to the system were performed in order to prevent receiver overheating and optimize system performance. Further system improvements are discussed, in addition to future investigations that would be useful for advancing sustainable energy solutions.
-------------------------------------

10133919_183 - 0.999880920149 - technology_and_computing
[thread, latency, design, parallelism, hardware, cmp, execution, performance, support, core]

Architectural support for efficient on-chip parallel execution
Exploitation of parallelism has for decades been central to the pursuit of computing performance. This is evident in many facets of processor design: in pipelined execution, superscalar dispatch, pipelined and banked memory subsystems, multithreading, and more recently, in the proliferation of cores within chip multiprocessors (CMPs). As designs have evolved, and the parallelism dividend of each technique have been exhausted, designers have turned to other techniques in search of ever more parallelism. The recent shift to multi-core designs is a profound one, since available parallelism promises to scale farther than at prior levels, limited by interconnect degree and thermal constraints. This explosion in parallelism necessitates changes in how hardware and software interact. In this dissertation, I focus on hardware aspects of this interaction, providing support for efficient on-chip parallel execution in the face of increasing core counts. First, I introduce a mechanism for coping with increasing memory latencies in multithreaded processors. While prior designs coped well with instruction latencies in the low tens of cycles, I show that long latencies associated with stalls for main memory access lead to pathological resource hoarding and performance degradation. I demonstrate a reactive solution which more than doubles throughput for two-thread workloads. Next, I reconsider the design of coherence subsystems for CMPs. I show that implementation of a traditional directory protocol on a CMP fails to take advantage of the latency and bandwidth landscape typical of CMPs. Then, I propose a CMP-specific customization of directory-based coherence, and use it to demonstrate overall speedup, reduced miss latency, and decreased interconnect utilization. I then focus on improving hardware support for multithreading itself, specifically for thread scheduling, creation, and migration. I approach this from two complementary directions. First, I augment a CMP with support for rapidly transferring register state between execution pipelines and off-core thread storage. I demonstrate performance improvement from accelerated inter -core threading, both by scheduling around long-latency stalls as they occur, and by running a conventional multi- thread scheduler at higher sample rates than would be possible with software alone. Second, I consider a key bottleneck for newly-forked and newly-rescheduled threads: the lack of useful cached working sets, and the inability of conventional hardware to quickly construct those sets. I propose a solution which uses small hardware tables that monitor the behavior of executing threads, prepares working-set summaries on demand, and then uses those summaries to rapidly prefetch working sets when threads are forked or migrated. These techniques as much as double the performance of newly-migrated threads
-------------------------------------

10175485_189 - 0.996000342214 - technology_and_computing
[semantic, performance, control, task, deficit, word, patient, comprehension]

Testing the semantic control hypothesis for stroke aphasics with semantic deficits
Some studies of stroke patients with semantic deficits have found no effect of word frequency on semantic tasks, as well as inconsistent performance across items and tasks.  A deficit in semantic control has been suggested as the source of the deficit - i.e., an inability to focus on semantic features appropriate to the task. In the present study, two stroke patients performed significantly better in single-distractor versions (low semantic control) than multiple-distractor versions of semantic tasks (high semantic control) of comprehension tasks, which appears consistent with the semantic control hypothesis. On the other hand, two aphasic patients showed substantially better performance for auditory than visual presentation of words in comprehension tasks – a finding that is not expected on the basis of semantic control. Experiment 1 evaluated whether performance on a multiple-distractor comprehension task could be predicted solely on the basis of performance on a single-distractor version using Luce’s choice axiom. Single distractor performance significantly predicted performance and no convincing evidence was obtained for a role for semantic control. Experiment 2, which examined the modality effect, showed that for one of the patients, worse performance with auditory presentation was most likely due to rapid decay of phonological representations.  For the other, worse performance was most likely due to a disruption to phonological representations of words or to their connection to semantic representations.  In all, the results suggest that word comprehension deficits in aphasia can result from a variety of sources and not all are due to semantic control deficits.
-------------------------------------

10134527_183 - 0.999936183666 - technology_and_computing
[antenna, ghz, db, design, gain, mm, high, efficiency, yagi-uda]

High efficiency planar and RFIC-based antennas for millimeter-wave communication systems
The dissertation presents the design and measurements of several planar and RFIC-based high efficiency antennas for mm-wave applications. The high-efficiency microstrip-fed endfire angled-dipole antenna is designed mainly for phased-array applications. It is built on both sides of a Teflon substrate (epsilonr = 2.2) and allows a wideband feed from the single-ended microstrip line to the differential dipole. The design results in wide radiation patterns for scanning purposes with a gain of around 2.5 dB at 20 - 26 GHz and a cross-polarization level of < -15 dB at 24 GHz. A mutual coupling of < -23 dB is measured between adjacent elements with 6.8 mm center-to center spacing (0.50 - 0.54lambda0 at 22 - 24 GHz). A variant of the angled-dipole antenna with a magnetic ground plane edge was also developed, and shows a measured gain of > 6 dB at 23.2 - 24.6 GHz and very low mutual coupling between elements (< -23 dB for a 6.8 mm spacing). Both antennas result in a radiation efficiency of > 93% when referenced to the microstrip line feed. The usefulness of these antennas as phased-array radiators is demonstrated by several eight-element linear arrays at 22 - 24 GHz with scan angle up to 50&deg. High-efficiency microstrip-fed and CPS-fed Yagi-Uda antennas have also been developed for point-to-point millimeter-wave communication systems. The antennas are built on Teflon substrates (epsilonr< = 2.2) ; and utilize 5 directors to result in a gain of 8 - 12 dB at 24 GHz and 60 GHz. A mutual coupling of < -20 dB is measured between two microstrip-fed Yagi-Uda antennas with a center-to center spacing of 8.75 mm (0.7lambda0 at 24 GHz), and a two-element array results in a measured gain of 11.5-13.0 dB at 22-25 GHz. The planar Yagi-Uda antennas result in high radiation efficiency (> 90%) and is suitable for short-range mm-wave radars and high data- rate communication systems. A differential version was also developed using a folded dipole feed and is compatible with fully-differential RFICs. Self-shielded microstrip-fed Yagi-Uda antenna has also been developed for 60 GHz communications. The antennas are built on a Teflon substrates (epsilonr = 2.2) with a thickness of 10 mils (0.254 mm). A 7-element design results in a gain > 9.5 dB at 58 - 63 GHz. The antenna shows excellent performance in free space and in the presence of metal- planes used for shielding purposes. A parametric study is done with metal plane heights (h) from 2 mm to 11 mm, and the Yagi-Uda antenna results in a gain > 12 dB at 58 - 63 GHz for h = 5 - 8 mm. A 60 GHz four-element switched-beam Yagi-Uda array is also presented with top and bottom shielding planes, and allows for 180&deg angular coverage with < 3 dB amplitude variations. This antenna is ideal for inclusion in complex platforms, such as laptops, for point-to-point communication systems, either as a single element or a switched-beam system. MM-wave planar monopole antennas have been also demonstrated. A triangular and a straight monopole antennas result in a measured S₁₁ < -10 dB at 20.7 - 37.9 GHz and 18 - 42 GHz respectively. Both antennas are suitable for ultra-wideband applications. These antennas show omni-directional patterns over almost the whole bandwidth but with high cross-polarization levels ( ̃equal to the co-polarization level). An alternate monopole design with a localized folded current choke was developed and results in lower cross-polarization levels ( -6 dB), but with S₁₁ < -10 dB at 23.1 - 26.7 GHz. A variant of this design with a magnetic ground plane results in substantial reduction in the cross-polarization level (-13 dB) but with a bandwidth of only 1 GHz (S₁₁ < - 10 dB at 23.5 - 24.8 GHz). The measured gain of the antennas are in the range of -4.0 dB to + 2.9 dB, depending on the design, and with high radiation efficiency (> 90%). Finally, a W-band high-efficiency, electromagnetically-coupled on-chip silicon microstrip antenna has been demonstrated. The antenna is composed of a quartz substrate placed on top of a commercial low- resistivity SiGe BiCMOS silicon chip. Design criteria for the microstrip antenna taking into account the dielectric and metal-density rules for the different layers of the BiCMOS silicon chip are presented. The antenna results in Sv(1)v(1) < -10 dB at 91.7 - 98.5 GHz, a gain of 0.7 - 3.9 dB and a radiation efficiency of 44 +/- 13% at 91 - 100 GHz. The design is scalable to NxM elements and to wafer-scale arrays. To our knowledge, this is the first high- efficiency Silicon wafer-scale antenna to date
-------------------------------------

10130051_178 - 0.999852615149 - technology_and_computing
[network, reference, maximization, delay, bibliographical, algorithm, throughput, time-varying, reconfiguration]

Scheduling algorithms for throughput maximization in time-varying networks with reconfiguration delays
Includes bibliographical references (p. 247-258).
-------------------------------------

10131085_183 - 0.931379760688 - technology_and_computing
[model]

Role of Context in Imprinting
Pekin ducklings (Anas platyrhynckos) were exposed either to a white or harlequin duck model. When tested for their preferences with both models simultaneously present, the harlequin was more often preferred. If tested in the presence of another strange object (a stuffed barn owl, Tytus alba), the harlequin-trained ducklings more often deviated from chosing their training model than did the white-trained ducks, i.e., a reversal of effects. Apparently, the context of the test interacts with the characteristics of the model in a way that confounds predictions.
-------------------------------------

10130060_178 - 0.999998459164 - technology_and_computing
[system, device, reference, mechanical, characterization, nano-particle, bibliographical, microelectromechanical, property, film]

Mechanical property characterization of metal nano-particle films for microelectromechanical systems devices
Includes bibliographical references (p. 79-83).
-------------------------------------

10132739_183 - 0.920263353909 - technology_and_computing
[bus, access, wheelchair, impaired, user, modification, transportation]

The Impact of Information Access on Travel Behavior of Blind or Vision Impaired People
To date, most attention and compliance to the ADA mandates for equal access to transportation has been focused on the non-ambulatory/wheelchair bound traveler. These modification costs have been tremendous. Buses and trains have had to be retrofitted or new equipment purchased to provide wheelchair lifts and designated seating areas. Much transit infrastructure has been totally rebuilt to allow for elevators to bypass struts, level access boarding and other costly structural modifications. Not so subtle grumbling is heard when few wheelchair users are seen in these facilities or on the expensive retrofitted buses. The blind and visually impaired in this country represent a significantly large group of disabled persons (almost three times the number of wheelchair users) who also need help with transportation modifications The good news, uncovered in the empirical analysis resulting from our survey, is that their needs do not seem to require anywhere near the massive financial outlays required by the adaptations for wheelchair users. Traveling for visually impaired people means moving through a world lacking many or all of the visual cues that sighted travelers, and many transit providers, take for granted. The absence of visual cues such as bus stop signs, bus numbers, bus schedules, and street signs are the main barriers to equal access to transportation reported in this study. This group’s main need is simply more and better INFORMATION.
-------------------------------------

10134285_183 - 0.936555574822 - technology_and_computing
[datum, structure, speculative, state, non-speculative, misspeculation, dynamic, dissertation, cord]

Speculative Parallelization on Multicore Processors
With the advent of multicore processors, extracting thread level parallelism from a sequential program has become crucial for improving performance. However, many sequential programs cannot be easily parallelized due to the presence of dependences. To solve this problem this dissertation presents a thread-based execution model, called Copy-or-Discard (CorD), that supports speculative parallelization. In CorD, the state of speculative threads is maintained separately from the non-speculative computation state. If speculation is successful, the results of the speculative computation are committed by copying them into the non-speculative state. If a misspeculation is detected, no costly recovery mechanisms are needed as the speculative state can be simply discarded.To illustrate the applicability of CorD, this dissertation first shows how to apply it to streaming applications. Optimizations are proposed to reduce data copying overhead. A lightweight scheme based on version comparison is also presented to detect misspeculations.  It is observed that when misspeculation rate becomes high, the benefits of parallelism are usually nullified. To address this problem two techniques, Multiple Speculations and Incremental Recovery, are proposed. The first technique creates multiple versions of speculatively-executed code using different value predictions. If any one of these versions is found to be correct, the speculation is successful. The second technique focuses on reducing misspeculation cost. Instead of discarding all results, it allows saving and reuse of the results that are not affected by the variables that cause the misspeculation. Finally, this dissertation shows the applicability of CorD in the presence of dynamic data structures. Such data structures pose many new challenges. The copying of data structures from non-speculative to speculative state is expensive due to the large sizes of data structures. The copying of updated data structures from speculative state to non-speculative state are complex due to the changes in the shape of dynamic data structures. In addition, translating pointers internal to dynamic data structures between their non-speculative and speculative memory addresses has to be addressed. This dissertation proposes an augmented design for the representation of dynamic data structures such that all of the above operations are performed efficiently.
-------------------------------------

10139099_183 - 0.993257354173 - technology_and_computing
[cnt, thermal, array, assembly, interface, measurement, technique, contact, substrate, mwnt]

Synthesis and metrology of conducting carbon nanotube assemblies
Since its discovery, the carbon nanotube (CNT) has been proposed as one of the ultimate materials for its electrical, thermal and mechanical properties due to its incredibly strong sp<super>2</super> bonds, low defect density, and large aspect ratio. Many experimental results on individual CNTs have confirmed these outstanding theoretically predicted properties. However, scaling these properties to the macroscopic regime has proved to be challenging. This work focused on the synthesis and measurement of highly conducting, macroscopic, CNT assemblies. Scaling up the synthesis of vertically aligned multiwalled CNT (MWNT) forests was investigated through the development of a large, 100mm, wafer scale, cold wall chemical vapor deposition chamber. In addition to the synthesis, two distinct CNT assemblies have been investigated. A linear morphology where CNTs are strung in series for electrical transport (CNT wires) and a massively parallel 2D array of vertically aligned CNTs for Thermal Interface Material (TIM) applications.Poymer-CNT wire composites have been fabricated by developing a coaxial CNT core-polymer shell electrospinning technique. The core-shell interactions in this system have been studied by way of Hansen's solubility parameters. The most well defined CNT core was achieved using a core solvent that is semi-immiscible with the shell solution, yet still a solvent of the shell polymer. Electrical characterization of the resulting CNT core has shown a two orders of magnitude increase in conductivity over traditional, homogeneously mixed, electrospun CNT wires.A number of vertically aligned MWNT assemblies were studied for their thermal interface properties. Double-sided Silicon substrate (MWNT-Si-MWNT) TIM assemblies were characterized using a DC, 1D reference bar, thermal measurement technique. While attempts to control MWNT density via a micelle template technique produced only 'spaghetti like' CNTs, sputter deposited catalyst provided stark variations in array density. Relevant array morphologies such as density, height, and crystallinity were studied in conjunction with their thermal performance. A Euler buckling model was used to identify the transition between increasing and decreasing resistance with density over array height, these two regimes are explained by way of contact analysis.Self catalyzing Fecralloy substrate MWNT TIMs were studied in a similar vein to the Silicon based assemblies. This substrate was investigated because of its malleability, ease of CNT synthesis and increased CNT adhesion. The growth behavior was studied with respect to the array morphologies, i.e. array height, density, crystallinity, and diameter, while the contact resistance was evaluated using a DC, 1D reference bar technique. The best performing samples were found to have a factor of two increase over their Si counterparts. Temperature dependent thermal measurements offer insight into the interfacial phonon conduction physics and are found to agree with other temperature dependent studies, suggesting inelastic scattering at the MWNT-Cu interface. Due to the challenges associated with deliberately controlling a single array morphology, a statistical approach was used for identifying the influences of the multivariate array morphology on contact resistance. Showing the strongest correlation with array height, following a R ~ L<super>-0.5</super>. Several models were investigated to help explain this behavior, although little insight is gained over the empirical relations.To better characterize these MWNT TIM assemblies two experimental techniques were developed. A transient 3&omega; thermal measurement technique was adapted to characterize the thermal performance of CNT TIMs, offering insight into the limiting resistance in a mulilayer material stack. The MWNT-growth substrate interface was found to dominate in the Si samples while the MWNT-opposing substrate interface dominated in the Fecralloy samples. These measurements strongly supported the DC thermal measurements and the qualitative observations of substrate adhesion. Additionally, a new technique for observing nano sized contacts was established by viewing contact loading through an electron transparent membrane, imaged under an SEM. The contrast mechanism is explained by a voltage contrast phenomenon developed by trapped charges at the interface. The resolution limits have been studied by way of electron beam interactions and the use of Monte Carlo simulations, showing nanometer resolution with appropriate experimental conditions. The real MWNT contact area was found to be less than 1/100<super>th</super> the apparent contact area even at moderate pressures and the number of contacting CNTs is approximately 1/10<super>th</super> the total number of CNTs. These results confirm experimental measurement values for van der Waals adhesion strengths and thermal interface resistance.
-------------------------------------

10137612_183 - 0.999992674966 - technology_and_computing
[receiver, user, channel, design, applicable, dissertation, multiple]

Receiver designs for multiuser underwater acoustic communications
This dissertation focuses on multiuser communications through shallow, underwater acoustic channels. These channels are characterized by channel impulse responses with long delay spreads undergoing rapid fluctuations with respect to the digital signaling time. When multiple users (e.g. AUVs, gliders, or sensor nodes) need to transmit information to a common receiver, they must share the channel in some fashion. The designs presented in this dissertation utilize a sharing scheme known as Space Division Multiple Access (SDMA), where the inherent disparity in the impulse responses sampled at different spatial locations are leveraged by the system to provide users with interference-free uplinks to the common receiver. Compared to other channel sharing methods, SDMA benefits from high data throughput and a low reliance on feedback from the receiver, two desirable qualities in a bandwidth limited, rapidly evolving environment. The receivers discussed throughout this dissertation will employ successive decoding techniques to retrieve each user's information independently but will use knowledge from previous decoding cycles to model and remove multiple access interference along the way. With multiple iterations of estimation and interference cancellation, these receivers will progress towards the goal of providing each and all of the users with interference-free uplinks to the receiver. Three receivers will be discussed in this dissertation with each successive design more generally applicable than the previous: one will be applicable in time-invariant environments between geographically fixed users and a fixed, multiple-element receiver, the next will be applicable in a time-varying environment between fixed users and a fixed receiver array, and the final design will be applicable in situations with users in motion. All of the receivers discussed will require direct knowledge of the impulse response and will employ sparse channel estimation techniques to acquire this information and track any changes while decoding. The capabilities of all of the receivers will be analyzed with data collected during at-sea experiments
-------------------------------------

10135261_183 - 0.999999399424 - technology_and_computing
[network, function, node, computing, computation]

Network computing : limits and achievability
Advancements in hardware technology have ushered in a digital revolution, with networks of thousands of small devices, each capable of sensing, computing, and communicating data, fast becoming a near reality. These networks are envisioned to be used for monitoring and controlling our transportation systems, power grids, and engineering structures. They are typically required to sample a field of interest, do 'in-network' computations, and then communicate a relevant summary of the data to a designated sink node(s), most often a function of the raw sensor measurements. In this thesis, we study such problems of network computing under various communication models. We derive theoretical limits on the performance of computation protocols as well as design efficient schemes which can match these limits. First, we begin with the one -shot computation problem where each node in a network is assigned an input bit and the objective is to compute a function f of the input messages at a designated receiver node. We study the energy and latency costs of function computation under both wired and wireless communication models. Next, we consider the case where the network operation is fixed, and its end result is to convey a fixed linear transformation of the source transmissions to the receiver. We design communication protocols that can compute functions without modifying the network operation. This model is motivated by practical considerations since constantly adapting the node operations according to changing demands is not always feasible in real networks. Thereafter, we move on to the case of repeated computation where source nodes in a network generate blocks of independent messages and a single receiver node computes a target function f for each instance of the source messages. The objective is to maximize the average number of times f can be computed per network usage, i.e., the computing capacity. We provide a generalized \textit{min- cut} upper bound on the computing capacity and study its tightness for different classes of target functions and network topologies. Finally, we study the use of linear codes for network computing and quantify the benefits of non-linear coding vs linear coding vs routing for computing different classes of target functions
-------------------------------------

10134983_183 - 0.904913770199 - technology_and_computing
[response, noise, fathometer, array, reflection, peak, passive, depth, microseism, time]

Geophysical inversion with adaptive array processing of ambient noise
Land-based seismic observations of microseisms generated during Tropical Storms Ernesto and Florence are dominated by signals in the 0.15 - 0.5Hz band. Data from seafloor hydrophones in shallow water (70m depth, 130 km off the New Jersey coast) show dominant signals in the gravity- wave frequency band, 0.02 - 0.18Hz and low amplitudes from 0.18 - 0.3Hz, suggesting significant opposing wave components necessary for DF microseism generation were negligible at the site. Both storms produced similar spectra, despite differing sizes, suggesting near-coastal shallow water as the dominant region for observed microseism generation. A mathematical explanation for a sign-inversion induced to the passive fathometer response by minimum variance distortionless response (MVDR) beamforming is presented. This shows that, in the region containing the bottom reflection, the MVDR fathometer response is identical to that obtained with conventional processing multiplied by a negative factor. A model is presented for the complete passive fathometer response to ocean surface noise, interfering discrete noise sources, and locally uncorrelated noise in an ideal waveguide. The leading order term of the ocean surface noise produces the cross-correlation of vertical multipaths and yields the depth of sub-bottom reflectors. Discrete noise incident on the array via multipaths give multiple peaks in the fathometer response. These peaks may obscure the sub- bottom reflections but can be attenuated with use of Minimum Variance Distortionless Response (MVDR) steering vectors. A theory is presented for the Signal-to-Noise- Ratio (SNR) for the seabed reflection peak in the passive fathometer response as a function of seabed depth, seabed reflection coefficient, averaging time, bandwidth and spatial directivity of the noise field. The passive fathometer algorithm was applied to data from two drifting array experiments in the Mediterranean, Boundary 2003 and 2004, with 0.34s of averaging time. In the 2004 experiment, the response showed the array depth varied periodically with an amplitude of 1 m and a period of 7 s consistent with wave driven motion of the array. This introduced a destructive interference which prevents the SNR growing with averaging time, unless the motion is removed by use of a peak tracker
-------------------------------------

10135191_183 - 0.99812396369 - technology_and_computing
[control, design, single-legged, reaction, wheel]

Design and control of multimodal single-legged vehicles with variable geometry reaction wheel arrays
This dissertation presents the mechanical and control design of single-legged reaction wheel stabilized robots with wheeled and monopedal locomotion capabilities. The incorporation of variable-geometry reaction wheel arrays (RWAs) increases control authority, as compared to conventional RWAs, and is facilitated by the design of spring-loaded mechanical linkages, which complement high- speed/low-torque actuation and enable the directed release of gradually-accumulated spring energy. The design of the final prototype enables roving, self-uprighting, quasi- static stair-climbing maneuvers, and conventional or end- over-end monopedal locomotion. Linear time-varying (LTV) linear quadratic regulator (LQR) control gains for the stabilization of continuous single-legged hopping maneuvers are solved for, based on the linearization of equations of motion that incorporte nonlinear dampers in order to emulate the no-slip and no-penetration conditions. These controllers are demonstrated in simulation and have been tested on the latest physical prototype. Comparison against high-speed video has revealed that significant angular estimate drift is a key limiting factor towards robust stabilization in practice. A key challenge arises from the excitation of structural modes during the touchdown portion of continuous hopping. The frequencies of these vibrations lie within the controller bandwidth, due to the limited control authority of the system, leading to instability from positive feedback. Significant over-estimation of the forward velocity has been observed to result from foot slip near takeoff. A method of estimating the instantaneous rotation center position using offset accelerometers is developed in order to detect foot slip
-------------------------------------

10133590_183 - 0.999992209188 - technology_and_computing
[mosfet, model, design, modeling]

Design and modeling of non-classical MOSFETs
As bulk CMOS scaling is approaching the limit that is imposed by gate oxide tunneling, body doping, band-to-band tunneling, etc., non-classical MOSFET is becoming an intense subject of very large-scale integration (VLSI) research. Among a variety of non-classical MOSFETs, multiple-gate (MG) MOSFETs which are still based on Si have been proposed to scale down CMOS technology more aggressively because of better control of short-channel effects (SCEs), whereas novel MOSFETs utilizing III-V materials instead of Si are suggested to achieve CMOS performance breakthrough even without scaling down too aggressively due to the large mobility of mobile carriers. This dissertation focuses on the design and modeling of these two categories of non-classical MOSFETs. Actually, many different types of Si-based MG MOSFETs have been designed and even fabricated in the last two decades, including double-gate (DG) MOSFETs, surrounding-gate (SG) MOSFETs, quadruple-gate (QG) MOSFETs, triple-gate (TG) MOSFETs, Pi-gate MOSFETs, Omega-gate MOSFETs, and so on. Although the design work has been pretty much done, specific compact models for these MG MOSFETs other than BSIM, PSP, and HiSIM are in urgent need, because the charge sheet approximation is no longer appropriate for MG MOSFETs due to the so-called "volume inversion" effect. In this dissertation, we will first introduce the complete non-charge-sheet based analytic models of drain current, terminal charges and capacitance coefficients for long channel symmetric DG and SG MOSFETs. The DG and SG models will be generalized to a unified analytic drain current model for all kinds of MG MOSFETs, with some non-trivial yet reasonable approximations. Efforts will also be focused on making the physics-based model more versatile and computationally efficient. On the contrary, the research on III-V MOSFETs is still in the primary phase. Compact modeling for III-V MOSFETs is not being considered in the current stage because the device technology itself is far away from maturity, and the interest of this dissertation is in device design and basic physical modeling. With SCEs treated as the top-drawer consideration, a baseline device design of III-V MOSFET for sub-22nm scaling is proposed based on the thin-BOX-SOI -like structure. Physical modeling of capacitances in III- V MOSFETs has also been carried out to gain a more clear picture of capacitance degradation due to small density-of -states (DOS)
-------------------------------------

10129920_178 - 0.998827082702 - technology_and_computing
[force, afosr, systems, states, united, attenuation, office, separation, timescale, retroactivity]

Retroactivity Attenuation in Bio-Molecular Systems Based on Timescale Separation
United States. Air Force Office of Scientific Research (AFOSR Award FA9550-09-1-0211)
-------------------------------------

10134053_183 - 0.984041456408 - technology_and_computing
[wall, site, function, maya]

La muralla de Chichén : excavations of a Maya site perimeter wall
Site perimeter walls in the Maya area have traditionally been assumed to represent defensive features, and as such are used as evidence for the presence of increasingly violent warfare, especially in the Late and Terminal Classic periods. Although walls may be used for defense, simply equating them with warfare is a dangerous oversimplification. Site walls may have multiple and concurrent or changing functions, and I identify three hypotheses for the primary functions of site walls : the defensive hypothesis, the use of walls as water management features, and their use as symbolic structures meant to define and restrict sacred or elite space. I apply these hypotheses to data collected during 2009 excavations of a portion of the site wall that surrounds the center of Chichén Itzá, a Late/Terminal Classic site in the northern Maya lowlands. I argue that the primary purpose for the initial construction of the Chichén wall was the delineation of sacred or elite space, with a secondary function of water management incorporated into its design and construction strategy. A defensive function may have existed after the original construction of the wall, and this function likely arose due to new concerns regarding the potential need for defense that accompanied the process of political decentralization that occurred during the Terminal Classic/Early Postclassic period in the Maya area and at this site
-------------------------------------

10129765_178 - 0.985395774652 - technology_and_computing
[capacity, bibliographical, semiconductor, reference, management, inventory, policy, capital, operations, plant]

Operations improvement in a semiconductor capital equipment manufacturing plant : capacity management and inventory policies
Includes bibliographical references (p. 109-110).
-------------------------------------

10134498_183 - 0.997239169807 - technology_and_computing
[packet, error, loss, video, method, concealment, encoder, algorithm, decoder]

Comparison of algorithms for concealing packet losses in the transmission of compressed video
Information loss in video tends to reduce its visual quality since some glitches or artifacts can be detected by humans. Over the past few years, great efforts have been made to reduce packet loss and its visibility in videos. Forward error correction (FEC) methods can be used at the encoder to prevent packet loss. Error concealment methods can be used at the decoder to reduce the visibility of lost packets if the error correction algorithms have failed. One way to prevent the decrease of quality in videos due to packet loss is to tag packets at the encoder according to their priority levels. A problem with this prioritization method is that typically the encoder has no information about what kind of error concealment algorithm the decoder uses. This thesis presents the experiment that we performed in order to determine to what extent the encoder prediction of packet importance is independent of the error concealment method used at the decoder. In this experiment, four different types of packet loss were introduced into nine different videos. Each lossy version of each video was then decoded using five types of error concealment methods. A Visual Quality Metric was then used to rate the concealed videos. Results show that for each type of loss, the error concealment methods did not differ significantly, thus indicating that it is possible to create a packet priority tagging algorithm that is largely independent of the error concealment method used at the decoder
-------------------------------------

10139984_183 - 0.985625532452 - technology_and_computing
[arm, device, pilot, arom, study, training, ra]

The Resonating Arm Exerciser: design and pilot testing of a mechanically passive rehabilitation device that mimics robotic active assistance
Abstract
				
				
					
						Background
					Robotic arm therapy devices that incorporate actuated assistance can enhance arm recovery, motivate patients to practice, and allow therapists to deliver semi-autonomous training. However, because such devices are often complex and actively apply forces, they have not achieved widespread use in rehabilitation clinics or at home. This paper describes the design and pilot testing of a simple, mechanically passive device that provides robot-like assistance for active arm training using the principle of mechanical resonance.
				
				
					
						Methods
					The Resonating Arm Exerciser (RAE) consists of a lever that attaches to the push rim of a wheelchair, a forearm support, and an elastic band that stores energy. Patients push and pull on the lever to roll the wheelchair back and forth by about 20&#8201;cm around a neutral position. We performed two separate pilot studies of the device. In the first, we tested whether the predicted resonant properties of RAE amplified a user&#8217;s arm mobility by comparing his or her active range of motion (AROM) in the device achieved during a single, sustained push and pull to the AROM achieved during rocking. In a second pilot study designed to test the therapeutic potential of the device, eight participants with chronic stroke (35 &#177; 24&#8201;months since injury) and a mean, stable, initial upper extremity Fugl-Meyer (FM) score of 17 &#177; 8 / 66 exercised with RAE for eight 45&#8201;minute sessions over three weeks. The primary outcome measure was the average AROM measured with a tilt sensor during a one minute test, and the secondary outcome measures were the FM score and the visual analog scale for arm pain.
				
				
					
						Results
					In the first pilot study, we found people with a severe motor impairment after stroke intuitively found the resonant frequency of the chair, and the mechanical resonance of RAE amplified their arm AROM by a factor of about 2. In the second pilot study, AROM increased by 66% &#177; 20% (p = 0.003). The mean FM score increase was 8.5 &#177; 4 pts (p = 0.009). Subjects did not report discomfort or an increase in arm pain with rocking. Improvements were sustained at three months.
				
				
					
						Conclusions
					These results demonstrate that a simple mechanical device that snaps onto a manual wheelchair can use resonance to assist arm training, and that such training shows potential for safely increasing arm movement ability for people with severe chronic hemiparetic stroke.
-------------------------------------

10133962_183 - 0.999969553219 - technology_and_computing
[energy, sensor, transmission, datum, network, fusion, estimation, multihop, cost, progressive]

Energy Efficient Distributed Data Fusion In Multihop Wireless Sensor Networks
This thesis addresses a transmission energy problem for wireless sensor networks. There are two types of wireless sensor networks. One is single-hop sensor network where data from each sensor is directly transmitted to a fusion center, and the other is multihop sensor network where data is relayed through adjacent sensors. In the absence of a moving agent for data collection, multihop sensor network is typically much more energy efficient than single-hop sensor network since the former avoids long distance data transmission. Progressive data fusion is a distributed fusion method that fuses data as they hop through sensors, which is effective to further reduce the energy cost. With the knowledge of a routing tree and all channel state information, the transmission energy allocated for each sensor can be pre-determined to even further reduce the energy cost while satisfying a pre-determined performance. In this thesis, we develop several energy planning algorithms for the above purpose. Specifically we designed two energy planning algorithms for progressive estimation with digital transmissions between sensors and one energy planning algorithm for progressive estimation with analog transmission. We also show that digital transmission is more efficient in transmission energy than analog transmission if the available transmission time-bandwidth product for each link and each observation sample is not too limited.We also study energy cost for consensus estimation which is a distributed fusion method for peer-to-peer multihop sensor networks. The impact of fusion weights and energy allocation for each sensor is also investigated. We demonstrate that to achieve an approximately same performance, the total energy cost for consensus estimation can be much higher than that for progressive estimation, but the peak energy for the former is less thanthat for the latter.
-------------------------------------

10175387_189 - 0.999444756434 - technology_and_computing
[space, map, surface, hyperbolic, harmonic, differential, component, structure, hopf, cone]

The Hopf differential and harmonic maps between branched hyperbolic structures
Given a surface of genus g with fundamental group π, a representation of π into PSL(2,R) is a homomorphism that assigns to each generator of π an element of P SL(2, R). The group P SL(2, R) acts on Hom(π, P SL(2, R)) by conjugation. Define therepresentationspaceRg tobethequotientspaceHom(π,PSL(2,R))\PSL(2,R). Associated to each representation ρ is a number e(ρ) called its Euler class. Goldman showed that the space Rg has components that can be indexed by Euler classes of rep- resentations, and that there is one component for each integer e satisfying |e| ≤ 2g−2. The two maximal components correspond to Teichmu ̈ller space, the space of isotopy classes of hyperbolic structures on a surface. Teichmu ̈ller space is known to be homeomorphic to a ball of dimension 6g − 6. The other components of Rg are not as well understood.
The theory of harmonic maps between non-positively curved manifolds has been used to study Teichmu ̈ller space. Given a harmonic map between hyperbolic surfaces, there is an associated quadratic differential on the domain surface called the Hopf differential. Wolf, following Sampson, proved that via the Hopf differential,
harmonic maps parametrize Teichmu ̈ller space. This thesis extends his work to the case of branched hyperbolic structures, which correspond to certain elements in non- maximal components of representation space. More precisely, a branched hyperbolic structure is a pair (M, σ|dz|2) where M is a compact surface of genus g and σ|dz|2 is a hyperbolic metric with integral order cone singularities at a finite number of points expressed in terms of a conformal parameter.
Fix a base surface (M, σ|dz|2). For each target surface (M, ρ|dw|2) with the same number and orders of cone points as (M,σ|dz|2), there is a unique harmonic map w : (M,σ|dz|2) → (M,ρ|dw|2) homotopic to the identity that fixes the cone points of M pointwise. Thus we may define another map from the space of branched hyperbolic structures with the same number and orders of cone points to the space of meromorphic quadratic differentials on the base surface M. This map, Φ, takes the harmonic map w associated with a metric ρ|dw|2 to the Hopf differential of w. This thesis shows that the map Φ is injective.
-------------------------------------

10136617_183 - 0.938322776775 - technology_and_computing
[moduli, tool, problem, group, subgroup, scheme, witt]

On Lattice-Like Subgroups of Witt Group Schemes, and Associated Moduli Spaces
In this work we define a moduli problem for subgroups of powers of Witt group schemes which we believe to be a good analogue for the lattices used in the construction of the classical affine Grassmannian. We then develop some tools for describing these subgroups. With these tools in hand we are able to show that the moduli problem is representable in general, and we construct the representing scheme in some cases.
-------------------------------------

10136143_183 - 0.999999322908 - technology_and_computing
[power, amplifier, wireless, system, recording, low, digital, processing, spike, level]

A System-Level Analysis of a Wireless Low-Power Biosignal Recording Device
Development of brain-machine interfaces and treatment of neurological diseases can benefit from analysis of recorded data from implanted electrodes. Existing wireless neural recording systems are often bulky, dissipate too much heat to be implanted, or only have a small number of channels. Furthermore, advances in micro-machined electrodes provide the possibility of high-density recordings, but the companion electronics do not provide enough simultaneous channels with low enough power, wireless telemetry, or a small form-factor. A system level view of wireless recording-circuitry which could overcome these deficiencies is described in this work. The overall system comprises of an analog front end (AFE), digital signal processing (DSP), and transmitter (TX). Each block is analyzed, and system-level specifications are derived. Based on these specifications, each block can be optimized for low power and small area. The analog front-end uses open-loop amplifiers to support lower voltage operation than previously published work. A prototype amplifier was also fabricated to measure performance in a 65-nm CMOS process that is needed for low-power digital signal processing. The amplifier performance was comparable to other recently published amplifiers with 2.5 &mu;V noise in 10 kHz bandwidth while dissipating 17.2 &mu;V from a low 1 V supply. The use of programmable bias currents in the amplifier, to exploit the trade-off between noise and power, was proposed to set each individual amplifier's noise level (and power) to meet requirements for accurate spike detection. Literature reviews of digital-signal processors and transmitters are used to construct approximate models of power versus performance. These models are then used to investigate the overall system power with different levels of digital processing. With a target application of neural spike recording, four modes (raw data, spike detection, feature extraction, and clustering) were analyzed. A system that uses feature extraction yields the lowest overall power, supports 400 channels with a practical wireless link, and consumes approximately 8 mW.
-------------------------------------

10139699_183 - 0.999989748267 - technology_and_computing
[digitizer, oversampling, adc, ccd]

Multiplexed Oversampling Digitizer in 65 nm CMOS for Column-Parallel CCD Readout
A digitizer designed to read out column-parallel charge-coupled devices (CCDs) used for high-speed X-ray imaging is presented.  The digitizer is included as part of the High-Speed Image Preprocessor with Oversampling (HIPPO) integrated circuit.  The digitizer module comprises a multiplexed, oversampling, 12-bit, 80 MS/s pipelined Analog-to-Digital Converter (ADC) and a bank of four fast-settling sample-and-hold amplifiers to instrument four analog channels.  The ADC multiplexes and oversamples to reduce its area to allow integration that is pitch-matched to the columns of the CCD.  Novel design techniques are used to enable oversampling and multiplexing with a reduced power penalty.  The ADC exhibits 188 ?V-rms noise which is less than 1 LSB at a 12-bit level.  The prototype is implemented in a commercially available 65 nm CMOS process.  The digitizer will lead to a proof-of-principle 2D 10 Gigapixel/s X-ray detector.
-------------------------------------

10131718_183 - 0.947361473389 - technology_and_computing
[driver, behavior, interactive, simulation]

Interactive Simulation for Modeling Dynamic Driver Behavior in Response to ATIS
It has been contended that in-laboratory experimentation with interactive microcomputer simulation can substitute for the lack of real-world applications and provide a useful approach to data collection and driver behavior analysis. With the rapid development but limited real-world deployment of Advanced Traveler Information Systems, interactive simulation has quickly grown in popularity among researchers studying dynamic driver behavior. This paper discusses the development and implementation of FASTCARS (Freeway and Arterial Street Traffic Conflict Arousal and Resolution Simulator), an interactive microcomputer-based animated simulator designed for in-laboratory experimentation to assist in the estimation and calibration of predictive models of driver behavior under the influence of real-time information.
-------------------------------------

10137432_183 - 0.999902724426 - technology_and_computing
[protocol, localization, location, time-based, accuracy, secure, node, criterion, application, resolution]

Accurate and Secure Time-Based Localization With 802.11-Compatible Entities
Mobile computing is growing at an incredible pace in the world around us. With the ubiquity of personal mobile devices, new application areas continue to emerge in the wireless networking domain. One emerging area that has recently been the focus of extensive research is location-based applications. For such applications, the idea of authentication includes verification of the physical location of the node, in addition to verifying its cryptographic identity.Although mobile entities are equipped with mechanisms like GPS to find their own locations, a location-based application cannot trust a node to report its true location. Due to the privileges associated with the physical location, there is incentive for a node to claim a false location. Therefore, there must be a mechanism in place to determine the location of a possibly malicious node without trusting it. Secure localization protocols enable a group of mutually trusted nodes (called verifiers) to collectively  determine the location of a possibly malicious node (called prover).  In this dissertation, we consider time-based secure localization protocols. Two important criteria must be satisfied in the design of such protocols: correctness and proper timing resolution. The correctness  criteria is satisfied when we ensure that the protocol is secure against location cheating, and that the localization algorithm is executed as designed. The timing resolution criteria is satisfied when we ensure that the protocol can be implemented in the target system, and the accuracy of the computed location meets the accuracy requirement of the location-based application. The target system considered in this dissertation is an 802.11-based network and the target accuracy is on the order of a few meters. Prior works on this topic either focus on the issue of correctness, or on the issue of proper timing resolution. None of the existing protocols addresses both the criteria simultaneously. Furthermore, none of the existing protocols have been designed for, or implemented with 802.11-compatible entities. In this dissertation, we propose a new time-based localization protocol called ``Elliptical Multilateration", which simultaneously satisfies both the criteria: correctness and timing resolution. Our protocol also conforms to the 802.11 standard, and can be implemented with off-the-shelf 802.11-compatible hardware. In the first part of the dissertation, we identify the challenges faced in designing secure time-based localization protocols for 802.11-based networks. We introduce a new protocol that addresses these challenges. Through formal analysis, we prove that our protocol addresses the correctness criterion. The second part of the dissertation focuses on the issue of proper timing resolution. We identify the factors which have so far prevented implementation of time-based localization protocols in 802.11-based networks. We explain why the 802.11 standard does not support time-based localization with accuracy on the order of a few meters. We address this issue by proposing the addition of required architectural support. Next, we quantify the effect of clock synchronization on accuracy of time-based localization. We show how to use statistical averaging to improve accuracy beyond the limits imposed by the physical layer hardware. In secure localization it is desirable to complete the localization process fast, over minimum number of message exchanges. We propose a new algorithm which leverages the maximum likelihood method for speedy localization. Our method reduces the number of message exchanges required in hyperbolic multilateration by at least 50%, and often more, in comparison to the conventional method, without compromising accuracy. Overall, we show that it is possible to design time-based secure localization protocols that can be implemented with 802.11-compatible entities, such that the positioning accuracy is on the order of a few meters.
-------------------------------------

10134272_183 - 0.963987921116 - technology_and_computing
[dimension-free, matrix, function, variable, noncommutative, problem, ball, analytic, nc, class]

NC ball maps and changes of variables
In this dissertation, we analyze problems set in a dimension-free, noncommutative setting. To be more specific, we use a class of functions defined by power series in noncommutative variables and evaluate these functions on sets of matrices of all sizes - hence the dimension-free term. These types of functions have recently been used in the study of dimension-free linear system engineering problems HP07, CHSY03. Here we analyze a class of functions called NC analytic with the intention of understanding changes of variables in dimension-free classes of problems in matrix variables. To this end, we force geometric constraints on our analytic functions and ask how this affects the algebraic structure of the series defining them. In particular, we present a characterization of maps that send dimension-free matrix balls to dimension-free matrix balls and carry the boundary to the boundary. We study this problem in various cases restricting the variables (and matrices) to have additional symmetric structure and in cases where the variables (and matrices) have no such restrictions. These characterizations are then used to study the more general question of understanding when a dimension-free set is bianalytic to a dimension-free ball. In addition to our study of NC analytic functions, we present a result on a representation of noncommutative rational expressions. Recently there have been studies linking convexity of noncommutative rational functions to linear matrix inequalities, LMIs HMV06. The algorithm presented in the final chapter of the thesis presents a necessary step to automatically converting inequalities involving convex rational expressions into LMIs
-------------------------------------

10135386_183 - 0.944175808558 - technology_and_computing
[performance, feedback, user, threshold, asymptotic, channel, sinr, random, distribution, broadcast]

Feedback reduction techniques and fairness in multi-user MIMO broadcast channels with random beamforming
With the rise of cellular communications, the broadcast channel has gained prominence since it is an effective model of the cellular downlink channel. Multiple-input multiple-output (MIMO) communications has also gained in popularity due to its theoretically promised performance benefits over traditional single antenna systems. Combining these two theoretical objects yields the the MIMO, or vector, broadcast channel. To achieve the highest possible performance it is known that channel state information (CSI) from each user in the broadcast channel must be known at the transmitter. This requires each user to feed back this information, which is an unwanted overhead. The first part of the dissertation discusses methods to reduce this feedback overhead by considering use of multiple receive antennas. Under the random beamforming transmit methodology, the feedback is reduced by considering only feeding back the largest SINR value observed at each user. Analysis of this reduced feedback scheme involves finding the distribution of the largest SINR over correlated random variables. Each user can also use the additional receive antennas to perform LMMSE reception. The distribution of the post-processed SINR is found and used to compute the system performance. Feedback can be reduced after LMMSE reception by feeding back only the largest post-processed SINR. Bounding techniques on the distribution function of the maximum post-processed SINR are used to evaluate the performance of this scheme. Fixed finite thresholds are shown to have no asymptotic effects on the system performance. The second part of the thesis considers the design of thresholds as a function of the number of users to reduce feedback. The asymptotic properties of any successful threshold are derived, namely any threshold T(n) in the class o(log n) asymptotically achieves the optimal sum-rate scaling rate while any threshold T(n) in the class omega( \log n) loses all multi-user diversity. Under three proposed system performance metrics, the optimal threshold is obtained. These metrics are : constraining the average number of users providing feedback, constraining the probability that no user feeds back, and constraining the rate lost due to thresholding. The final portion of the dissertation address the question of fairness in the random beamforming technique. To improve fairness the proportional fair sharing algorithm is proposed as a substitute for the greedy scheduling algorithm. Under the Rayleigh fading model, the asymptotic performance is found to be identical to the asymptotic performance of the greedy algorithm. The rate of convergence of the proportional fair sharing algorithm to the asymptotic performance limit is found. The distribution of the distance from the asymptotic limit is computed. Additional analysis is performed on the use of thresholds under proportional fair sharing
-------------------------------------

10130198_178 - 0.989645502229 - technology_and_computing
[scheme, convective, transport, model, convection, cumulus]

Off-line algorithm for calculation of vertical tracer transport in the troposphere due to deep convection
A modified cumulus convection parametrisation scheme is presented. This scheme computes the mass of air transported upward in a cumulus cell using conservation of moisture and a detailed distribution of convective precipitation provided by a reanalysis dataset. The representation of vertical transport within the scheme includes entrainment and detrainment processes in convective updrafts and downdrafts. Output from the proposed parametrisation scheme is employed in the National Institute for Environmental Studies (NIES) global chemical transport model driven by JRA-25/JCDAS reanalysis. The simulated convective precipitation rate and mass fluxes are compared with observations and reanalysis data. A simulation of the short-lived tracer superscript 222Rn is used to further evaluate the performance of the cumulus convection scheme. Simulated distributions of superscript 222Rn are evaluated against observations at the surface and in the free troposphere, and compared with output from models that participated in the TransCom-CH4 Transport Model Intercomparison. From this comparison, we demonstrate that the proposed convective scheme in general is consistent with observed and modeled results.
-------------------------------------

10134434_183 - 0.993348638441 - technology_and_computing
[nw, application, as-synthesized, material, synthesis, system, process, nanomaterial]

Synthesis, Characterization and Applications of Novel Nanomaterial  Systems and Semiconducting Nanowires
The synthesis of novel nanomaterials systems and one-dimensioal nanostructures have created a great deal of excitement in the area of nanoelectronics and biomedical applications. The research presented in this dissertation is an effort to contribute to the realization of applications in both of the forementioned fronts of Nanotechnology. 	In the first approach, a mutant icosahedral Cow Pea Mosaic Virus (CPMV) is employed as an scaffold to covalently attached two different types of inorganic nanomaterials; namely, ZnS(shell)/CdSe(core) semiconductor quantum dots and iron oxide (&gamma-Fe2O3) nanoparticles. 	In the CPMV-QD(1,2) system a bistability behavior is observed and conductive atomic force microscopy (CAFM) is extensively used to characterize and implement a hybrid memory element at the nanoscale. 	In the CPMV-&gamma-Fe2O3 system the conjugation of iron oxide (IO) nanoclusters is extensively characterized by magnetic force microscopy (MFM) and an increase in the local magnetic field strength opens up the potential use of this system in bioimaging applications.	On the second approach, crystalline nanowires (NWs) of semiconducting compound materials Copper Indium Disulfide (CuInS<sub>2</sub>) and Indium Antimonide (InSb) are synthesized using a sono-electrochemical deposition process. Using sonoelectrochemistry overcomes many of the challenges related to the crystal quality that are inherent to regular electrochemistry and allows a greater control in the engineering of the electronic and optoelectrical properties of the as-synthesized NW	On one hand, CuInS<sub>2</sub> is a promising materials as an absorbent material for solar cell applications. During the synthesis process the effect of the partial precursor electrolyte ratios on the optoelectrical properties of the as-synthesized NWs is extensively study. As-synthesized I-rich NWs are found to be photoconductive while C-rich NWs are not.	On the other hand, InSb is an attractive material to develop high speed and low power logical applications. During the synthesis process, the depositon potential was selectively controlled as a way to modify the composition and stoichiometric ratio of the as-synthesized NWs. The transport characteristics of as-synthesized NWs were modified between n and p-type semiconducting behavior.
-------------------------------------

10134337_183 - 0.979902397916 - technology_and_computing
[metal, hybrid, structure, pd, deposition, gas]

Synthesis and Characterization of Hybrid Metal-Metal and Metal-Oxide Nanostructure Decorated Single Walled Carbon Nanotube Devices
The overall objective of this work is the development of a high density sensor array using singe wall carbon nanotube (SWNT) hybrid structures as the platform material for the realization of highly sensitive, selective and discriminative gas sensors for monitoring of industrial and automotive emissions in the environment.  To conceive such desired sensor characteristics SWNTs were surface functionalized with metal and metal oxide active materials i.e. ZnO, Ag, Pt, Pd, Au to target specific analytes.  Detailed analysis of ZnO electrodeposition on to SWNTs was performed to investigate structure property relations between ZnO particle size, density, material quality, catalyst type, and combination thereof, towards the sensing performance and analyte specificity.  	Room temperature gas sensing performance of environmentally significant gases such NH3, SO2, H2S, NO2, CO, CO2, benzene derivatives, alkanes, and alcohols was evaluated using the metal and metal oxide hybrid structures.  The ZnO-SWNT hybrid structures exhibited characteristic selectivity towards H2S, with enhanced sensitivity, response and recovery time contingent of crystal orientation; additionally as synthesized ZnO-SWNT hybrid structures displayed conductance response cognizance of xylene isomers (ortho, para, meta).  Furthermore, at the hand of sequential incorporation of a metal catalysts (Au, Ag, Pt, and Pd), the ZnO-SWNT system showed distinct charge dependant pattern recognition towards less detectable analytes such as SO2 and NO2; metal-SWNT hybrid structures with Pd as the second deposited metal demonstrated enhanced sensitivity towards NO2 and NH3 due to the enhanced dissociation of the molecular adsorbate on the catalytically active Pd nanoparticle surface.  Understanding of the deposition process was then extended to the deposition of binary metals Au, Pd, Pt, followed by sequential deposition of a combination of metals. Emphases was placed on both the gas sensing aspect of the resultant hybrid structures, and also on the ability to site specifically control the deposition of the second metal.   A detailed examination of these hybrid nanostructures and the nature of their responses towards different gaseous environments was studied and compared with the performance of unfunctionalized carbon nanotubes.
-------------------------------------

10129977_178 - 0.962479266655 - technology_and_computing
[u-pb, process, dynamic, reference, high-precision, geochronologic, model, bibliographical, datum, earth]

Integrating high-precision U-Pb geochronologic data with dynamic models of earth processes
Includes bibliographical references.
-------------------------------------

10131668_183 - 0.998407414168 - technology_and_computing
[technology, magnetic, transrapid, levitation, system]

The Transrapid Magnetic Levitation System: A Technical and Commercial Assessment
Magnetic levitation is the first truly new transportation technology in one hundred years and comes at a time when advances in transportation are sorely needed. The German Transrapid magnetic levitation train is the first commercially available system to use this technology. Based on technology that originated in the United States, the Transrapid system has been under development for approximately 15 years in Germany. During the last decade a number of serious proposals have been made for its use in the United States and Germany. However, to date, non have left the planning stage and all remain speculative at best. This is a function of the high level of risk associated with such a new technology, which has yet to prove itself in commercial operations, despite holding great promise for high levels of service.
-------------------------------------

10133214_183 - 0.902639284419 - technology_and_computing
[transit, integration, service, local, area, level, authority, operator]

An Analysis of the Impacts of British Transport Reforms on Transit Integration in the Metropolitan Areas
By the 1990s, many experts concluded that transit privatization in Britain had produced positive impacts on service provision in London, but that deregulation outside of the capital had resulted in a number of negative impacts to passengers, most notably, rising fares, lower service frequencies in some areas, and declining levels of service integration. In an attempt to improve mobility at the local level, the incoming Labour Government effectively devolved transport planning powers to local authorities, requiring that they submit five-year Local Transport Plans in order to receive funding. Empowering legislation specifically identified service integration as a means through which to improve transit and provide a viable alternative to the auto. More recently, however, experts have surmised that local strategies in the Metropolitan Areas (Mets) have yielded limited gains in the area of service integration, in contrast to the experience of London.
      While some politicians believe that re-regulation of the transit industry in the Mets would automatically resolve integration issues, interview results suggested that there are additional factors that keep transit providers from effectively collaborating with one another. For example, existing competition law prevents transit operators from freely communicating with others, virtually eliminating the prospect of collaborative responses to common concerns. Other factors influencing the level of integration include the ease with which local authorities voluntarily band together to provide service links, and the level of trust that transit operators have in local authorities. In addition, the interviews revealed that the integration of transit is more easily achieved where operators sense that authorities want to engage in horizontal integration and do not have a hidden agenda.
      Beyond providing a better understanding of transit integration and possible reasons for past failures in the coordination of services, this study suggests ways of encouraging the sort of collaborative planning that can effectively bring together operators to work on improving service links in common areas. Attention to these issues is essential, not only to avoid disruptive, interoperator conflicts, but also to provide the conditions necessary to collectively offer a seamless, integrated transit service that provides significant benefits to passengers and society at large.
-------------------------------------

10134360_183 - 0.999968163138 - technology_and_computing
[relative, network, synchronization, estimator, clock, delay, estimation, frequency, interval, independent]

Model-based Estimation and Inference Procedures for Clock Synchronization
Accurate clock synchronization is essential in the operation of networks. In applications in which it can be performed frequently, the process generally relies upon the estimation of the relative offset of two different clocks. However, due to limited power resources in Wireless Sensor Networks, it cannot be performed often. Thus in order to obtain a longer lasting synchronization, the relative frequency of two clocks is also estimated. In the literature, there are estimators for relative offset and frequency based on a variety of techniques including graphical methods, maximum likelihood estimation, and least squares estimation. However, there has been no extensive study to compare the performance of these estimators. In this dissertation, the existing estimators are examined in detail and several alternative least squares-based estimators are developed and compared with the existing estimators in an exhaustive study.Although there is a large literature on estimating relative offset and frequency, there is little to no research regarding inference. This topic is of potentially great importance because it may possibly lead to limiting unneeded synchronization, resulting in better conservation of network power resources. Recent work has focused on the construction of confidence intervals for relative offset and while it does not address relative frequency, it is an important first step towards developing procedures that do. In this work, it has been assumed that the network delays that occur during the synchronization process are independent, but this may not be appropriate in all applications. The network delays are often modeled as independent exponential random variables. Thus to improve upon the existing confidence interval methodology, in this dissertation the use of bivariate exponential distributions is introduced to capture the anticipated correlation between the network delays. Moreover, an alternative confidence interval procedure for offset is developed and is then used to illustrate how the assumption of independent network delays can potentially lead to improper inference about relative offset.
-------------------------------------

10136782_183 - 0.999124424768 - technology_and_computing
[morse, point, function, critical, distance, theory, riemannian, index]

Sub-Index for Critical Points of Distance Functions
Morse theory is based on the idea that a smooth function on a manifold yields data about the topology of the manifold.  In this way it provides a tool for visualizing the shape of a space.  Specifically, Morse's Isotopy Lemma tells us that the homotopy type of a manifold does not change in regions without critical points.  The topology only changes in the presence of a critical point.  Morse's Theorem states that the specific topological change is determined by the index of the Hessian at each critical point.  In Morse Theory a smooth function is essential so that the differential and Hessian exist.In Riemannian geometry, the distance function is not smooth everywhere.  This means the differential as well the Hessian do not exist and Morse Theory cannot be applied.  In order to generalize Morse Theory to this non-smooth function, an alternate definition of critical point and index are required.  Grove and Shiohama developed a definition of critical point for the Riemannian distance function and used it to generalize Morse's Isotopy Lemma.  Their generalization had a profound impact on the study of Riemannian geometry.  Since no definition of index currently exists, Morse's Theorem has not been generalized.The purpose of this dissertation is to define a new notion, called sub-index, for critical points of  Riemannian distance functions.  We show that Morse's connectedness corollary holds for the distance function when index is replaced by sub-index.
-------------------------------------

10138875_183 - 0.999364734426 - technology_and_computing
[toolkit, ieq, performance, building, evaluation, case, feedback, rating, study, method]

Commercial Building Indoor Environmental Quality Evaluation: Methods and Tools
In order to address the previously identified need for better methods and tools for evaluating building performance, this project seeks to:
      1. Develop a hardware and software toolkit for facilitating the evaluation of IEQ performance in commercial buildings based on the ASHRAE/CIBSE/USGBC Performance Measurement Protocols.
      2. Evaluate the success of the toolkit through a case study
      3. Explore IEQ models as a method for rating IEQ performance
      4. Provide an example implementation of the PMP and suggestions for improvement
      The toolkit aims to simplify the process of building performance evaluation, tying together the multiple pieces needed to appropriately evaluate performance. In doing so, the toolkit hopes to be a prototype for future cost-effective, commercially available toolkits.
      The toolkit’s success will be evaluated through a case study that provides feedback on toolkit procedures and features from practitioners. In addition to feedback on the toolkit, the case study will provide a source of data to explore three of the IEQ rating systems discussed in section 1.5. Specifically, issues of spatial and temporal resolution are explored as they relate both to IEQ ratings and procedural feasibility.
      Finally, this project aims to provide critical feedback on the PMP in an effort to widen the appeal of IEQ evaluation to current practitioners and other potential interested parties such as LEED.
-------------------------------------

10135377_183 - 0.999969125159 - technology_and_computing
[datum, analytic, processing, state, stateful, architecture, imr, groupwise, cbp, model]

Architectures for stateful data-intensive analytics
The ability to do rich analytics on massive sets of unstructured data drives the operation of many organizations today and has given rise to a new class of data-intensive computing systems. Many of these analytics are update-driven, they must constantly integrate new data in the analysis, and a fundamental requirement for efficiency is the ability to maintain state. However, current data-intensive computing systems do not directly support stateful analytics, making programming harder and resulting in inefficient processing. This dissertation proposes that state become a first-class abstraction in data-intensive computing. It introduces stateful groupwise processing, a programming abstraction that integrates data -parallelism and state, allowing sophisticated, easily parallelizable stateful analytics. The explicit modeling of state abstracts the details of state management, making programming easier, and allows the runtime system to optimize state management. This work investigates the use of stateful groupwise processing in two distinct phases in the data management lifecycle : (i) the extraction of data from its sources and online analysis, and (ii) its storage and follow-on analysis. We propose two complementary architectures that manage data in these two phases. This work proposes In-situ MapReduce (iMR), a model and architecture for efficient online analytics. The iMR model combines stateful groupwise processing with windowed processing for analyzing streams of unstructured data. To allow timely analytics, the iMR model supports reduced data fidelity through partial data processing and introduces a novel metric for the systematic characterization of partial data. For efficiency, the iMR architecture moves the data analysis from dedicated compute clusters onto the sources themselves, avoiding costly data migrations. Once data are extracted and stored, a fundamental challenge is how to write rich analytics to gain deeper insights from bulk data. This work introduces Continuous Bulk Processing (CBP), a model and architecture for sophisticated dataflows on bulk data. CBP uses stateteful groupwise processing as the building block for expressing analytics, lending itself to incremental and iterative analytics. Further, CBP provides primitives for dataflow control that simplify the composition of sophisticated analytics. Leveraging the explicit modeling of state, CBP executes these dataflows in a scalable, efficient, and fault-tolerant manner
-------------------------------------

10138958_183 - 0.743583938803 - technology_and_computing
[memory, execution, consistency, coherence, processor]

Energy Efficient Memory Speculation With Memory Latency Tolerance Supporting Sequential Consistency Without A Coherence Protocol
Modern out-of-order processor architectures focus significantly on the high performance execution of memory operations. Because memory instructions pose ordering requirements, their execution becomes a significant bottleneck for out-of-order execution, particularly slow executing loads.Many high-overhead structures such as StoreSets, Load Queues and Store Queues are included in these processors to support memory speculation in an attempt to relax these ordering hazards whereever possible. However, each of these structures presents a new and significant source of energy consumption and design complexity to processor architects.The execution of memory instructions becomes further complicated by the introduction of multi-core processors. Memory coherence is needed which often requries a coherence protocol and interconnection network. Additionally, the timing and ordering of memory instructions' execution between cores can have critical impacts on program functionality and output which programmers must concern themselves with in the form of a memory consistency model. This work proposes a decoupled memory execution verification mechanism that supports memory speculation without costly, complex, and scaling limited structures. This in-orderverification can reduce the average energy dissipation by over 16% with a simpler design that removes the Load and Store queues, StoreSets, and even invalidation-based cache coherence protocols. These benefits are realized by a system providing the straight-forward and intuitiveSequential Consistency memory consistency model.
-------------------------------------

10134898_183 - 0.999968989497 - technology_and_computing
[stream, address, memory, compression, application, hpc, resource, technique, subsystem, performance]

HPC application address stream compression, replay and scaling
As the capabilities of high performance computing (HPC) resources have grown over the last decades, a performance gap has developed and expanded between the processor and memory. Processor speeds have improved according to Moore's law, while memory bandwidth has lagged behind. The performance bottleneck created by this gap, termed the "Von Neuman bottleneck," has been the driving force behind the development of modern memory subsystems. Many advances have been made aimed at hiding this memory bottleneck. Multi-level cache structures with a variety of implementation policies have been introduced. Memory subsystems have become very complex and the effectiveness of their structure and policies vary according the behavior of the application running on the resource. Memory simulation studies aid in the design of memory subsystems and in acquisition decisions. During a typical acquisition, candidate resources are evaluated to determine their appropriateness for a pre-defined workload. Simulation-aided models provide performance predictions when the hardware is not available for full testing ahead of purchase. However, address streams of full applications may be too large for direct use, complicating memory subsystem simulation. Memory address streams are extremely large. They can grow at a rate of over 2.6 TB/hour per core. HPC workloads contain applications that run for days across hundreds of processors, generating address streams whose handling is intractable. However, the memory address streams contain a wealth of information about the behavior of applications, that is largely inaccessible. This work describes a novel compression technique, specifically designed to make the information within HPC application address streams accessible and manageable. This compression method has several advantages over previous methods: extremely high compression rates, low overhead, and a human readable format. These attributes of the compression technique enable further, previously problematic, studies. High compression ratios are a necessity for application address streams. Address streams are very large, making them challenging to collect and store. Furthermore, any simulation experiment performed using the stream will be limited by disk speeds, since there is no other plausible place to store and retrieve such volumes of data. The compression technique presented has demonstrated compression ratios in the hundreds of thousands of times. This leads to file sizes that can easily be emailed between collaborators and the format can be replayed at least as fast as disk speeds. The collection overhead for an address stream must be low. The collection takes place on an HPC resource, and HPC resource time is costly. This compression technique has an unsampled average slowdown of 90X. This slowdown is an improvement of the state-of-the- art. The compressed address stream profiles are human readable. This attribute enables new and interesting uses of application address streams. It is possible to experiment with hypothetical code optimizations using simulation or other metrics rather than actually implement the optimizations. Strong scaling analysis of memory behavior is historically challenging. High-level metrics such as execution time and cache miss rates do not lend well to strong scaling studies because they hide the true complexity of the application-machine interactions. This work includes a strong scaling analysis in order to demonstrate the advanced capabilities that can be built upon this compression technique
-------------------------------------

10133367_183 - 0.999915621381 - technology_and_computing
[datum, traffic, detector, approach, loop]

Traffic flow reconstruction using mobile sensors and loop detector data
In order to develop efficient control strategies to improve traffic conditions on freeways, it is necessary to know the state of the freeway at any point in time and space. Using data collected from stationary detectors –such as loop detector stations– the density field can be currently reconstructed to a certain accuracy. Unfortunately, deploying this type of infrastructure is expensive, and its reliability varies. This article proposes and investigates new algorithms that make use of data provided by mobile sensors, in addition to that collected by stationary detectors, to reconstruct traffic flow. Two approaches are proposed and evaluated with traffic data. The first approach is based on data assimilation methods (so-called nudging method) and the second is based on Kalman filtering. These approaches are evaluated using traffic data. Results show that the proposed algorithms appropriately incorporate the new data, improving significantly the accuracy of the estimates that consider loop detector data only.
-------------------------------------

10131310_183 - 0.931379760688 - technology_and_computing
[model, behavioural]

Behavioural Screening and Selection Through Affinity:  The Case of Polygyny in Paper Wasps (Polistes dominulus)
An evolutionary model for emergence of polygynous foundation in polistine paperwasps is proposed. Adapted from Hamilton's model, the model is based on selection through affinity, a process of genetic assortment. This would involve selective association of founding wasps on the basis of common possession of a behavioural trait. The model, therefore, invokes a form of behavioural screening in a genetically heterogeneous population. The validity of the hypothesis and the nature of the trait concerned are considered in relation to observations on formation of Polistine polygynous associations and between species comparisons. 
-------------------------------------

10138037_183 - 0.965165132122 - technology_and_computing
[dynamic, drem, regulatory, network, datum, interaction, gene, expression, species, static]

DREM 2.0: Improved reconstruction of dynamic regulatory networks from time-series expression data
Abstract
            
            
               
                  Background
               
               Modeling dynamic regulatory networks is a major challenge since much of the protein-DNA interaction data available is static. The Dynamic Regulatory Events Miner (DREM) uses a Hidden Markov Model-based approach to integrate this static interaction data with time series gene expression leading to models that can determine when transcription factors (TFs) activate genes and what genes they regulate. DREM has been used successfully in diverse areas of biological research. However, several issues were not addressed by the original version.
            
            
               
                  Results
               
               DREM 2.0 is a comprehensive software for reconstructing dynamic regulatory networks that supports interactive graphical or batch mode. With version 2.0 a set of new features that are unique in comparison with other softwares are introduced. First, we provide static interaction data for additional species. Second, DREM 2.0 now accepts continuous binding values and we added a new method to utilize TF expression levels when searching for dynamic models. Third, we added support for discriminative motif discovery, which is particularly powerful for species with limited experimental interaction data. Finally, we improved the visualization to support the new features. Combined, these changes improve the ability of DREM 2.0 to accurately recover dynamic regulatory networks and make it much easier to use it for analyzing such networks in several species with varying degrees of interaction information.
            
            
               
                  Conclusions
               
               DREM 2.0 provides a unique framework for constructing and visualizing dynamic regulatory networks. DREM 2.0 can be downloaded from: http://www.sb.cs.cmu.edu/drem.
-------------------------------------

10134346_183 - 0.807671473984 - technology_and_computing
[spin, structure, semiconductor, dependent, interface]

Tuning of Spin Dependent Reflection at Ferromagnet/GaAs Interfaces
Semiconductor spintronics aims to integrate memory and logic functions by utilizing the electron's spin degree of freedom in addition to its charge. The building blocks for semiconductor spintronic devices are the ferromagnet (FM)/semiconductor hybrid structures, where the interface plays the key role in spin injection and detection. My graduate research focuses on exploring spin dependent properties of FM/GaAs hybrid structures by ultrafast optics and possible route for tuning of spin polarization through interface modification. In Fe/MgO/GaAs structures, the ultrathin MgO interlayer at the interface causes an unexpected sign reversal in spin dependent reflection, despite being a nonmagnetic insulator. We find that the interfacial bonding with Mg is responsible for the sign reversal. In the Fe3O4/GaAs system, we observed oscillatory spin polarization and magneto-optic Kerr effect with respect to Fe3O4 film thickness, which we attribute to the formation of spin-polarized quantum well states.
-------------------------------------

10135066_183 - 0.975090345181 - technology_and_computing
[decision, system, information, maker, advocate, adversarial, fre]

Essays on communication games with multiple informants and their applications to legal systems
Chapter 1 studies the properties of the two most commonly used legal institutions, the inquisitorial system and the adversarial system. In the former, the decision maker takes a decision based on her own acquired information, whereas under the latter the decision maker requires the advocates to present their acquired information prior to making a decision. When information is both verifiable and costly to collect, I show that the decision maker expects to make fewer decision making errors in the adversarial system than in the inquisitorial system. The main factor behind this result is that the advocate with the burden of proof values information more and, consequently, works harder to collect information under the adversarial system than the impartial decision maker under the inquisitorial system. This larger effort exerted by the advocates leads to more informed decision making under the adversarial system in spite of the advocates' incentives to distort information. Chapter 2 studies the problem of an uninformed decision maker who acquires expert advice prior to making a decision. I show that it is less costly to hire partisan agents than impartial agents, especially under advocacy, and that the decision maker prefers partisan advocacy to other forms of institutions. I also extend the literature, originating with Dewatripont and Tirole (1999), to a setting with contracts that condition on information provided and not just the decision made. Chapter 3 studies the robustness of fully revealing equilibria (FRE) in multidimension-multisender cheap talk games. A FRE is outcome-robust (strategy-robust) if there is an equilibrium whose outcome (strategy) is close to the FRE outcome (strategy) when the noise in senders' observations is small. I show that there is no outcome- robust FRE in the model of Levy and Razin (2007), and discuss the connections between these new notions of robustness and the existing stability concepts studied in the literature
-------------------------------------

10137011_183 - 0.993075851028 - technology_and_computing
[machine, music, human, computer, audition]

Machines that understand music
Machine learning, signal processing and data mining are being combined to analyze audio content in a relatively new field of research called computer audition. This thesis develops and describes a number of computer audition methods and shows how they can be applied to solve challenges including automatic tagging, similarity and recommendation, search and discovery, and segmentation of music content. To achieve these advances in music understanding requires human guidance. A further contribution of this work is to pioneer game-powered machine learning that uses crowdsourced human intelligence to guide the training of machine algorithms. By leveraging human perception with machine automation, the work described in this thesis presents a comprehensive approach to computer audition that leads to the development of machines that understand music
-------------------------------------

10134123_183 - 0.969735538427 - technology_and_computing
[visualization, field, vector, seismic, glyph, time-dependent, novel]

Visualization of time-dependent seismic vector fields with glyphs
Seismic simulations allow us to study earthquakes in a manner not feasible with the real world. Simulations of earthquakes produce time-dependent vector fields that contain interesting geophysics. Prior visualization strategies focused on slices and volumetric rendering of scalar fields which reduces the observable phenomena. This thesis studies visualization techniques implemented in an interactive glyph visualization application called "GlyphSea" that allows scientists to explore seismic velocity fields. This work draws from a large body of work in glyph rendering and focuses on time-dependent seismic vector fields and is the result of collaboration between domain experts in visualization and seismology. Through the study of vector visualization, several novel techniques were formed. A novel procedural dipole and cross mark texturing enhancement encodes unambiguous vector orientation on any geometry with volume. A novel lattice method was created to show neighborhood which also enables glyph distinction. Visualization is further enhanced by using screen space ambient occlusion, jitter, halos, and displacement
-------------------------------------

10133687_183 - 0.999719409279 - technology_and_computing
[ramp, motion, method, amplitude, angle, system, pitch, vessel, degree, control]

Control and optimization of wave-induced motion of ramp- interconnected craft for cargo transfer
A two vessel interconnected by a ramp system was modeled using SimMechanics toolbox in Simulink. Both vessels were modeled as half cylinders and the ramp as a rectangular solid. Although the equations of motion for the system were derived, the SimMechanics model proved to be more efficient to implement certain control and optimization techniques and is emphasized throughout the thesis. This thesis documents different attempts to control and optimize the various motions of the system using passive and active methods. The passive methods include extremum seeking tuning of two parameters namely the ramp length and wave heading angle to reduce the pitch angle amplitude at the joint connecting the ramp and T-Craft. The second method employed mimics automotive shock absorbers to reduce relative motion between each vessel and ramp to reduce overall ramp motions. In both methods the results concur with the goal of the problem statement of stabilizing ramp/vessel motions. Applying the ES algorithm to tune the ramp length and wave heading angle reduced the pitch amplitude by 67% (from approx. 15 to 5 degrees) and applying the shock absorbers in the pitch joint case of the system reduced the pitch angle amplitude by two orders of magnitude (from approx. 10 to 0.1 degrees). The active method explored is installing a control moment gyroscope on the T-Craft to stabilize its roll motion. The results show that roll motion is decreased to lie within the stability region of one degree in amplitude and have a feasible size and weight requirement
-------------------------------------

10133867_183 - 0.984498506757 - technology_and_computing
[dynamic, test, beam, failure, sandwich, behavior, specimen, blast]

Non-explosive simulated blast loading of composite sandwich beams
A dynamic loading method for simulating explosive blast was developed using a crushing foam projectile launched by a gas gun at velocities ranging from 30-60 m/s. This test method is used to load composite specimens to study the dynamic failure of carbon/epoxy sandwich beams. The beams consist of an end-grain balsa core and carbon fiber laminate facesheets. A focused study was conducted on the failure behavior of the facesheets, in isolation of core- failure effects, as well as the behavior of the full sandwich structure. Dynamic three point bending was conducted using the crushing foam loading methodology to impart simulated dynamic blast loading at relevant strain rates onto the beam specimens. Slower, more controlled dynamic tests, as well as quasi-static tests, were also performed using a servohydraulic test machine to obtain a highly detailed view of the specimens' behavior during the failure process
-------------------------------------

10136097_183 - 0.998552214805 - technology_and_computing
[system, impairment]

Analysis and Simulation of Impairments in Xampling Based Receivers
Recent advances in Compressed Sensing theory has lead researcher to apply these techniques to the design of sampling systems which can sample spectrally sparse signals with a wide bandwidth below the Nyquist rate. Xampling is a framework proposed to design such Sub-Nyquist sampling systems. This work presents results and analysis of simulations of impairments in of such systems. The impacts of hardware impairments, choice of reconstruction algorithm, signal energy dynamic range and channel mismatches on system performance are studied. Results show that if a system is designed with a sufficient number of channels then it may be able to maintain performance despite impairment.
-------------------------------------

10138893_183 - 0.995147699221 - technology_and_computing
[dice, implementation, time, model]

A 4-stated DICE: quantitatively addressing uncertainty effects in climate change
We introduce a version of the DICE-2007 model designed for uncertaintyanalysis. DICE is a wide-spread deterministic integrated assessment model of climatechange. However, climate change, long-term economic development, and theirinteractions are highly uncertain. A thorough empirical analysis of the effects ofuncertainty requires a recursive dynamic programming implementation of integratedassessment models. Such implementations are subject to the curse of dimensionality.Every increase in the dimension of the state space is paid for by a combinationof (exponentially) increasing processor time, lower quality of the value function andcontrol rules approximations, and reductions of the uncertainty domain. The paperpromotes a four stated recursive dynamic programming implementation of the DICEmodel. Our implementation solves the infinite planning horizon problem for an arbitrarytime step. Moreover, we present a closed form continuous time approximationto the exogenous (discretely and inductively defined) processes in DICE and presenta Bellman equation for DICE that disentangles risk attitude from the propensity tosmooth consumption over time.
-------------------------------------

10135575_183 - 0.76401236489 - technology_and_computing
[layer, tag, correlation, crf, technique, prediction, result, f1m, datum, two-layer]

Exploiting tag correlations to improve multilabel learning
This thesis looks at applying tags to musical songs as a multilabeling problem. We focus on the CAL500 dataset which summarizes 1704 student reviews into tags for 502 songs. This summarization loses information, so we create the CAL1700 dataset which uses each of the student reviews to generate a single multilabel. We develop a two-layer technique to exploit tag correlations. The first layer makes tag predictions based on data features. The second layer applies correlation information to these predictions to create the final prediction. Our two-layer technique differs from previous stacking methods because it trains both layers on the full training set. Our second layer is a weighted combination of first layer predictions. We look at learning tag correlations using linear-chain conditional random field models (CRF) and using these CRFs as the first layer in our two-layer technique. We exceed previously published results for CAL500 data using our two -layer technique with a linear-chain CRF first layer. We train this CRF using CAL1700 labels. We achieve 0.288 F1- macro (F1M) and 0.401 mean average precision (MAP) scores using 100 data features. The previous best was 0.207 F1M and 0.394 MAP. Our F1M results are significantly better than previously published results
-------------------------------------

10130189_178 - 0.956474967045 - technology_and_computing
[thruster, monitoring, small, characterization, singapore-mit, alliance, environmental, robust, wake, technology]

Thrust and wake characterization in small, robust ultrasonic thrusters
Singapore-MIT Alliance for Research and Technology. Center for Environmental Sensing and Monitoring
-------------------------------------

10130225_178 - 0.999998333642 - technology_and_computing
[discrete, system, grant, transmission, united, states, choosing, office, naval, line]

AC transmission system planning choosing lines from a discrete set
United States. Office of Naval Research (Grant N00014-02-1-0623)
-------------------------------------

10137475_183 - 0.882086302303 - technology_and_computing
[graphite, system, monolayer, isotope, layer, krypton]

Molecular Dynamics Study of Krypton Isotopes Physisorbed on Graphite
Desorption of adsorbed Krypton isotopes is studied via Molecular Dynamics as a function of temperature, density and adsorbed layers. The model simulates a 50-50 mixture of Krypton isotopes where the multilayer systems are built around the first layer which is commensurate with the graphite substrate. It is shown that desorption of an isotropic mixture in a multilayered system (layers > 2) exhibits a separation that is equivalent to Graham's Law while a monolayer system exhibits a separation of the isotopes that is significantly higher. The enhanced effect seen in the monolayer studies is believed to be due to the confinement provided by the graphite substrate. The validation was performed by calculating the vapor pressure of the system as well identifying the melting point of a commensurate monolayer of Kr physisorbed onto graphite. The results of the validation agree within 6% of the experimental results. The limitations of the model as well as possible consequences are presented as well.
-------------------------------------

10130317_178 - 0.999822014035 - technology_and_computing
[model, input-output, augmented, solution, matrix, multiregional, multiplier, mrio]

Multiregional input-output multipliers and the partitioned matrix solution of the augmented MRIO model
Bibliography: leaves 389-415.
-------------------------------------

10138309_183 - 0.989171471287 - technology_and_computing
[model, load, baseline, building, accuracy]

Statistical Analysis of Baseline Load Models for Non-Residential Buildings
Policymakers are encouraging the development of standardized and consistent methods to quantify the electric load impacts of demand response programs. For load impacts, an essential part of the analysis is the estimation of the baseline load profile. In this paper, we present a statistical evaluation of the performance of several different models used to calculate baselines for commercial buildings participating in a demand response program in California. In our approach, we use the model to estimate baseline loads for a large set of proxy event days for which the actual load data are also available. Measures of the accuracy and bias of different models, the importance of weather effects, and the effect of applying morning adjustment factors (which use data from the day of the event to adjust the estimated baseline) are presented. Our results suggest that (1) the accuracy of baseline load models can be improved substantially by applying a morning adjustment, (2) the characterization of building loads by variability and weather sensitivity is a useful indicator of which types of baseline models will perform well, and (3) models that incorporate temperature either improve the accuracy of the model fit or do not change it.
-------------------------------------

10137130_183 - 0.999816674394 - technology_and_computing
[noise, design, rf, wideband, analysis]

Noise in Large-Signal, Time-Varying RF CMOS Circuits: Theory & Design
RF CMOS design is now a mature field and CMOS radio transceivers have become standard in most consumer wireless devices. Like any wireless RF design, at the heart of the endeavor is the requirement to frequency translate signals between baseband and RF with minimal introduction of noise and distortion. This translation is generally accomplished using time-varying, strongly nonlinear circuits, whose operation and noise performance cannot be understood using standard LTI circuit analysis techniques. This work seeks to address some of the design and analysis challenges posed by a variety of these non-linear, time-varying CMOS RF circuits, specifically in the context of low noise design.First, a new wideband receiver architecture is proposed and analyzed.  Using two separate passive-mixer-based down-conversion paths, noise cancelling is enabled, but voltage gain is avoided at unwanted blocker frequencies. This approach significantly relaxes the trade-off between noise, out-of-band linearity and wideband operation.Second, using a phasor-based analysis method, new theoretical results relating to noise mechanisms in LC oscillators are described.  Amplitude noise and Q-degradation is quantified for the first time, while the analysis method is also used to re-derive a fundamental limit to the achievable phase noise of any LC oscillator.Finally, a low-noise, wideband PLL is described that is suitable for emerging mm-wave standards. This design demonstrates that CMOS technology is capable of delivering a high-performance wideband VCO, even at mm-wave frequencies.
-------------------------------------

10140009_183 - 0.999885667818 - technology_and_computing
[beam, rate, design, source, dynamics, high-repetition, light]

BEAM DYNAMICS STUDIES OF A HIGH-REPETITION RATE \\ LINAC-DRIVER
 FOR A 4TH GENERATION LIGHT SOURCE
We present recent progress toward the design of a super-conducting linac driver for a high-repetition rate FEL-based  soft x-ray light source. The machine is designed to accept beams generated by the APEX  photo-cathode gun operating with MHz-range repetition rate and deliver them to an array of SASE and seeded FEL beamlines. We review the current baseline design and report results of beam dynamics studies.
-------------------------------------

10134565_183 - 0.99999944428 - technology_and_computing
[underwater, sensor, modem, network, wireless, small, dense, radio, transducer, design]

Design of a low-cost underwater acoustic modem for short- range sensor networks
Small, dense, wireless sensor networks are beginning to revolutionize our understanding of the physical world by providing fine resolution sampling of the surrounding environment. The ability to have many small devices streaming real-time data physically distributed near the objects being sensed brings new opportunities to observe and act on the world which could provide significant benefits to mankind. While wireless sensor-net systems are beginning to be fielded in applications today on the ground, underwater sensor nets remain quite limited by comparison. Still, a large portion of ocean research is conducted by placing sensors (that measure current speeds, temperature, salinity, pressure, bioluminescence, chemicals, etc.) into the ocean and later physically retrieving them to download and analyze their collected data. Real-time underwater wireless sensor networks that do exist are often sparsely deployed over wide areas. The existence of small, dense wireless sensor networks on land was made possible by the advent of low-cost radio platforms. These radio platforms cost a few hundred U.S. dollars enabling researchers to purchase many nodes with a fixed budget allowing for dense, short-range deployment. The aquatic counterpart to the terrestrial radio is the underwater acoustic modem. Existing underwater acoustic modems' power consumption, ranges, and price points are all designed for sparse, long-range, expensive systems rather than for small, dense, and inexpensive sensor-nets. It is widely recognized that an aquatic counterpart to inexpensive terrestrial radio would be required to enable deployment of small dense underwater wireless sensor networks for advanced underwater ecological analyses. This thesis describes the full design of an underwater acoustic modem for small, dense wireless sensor networks starting with the most critical component from a cost perspective - the transducer. The design replaces a commercial underwater transducer with a homemade underwater transducer using inexpensive piezoceramic material and builds the rest of the modem's components around the properties of the homemade transducer to extract as much performance as possible. By building the modem from inexpensive components, the design provides a low-cost, low-power alternative to existing commercial modems for use in short-range networks
-------------------------------------

10133885_183 - 0.99976125206 - technology_and_computing
[architecture, level, parallel, memory]

Parallel and Scalable Architectures for Video Encoding
As the latest video compression standard, H.264/AVC exhibits great compresion performance than its previous ancestors. Many new features are used to achieve much better rate-distortion eciency and subjective quality, but the high computational complexity and intensive memory access are the penalties. Such high requirement of memory and computational resources leads to long processing cycles and high power consumption. This made real-time encoding of H.264/AVC hard to implement.To address these diculties, this thesis is focused on fast algorithm, data reuse and parallel architectures of H.264/AVC encoder. For data reuse, we proposed a partially forward processing algorithm (PFPA) which reuses the reference information to avoid duplicated reference data loading. For fast algorithms, we studied the statistical features of fractional motion estimation (FME) and proposed a FME mode reductionscheme. For parallel architectures, we proposed two solutions for block level and MB level parallelization respectively. At the block level, we proposed a FME parallel architecture which achieved both memory and processing cycle eciency (reduced about 67%memory accesses and about 50% processing cycles compared with most of state of the art architectures). At the MB level, we proposed wavefront architecture. Theoretically, this architecture can extend a multi-core encoder to a system with any desired numberof cores without sacricing encoding quality.Both JM model and Tensilica XTMP are used to verify the proposed architectures. Architecture implementation detail are discussed and cycle-accurate test results show good performance improvements with very small overhead. From dual-core to three-core and quad-core, the overhead of the P-Core performance are 0.8% and 1.3%for I-frames; 1.7% and 2.4% for P-frames. The speed-ups from dual-core to three-core and quad-core are 1.49 and 1.97 for I-frames; 1.47 and 1.95 for P-frames. System up-scaling methodologies are also covered at the end of this thesis.
-------------------------------------

10129741_178 - 0.858373703614 - technology_and_computing
[bibliographical, cube, reference, model, analysis, energy]

Energy analysis using a cube model
Includes bibliographical references (p. 233-241).
-------------------------------------

10137549_183 - 0.872567506355 - technology_and_computing
[datum, method, path, assimilation, study, swe, equation, vector, twin, mhmc]

Methods for data assimilation in chaotic systems : examples from simple geophysical models
Data assimilation has wide ranging applications, including neuroscience, oceanography and climate science. In this dissertation we will examine data assimilation as a tool for systems of partial differential equations on a discretized spacial grid, using simple geophysical models as a twin for our study. We will use the 1 layer shallow water equations (SWE), and describe how to extend the method to a 2 layer SWE. Although we only used the SWE for this dissertation, we examine how we would use the barotropic vorticity equations (BVE) as the twin in the same study. We will examine two different methods for performing data assimilation on chaotic systems. The first method relies on the measurements to smooth the synchronization manifold, allowing a nonlinear optimizer to correctly determine the most likely path, or the path which minimizes the cost function. The second method we call Metropolis-Hastings Monte Carlo (MHMC) integration scheme. MHMC also allows retention of a group of path samples whose statistics reflect the probability of each path, allowing histograms of state vector values for analysis or inputs to particle filter methods for prediction. The study uses MHMC with the SWE as twin. in this chapter we will examine a data set used for the study. We then describe he various numbers of state vectors needed as data, and the increase in the quality of the fit. We determine the number of state vectors needed as measurements to accurately predict the unmeasured ones
-------------------------------------

10136410_183 - 0.963093095549 - technology_and_computing
[database, hypothesis, historical]

A Historical Database of Sociocultural Evolution
The origin of human ultrasociality—the ability to cooperate in huge groups of genetically unrelated individuals—has long interested evolutionary and social theorists, but there has been little systematic empirical research on the topic. The Historical Database of Sociocultural Evolution, which we introduce in this article, brings the available historical and archaeological data together in a way that will allow hypotheses concerning the origin of ultrasociality to be tested rigorously. In addition to describing the methodology informing the set-up of the database, our article introduces four hypotheses that we intend to test using the database. These hypotheses focus on the resource base, warfare, ritual, and religion, respectively. Ultimately the aim of our database is to offer a ‘rapid discovery science’ route to the study of the past. We believe our approach is not only highly complementary with existing traditions of enquiry in history and archaeology but will extend their intellectual scope and explanatory power.
-------------------------------------

101379_108 - 0.999989776351 - technology_and_computing
[service, network, mobility, wireless, multimedia, wlan, architecture, ip-based, system, infrastructure]

QoS Provisioning and Mobility Management for IP-Based Wireless LAN
<p>Today two major technological forces drive the telecommunication era: the wireless cellular systems and the Internet. As these forces converge, the demand for new services, increasing bandwidth and ubiquitous connectivity continuously grows. The next-generation mobile systems will be based solely or in a large extent, on the Internet Protocol (IP). This thesis begins by addressing the problems and challenges faced in a multimedia, IP-based Wireless LAN environment. The ETSI HiperLAN/2 system has been mainly selected as the test wireless network for our theoretical and simulation experiments. Apart from the simulations, measurements have been taken from real life test scenarios, where the IEEE 802.11 system was used (UniS Test-bed). Furthermore, a brief overview of the All-IP network infrastructure is presented. An extension to the conventional wireless (cellular) architecture, which takes advantage of the IP network characteristics, is considered. Some of the trends driving the 3G and WLANs developments are explored, while the provision of quality of service on the latter for real-time and non-real-time multimedia services is investigated, simulated and evaluated. Finally, an efficient and catholic Q0S framework is proposed. At the same time, the multimedia services should be offered in a seamless and uninterrupted manner to users who access the all-IP infrastructure via a WLAN, meeting the demands of both enterprise and public environments anywhere and anytime. Thus providing support for mobile communications not only in terms of terminal mobility, as is currently the case, but also for session, service and personal mobility. Furthermore, this mobility should be available over heterogeneous networks, such as WLANs, IJMTS, as well as fixed networks. Therefore, this work investigates issues such as, multilayer and multi-protocol (SIP-Mobile IP-Cellular IP) mobility management in wireless LAN and 3G domains. Several local and global mobility protocols and architectures have been tested and evaluated and a complete mobility management framework is proposed. Moreover, integration of simple yet efficient authentication, accounting and authorisation mechanisms with the multimedia service architecture is an important issue of IP-based WLANs. Without such integration providers will not have the necessary means to control their provided services and make revenue from the users. The proposed AAA architecture should support a robust AAA infrastructure providing secure, fast and seamless access granting to multimedia services. On the other hand, a user wishing a service from the All-IP WLAN infrastructure needs to be authenticated twice, once to get access to the network and the other one should be granted for the required service. Hence, we provide insights into these issues by simulating and evaluating pre-authentication techniques and other network authentication scenarios based on the wellknown IEEE 802.lx protocol for multimedia IP-based WLANs.</p>
-------------------------------------

10136764_183 - 0.999888557394 - technology_and_computing
[flash, policy, file, site, crossdomain]

Analyzing the flash crossdomain policies
Adobe Flash is a multimedia platform used for developing rich internet applications. Flash also facilitates sharing resources and data between flash files on different domains. Sites that allow sharing of content need to host a crossdomain policy file, crossdomain.xml which has a list of websites that can access resources from this site. This thesis attempts to survey the crossdomain.xml Flash policy file present accross the Alexa top 50,000 websites. We found that 3609 out of the 47197 surveyed sites had unrestricted crossdomain Flash access. These numbers suggest that Flash crossdomain.xml policy files are liable to misconfiguration. We also propose some mitigation techniques for the same
-------------------------------------

10137533_183 - 0.853818317035 - technology_and_computing
[model, result]

Strong Correlations in Lattice Models
The primary aim of this thesis is to understand the effects of electronic frustration generated by local interactions of the Hubbard U type on a lattice. Specifically we attempt to study the strongly interacting liquid phase of the Hubbard model and the so called extremely correlated phase of the t-J model without any broken symmetries. Of particular interest is the fate of the Fermi Liquid state under the influence of a large U. Exact solutions in solvable limits inform most of the physical intuition. Perturbative methods such as an infinite order calculation in the particle-particle channel ladder expansion yield fruitful results. Finally the non-perturbative field theory of Schwinger is used to generate a calculation scheme for the Green's function of the non-canonical Fermions of the t-J model. Numerical results are compared with important experimental results from the high-T<sub>c</sub> Cuprate compounds.
-------------------------------------

10175396_189 - 0.968922624636 - technology_and_computing
[array, area, surface, form, study, commercial, nanosphere, optical, structure, property]

Characterizing Engineered Nanomaterials: From Environmental, Health and Safety Research to the Development of Shaped Nanosphere Lithography for Metamaterials
In this thesis two issues in nanotechnology have been addressed. The first is the comprehensive characterization of engineered nanomaterials prior to their examination in toxicology and environmental studies.  The second is the development of a method to produce nanostructure arrays over large areas and for low cost.
	A major challenge when assessing nanomaterial’s risks is the robust characterization of their physicochemical properties, particularly in commercial products.  Such data allows the critical features for biological outcomes to be determined.  This work focused on the inorganic oxides that were studied in powdered and dispersed forms as well as directly in consumer sunscreen products.  The most important finding was that the commercial sunscreens that listed titania or zinc oxide as ingredients contained nanoscale materials. Cell free photochemical tests revealed that ZnO particles without any surface coating were more active at generating ROS than surface coated TiO2 nanoparticles. These studies make clear the importance of exposure studies that examine the native form of nanomaterials directly in commercial products. 
	The second part of this thesis presents the development of a new method to fabricate gold nanoring and nanocrescent arrays over large areas; such materials have unique optical properties consonant with those described as metamaterials. A new shaped nanosphere lithography approach was used to manipulate the form of silica nanospheres packed onto a surface; the resulting array of mushroom structures provided a mask that after gold evaporation and etching left either golden rings or crescents over the surface.  The structures had tunable features, with outer diameters ranging from 200 to 350 nm for rings and crescent gap angles of ten to more than a hundred degrees.  The use of a double mask method ensured the uniform coverage of these structured over 1 cm2 areas. Experimental and theoretical investigations of the optical properties of the arrays revealed the optical resonances in the infrared region.  Finally, in the course of developing the nanorings, etch conditions were developed to deposit large area arrays of polystyrene nanodoughnuts with diameters from 128 to 242 nm. These non-conductive structures provide an ideal template for further attachment of magnetic of optically emissive nanoparticles.
-------------------------------------

10130041_178 - 0.884433527985 - technology_and_computing
[area, job-search, model, bibliographical, metropolitan, footnote, migration]

A job-search model of migration between metropolitan areas.
Includes bibliographical footnotes.
-------------------------------------

10130444_178 - 0.999441838067 - technology_and_computing
[cooperative, navigation, monitoring, distribution, alliance, environmental, framework, measurement, technology, singapore-mit]

A measurement distribution framework for cooperative navigation using multiple AUVs
Singapore-MIT Alliance for Research and Technology. Center for Environmental Sensing and Monitoring
-------------------------------------

10134790_183 - 0.857012812993 - technology_and_computing
[model, class, design, function, criterion]

Optimum Designs for Identification and Discrimination within a Class of Competing Linear Regression Models
We consider the problem of finding optimum designs for model  identification and discrimination where the dependence of the response variable Y on an explanatory variable X can be described by at most a third order model. We therefore consider a class that includes all the models up to a maximum of third order with linear, quadratic, and cubic coefficients present. In addition all models have an intercept parameter. A general class of designs with 4 distinct points x_1, x_2, x_3, and x_4 is considered with replications n1; n2; n3, and n4 respectively, satisfying n_1 + n_2 + n_3 + n_4 = n where n is known in advance. While discriminating between two models from the class of models considered, the true model may or may not be one of them. We define the predictive criterion function I and the fitting criterion function J. When the functions I and J are dependent on more than one model parameters, we define the additional criterion functions K_I and K_J. We use the proposed optimality criterion functions to obtain the optimal designs for the model identification and discrimination.
-------------------------------------

10130286_178 - 0.834185241908 - technology_and_computing
[multistory, building, precast, pertinent, analysis, office, common, design, material, concrete]

Precast concrete multistory office building
Preface: By presenting the three individual theses with a common analysis of material pertinent to the design of each thesis it is hoped that a more rich and extensive result has been attained.
-------------------------------------

101661_108 - 0.99732632357 - technology_and_computing
[information, communication, accessibility, informational, disabled]

Enabling Disabled Tourists? Accessibility Tourism Information Schemes
<p>Embedded in information search theory and based on exploratory research, this paper investigates accessibility tourism information schemes as information communication sources and their potential to fulfill the informational needs of tourists with a disability. Five interrelated need components are identified: richness and reliability of information, appropriate information sources, communication tools and customer-orientation. The results show that the existing schemes studied only partly comply with informational requirements. Major limitations include their high fragmentation and lack of geographical reach, which result in deficiencies in quality, depth and breadth of information, appropriate customer-oriented and communication services using mainstream media. To achieve information satisfaction and fully enable disabled people, it is advocated that a pan-European approach to accessibility is necessary to remove informational barriers that currently restrict travel options of disabled individuals.</p>
-------------------------------------

10134491_183 - 0.999998396119 - technology_and_computing
[datum, network, approach, optical, application, stream, digital, parallel, challenge, cross-stream]

Toward petabyte digital content transfer and preservation over optical networks
Optical networks have become the central elements in many scientific applications. And the emerging of those data intensive applications also brings about new challenges to the approaches on how to efficiently use the abundant bandwidth provided by optical networks. From some of the applications, we observe a parallel data access model, and are thus motivated to study a new approach to use parallel data streams to better feed the parallel data access model. Parallelization of data streams leads to new challenges which are not sufficiently addressed in previous related works. The new approach we propose in this thesis improves existing approaches through tightly coupling data streams using Cross-Stream coding. It can effectively resolve unevenly distributed code packet loss at the receiving side to eliminate the necessity of frequently sending acknowledgments back to the senders requesting missing data. We further prove that the Cross- Stream coding approach is scalable, and give out practical algorithms to find optimal solutions for large number of parallel streams, for any given objective Cross-Stream packet loss pattern. We experimented with our Cross-Stream coding approach using computer clusters. We also observe, from other applications, that optical networks can improve traditional digital archiving methods, and will include our preliminary research of using optical networks to facilitate a workflow based digital archiving system, named CineGrid Exchange. The CineGrid Exchange has been practically online for about a year, storing and managing 20 Tera bytes digital contents. In summary, both the requirements of large scale network applications and the available bandwidth of optical networks increase faster than the development of middleware between them. And there exist significant challenges to alleviate the difference. The research in this thesis is motivated by these challenges, and explores possible directions towards future petascale data transfer and preservation
-------------------------------------

10139275_183 - 0.883445862079 - technology_and_computing
[hpc, file, workshop, doe]

The Fifth Workshop on HPC Best Practices: File Systems and Archives
The workshop on High Performance Computing (HPC) Best Practices on File Systems and Archives was the fifth in a series sponsored jointly by the Department Of Energy (DOE) Office of Science and DOE National Nuclear Security Administration. The workshop gathered technical and management experts for operations of HPC file systems and archives from around the world.  Attendees identified and discussed best practices in use at their facilities, and documented findings for the DOE and HPC community in this report.
-------------------------------------

10138397_183 - 0.906167776152 - technology_and_computing
[window, frame, u-factor, performance]

Key Elements of and Materials Performance Targets for Highly Insulating Window Frames
The thermal performance of windows is important for energy efficient buildings. Windows typically account for about 30-50 percent of the transmission losses though the building envelope, even if their area fraction of the envelope is far less. The reason for this can be found by comparing the thermal transmittance (U-factor) of windows to the U-factor of their opaque counterparts (wall, roof and floor constructions). In well insulated buildings the U-factor of walls, roofs an floors
can be between 0.1-0.2 W/(m2K). The best windows have U-values of about 0.7-1.0. It is therefore obvious that the U-factor of windows needs to be reduced, even though looking at the whole energy balance for windows (i.e. solar gains minus transmission losses) makes the picture more complex. In high performance windows the frame design and material use is of utmost importance, as the frame performance is usually the limiting factor for reducing the total window U-factor further. This paper
describes simulation studies analyzing the effects on frame and edge-of-glass U-factors of different surface emissivities as well as frame material and spacer conductivities. The goal of this work is to define materials research targets for window frame components that will result in better frame thermal performance than is exhibited by the best products available on the market today.
-------------------------------------

10135658_183 - 0.999881540491 - technology_and_computing
[model, heater, signal, height, input, datum, hard, element, measurement, discrete-time]

Nano-scale positioning, control and motion planning in hard disk drives
In this dissertation, we focus on optimization of cross- track and vertical positioning of the read/write element over the data track. First, a data-based approach is presented for modeling and controller design of a dual- stage servo actuator in a hard disk drive. Based on discrete-time models, different dual-stage track-following controllers were designed using classic and H-infinity loop shaping techniques. The controllers were implemented in real-time. Next, an input shaping algorithm based on convex optimization techniques is presented for closed- loop discrete-time linear time-invariant (LTI) system. The proposed algorithm allows closed-loop signals to be subjected to linear constraints on amplitude and rate of change. As an illustrative example the seeking process in a hard disk drive is investigated and experimentally verified. To study the dependence of the read signal on cross-track and vertical motion, a straightforward analytical model for the read back signal is derived for perpendicular and longitudinal magnetic recording. The model captures the contribution of a single bit rather than the contribution of a bit transition which makes it applicable to patterned media as well as continuous media. In addition, a novel method of measuring the relative head -medium spacing based on the measurement of the read back signal from servo sectors is developed. The spacing measurement is tested experimentally on a spin stand where the flying height is varied using the resistance heater element in a thermal flying height control slider. In addition, voltage step response measurements were obtained for data based modeling. Finally, a dynamic model of the resistance heater in a thermal flying height control (TFC) slider is identified based on experimentally obtained step -input data. A generalized realization algorithm is used for identification of a discrete-time dynamic model of the resistance heater. Based on the identified model and convex optimization techniques, a computational scheme is proposed to obtain optimized feed forward input profiles to the heater element that minimize repeatable flying height variations. The optimized input signals were applied to the heater and greatly reduced flying height variations were observed in spinstand experiments
-------------------------------------

10133640_183 - 0.802369463454 - technology_and_computing
[system, modeling, molecular, geometry, chemistry, device, transport, engineering, application, charge]

High Throughput Ab initio Modeling of Charge Transport for Bio-Molecular-Electronics
Self-assembled nanostructures, composed of inorganic and organic materials, have multiple applications in the fields of engineering and nanotechnology. Experimental research using nanoscaled materials, such as semiconductor/metallic nanocrystals, nanowires (NW), and carbon nanotube (CNT)-molecular systems have potential applications in next generation nano electronic devices. Many of these molecular systems exhibit electronic device functionality. However, experimental analytical techniques to determine how the chemistry and geometry affects electron transport through these devices does not yet exist. Using theory and modeling, one can approximate the chemistry and geometry at the atomic level and also determine how the chemistry and geometry governs electron current. Nanoelectronic devices however, contain several thousand atoms which makes quantum modeling difficult. Popular atomistic modeling approaches are capable of handling small molecular systems, which are of scientific interest, but have little engineering value. The lack of large scale modeling tools has left the scientific and engineering community with a limited ability to understand, explore, and design complex systems of engineering interest. To address these issues, I have developed a high performance general quantum charge transport model based on the non-equilibrium Green function (NEGF) formalism using density functional theory (DFT) as implemented in the FIREBALL software. FIREBALL is a quantum molecular dynamics code which has demonstrated the ability to model large molecular systems. This dissertation project of integrating NEGF into FIREBALL provides researchers with a modeling tool capable of simulating charge current in large inorganic / organic systems. To provide theoretical support for experimental efforts, this project focused on CNT-molecular systems, which includes the discovery of a CNT-molecular resonant tunneling diode (RTD) for electronic circuit applications. This research also answers basic scientific questions regarding how the geometry and chemistry of CNT-molecular systems affects electron transport.
-------------------------------------

10134763_183 - 0.714154450666 - technology_and_computing
[transit, mass, rail, density]

Mass Transit & Mass: Densities Needed to Make Transit Investments Pay Off
Rail transit has become a political lightning rod. Critics view rail proposals as among the most flagrant forms of pork barrel politics today. Advocates counter-argue that expanding the nation's rail transit offerings will yield many under-appreciated environmental and societal benefits, including reduced carbon emissions and dependency on foreign oil supplies.
      If fixed-guideway transit is to yield appreciable dividends, there must be a close correspondence between transit investments and urban development patterns. All to often, rail transit investments in the U.S. have been followed by growth that is oriented to highway rather than transit corridors. Put simply, "mass transit" needs "mass" - i.e. density. For investments to pay off, there must be an unwavering local commitment to substantially raise population and employment densities along transit corridors. While dense areas average higher transit capital costs as well as higher ridership, our analysis suggests that many transit stations in the U.S. have nearby job or population densities that are too low to support cost-effective transit service. The thresholds in this study can provide cities and towns with targets for zoning around existing and proposed transit stations based on projected costs.
-------------------------------------

10139349_183 - 0.999928143194 - technology_and_computing
[computing, cloud, project, magellan]

The Magellan Final Report on Cloud Computing
The goal of Magellan, a project funded through the U.S. Department of Energy (DOE) Office of Advanced Scientific Computing Research (ASCR), was to investigate the potential role of cloud computing in addressing the computing needs for the DOE Office of Science (SC), particularly related to serving the needs of mid- range computing and future data-intensive computing workloads. A set of research questions was formed to probe various aspects of cloud computing from performance, usability, and cost. To address these questions, a distributed testbed infrastructure was deployed at the Argonne Leadership Computing Facility (ALCF) and the National Energy Research Scientific Computing Center (NERSC). The testbed was designed to be flexible and capable enough to explore a variety of computing models and hardware design points in order to understand the impact for various scientific applications. During the project, the testbed also served as a valuable resource to application scientists. Applications from a diverse set of projects such as MG-RAST (a metagenomics analysis server), the Joint Genome Institute, the STAR experiment at the Relativistic Heavy Ion Collider, and the Laser Interferometer Gravitational Wave Observatory (LIGO), were used by the Magellan project for benchmarking within the cloud, but the project teams were also able to accomplish important production science utilizing the Magellan cloud resources.
-------------------------------------

10130142_178 - 0.99998535108 - technology_and_computing
[network]

Energy aware network coding in wireless networks
Includes bibliographical references (p. 97-104).
-------------------------------------

10129943_178 - 0.999975427379 - technology_and_computing
[three-dimensional, structure, system, psl, manufacturing, complex]

Design and optimization of a light-emitting diode projection micro-stereolithography three-dimensional manufacturing system
The rapid manufacture of complex three-dimensional micro-scale components has eluded researchers for decades. Several additive manufacturing options have been limited by either speed or the ability to fabricate true three-dimensional structures. Projection micro-stereolithography (PμSL) is a low cost, high throughput additive fabrication technique capable of generating three-dimensional microstructures in a bottom-up, layer by layer fashion. The PμSL system is reliable and capable of manufacturing a variety of highly complex, three-dimensional structures from micro- to meso-scales with micro-scale architecture and submicron precision. Our PμSL system utilizes a reconfigurable digital mask and a 395 nm light-emitting diode (LED) array to polymerize a liquid monomer in a layer-by-layer manufacturing process. This paper discusses the critical process parameters that influence polymerization depth and structure quality. Experimental characterization and performance of the LED-based PμSL system for fabricating highly complex three-dimensional structures for a large range of applications is presented.
-------------------------------------

10131037_183 - 0.999385192537 - technology_and_computing
[annotated, cross-cultural, standard, codebook, sample, cumulative]

STDS00.COD
This is the Annotated Cumulative Codebook: Standard Cross-Cultural Sample.
-------------------------------------

10135987_183 - 0.714145118259 - technology_and_computing
[game, tool, designer, design, level, content, generation]

Expressive Design Tools: Procedural Content Generation for Game Designers
Games are shaped by the tools we use to make them and our ability to model the concepts they address. Vast improvements in computer graphics technology, processing power, storage capacity, and physics simulations have driven game design for the past forty years, leading to beautiful, spacious, detailed, and highly immersive worlds supporting games that are, for the most part, fundamentally about movement, collision, and other physics-based concepts. Designers use increasingly complex tools that support the creation of these worlds. The games industry is undoubtedly thriving on these technologies, but what comes next? The tools that game designers use today are highly complex--too complex for a growing population of novice designers, and offering little support for reasoning about gameplay--and there are entire genres of games that cannot be made using the tools available to us now. This dissertation presents the use of procedural content generation to create <bold>expressive design tools</bold>: content generators that are accessible to designers, supporting the creation of new kinds of design tools and enabling the exploration of a new genre of game involving the deep integration of procedural content generation into game mechanics and aesthetics. The first of these tools is Tanagra, the first ever AI-assisted level design tool that supports a designer creating levels for 2D platforming games. Tanagra guarantees that levels created in the tool are playable, and provides the designer with the ability to modify generated levels and directly control level pacing. The second tool is Launchpad, which supports a designer controlling both component and pacing features of generated levels; its companion game <italic>Endless Web</italic> uses the generator to create an infinite world for players to explore and alter through their choices. <italic>Endless Web</italic> is one of a handful of games in a new genre enabled by content generation: PCG-based games. Finally, this dissertation presents a novel method for understanding, visualizing, and comparing a generator's expressive range, thus allowing designers to understand the implications of decisions they will make during the design process.
-------------------------------------

10138296_183 - 0.999101139718 - technology_and_computing
[ps2, upgrade, e-cloud, effect]

E-Cloud Effects on Singe-Bunch Dynamics in the Proposed PS2
One of the options considered for future upgrades of the
LHC injector complex entails the replacement of the PS
with the PS2, a longer circumference and higher energy
synchrotron. Electron cloud effects represent an important
potential limitation to the achievement of the upgrade
goals. We report the results of numerical studies aiming
at estimating the e-cloud density thresholds for the occurrence
of single bunch instabilities.
-------------------------------------

10134921_183 - 0.999674866113 - technology_and_computing
[frequency-domain, domain, response, code, time-domain, target, datum, impulse, time, marine]

A comparison of marine time-domain and frequency-domain controlled source electromagnetic methods
The frequency-domain marine controlled source electromagnetic (CSEM) method has recently become a tool in determining subsurface resistivity related to hydrocarbon formations in the deep water environment. In shallow water, this frequency-domain method is subject to airwave saturation that severely limits sensitivity to targets at depth. It has been suggested that time-domain CSEM may offer an improved resolution to these deep targets, as well as increased sensitivity to resistors in the presence of the airwave. In order to examine and test these claims, a modeling code has been developed for computing time-domain responses for layered 1D models with arbitrarily located and oriented transmitters and receivers. The code extends the open-source frequency domain code Dipole1D by efficiently computing the time- domain, step-on, and impulse responses by Fourier transformation of the frequency-domain kernels. Impulse responses are used along with pseudo-random binary sequences (PRBS) to generate synthetic time-domain data. A realistic noise model and waveform scaling effects are then applied to synthetic step-on, PRBS, and the frequency -domain SIO "Waveform D" data generated from this code. Wiener deconvolution is applied to recover impulse responses from the PRBS data, allowing for a systematic examination of the sensitivity and resolution of time- domain and frequency-domain CSEM to representative targets of interest for offshore hydrocarbon exploration. These studies suggest that there is no large ad- vantage to time -domain techniques, as previously suggested, and rather that the frequency-domain Waveform D should give better results in the presence of noise for the shallow marine setting
-------------------------------------

10131002_183 - 0.996981834973 - technology_and_computing
[datum, code, whiting, weather]

STDS08.COD:  Climate Data from Weather Stations
This file describes codes for variables v179-v199 of the SCCS data set. They are from previously unpublished codes done by John W. M. Whiting, originally referenced in "Winter temperature as a constraint to the migration of preindustrial peoples" Whiting et al. American Anthropologist 84:279-298 (1982). The weather data are cited as coming from Walter, H., and H. Leith (1964) Klimadiagramm-Weltatlas, Jena: Gustav Fischer.
-------------------------------------

10138256_183 - 0.989031662792 - technology_and_computing
[power, reactive, der, cost, support, voltage]

Assessment of the Economic Potential of Microgrids for Reactive Power Supply
As power generation from variable distributed energy resources (DER) grows, energy flows in the network are changing, increasing the requirements for ancillary services, including voltage support. With the appropriate power converter, DER can provide ancillary services such as frequency control and voltage support. This paper outlines the economic potential of DERs coordinated in a microgrid to provide reactive power and voltage support at its point of common coupling. The DER Customer Adoption Model assesses the costs of providing reactive power, given local utility rules. Depending on the installed DER, the cost minimizing solution for supplying reactive power locally is chosen. Costs include the variable cost of the additional losses and the investment cost of appropriately over-sizing converters or purchasing capacitors. A case study of a large health care building in San Francisco is used to evaluate different revenue possibilities of creating an incentive for microgrids to provide reactive power.
-------------------------------------

10133897_183 - 0.789237424122 - technology_and_computing
[library, synthetic, oscillator, unit, copy, gene, repressor, strain]

Creation and observation of a library of synthetic bacterial oscillators
Synthetic biology utilizes computer science, physics and molecular biology to create novel gene circuits to further our knowledge of naturally occurring systems. This thesis addresses the construction and observation of a library of synthetic bacterial gene oscillators. The study furthers the characterization of known synthetically designed systems. The library is based on a change in copy number of activator and repressor units in the dual feedback oscillator network. By changing the copy number of activator or repressor units, oscillators with a wide range in periods have been created. Twelve different strains were constructed by varying the origins of replication. Three of the strains contain an integrated repressor unit, acting as a single copy in the cell. A high throughput method to observe the strains has also been created to accompany the library. A custom created microfluidic chip is used to observe the cells using fluorescence microscopy, while an automated tracking program was made to record the oscillations. The library and high throughput method has created data used to further characterize synthetic gene circuits
-------------------------------------

10134209_183 - 0.76162771388 - technology_and_computing
[rf, filter, switch, tunable, mems, mem, device, capacitive]

Stress-tolerant and temperature-stable RF MEMS capacitive switches and tunable filters
This dissertation presents RF MEMS capacitive switches which are based on a thin-film aluminum circular beam geometry that exhibit reduced sensitivity to both initial residual stress and stress-changes versus ambient temperature. The device symmetry also facilitates low- series-inductance compact device arrays for high-value capacitances. These switches are built in the Raytheon RF MEMS process and show an 8-10x improvement in temperature stability over the standard fixed-fixed beam designs. Also, cascadable RF MEMS switched capacitors are demonstrated that are suitable for VHF and UHF tunable filters and reconfigurable matching networks. These devices are fabricated in the UCSD and Raytheon RF MEMS processes and result in a near ideal capacitor impedance over a 30:1 frequency range. The circular geometry is then used to demonstrate an RF MEMS switched capacitor with 10 W power handling at 10 GHz under hot-switching conditions that maintains a relatively-low (< 30 V) pull-in voltage. The device consists of separate RF and DC electrodes, which are defined underneath a temperature-stable circular beam, to result in both increased restoring force above the RF electrode and higher RF self-actuation voltage. This thesis also presents a compact low-loss tunable X-Band bandstop filter that is implemented on a quartz substrate using both miniature RF-MEMS capacitive switches and GaAs varactors. The 2-pole filter is based on capacitively loaded folded-lambda resonators that are coupled to a microstrip line, and the filter analysis includes the effects of non-adjacent inter-resonator coupling. The RF MEMS loaded filter results in a measured 25 dB improvement in power handling and linearity compared to the GaAs varactor design. Finally, a 1.6-2.4 GHz suspended 3-pole RF MEMS tunable filter is presented. The filter results in an insertion loss of 1.34-3.03 dB over the tuning range and a 3-dB bandwidth of 201-279 MHz. This design results in a tunable Q_u of 50-150 over the frequency range, and to our knowledge, is the first suspended RF MEMS filter with the best Q_u. The Appendix presents in full detail the UCSD RF MEMS capacitive switch process on a high- resistivity silicon substrate
-------------------------------------

10137722_183 - 0.999940928188 - technology_and_computing
[algorithm, online, cost, problem, file, variant, fastpc]

Topics in Approximation Algorithms
This thesis focuses on approximation and online algorithms for a few different problems.1) There have been continued improvements in the approximation algorithms for packing and covering problems. Some recent algorithms, including Fastpc, have provable good worst-case running times, but it is not known how they perform in practice compared to the simplex and interior-point algorithms that are widely used to solve these problems. We present an empirical comparison of these algorithms, using our own implementation of Fastpc and CPLEX implementations of simplex and interior-point methods. We use a variety of inputs for this experimental study. We find that Fastpc is slower for small problem instances, but its performance, relative to CPLEX algorithms, improves as the instances get bigger. Our experiments show that for reasonably large inputs Fastpc performs better than the CPLEX algorithms.2) We give deterministic algorithms for some variants of online file caching by reducing the problems to online covering. The variants considered in this study include one or both of the following features: (i) a rental cost for each slot occupied in the cache, and (ii) zapping a file by paying a cost so that the zapped file does not occupy any space in the cache and does not incur any retrieval cost. The rental cost is motivated by the idea of energy efficient caching where caching systems can save power by turning off slots not being used to store files. Our approach is based on the online covering algorithm by Koufogiannakis and Young. We give deterministic lower bounds for these variants, which are tight within con-stant factors of the upper bounds for most of the cases. We also give randomized algorithms and lower bounds for the variants with rental cost.3) We introduce online Huffman coding. In Huffman coding, the symbols are drawn from a probability distribution and revealed one by one, and the goal is to find a minimum cost prefix-code for the symbols. In the online version, the algorithm has to assign a codeword to a symbol when it is revealed for the first time. We propose an online greedy algorithm and show that it is constant-competitive for online Huffman coding. We also show a lower bound of 10/9 on the competitive ratio of any deterministic online algorithm.
-------------------------------------

10136663_183 - 0.99999625517 - technology_and_computing
[antenna, power, active, design, oscillator, order, high, zor, cpw]

A Power Efficient Active Integrated Antenna Using Zeroth Order Antenna
A microwave oscillator is integrated with a zeroth order resonance (ZOR) antenna to build an active oscillating antenna using the coplanar waveguide (CPW) technology. In this thesis, the basic metamaterial concept is introduced to understand the ZOR antenna and negative resistance method is also introduced to explain the design procedure of the oscillator type active integrated antenna (AIA). The CPW technology is utilized to design the entire circuit. The CPW TL gives a lot of advantages such as fabrication simplicity and design freedom. The ZOR antenna is utilized due to its electrically small size and high Q-factor which results in good phase noise of -92.5dBc/Hz at 100kHz offset. The proposed AIA is designed to achieve high DC-RF power conversion efficiency by suppressing higher order harmonic power. As a result, the radiated power of 14.57dBm (28.64mW) and the effective isotropic radiated power of (EIRP) 16.11dBm (40.83mW) is obtained with high DC to RF efficiency (27.2%), which is much higher than other active oscillator type AIA designs.
-------------------------------------

10136172_183 - 0.977400574844 - technology_and_computing
[end, processing, transcription, poly, cleavage, coupling, signal, system, rna, activator]

Molecular Basis of Coupling 3'-end Processing to Transcription in Mammals
A major focus of my studies is to understand how mRNA transcription is functionally interconnected, or coupled, to 3'-end processing (cleavage and polyadenylation) of mRNAs .  A product of this coupling effect is that transcription can affect and enhance 3'-end processing.  To understand how transcription is able to affect 3'-end processing, I investigated a few mechanisms that have been suggested in the literature to be the primary modes of coupling 3'-end processing to transcription: recruitment of processing factors to the promoter, cotranscriptional recruitment of processing factors during gradual extrusion of the poly(A) signal from the polymerase, and phosphorylation of C-terminal domain (CTD) of RNA polymerase II (Chapter 3).  Using an in vitro system that can support 3'-end processing that is functionally coupled to transcription, I tested the extent to which each of these mechanisms contribute to coupling in our in vitro system.  I demonstrate that recruitment of processing factors to the promoter or to the transcription elongation complex prior to transcription of the poly(A) signal is not necessary for coupled 3'-end processing in our in vitro system.  I found that assembly of cleavage and polyadenylation factors during gradual extrusion of the poly(A) signal from the polymerase is not important for coupling.  I also show that the phosphates put on the C-terminal domain (CTD) of the transcribing polymerase at or shortly after the time of transcription initiation are unimportant for coupling in our system.  Surprisingly, nothing that happens at the promoter is necessary to direct coupling since transcription elongation complexes (TEC) initiated on oligo(dT) tailed templates can still engage in 3'-end processing that is an order of magnitude more efficient than processing uncoupled from transcription.  Therefore, these results argue that the ability to couple 3'-end processing to transcription is intrinsic to the structure of the TEC.  Consistent with this, I found that coupling is inhibited if the RNA tether which connects the RNA transcript to the polymerase is severed.  Based on these results, we propose that coupling is somehow mediated by having the poly(A) signal held close to the CTD via an intact RNA tether in the context of the TEC.Another aspect of this coupling phenomenon was demonstrated by Blencowe's group in which they reported that transcriptional activators can contribute to 3'-end cleavage in vivo.  It is intriguing that transcriptional activators that are recruited to the promoter can affect a process that occurs at the other end of the gene.  To understand this further, I investigated the effect of transcriptional activators on 3'-end processing in vitro (Chapter 5).  I found that while transcriptional activators can dramatically enhance transcription, they did not alter 3'-end processing levels significantly in a coupled in vitro system.  However, I did observe that transcription initiates randomly in the absence of a transcriptional activator to direct promoter-specific transcription.  At face value, these results suggest that the mechanism by which transcriptional activators affect 3'-end processing cannot be supported in our in vitro system.   However, my results do suggest a possibility that transcriptional activators may affect cleavage levels by redirecting initiation from cryptic promoters to promoter-specific initiation.It has been suggested that coupling of 3'-end processing to transcription may be in part mediated by speeding up the assembly of the cleavage and polyadenylation apparatus (CPA) at the poly(A) signal.  One way that this is thought to be achieved is by early recruitment of processing factors to the promoter during initiation and its subsequent transfer to the elongating polymerase so that assembly can begin as soon as the poly(A) signal is transcribed.  While this may be the case, very little is known regarding the actual assembly process of the CPA and the rate-limiting step that may be subject to regulation.  Therefore, I sought to dissect the assembly pathway of the CPA in hopes of revealing how 3'-end processing may be regulated by transcription.  I decided to begin by resolving whether ATP is a required cofactor for 3'-end processing since there have been conflicting reports in the literature.  Interestingly, I found that ATP is indeed a required cofactor for 3'-end cleavage in our in vitro system since withholding ATP can inhibit cleavage (Chapter 4).  I found that this inhibition is reversible since cleavage resumes upon re-addition of ATP.  I also demonstrate that a partial CPA can form in the absence of ATP and this partial apparatus consists of at least CPSF and CstF, the core cleavage and polyadenylation complexes.  Based on preliminary data, the rate-limiting step of the cleavage pathway occurs during or after the ATP-requiring step.  These results suggest that ATP may play an important role in regulation of 3'-end cleavage.  The significance of an ATP requirement for 3'-end cleavage in our in vitro system will be discussed. An interesting product of the interconnection between 3'-end processing to transcription is the possibility that there may be a coordinated surveillance mechanism that can assess the quality of the transcript being made and decides the fate of the transcript by directing it to process or to degrade.  Here, we studied poly(A)-dependent pausing in vitro, which has been proposed as a surveillance checkpoint, and poly(A)-dependent degradation of unprocessed transcripts from weak poly(A) signals (Chapter 2).  We confirm directly, by measuring the length of RNA within isolated transcription complexes, that a newly transcribed poly(A) signal reduces the rate of elongation by RNA polymerase II, resulting in the accumulation of these complexes downstream of the poly(A) signal.  We then show that if the RNA in these elongation complexes contain a functional but unprocessed poly(A) signal, degradation of the transcripts ensues.  We propose that during normal 3'-end processing, a decision is made whether to process or to degrade.  In the case of weak poly(A) signals, where cleavage at the poly(A) site is slow, the default pathway to degradation predominates.
-------------------------------------

10137133_183 - 0.945417081504 - technology_and_computing
[transfer, minimum, fuel, orbital, perturbation, solution, method, thrust, low-thrust, preliminary]

Optimum Low Thrust Elliptic Orbit Transfer using Numerical Averaging
Low-thrust electric propulsion is increasingly being used for spacecraft missions primarily due to its high propellant efficiency. Since analytical solutions for general low-thrust transfers are not available, a simple and fast method for low-thrust trajectory optimization is of great value for preliminary mission planning. However, few low-thrust trajectory tools are appropriate for preliminary mission design studies. The method presented in this paper provides quick and accurate solutions for a wide range of transfers by using numerical orbital averaging to improve solution convergence and include orbital perturbations. Thus allowing preliminary trajectories to be obtained for transfers which involve many revolutions about the primary body. This method considers minimum fuel transfers using first order averaging to obtain the fuel optimum rates of change of the equinoctial orbital elements in terms of each other and the Lagrange multipliers.  Constraints on thrust and power, as well as minimum periapsis, are implemented and the equations are averaged numerically using a Gaussian quadrature. The use of numerical averaging allows for more complex orbital perturbations to be added without great difficulty. Orbital perturbations due to solar radiation pressure, atmospheric drag, a non-spherical central body, and third body gravitational effects have been included. These perturbations have not ben considered by previous methods using analytical averaging. Thrust limitations due to shadowing have also been considered in this study. To allow for faster convergence of a wider range of problems, the solution to a transfer which minimizes the square of the thrust magnitude is used as a preliminary guess for the minimum fuel problem. Thus, this method can be quickly applied to many different types of transfers which may include various perturbations. Results from this model are shown to provide a reduction in propellant mass required over previous minimum fuel solutions. Minimum time transfers are also solved and compared to minimum fuel.
-------------------------------------

10130352_178 - 0.999308309135 - technology_and_computing
[bibliographical, system, design, reference, micro-based, process, computer-aided]

Drafting systems, Towards a design process for micro-based computer-aided
Includes bibliographical references (p. 67).
-------------------------------------

10130918_183 - 0.971466359419 - technology_and_computing
[press, note, toll, machine, business, robert, university, bibliographic, xiii, essay]

<Em>The Entertainment Machine: American Show Business in the Twentieth Century. By Robert C. Toll. New York: Oxford University Press, 1982. Pp. xiii + 284. Illustrations, notes, bibliographic essay. $12.95 (paper) $29.95 (cloth).
No abstract
-------------------------------------

10137037_183 - 0.999993592531 - technology_and_computing
[secure, computation, protocol, party, notion, adversary, dissertation]

New Frontiers in Secure Computation
The notion of secure computation is central tocryptography. Introduced in the seminal works of Yao FOCS'82, FOCS'86 and Goldreich, Micali and Wigderson STOC'87, secure multiparty computation allows a group of (mutually) distrustful parties to jointly compute any functionality over their individual private inputs in such a manner that the honest parties obtain the correct outputs and no group of malicious parties learn anything beyond their inputs and the prescribed outputs. General feasibility results for secure computation were given by Yao FOCS'86 and Goldreich et al. STOC'87 more than two decades ago. Subsequent to these works, designing secure computation protocols that can tolerate more powerful adversaries and satisfy stronger notions of security has been a very active area of research. In this dissertation, we study two such new frontiers in the area of secure computation. In the first part of this dissertation, we initiate a study of designing leakage-resilient interactive protocols. Specifically, we consider the scenario where an adversary, in addition to corrupting a subset of parties, can leak (potentially via physical attacks) arbitrary information from the secret state of any honest party. This is in contrast to the standard notion of secure computation where it is assumed that the adversary only has ``black-box'' access to the honest parties. In particular, we formalize a meaningful definition of leakage-resilient zero knowledge proof systems and provide constructions that satisfy our notion. We also discuss various applications of our results. The second part of this dissertation concerns with the general question of whether it is possible to securely run cryptographic protocols over an insecure network environment such as the Internet. It is well-known that the standard notion of secure computation is only relevant to the "stand-alone" setting where a single protocol is being executed in isolation; as such it does not guarantee security when multiple protocol sessions may be executed concurrently under the control of an adversary who is present across all sessions. We consider the open problem of constructing secure password-based authenticated key exchange protocols in such a setting in the "plain model" (i.e., without assuming any trusted infrastructure or random oracles). We give the first construction of such a protocol based on standard cryptographic assumptions. Our results are in fact much more general, and extend to other functionalities w.r.t. a (necessarily) weakened notion of concurrently secure computation. The results presented in this dissertation stem from two papers which are joint works with Sanjam Garg and Amit Sahai, and with Vipul Goyal and Rafail Ostrovsky, respectively.
-------------------------------------

10133165_183 - 0.996646979485 - technology_and_computing
[communication, internet, travel, effect, activity, chain, methodology, causal, specific, study]

Communication Chains: A Methodology for Assessing the Effects of the Internet on Communication and Travel
Although numerous researchers have investigated the impact on travel of specific telecommunications applications like telecommuting, only rarely has a broader look at the impact of electronic communications on multiple communications media, including travel, been attempted. This is no doubt due in part to the measurement challenges associated with such an attempt. Time-use diaries, activity diaries, or communication logs can provide opportunities for cross-sectional analysis, typically at a single point (or small interval) of time. However, these tools generally do not identify the chain of communication events cascading from a specific message, and thus are unable to capture behavioral linkages between events. Thus, for example, if a cross-sectional study using such a tool finds that greater Internet use is associated with more travel, it has not been established that the Internet use caused the travel; a third-party variable such as income, or a gregarious or variety-seeking personality, may be responsible for both effects separately. On the other hand, following communication chains in a precise and quantifiable way is a daunting task for both the researcher and the respondent.
      The methodology proposed in this study offers a practical middle ground between no data and perfect data on causal linkages. Specifically, for a given Internet activity, we simply ask the respondent to identify its causal antecedent and its likely communication consequences, by checking off the appropriate responses from a list. Obviously, by sacrificing precision and detail we lose the ability to quantify those consequences (e.g., in terms of miles of travel generated or substituted). However, we argue that the qualitative view of causal chains provided by this approach can offer valuable insights not possible from single cross-sectional studies alone.
      We analyze the relative strengths of effects of a given Internet activity in three ways. Using a 1994 sample of 148 early Internet adopters to illustrate the application of the methodology, we first tabulate the total presence, within the sample, of specific types of effects, to assess the net effect of Internet activities on other communication media. Second, we tabulate the frequency with which specific combinations of effects, or “pattern vectors,” occur in the sample. Finally, we use cluster analysis to identify latent types of communication chains having relatively similar causal relationships between Internet activities and other media.
      To establish a theoretical background and context for this study, we briefly summarize the relevant literature and develop a conceptual framework for understanding interactions among communications media. After introducing the survey instrument and methods of analysis, we apply the proposed methodology and discuss key empirical findings.
-------------------------------------

10135249_183 - 0.959931805159 - technology_and_computing
[brownian, motion, fractional, solution]

Smooth densities for solutions to differential equations driven by fractional Brownian motion
The fractional Brownian motions are a family of stochastic processes which resemble Brownian motion in many key ways, yet lack the quality of independence of increments. This dissertation focuses on proving smoothness of densities for solutions to differential equations driven by fractional Brownian motion, provided the vector fields satisfy a particular stratification condition. This result is achieved using the methods of Malliavin calculus. Examples of such solutions include the area process for any two-dimensional projection of the fractional Brownian motion
-------------------------------------

10136557_183 - 0.999995379559 - technology_and_computing
[efficient, secure, protocol, input, random, security, party, knowledge, question, primitive]

Efficient Secure Computation and Randomness
The question of how to construct optimally efficient secure protocols is a central question in cryptography and in the computer security world at large.   We focus in this work on several facets of this question, with a particular view towards the role of randomness in secure computation.The use of random inputs is ubiquitous in cryptographic primitives.  However, the ability to consistently draw from large random sources may be a barrier in practice.  We examine the existence of optimally strong <italic>pseudorandom sources</italic>  with particularly efficient implementations; namely, we construct exponentially hard pseudorandom generators that can be computed by circuits that have size linear in the generator output.  Conversely, we also examine efficient protocols that rely only minimally on their random inputs. In the <italic>resettable security model</italic>, parties may be forced to use the same random input across polynomially many interactions with other parties.  We examine this security model in the case of zero knowledge proofs, which are a primitive frequently required in secure computation protocols when one party must prove that they have executed the protocol correctly to another party without revealing secret inputs and compromising security. Finally, we examine the secure and efficient implementation of a specific functionality, including its various required cryptographic primitives (such as zero knowledge arguments of knowledge).  In particular, we construct a protocol that securely realizes <italic>pattern matching</italic>, including single character wildcards and substring matching.
-------------------------------------

10134956_183 - 0.836843446772 - technology_and_computing
[method, model, regression, probit]

New Methods for Solving Maximum Likelihood Estimating Equations of Logistic and Probit Regression Models
Several iterative methods are available in literature for solving the Maximum Likelihood Estimating Equations (MLEEs) of logistic and probit regression models. Generalized Self Consistency (GSC) method is such an existing iterative method. We introduce a new idea using the paired observations and combine it with the GSC method for both logistic and probit regression models and propose several new methods for solving MLEEs. For probit regression model, we introduce a linear approximation method for finding the exact solution of MLEEs. We illustrate the proposed methods with a real data as well as a simulated data and compare their performances with the existing methods. We investigate some theoretical properties of our estimates. We also present a meaningful method of choosing the initial values of parameters for the iterative methods.
-------------------------------------

10129706_178 - 0.999882989703 - technology_and_computing
[system]

A cost design system for residential building systems.
Includes bibliographical references.
-------------------------------------

10133012_183 - 0.999992885637 - technology_and_computing
[datum, network, seismic]

Seismic Network
The Seismic Applications group is working to increase the practical limits of the density of seismic deployments and move to more rapid data gathering in temporary deployments. Temporary deployments in seismology typically consist of stand-alone sites in order to decrease installation time. Data must be collected either periodically from each site or at the end of the experiment and incorporated into a data base for analysis. Permanent seismic installations do transmit data immediately to central data bases through ethernet, point-to-point radio, and modem. We add the ’¡Èreal-time’¡É data collection of permanent installations to temporary networks with multi-hop networking. Off-the-shelf 802.11 wireless radio cards are incorporated with an Intel Stargate computer running Roofnet networking software. In-house software is developed to collect data and route it hop-by-hop through the network to a central data repository. Hop-by-hop data movement allows data to pass effectively through the radio network despite breaks or flaky links within the network.
-------------------------------------

10137286_183 - 0.999997457341 - technology_and_computing
[fpga-rr, fpga, architecture, area, novel]

FPGA-RR: A Novel FPGA Architecture with RRAM-Based Reconfigurable Interconnects
In this paper we introduce a novel FPGA architecture with RRAM-based reconfiguration (FPGA-RR). This architecture focuses on the redesign of programmable interconnects, the dominant part of FPGA. By renovating the routing structure of FPGA using RRAMs, the architecture achieves significant benefits concerning area, performance and energy consumption. The implementation of FPGA-RR can be realized by the existing CMOS-compatible RRAM fabrication process. A customized CAD flow is provided for FPGA-RR, with an advanced P&R tool named VPR-RR developed for FPGA-RR to deal with its novel routing structure. We use the flow to verify the benefits of area, performance and power of FPGA-RR over the 20 largest MCNC benchmark circuits. Results show that FPGA-RR achieves 6.82x area savings, 3.09x speedup and 4.33x energy savings.
-------------------------------------

10136922_183 - 0.960348260826 - technology_and_computing
[noise, part, khz, pll, loop, fractional-n, phase]

A Jitter-Cleaning Fractional-N Frequency Synthesizer with 10 Hz-40 kHz Digitally Programmable Loop Bandwidth
This dissertation contains three parts. In the first part, the analysis and circuits of a jittercleaning fractional-N frequency synthesizer is presented. In the second part, a low phase noise and low I/Q mismatch quadrature VCO is presented. In the third part, a low phase noise digital PLL is presented.For the first part, the design utilizes a dual-loop architecture, which is suitable for integration in an SoC environment. The primary loop is a digital PLL with a second-order noise shaping phase-error ADC. The secondary loop is a fractional-N PLL implementing the digitally controlled oscillator inside the primary loop, and it locks to an external clean reference clock to reduce the phase noise and to improve the frequency stability of the on-chip oscillator. For the second part, a tail-tank coupling technique that combines two complementary differential LC-VCOs to form a quadrature LC-VCO is presented. This technique reduces phase noise by providing additional energy storages for noise redistribution and by canceling out most of the noise injected by transistors when they operate in the triode region. The resulting noise factor is close to the theoretical minimum value.For the third part, a 2.8 to 3.2 GHz fractional-N digital PLL is presented. A divider with two-stage retiming improves linearity to reduce fractional spurs without increasing the in-band noise floor. An ADC is employed to boost TDC resolution by five times to achieve 2 ps effective resolution. A dither-less DCO with an inductively coupled fine-tune varactor bank improves tuning step-size to 20 kHz. With a 52 MHz reference clock and a loop-bandwidth of 950 kHz, this prototype achieves 230 fs rms jitter integrated from 1 kHz to 40 MHz offset while drawing 17 mW from a 1.8V supply. A FOM of -240.4 dB is achieved.
-------------------------------------

10136510_183 - 0.886394714981 - technology_and_computing
[set, object, problem, part, result, statistics, pattern]

Patterns and statistics on words
We study the enumeration of combinatorial objects by number of occurrences of patterns and other statistics. This work is broken into three main parts. In the first part, we enumerate permutations, compositions, column- convex polyominoes, and words by patterns relating consecutive entries. We show that there is a hierarchy of enumeration problems on these sets of objects, such that the problems in one set may be reformulated in terms of the higher sets, then solved using powerful techniques developed for those sets. We use this viewpoint to solve an open problem due to Kitaev and to produce many extensions of existing results and interesting new results. In the second part, we use the same viewpoint to generalize a theorem due to Garsia and Gessel on the major index statistic. We give many specializations and slight extensions of this result to apply it to a variety of combinatorial objects and variations of the statistic. In the third part, we present general method for finding bijections between sets of objects that preserve various statistics. We use this method to solve problems posed by Claesson and Linusson and by Jones, and we also present several new results
-------------------------------------

10136246_183 - 0.999996969688 - technology_and_computing
[object, bci, computer, accuracy, user, dissertation, interface, system, algorithm, average]

Investigation of Brain Computer Interface as a New Modality in Computer Aided Design/Engineering Systems
Brain-computer interfaces (BCIs) are recent developments in alternative technologies of human computer interaction. These interfaces aim to interpret the brain's activity as user intentions in active BCI systems or cognitive/ emotional state in passive BCI systems.This dissertation focuses on implementation of BCIs in different aspects of Computer Aided Design system. Specifically, the dissertation explores the use of BCI in creating, selecting and modifying objects in CAD systems. Geometry creation is achieved through visual imagery by recording and analyzing EEG signals when subjects imagine distinct shapes. The algorithms developed in this research successfully recreated the primitive shape that a subject imagined with an average accuracy of 44.6% (chance accuracy is 20%). The research further indicated that geometrical properties of objects such as roundness and parallel extrusion are also salient in classifying imagined objects. Selection and modification of objects is obtained by developing algorithms based on the P300 characteristic of the EEG signal and motor imagery.  The classification result indicates the proposed method can select the target object /face with an average accuracy of 74%. Furthermore, the dissertation discusses a method to estimate user emotions of satisfaction and frustration that follow the successful or unsuccessful execution of a user command by the computer. This estimate is derived based on a combination of relative power spectral density and largest Lyapunov exponents. The results show that the algorithms can determine the user's satisfaction level with an average accuracy of 79%.
-------------------------------------

10133485_183 - 0.999983014602 - technology_and_computing
[node, energy, datum, scheduling, backbone, network, savings]

Resource management in heterogeneous wireless sensor networks
In heterogeneous wireless sensor networks (HWSNs) such as HPWREN, the sensed data needs to be routed through multiple hops before reaching the main high-bandwidth data links. The routing is done by battery-powered nodes using license free radios such as 802.11. In this context, minimizing energy consumption is critical to maintaining operational data links. This thesis presents a novel routing mechanism for HWSNs that achieves large energy savings while delivering data efficiently. This mechanism sits on top of the unmodified MAC layer so that legacy network devices can be used, and expensive hardware/ software modifications are avoided. Thus, our approach is inexpensive and easily deployable. Our solution includes scheduling and routing algorithms. The TDMA-based distributed scheduling algorithm limits the number of active nodes and allows a large portion of nodes to sleep thus saving energy. Simulations and measurements on a testbed network show that scheduling can achieve as much as 85% power savings and up to 10% increase in throughput. Scheduling is combined with the creation of a backbone of nodes that provides connectivity and delivers data to the proper destinations. The nodes of the backbone stay awake continuously for a predefined amount of time. Since it is an energy expensive task, they are dynamically selected so that those nodes that have more energy are more likely to become part of the backbone. Simulation results show that the combined scheduling and forwarding backbone approach achieves up to 60% energy savings per battery operated node and also have better performance when compared to existing techniques
-------------------------------------

10131949_183 - 0.999998801834 - technology_and_computing
[system, systems, datum, ati, gi]

On Real-Time Distributed Geographical Database Systems
Advanced Traveler Information Systems (ATIS) under the intelligent Vehicle Highway Systems (IVHS) context require efficient information retrieval and updating in a dynamic environment and at different geographical scales. Some problems in ATIS can be solved based on the functionalities provided by GIS systems. However, extra requirements such as real-time response are not readily met in existing GIS systems. We investigate the use of GIS-based systems for applications in ATIS and we propose a new system architecture based on existing GIS technology and distributed computing technology. Issues on data modeling, data representation, storage and retrieval, data aggregation, and parallel processing of on-line queries in the proposed GIS-based systems are discussed.
-------------------------------------

10139787_183 - 0.996889779024 - technology_and_computing
[datum, model, levee, pressure, pore, settlement]

Remote monitoring of a model levee constructed on soft peaty organic soil
Remote data monitoring was performed to measure settlement and pore pressure in soft peat beneath a model levee constructed in the Sacramento / San Joaquin Delta. Consolidation and secondary compression behavior was monitored following construction and dynamic testing of the model levee using piezometers embedded in the peat and an in-place horizontal inclinometer at the base of the model levee. A solar powered data acquisition system was used to gather the data, and a modem transmitted the data to a web-based interface. This system enabled us to (i) know when primary consolidation had finished prior to testing, (ii) observe the large influence of secondary compression on observed settlements, and (iii) observe a lack of any significant post-cyclic settlement. The initial change in pore pressure was predicted well using Skempton's pore pressure parameters.
-------------------------------------

10134580_183 - 0.916994151962 - technology_and_computing
[source, method, ica, speech, algorithm, beamforming, robust, problem, frequency, adaptive]

Microphone array processing for speech : dual channel localization, robust beamforming, and ICA analysis
Compared with single channel speech processing, multi- microphone based speech processing methods are capable of high interference suppression in noisy environments because of their spatial filtering capability. This dissertation develops novel microphone array speech processing methods in a variety of configurations and also analyzes and provides insights into existing popular techniques. First we develop a two microphone based source localization technique for multiple speech sources utilizing speech specific properties and the generalized mixture decomposition clustering algorithm. Voiced speech is sparse in the frequency domain and can be represented by sinusoidal tracks via sinusoidal modeling which provides high local SNR. By utilizing the inter-channel phase differences (IPD) between the dual channels on the sinusoidal tracks, the source localization of the mixed multiple speech sources is turned into a clustering problem on the IPD vs. frequency plot. The generalized mixture decomposition algorithm (GMDA) is used to cluster the groups of points corresponding to multiple sources and thus estimate the DOA of the sources. Our next work considers data dependent adaptive beamformers, which are known to have high resolution and interference rejection capability when the array steering vector is accurately known. However, these methods degrade severely if steering vector error exists and so robust variants are needy to remedy this sensitivity. We compare and analyze recent developments in adaptive beamforming. We then develop a robust broadband adaptive beamforming algorithm which combined the robustness of the delay-and-sum beamforming in the look direction with the high interference rejection capability of adaptive beamforming algorithm. Based on J. Li and P. Stoica's work on robust Capon beamforming, we develop variants of the constrained robust Capon beamformer that attempt to limit the search in the underlying optimization problem to a feasible set of steering vectors thereby achieving improved performance. Another class of promising multi-channel signal separation algorithms that complement beamforming methods are blind source separation methods. We analyze and provide insight into one such class of blind source separation methods, independent component analysis (ICA) methods. For separating convolutively mixed source signals, the frequency domain ICA approach is often used because it simplifies the time domain convolutive mixing problem into the instantaneous mixing problem in each frequency bin. We examine and provide insights into the frequency domain ICA methods for source separation in reverberant environments. Concentrating on the bin-wise ICA methods, a significant contribution of this work is to show that signals modeled using Gaussian scale mixtures (GSM) density can be separated using ICA even though they might be dependent on each other as long as the the frame dynamics of the source signals are different almost surely. We also analyze the stability conditions of the complex maximum likelihood ICA /IVA. Lastly, in an attempt to make the best of ICA and beamforming methods, we propose two approaches for combining geometric information with ICA algorithms to solve the permutation problem in a scenario where approximate information about the direction of the desired source is known
-------------------------------------

10135712_183 - 0.999961226922 - technology_and_computing
[mode, space, demand, cost, city, structure, transport, system]

Allocation of Space and the Costs of Multimodal Transport in Cities
Cities worldwide face growing demand for mobility with limited transportation infrastructure. This dissertation addresses how street space should be allocated and how transport modes should be operated for di fferent city structures. City structure is characterized by the density of travel demand and the amount of space available for transportation. Several costs are associated with transportation systems, including time, money, space, and externalities. Building on macroscopic models of traffic and transit operations in urban networks, the relationship between the costs of providing mobility with various transport modes and the structure of the city served is modeled recognizing that vehicles require space. Cities served by an individual mode (e.g., cars) and/or a collective mode (e.g., buses) are analyzed for three cases: constant demand over time (travelers can choose their mode); evening peak demand (travelers can choose their mode); morning peak demand (travelers can choose mode and departure time). In all cases, the system optimal use of space and modes which minimizes total system costs is identifi ed along with a pricing strategy to achieve the optimum at user equilibrium. The results of this study show systematically how to allocate street space, operate transport systems, and price modes to minimize the costs of mobility for any city structure.
-------------------------------------

10137615_183 - 0.996571748714 - technology_and_computing
[heuristic, graph, set, algorithm, selection]

Statistical Heuristic Selection for Graph Coloring
Although a heuristic algorithm's usefulness is grounded in its empirical performance on a set of problem instances, much of recent graph coloring heuristic development neglects statistical methodology in several important ways.  First, heuristic parameters are often set in an ad-hoc, irreproducible fashion.  Second, heuristic parameters are often tuned and evaluated on the same set of instances, causing over-tuning.  Last, the common winner-take-all approach limits a heuristic's application to a very specific set of graph instances.To address the above issues, we employ machine learning techniques to perform instance-based algorithm selection from a set of diverse heuristic algorithms, including multiple parameterizations of the same algorithm.  The implemented strategy improves on a winner-take-all strategy by over a color on average on a set of IID random graphs when allowed a maximum runtime of approximately 15 minutes, and in general achieves near-optimal heuristic selection on unseen graphs.
-------------------------------------

10133101_183 - 0.868121403932 - technology_and_computing
[marzena, america, private, modem, subjectivity, political, change, grzegorczyk, palgrave, latin]

GRZEGORCZYK, MARZENA. Private Topographies: Space, Subjectivity, and Political Change in Modem Latin America. New York: Palgrave Macmillan, 2005. 191 pp.
No abstract
-------------------------------------

10134188_183 - 0.999976472771 - technology_and_computing
[system, design, market-based, user, resource, traditional]

Practical market-based resource allocation
The arrival of large-scale open platforms such as cloud- computing infrastructures and petascale clusters is fueling the emergence of a new class of users and applications with large and diverse resource needs. Optimizing for traditional system-centric metrics such as response time or throughput does not capture the diverse and often conflicting needs of different users and resource providers. Collaboration between economists and computer scientists over the past ten years has lead to many important results describing designs of market-based mechanisms to address exactly these issues. However, there is very little progress in building these systems due to apprehensions about their potential unfairness and fragility in a live deployment. The resulting reality is that systems continue to rely on traditional and inadequate designs despite theoretical results demonstrating the potentially significant value added to all stakeholders by designing market-based systems. This dissertation investigates the sensitivity, applicability, and practicality of a market-based policy to increase the efficiency of resource allocations --- measured using a utilitarian social welfare metric --- in real, large-scale computing systems. We build upon previous work in this space by moving beyond "paper designs'' and using an iterative process of implementation, deployment, measurement, modeling and simulation. We begin by presenting Mirage, our implementation of a microeconomic resource allocation system for a sensor network, which is still in use today. In this context, we identify the compromises that deployed market-based systems must make when using traditional theoretical designs due to computational challenges and various imperfect conditions of a live deployment. Nonetheless, based upon user data, we find that these systems are still expected to be robust to typical user errors seen in traditional computing systems. We also describe extensions of existing designs to address some of these limitations. Finally, we discuss our experience with deploying systems with real users and applications and conclude by discussing the forthcoming challenges in pushing more widespread adoption of these market-based designs in real systems
-------------------------------------

10136983_183 - 0.999920998243 - technology_and_computing
[device, energy, photovoltaic, charge, bulk, organic, heterojunction, power, improvement]

Fabrication and Characterization of Organic/Inorganic Photovoltaic Devices
Energy is central to achieving the goals of sustainable development and will continue to be a primary engine for economic development. In fact, access to and consumption of energy is highly effective on the quality of life. The consumption of all energy sources have been increasing and the projections show that this will continue in the future. Unfortunately, conventional energy sources are limited and they are about to run out. Solar energy is one of the major alternative energy sources to meet the increasing demand. Photovoltaic devices are one way to harvest energy from sun and as a branch of photovoltaic devices organic bulk heterojunction photovoltaic devices have recently drawn tremendous attention because of their technological advantages for actualization of large-area and cost effective fabrication. The research in this dissertation focuses on both the mathematical modelling for better and more efficient characterization and the improvement of device power conversion efficiency. In the first part, we studied the effect of incident light power on the space charge regions of the Schottky barriers of the organic bulk heterojunction photovoltaic devices, the current-voltage characteristics and performance of the devices and built a current-voltage model for the devices that involves these effects.  The incident light power showed an effect on the Schottky barriers of the devices by changing the width of the space charge regions. This change directly affects the reverse bias current-voltage curves by increasing the current values and the slope of the curves. But under excessive incident light power; the space charge regions merge, the devices break down and work as ohmic devices. In the second part, we combined the two improvement methods, improving the charge carrier transport and improving absorption of the organic bulk heterojunction photovoltaic devices. For charge carrier transport improvement, we presented deoxyribonucleic acid complexes as hole collecting and electron blocking layer on the anode side of the devices by using them as band energy diagram arranging layer, for absorption improvement with plasmonic effect of the particles, we present colloidal platinum nanoparticles as the surface plasmons. Deoxyribonucleic acid complex layer improved the device performance by improving the charge carrier hopping efficiency of the devices by arranging the band energy diagram in order to collect holes easily and block electrons diffusing to anode electrodes. Colloidal platinum nanoparticles layer improved the device performance by increasing the light-harvesting efficiency of the devices by increasing the rate of photon absorption. This proves that the colloidal platinum nanoparticles can be used as surface plasmons in organic bulk heterojunction photovoltaic devices. Because peak their extinction spectra matches with the peak of absorbance of poly(3-hexylthiophene) (P3HT): 6,6-phenyl-C61-butyric acid methyl ester (PCBM). Combination of these two novel materials in the same device showed a significant improvement as a 26% increase in the power conversion efficiency of the devices. The research conducted in this dissertation offers promising potential of organic bulk heterojunction photovoltaic devices as one of the clean and affordable alternative energy sources for supplying increasing demand on energy.
-------------------------------------

10137686_183 - 0.999885787395 - technology_and_computing
[system, notation, koonce, possibility]

Alternate fingerings for the flute : Paul Koonce's Escape tone and the possibilities of notation
This dissertation explores the possibilities of the notation system used in Escape tone by Paul Koonce. Alternate fingering sources and systems are discussed and examined, as well as the possibilities for the use of Koonce's notation system in other works. The Koonce notation system is applied to Sciarrino's L'Opera per flauto: Fra i testi dedicati alle nubi to see if it is a viable notation system for contemporary music
-------------------------------------

10134811_183 - 0.968193685486 - technology_and_computing
[digital, technology, art, society]

Loss, meaning, and melancholy in our digital society
As society has absorbed the cornucopia of digital technologies of the late twentieth and early twenty-first centuries, artists have also absorbed these technologies and used them as tools for art production. The use of the technology as a tool can be said to be a first reaction to a new form. However, when the technology itself and the society that absorbs the technology go unexamined, it is problematic for the subsequent growth of the technology, the society, and the art produced in concert with those technologies. My artworks attempt to use digital technologies in alternative ways to explore the technologies themselves and their impact on our society. Our digital society is often viewed through the lens of hyperreality, the theory that argues that symbolic meaning is excessively detached from reality. This is an obvious, but likewise obviously superficial analysis of the complexities of our contemporary digital lives. Much of the analysis of digital art focuses on the digital as a medium that is detached from reality. There is a certain materiality that is lacking in digital art, and the modes of consuming digital art are significantly different non- digital modes of reception. My argument is not to dispute this apparent disconnect between digital art and the "real" world, but to suggest that there are more profound human issues in digital culture. In my art, I draw on the theory of permanent present, nostalgia, and melancholy. In the era of the permanent present, the experience of life is marked by a yearning for a rupture to break the flatness of the present
-------------------------------------

10136076_183 - 0.846644925364 - technology_and_computing
[method, laboratory, repeatability, source, reproducibility]

Evaluation of the repeatability and reproducibility of a suite of  PCR-based microbial source tracking methods
Many PCR-based methods for microbial source tracking (MST) have been developed and validated within research laboratories, but require inter-laboratory validation before implementation.  As part of the State of California Source Identification Protocol Project (SIPP), a blinded set of challenge filters were analyzed by three to five laboratories with a suite of PCR-based methods utilizing standardized methods.  Repeatability (within lab agreement) and reproducibility (between lab agreement) of results were assessed by multiple metrics and compared to previously observed values for other environmental methods.  Repeatability and reproducibility were found to be generally comparable to previously observed values for other methods (Median CV .001-.033 and .020-.106, respectively).  Variance component analysis showed contribution of laboratory to total variability to be larger but of similar magnitude to inherent intra-laboratory variability among laboratories using standardized methods.  Results among laboratories using non-standardized protocols for the same methods were also observed to have >2 log differences at times.  These findings verify the repeatability and reproducibility of these MST methods and highlight the need for standardization of protocols and consumables prior to implementation of larger scale microbial source tracking studies involving multiple laboratories.
-------------------------------------

10134288_183 - 0.994621929678 - technology_and_computing
[power, bus, ic, steiner, circuit, 3-d, communication, wire]

Physical planning to embrace interconnect dominance in power and performance
The growth of computer performance by Moore's law is currently limited by power consumption and waste heat removal. As feature size scales down, dynamic power of integrated circuits (IC) is increasingly dominated by interconnect wires. Reducing this power has become a focusing target for computer architects and circuit designers. In this dissertation, we study on two phases in IC design flow, floorplanning and interconnecting, to address the challenges on interconnect latency and power. The first and most direct way is to shorten the interconnect lengths. Current IC chips are manufactured on a 2-dimensional silicon die. Moving to 3-dimensional enables us to make the same circuit within a much smaller foot print, and therefore greatly reduce interconnect power. For 3-D floorplanning, we study the computational complexity of placing cuboids in 3-dimensional space under adjacency constraints. We use the basic graph-to-floorplan formulation and a 2.5-D variation with more strict layer constraints. In both cases, we find the problem is NP- complete by reductions from known NP-hard problems. The results, combined with previous work on the 2-D counterpart, show the fundamental hardness residing in 3-D structures, and reveal possible challenges in design complexity moving from 2-D to 3-D. Another way, when interconnect distances can no longer be reduced, is to minimize the wires involved in communication actions. Most multi-component system-on-chips use bus or bus matrix to connect all the sub-systems. Complying with standard bus protocols, we devise a bus matrix synthesis scheme to minimize dynamic power on communications, while maintaining the bandwidth capability and routing resources. The geometric basis is the shortest-path Steiner graph extended from Steiner arborescence (shortest -path Steiner tree), where optimized graph structures are used to replace the large wire nets in traditional bus architectures. Experimental results show that near-optimal communication power can be achieved without consuming excessive on-chip resources
-------------------------------------

10134473_183 - 0.995954835428 - technology_and_computing
[time, series, computational, algorithm, datum, solution, dataset, feature]

Learning from Time Series in the Resource-Limited Situations
Many data mining tasks are based on datasets containing sequential characteristics, such as web search queries, medical monitoring data, motion capture records, and astronomical observations. In these and many other applications, a time series is a concise yet expressive representation. A wealth of current data mining research on time series is focused on providing <italic>exact</italic> solutions in such small datasets. However, advances in storage techniques and the increasing ubiquity of distributed systems make realistic time series datasets orders of magnitude larger than the size that most of those solutions can handle due to computational resource limitations. On the other hand, proposed <italic>approximate</italic> solutions such as dimensionality reduction and sampling, suffer from two drawbacks: they do not adapt to available computational resources and they often require complicated parameter tuning to produce high quality results.In this dissertation, we discuss anytime/anyspace algorithms as a way to address these issues. Anytime/anyspace algorithms (after a small amount of setup time/space) are algorithms that always have a <italic>best-so-far</italic> answer available. The quality of these answers improves as more computational time/space is provided. We show that by framing problems as anytime/anyspace algorithms, we can extract the most benefit from the available computational resources and provide high-quality approximate solutions accordingly. We further argue that it is not always effective and efficient to rely on whole datasets. When the data is noisy, using distinguishing local features rather than global features could mitigate the effect of noise. Moreover, building a concise model based on local features makes the computational time and space much less expensive. We introduce a new time series primitive, <italic>time series shapelets</italic>, as a distinguishing feature. Informally, shapelets are time series subsequences which are in some sense maximally representative of a class. As we shall show with extensive empirical evaluations in diverse domains, classification algorithms based on the time series shapelet primitives can be interpretable, more accurate, and significantly faster than state-of-the-art classifiers.
-------------------------------------

10137213_183 - 0.793625552918 - technology_and_computing
[optical, thz, graphene, range, model, visible, conductivity, property, carrier, region]

Optical Properties of Graphene from the THz to the Visible Spectral Region
In this thesis, two models are developed to describe the optical properties of graphene from the THz to the visible spectral region. We show that the optical conductivity is mainly contributed by interband carrier transitions in the visible region and by intraband carrier scattering in the THz range. Optical properties such as refractive index and transmittance can be calculated theoretically from the model in the optical range. In the THz range, the Fermi energy and the scattering rate are two important parameters in the determination of the optical conductivity. One can use these physical quantities as fitting parameters and find their values by fitting the THz model to the experimental data. The fitted results are consistent with the data and one can predict the optical conductivity and refractive index at the frequency higher than the bandwidth of our experiments. Other physical properties such as the carrier density and the mobility can also be obtained from the fitted parameters. The resultant mobilities of monolayer graphene and bilayer graphene are consistent with the results of other groups.
-------------------------------------

10130018_178 - 0.997834135286 - technology_and_computing
[places, system, transportation, interchange, corridor, northeast]

Places of interchange in the northeast corridor transportation system
Includes bibliographies.
-------------------------------------

10133750_183 - 0.999989193742 - technology_and_computing
[program, monitoring, multicore, parallel, ecmon, support, software, ismd, tool, application]

IMPRESS: Improving Multicore Performance and Reliability via Efficient Software Support for Monitoring
With the advent of multicores, there is demand for monitoring parallelprograms running on multicores for enhancing reliability andperformance. Debugging tools such as data race detection anddeterministic replay debugging (DRD) require a parallel program to bemonitored at  runtime. Likewise, techniques such as  dynamicinformation flow tracking (DIFT) that  are used for preventingsecurity attacks also require runtime monitoring. Furthermore,techniques  such as speculative parallelization and speculativeoptimization, that strive to expose parallelism and increaseperformance of programs, also require runtime  monitoring --detecting misspeculation, which is an integral component of anyspeculative technique requiring the program to be monitored atruntime.While each of the above monitoring applications are quite different intheir purpose and implementation, they all share a common requirementin the context of monitoring a parallel program running on a multicore-- the need to detect and react to interprocessor shared memorydependences (ISMD). Current software  based monitoring tools, due totheir inability to detect and react to ISMDs efficiently, arerendered inapplicable for monitoring parallel programs running onmulticores. On the contrary, hardware based monitoring tools, whileapplicable in a multicore context, require specialized hardwaremodifications specific for each monitoring task.This dissertation IMPRESS strives to Improve Multicore Performance andand Reliability by providing Efficient Support for enabling Softwarebased monitoring.  To enable software based monitoring on multicores,this dissertation proposes ECMon -- lightweight and  general purposesupport for exposing  cache events to the software,  in effect,efficiently exposing ISMDs to the software. Using ECMon, a variety ofmonitoring  applications, which were  inapplicable on multicores, cannow be used to efficiently monitor  parallel program on  multicores.More specifically, a class of monitoring applications known  as shadowmemory tools such as DIFT for security, Memcheck and Eraser fordebugging, can now monitor parallel programs running on multicores atalmost the same execution overhead  as monitoring  sequentialprograms, using ECMon support.  ECMon can also be used to developnovel monitoring applications for increasing performance andreliability.  In particular, ECMon can be used for performingspeculative optimizations on parallel  programs which results in about12.5% execution time reduction in a set of seven  parallel programsconsidered. Furthermore, ECMon support can be used to record ISMDs  insoftware, which only results in 3 fold execution time slowdown.Finally ECMon can be used by servers to recover from memory errorswithout requiring heavy-weight  checkpointing or rollback.To summarize, this dissertation proposes light-weight and generalpurpose support in the form of exposing cache events to the software.Using this support, it is shown how parallel programs running onmulticores can be monitored efficiently for increasing performance andenhancing reliability.
-------------------------------------

10130124_178 - 0.795322046338 - technology_and_computing
[reaper, reference, machine, bibliographical, human, aircraft, action, mq-9]

The MQ-9 Reaper remotely piloted aircraft : humans and machines in action
Includes bibliographical references (p. 290-298).
-------------------------------------

10136957_183 - 0.999542377144 - technology_and_computing
[series, time, algorithm, primitive, exact, datum, shapelet, motif]

Exact Primitives for Time Series Data Mining
Data mining and knowledge discovery algorithms for time series data use primitives such as bursts, periods, motifs, outliers and shapelets as building blocks. For example a model of global temperature considers both bursts (i.e. solar fare) and periods (i.e. sunspot cycle) of the sun. Algorithms for finding these primitives are required to be fast to process large datasets. Because exact algorithms that guarantee the optimum solutions are very slow for their immense computational requirements, existing algorithms find primitives approximately. This thesis presents efficient exact algorithms for two primitives, time series motif and time series shapelet. A time series motif is any repeating segment whose appearances in the time series are too similar to happen at random and thus expected to bear important information about the structure of the data. A time series shapelet is any subsequence that describes a class of time series differentiating from other classes and thus can be used to classify unknown instances.We extend the primitives for different environments. We show exact methods to find motifs in three different types of time series data. They are the in-memory datasets suitable for batched processing, the massive archives of time series stored in hard drives and finally, the streaming time series with limited storage. We also describe an exact algorithm for logical-shapelet discovery that combines multiple shapelets to better describe complex concepts.We use efficient bounds to the goodness measures to increase the efficiency of the exact algorithms. The algorithms are orders of magnitude faster than the trivial solutions and successfully discover motifs/shapelets of real time series from diverse sensors such as EEG, ECG, EPG, EOG, Accelerometers and Motion captures. We show applicability of these algorithms as subroutines in high-level data mining tasks such as summarization, classification and compression.
-------------------------------------

10134930_183 - 0.999991537033 - technology_and_computing
[traffic, network, application, different, graph, network-wide, interaction, tdg, problem, behavior]

Analyzing Network-Wide Interactions Using Graphs: Techniques and Applications
The fundamental problem that motivates this dissertation is the need for better methods and tools to manage and protect large IP networks. In such networks, it is essential for administrators to profile the traffic generated by different applications (e.g., Web, BitTorrent, FTP) and be able to identify the packets of an application in the wild. This enables administrators to effectively accomplish the following key tasks: (a) Manage the network: It allows different policies to be applied to different applications, e.g., rate limit peer-to-peer (P2P) traffic during busy hours. (b) Protect the network: Profiling malicious traffic requires a strong separation from benign traffic; therefore, knowing the behavior of "good" application provides better separation from malicious activity.  Despite some significant efforts to solve the traffic profiling problem, none of the existing methods address all relevant problems.  The difficulty of the problem comes from the following three factors: (a) The intentions of application writers and users to hide their traffic using obfuscation (e.g., payload encryption); (b) The limited information about flows and IP-hosts when traffic is monitored at the Internet backbone; and (c) The continuous appearance of new applications as well as undocumented changes to existing network protocols.  In this dissertation, we propose a different way of looking at network traffic that focuses on the network-wide interactions of IP-hosts (as seen at a router). To facilitate the analysis of network-wide interactions, we represent traffic as a graph, where each node is an IP address, and each edge represents a type of interaction between two nodes. We use the term Traffic Dispersion Graph or TDG to refer to such a graph.  Intuitively, TDGs capture the "social behavior" of network hosts, which, as we show here, it is hard to obfuscate. For example, a P2P protocol cannot function while trying to hide its overlay network, as maintaining a network overlay is a fundamental behavior of a P2P protocol. This dissertation focuses on three key aspects of network-wide interactions: (a) The graph shapes and structures formed by different applications; (b) The distinctive dynamic network-wide behavior of network application (i.e., how the graphs change over time); and (c) The identification of communities formed by IP-hosts over the Internet.  Using the traffic analysis techniques we propose here, we develop novel traffic profiling solutions that are robust to obfuscation and can operate at the backbone, which are both very challenging to address with the current state-of-the-art. To evaluate the effectiveness of our methods, we use real-world traffic traces collected from six different networks. This dissertation presents the first work to explore the full capabilities of TDGs for profiling and analyzing traffic. Based on our results, we believe that TDGs can provide the basis for the next generation of traffic monitoring tools.
-------------------------------------

10136198_183 - 0.961879224249 - technology_and_computing
[design, level, gameplay, pattern]

The Science of Level Design: Design Patterns and Analysis of Player Behavior in First-person Shooter Levels
Level designers create gameplay through geometry, AI scripting, and item placement. There is little formal understanding of this process, but rather a large body of design lore and rules of thumb. As a result, there is no accepted common language for describing the building blocks of level design and the gameplay they create. This dissertation presents a set of level design patterns for first-person shooter (FPS) games, providing cause-effect relationships between level design patterns and gameplay. These relationships are explored through analysis of data gathered in an extensive user study.This work is the first scientific study of level design, laying the foundation for further work in this area. Data driven approaches to understand gameplay have been attempted in the past, but this work takes it to a new level by showing specific cause- effect relationships between the design of the level and player behavior.The result of this dissertation is a resource for designers to help them understand how they are creating gameplay through their art. The pattern collection allows them to explore design space more fully and create richer and more varied experiences.
-------------------------------------

10135036_183 - 0.999995625277 - technology_and_computing
[memory, flash, code, block, cell, error, design, number, erasure]

Coding for flash memories
Flash memories are, by far, the most important type of non -volatile memory in use today. They are employed widely in mobile, embedded, and mass-storage applications, and the growth in this sector continues at a staggering pace. Moreover, since flash memories do not suffer from the mechanical limitations of magnetic disk drives, solid- state drives have the potential to upstage the magnetic recording industry in the foreseeable future. The research goal of this dissertation is the discovery of new coding theory methods that supports efficient design of flash memories. Flash memory is comprised of blocks of cells, wherein each cell can take on q ̲> 2 levels. While increasing the cell level is easy, reducing its level can be accomplished only by erasing an entire block. Such block erasures are not only time-consuming, but also degrade the memory lifetime. Our main contribution in this research is the design of rewriting codes that maximize the number of times that information can be written prior to incurring a block erasure. Examples of such coding schemes are flash/floating codes and buffer codes, introduced by Jiang and Bruck et al. in 2007, and WOM- codes that were presented by Rivest and Shamir almost three decades ago. The overall goal in these codes is to maximize the amount of information written to a fixed number of cells in a fixed number of writes. Furthermore, the design of error-correcting codes in flash memories is extensively studied. It is shown how to modify WOM-codes to support an error-correction capability. Motivated by the asymmetry of the error behavior of flash memories and the work by Cassuto et al., a coding scheme to correct asymmetric errors is presented. An extensive empirical database of errors was used to develop a comprehensive understanding of the error behavior as well as to design specific error-correcting codes for flash memories. This research on flash memories is expanded to other directions. Wear leveling techniques are widely used in flash memories in order to reduce and balance block erasures. It is shown that coding schemes to be used in these techniques can significantly reduce the number block erasures incurred during data movement. Also, the design of parallel cell programming algorithms is studied for the specific constraints and behavior of flash cells
-------------------------------------

10132296_183 - 0.999986847976 - technology_and_computing
[video, system, visual]

Video Display for Study of Avian Visual Cognition: From Psychophysics to Sign Language
 We demonstrated four different ways of using video systems for research  in avian visual cognition: 1) recent developments of high vision TV systems made it possible to use the video system for psychophysical studies. Visual acuity measured with such a video system was comparable to those obtained by more traditional methods; 2) using image processing software, we could display unnatural animals, such as chimeras on the TV screen. We also reported that pigeons did not discriminate partially occluded conspecifics; 3) effects of exposure to visual stimuli upon on-going behavior were examined using suppression and conditioned suppression procedures; and 4) discrimination of moving images, namely two words of Japanese Sign Language, are reported.  
-------------------------------------

10138100_183 - 0.859062739449 - technology_and_computing
[system, hydrologic, change, chapter, study, result, condition, wetland, mar, resource]

Hydrologic System Response to Environmental Change: Three Case Studies in California
Hydrologic systems are vulnerable to anthropogenic and natural environmentalchanges. When these changes impair a system’s ability to function and serve as aresource, then restoration or mitigation may be needed. Successful management offreshwater resources requires a quantitative understanding of hydrologic processes and dynamics, and an assessment as to how hydrologic systems may respond to futurechanges. Some systems are sufficiently large or complex so as to defy direct control or restoration, but people can still benefit from understanding that will allow more reliable and thoughtful resource use, as part of a comprehensive management approach. The three chapters presented in this thesis examine hydrologic system response to a variety of environmental changes, including: (1) a recovering riparian wetland located downstream of a dam, (2) an overdrafted and seawater intruded coastal groundwater basin, and (3) a region experiencing an increase in the intensity of extreme precipitation events. In Chapter 1, our studies show that riparian wetland conditions can be improved while water is conserved in upstream reservoirs by utilizing surface infiltration to establish wetland saturation conditions, rather than lateral and upward groundwater transport. Results from the second study indicate that ~13% of the study area (29 of 220 km2 in the basin) may be suitable for managed aquifer recharge (MAR). Modeling suggests that MAR projects placed along the coast provide the greatest initial decrease in seawater intrusion, but MAR projects placed in suitable locations throughout the basin provides the greatest reduction in seawater intrusion over subsequent decades. In Chapter 3, we show that there has been a statistically significant increase in extreme precipitation, beyond proportional changes in mean annual precipitation, in the San Francisco Bay Area in the last 120 years. The extent of changes varies on a spatial scale of ~50 km, the scale at which city planning and risk management decisions should be based. The results of each chapter contribute to the fundamental understanding of hydrologic system dynamics, and demonstrate new field and computational methods. Results presented in Chapters 1 and 2 also compare the efficacy of hypothetical restoration and operational scenarios for improving resource conditions.
-------------------------------------

10134873_183 - 0.982596765263 - technology_and_computing
[sensor, hybrid, carbon, nanotube, property, high, polymer, sensitivity, application, d-fructose]

Synthesis, Characterization and Utility of Carbon Nanotube Based Hybrid Sensors in Bioanalytical Applications
ABSTRACT OF THE DISSERTATIONSynthesis, characterization and utility of carbon nanotube based hybrid sensors in bioanalytical applicationsbySushmee BadhulikaDoctor of Philosophy, Department of Electrical EngineeringUniversity of California, Riverside, USAProf. Ashok Mulchandani, ChairpersonThe detection of gaseous analytes and biological molecules is of prime importance in the fields of environmental pollution control, food and water - safety and analysis, and medical diagnostics.  This necessitates the development of advanced and improved technology that is reliable, inexpensive and suitable for high volume production. The conventional sensors available for catering to these needs are often traditional thin film based sensors which lack sensitivity due to the phenomena of current shunting across the charge depleted region when an analyte binds with them. Thus one dimensional nanostructures are heralded for wide applications which apart from eliminating the current shunting due to their one dimensional geometries also facilitate device miniaturization and low power operations.Carbon nanotubes are a class of popularly known one dimensional nanostructures that exhibit excellent electronic and mechanical properties. Their properties of small size, high strength, high electrical and thermal conductivity and high specific area have resulted in their wide spread applications in sensor technology. However, to overcome the issue of low sensitivity of pristine carbon nanotubes and to widen their scope, efforts have been made to configure hybrid devices that combine the properties of carbon nanotubes along with other materials like metals, metal oxides, conducting polymers etc. Conducting polymers exhibit electronic, magnetic and optical properties of metals and semiconductors while retaining the attractive mechanical properties and processing advantages of polymers.  Their high chemical sensitivity, room temperature operation and tunable charge transport properties has made them ideal for use as tranducing materials in chemical sensors. In this dissertation, various applications of carbon nanotube based hybrid devices such as CNT-conducting polymer and graphene-CNT based sensors have been demonstrated. Electrochemical polymerization enabled the synthesis of CPs and metal nanoparticles in a simple and efficient way on the surface of carbon nanotube based platforms thus resulting in the fabrication of hybrid devices exhibiting superior properties. In the first study towards demonstrating the synergistic behavior of the hybrid devices, PEDOT: PSS was electropolymerized on the surface of SWNTs and their response towards volatile organic compounds was evaluated. In terms of performance, when compared with bare SWNTs, these hybrid sensors exhibited better sensitivity over a wide dynamic range of concentrations of saturated vapors of VOCs. The second application involved electrodeposition of Polyaniline boronic acid on SWNTs by charge controlled electrochemical polymerization and their subsequent evaluation as chemiresistive sensors towards detection of D-fructose and D-glucose. The regeneration of the sensors based on the basis of the reversible nature of the binding between PABA and 1, 2 or 1, 3-diols at lower values of pH was also studied. These enzyme free sensors exhibited a limit of detection of 2.92 mM for D-fructose and 3.46 mM for D-glucose.  Molecular imprinting of conducting polymers (MICPs) is a very versatile technique that is used towards detecting biological entities such as receptors, enzymes, and antibodies with high specificity. A self-doped, molecularly imprinted conducting polymer (MICP) on aligned SWNTs surface has also been discussed which displayed better sensitivity towards D-fructose. Imprinting imparts higher selectivity towards D-fructose over D-glucose by template directed formation of imprinted cavities that are of same chemical nature in terms of shape, size and functionality as D-fructose. In the next study, nonenzymatic glucose sensor based on Platinum nanoflowers on multi-walled carbon nanotube-graphene hybrid has been developed. The hybrid sensor was   synthesized of Carbon nanotube (MWNT)/ graphene hybrid by one step chemical vapor deposition process and electrodeposition of high surface area platinum nanoflowers. Direct oxidation of glucose due to the electrocatalytic property of the Pt-nanoflowers resulted in the non-enzymatic detection of glucose in the physiological range at neutral pH.
-------------------------------------

10134141_183 - 0.816808407174 - technology_and_computing
[visual, layer, ica, model, processing, coding, feature, theory, image, input]

Computational models of early visual processing layers
Visual information passes through layers of processing along the visual pathway, such as retina, lateral geniculate nucleus (LGN), primary visual cortex (V1), prestriate cortex (V2), and beyond. Understanding the functional roles of these visual processing layers will not only help to understand psychophysical and neuroanatomical observations of these layers, but also would help to build intelligent computer vision systems that exhibit human-like behaviors and performance. One of the popular theories about the functional role of visual perception, the efficient coding theory, hypothesizes that the early visual processing layers serve to capture the statistical structure of the visual inputs by removing the redundancy in the visual outputs. Linear implementations of the efficient coding theory, such as independent component analysis (ICA) and sparse coding, learn visual features exhibiting the receptive field properties of V1 simple cells when they are applied to grayscale image patches. In this dissertation, we explore different aspects of the early visual processing layers by building computational models following the efficient coding theory. 1) We develop a hierarchical model, Recursive ICA, that captures nonlinear statistical structures of the visual inputs that cannot be captured by a single layer of ICA. The model is motivated by the idea that higher layers of the visual pathway, such as V2, might work under similar computational principles as the primary visual cortex. Hence we apply a second layer of ICA on top of the first layer ICA outputs. To allow the second layer of ICA to better capture nonlinear statistical structures, we derive a coordinate-wise nonlinear activation function that transforms the first layer ICA's outputs to the second layer ICA's inputs. When applied to grayscale image patches, the model's second layer learns nonlinear visual features, such as texture boundaries and shape contours. We apply the above model to natural scene images, such as forest and grassland, to learn some generic visual features, and then use these features for face and handwritten digit recognition. We get higher recognition rates than those systems built with features designed for face and digit recognition. (2) We show that retinal coding, the pre-cortical stage of visual processing, can also be explained by the efficient coding theory. The retinal coding model turns out to be another variation of Sparse PCA, a technique widely applied in signal processing, financial analysis, bioinformatics, etc. Compared with ICA, which removes the redundancy among the input dimensions, Sparse PCA removes redundancy among the input samples. We apply Sparse PCA to grayscale images, chromatic images, grayscale videos, environmental sound, and human speech, and learn visual and auditory features that exhibit the filtering properties of retinal ganglion cells and auditory nerve fibers. This work suggests that the pre-cortical stages of visual and auditory pathway might work under similar computational principles
-------------------------------------

10138573_183 - 0.840365907046 - technology_and_computing
[module, tissue, effect, cpg, analysis, dna, brain, methylation, blood]

Aging effects on DNA methylation modules in human brain and blood tissue
Abstract
Background
Several recent studies reported aging effects on DNA methylation levels of individual CpG dinucleotides. But it is not yet known whether aging-related consensus modules, in the form of clusters of correlated CpG markers, can be found that are present in multiple human tissues. Such a module could facilitate the understanding of aging effects on multiple tissues.

Results
We therefore employed weighted correlation network analysis of 2,442 Illumina DNA methylation arrays from brain and blood tissues, which enabled the identification of an age-related co-methylation module. Module preservation analysis confirmed that this module can also be found in diverse independent data sets. Biological evaluation showed that module membership is associated with Polycomb group target occupancy counts, CpG island status and autosomal chromosome location. Functional enrichment analysis revealed that the aging-related consensus module comprises genes that are involved in nervous system development, neuron differentiation and neurogenesis, and that it contains promoter CpGs of genes known to be down-regulated in early Alzheimer's disease. A comparison with a standard, non-module based meta-analysis revealed that selecting CpGs based on module membership leads to significantly increased gene ontology enrichment, thus demonstrating that studying aging effects via consensus network analysis enhances the biological insights gained.

Conclusions
Overall, our analysis revealed a robustly defined age-related co-methylation module that is present in multiple human tissues, including blood and brain. We conclude that blood is a promising surrogate for brain tissue when studying the effects of age on DNA methylation profiles.
-------------------------------------

10130261_178 - 0.7362578392 - technology_and_computing
[use, boston, easier, naval, civilian, charlestown, shipyard, bibliography]

Converting the Boston Naval Shipyard at Charlestown to civilian uses : easier said than done.
Bibliography: leaves 117-120.
-------------------------------------

10135206_183 - 0.995908481339 - technology_and_computing
[sensor, array, ppy, dopant, swnt, network, device, acid, nanotube, sulfonic]

Polypyrrole-Functionalized Single-Walled Carbon Nanotube Gas Sensor Arrays
The overall objective of this work is to fabricate and evaluate polypyrrole-single-walled carbon nanotubes hybrid structures based chemiresistive sensor arrays for sensitive, selective and discriminative sensing at room temperature of emissions from automobiles and industrial manufacturing. To conceive the sensor arrays single-walled carbon nanotubes (SWNTs) networks were aligned to bridge a 3 <em>f</em>Ým gap between a pair of prefabricated microelectrodes followed by coating with polypyrrole (PPY) with different dopants by electrochemical polymerization. Initially, the sensor¡¦s synthesis conditions in terms of PPY thickness on SWNTs networks by varying the electropolymerization charge of the monomer pyrrole in presence of LiClO4 dopant for the sensing of NH3 was optimized. Using the optimized polymerization charge of 1 <em>f</em>ÝC determined previously, arrays of SWNTs-PPY hybrid sensors were fabricated by replacing dopant LiClO4 by L-camphor sulfonic acid, D-camphor sulfonic acid, p-toluene sulfonic acid and sodium dodecyl sulfonate.Room temperature gas sensing performance of the PPY coated SWNTs network arrays to gases of environmental significance  such as NH3, NO2, H2S, SO2, CO and CO2 and volatile organic compounds such as benzene, toluene, ethyl benzene, p-xylene, methanol, n-hexane and acetone and humidity, was evaluated. Several folds enhancement in sensing performance was observed towards all the tested analytesfor hybrid devices when compared to bare SWNTs network devices. Differences in sensing performance were noticed for PPY coating with different dopants demonstrating the potential of using the array for discrimination of the tested analytes in a mixture by using chemometric techniques. The underlying sensing mechanism was also investigated by using the devices in chemFET mode configuration.
-------------------------------------

10137393_183 - 0.986607289715 - technology_and_computing
[image, datum, ofmrus, resolution, amount, time, parallel]

High-Resolution Optogenetic Functional Magnetic Resonance Imaging Powered by Compressed Sensing and Parallel Processing
Optogenetic functional magnetic resonance imaging (ofMRI) 1 is a powerful new technology that enables precise control of brain circuit elements while monitoring their causal outputs. To bring ofMRI to its full potential, it is essential to achieve high-spatial resolution with minimal distortions. With our proposed compressed sensing (CS) enabled method, high-spatial resolution ofMRI images can be obtained with a large field of view (FOV) without increasing spatial distortions and the amount of acquired data. The ofMRI data were sampled with passband balanced steady-state free precession (b-SSFP) 8, 17 fast stack-of-spiral sequence in order to achieve ultra-high-spatial resolution images in a short amount of time. Interleaves of data were randomly collected. The images were recovered from the undersampled k-space data by solving an unconstrained convex optimization problem, which balances the trade-off between data consistency and sparsity. The optimization problem can be solved by gradient descent combined with backtracking line search algorithms. Discrete cosine transform (DCT) were chosen as a sparsifying transform. The ofMRI image reconstruction was processed in parallel on a graphics processing unit (GPU) using C/C++ language supported by NVIDIA CUDA engine in order to achieve short reconstruction time. An existing nonequispaced fast Fourier transform (NFFT) algorithm 13, 14 was modified for our GPU parallel processing purpose. The results demonstrate that the compressed sensing reconstructed image has higher resolution while maintaining a precise activation map, compared to a fully sampled low-resolution image with the same amount of data and scan time. A 4-D image can be reconstructed in less than fifteen minutes, which allows compressed sensing ofMRI to become a practical application.
-------------------------------------

10135908_183 - 0.999992708312 - technology_and_computing
[device, mobile, learning, reading, learner, english, student, self-efficacy, language]

English language learners' reading self-efficacy and achievement using 1:1 mobile learning devices
Handheld technology devices allow users to be mobile and access the Internet, personal data, and third-party content applications in many different environments at the users' convenience. The explosion of these mobile learning devices around the globe has led adults to value them for communication, productivity, and learning. Outside of the school setting, many adolescents and children have access to, or own mobile devices. The use of these individual devices by children on a daily basis in schools is a relatively new phenomenon, with just four percent of elementary students doing so in classrooms in 2010(Gray, Thomas & Lewis, 2010). This mixed methods study researched a one-to-one implementation of percent devices in fourth- and fifth-grade elementary classrooms. The focus was to explore the mobile learning device's relationship to English language learners' reading achievement, to English language learners' self-efficacy in reading, and to explore the benefits and limitations of the device's daily use, as perceived by the students. The hypothesis was that the practice of reading and related literacy activities with mobile learning devices would augment English learners' vicarious learning experiences, and thereby effect student cognitive engagement, reading self-efficacy, and reading academic achievement. This study used validated surveys and assessments to measure students' beliefs about reading and their knowledge of reading. Additionally, English language learner interview data were also collected and analyzed to uncover perceived benefits and limitations of utilizing 1:1 mobile learning devices daily for literacy activities. Analysis of the data revealed significantly elevated levels of self-efficacy in reading for the experimental group with 1:1 handheld technology, while academic gains in reading for the experimental and control groups were statistically similar. Students in the experimental group described a virtually-enhanced socio-cultural context for communicating and learning with the handheld technology. Implications for practice, policy, and future research are discussed
-------------------------------------

10136329_183 - 0.999997480199 - technology_and_computing
[wireless, system, vibration, current, motor, fault, monitoring]

Multi-sensor Wireless System for Fault Detection in Induction Motors
This research presents a stand-alone multi-sensor wireless system for continuous real-time performance and condition monitoring of induction motors. The proposed wireless system provides a low-cost alternative to expensive condition monitoring technology available through dedicated current signature analysis or vibration monitoring equipment. The system employs multiple sensors (acoustic, vibration and current) mounted on a common wireless platform. The faults of interest are static and dynamic air-gap eccentricity and bearing damage.The Hilbert-Huang Transform (HHT) of vibration data and power spectral density (PSD) of current and acoustic signals are used as the features in a hierarchical classifier. The proposed wireless system can distinguish a faulty motor from a healthy motor with a probability of 99.9% of correct detection and less than 0.1%likelihood of false alarm. It can also discriminate between dierent fault categories and severity with an average accuracy of 95%.
-------------------------------------

10136948_183 - 0.988748804466 - technology_and_computing
[grid, vehicle, power, market, service, potential]

The Potential of Plug-in Hybrid and Battery Electric Vehicles as Grid Resources: the Case of a Gas and Petroleum Oriented Elecricity Generation System
Using data pertaining to the wholesale electricity market for Long Island, New York, a market unusually dependent on natural gas- and petroleum-fired generation, the article examines the potential uses of plug-in hybrid and battery electric vehicles for various electricity grid services.  The one area in which the vehicles could clearly play an economically favorable  role is frequency regulation services, whereby the vehicles would minutely fluctuate the power they feed into the grid or take from it, in order to keep the total power being fed into the grid in constant balance with the total demand for power.  The article also discusses the potential pitfalls in designing an institutional architecture for integrating the vehicles into the market for frequency regulation services.
-------------------------------------

10130133_178 - 0.999841406327 - technology_and_computing
[system, design, process, control, building]

Control systems for the building design process.
Bibliography: leaves 80-81.
-------------------------------------

10134556_183 - 0.999997808823 - technology_and_computing
[esd, protection, design, amplifier]

RF and Microwave Amplifier Design With ESD Protection
This dissertation is organized as follows. First, the concept and background of ultra wideband (UWB), orthogonal frequency-division multiplexing (OFDM), and Electrostatic discharge are reviewed. Second, the integration of OFDM-UWB and ESD protection and related unit architectures are discussed. Third, unit structures of the design, such as LNA, power amplifier, and ESD protection devices are also discussed and theoretical models are described for all the structures. A novel ESD design based on nano-phase-switching is developed and the ESD performance has been measured. Integration of ESD protection circuits with RF amplifier has been explored and discussed.
-------------------------------------

10130076_178 - 0.999830922316 - technology_and_computing
[example, transfer, reference, object, multiclass, bibliographical, learning, detection]

Transfer learning by borrowing examples for multiclass object detection
Includes bibliographical references (p. 31-33).
-------------------------------------

10134254_183 - 0.999996628529 - technology_and_computing
[network, wireless, localization, active, datum, algorithm, cellular, context-aware]

Context-aware computing for wireless networks
Context-aware computing has been the center of attention in computer science research for many years. Context-aware systems gather contextual data from their sensors, other cooperative nodes or persistent databases and adapt to this information without requiring explicit user intervention. In this thesis we first address the benefits of certain contextual data (such as network connectivity, communication bandwidth, cost of operation, user's location, as well as nearby people and objects) applied to wireless networks. Other important contextual data include social surrounding, environment related conditions, and time context (time of day, month, season, or year). As a result of advancements in technology, the accessing, storing, and incorporating of such massive amounts of data has become a mainstream service. We then develop active and passive localization algorithms for wireless networks. This thesis emphasizes context-aware models for the network layer of wireless and cellular networks rather than classical application layer context-awareness. We first propose a mobile client based active queue management technique which remotely controls the dedicated base station queue size significantly reducing the experienced packet latency. Here, mobile makes use of its knowledge of the underlying cellular technology to enhance the end user experience. We then introduce a packet size aware path setup mechanism for wireless mesh networks where routers benefit from packet size information in computing the optimal route. Both techniques are implemented on real hardware; implementation details and practical considerations are also provided. The second part of this thesis focuses on locating a target in wireless networks. First, we propose a localization algorithm that uses the multipath profile of a mobile device in a cellular network. This algorithm is implemented and evaluated using real data from a commercial cellular network. Finally, we provided linear least squares and neural network techniques for active and passive localization algorithms in a sensor network. The term active localization indicates that the target is active in the localization process, while passive localization refers to locating an uncooperative target
-------------------------------------

10137450_183 - 0.987268561729 - technology_and_computing
[trim, amplification, datum, ssd, workload, request, model, command, performance, device]

Model and analysis of trim commands in solid state drives
NAND flash solid state drives (SSDs) have recently become popular storage alternatives to traditional magnetic hard disk drives (HDDs), due partly to their superior performance in write speed. However, SSDs suffer from a decrease in write speed as they fill with data, due largely to write amplification, a phenomenon in which more writes than requested are performed by the device. Use of the Trim command is known to result in improvement in NAND flash SSD performance; the Trim command informs the SSD flash controller which data records are no longer necessary, and need not be copied during garbage collection. In this dissertation, we analytically model the amount of improvement provided by the Trim command for uniform random workloads in terms of effective overprovisioning, a measure of the device utilization. We show the effective spare factor is Gaussian with mean and variance dependent on the percentage of Trim requests in the workload and the manufacturer-provided physical overprovisioning, and the variance is also dependent on the total storage capacity of the SSD. We then utilize this information to compute the expected write amplification, and to verify our results by simulation. Our theoretical (formula-based) prediction suggests, and our simulations verify, that a considerable write amplification reduction is found as the proportion of Trim requests to Write requests increases. In particular, write amplification drops by almost half over the no-Trim case if just 10% of the requests are Trim instead of Write. We extend our models of effective overprovisioning and write amplification to allow for variation of the workload. We explore data of varying sizes as well as data with varying frequency of access. These extensions allow the flexibility needed to more closely model real-world workloads. Models predicting the write amplification for workloads including Trim requests can be used in real- world situations ranging from helping cloud storage data center operators manage resources to reduce costs without sacrificing performance to optimizing the performance of apps on mobile devices
-------------------------------------

10136803_183 - 0.986277498783 - technology_and_computing
[interface, harvesting, nanocrystal, energy, synthesis, nanomaterial, result, virus-polymer, application, oxide]

Engineering Nanomaterials towards Energy Harvesting and Virological Applications
Nanomaterials, defined as the materials with critical dimensions less than 100 nm, often exhibit unique properties in comparison with their bulk counterparts.  The capability to synthesize huge families of nanomaterials provides human being with unprecedented opportunities towards a more glory civilization. Focusing on nanomaterial synthesis and engineering of the nanocrystal-nanocrystal interface, virus-nanocrystal interface, and virus-polymer interface, this dissertation mainly contains three topics that are related to energy harvesting and health care, including: 1) The synthesis of thermoelectric nanocomposites from dissimilar nanocrystals as building blocks for energy harvesting. Lead telluride and titanium oxide nanocrystals were synthesized and phonon-glass electron-crystal structure was achieved from their homogeneous mixture. As a result, thermal conductivity was extremely suppressed which was similar to that of a superlattice structure and approached the lowest theoretical calculated result. 2) The synthesis of self-disinfection coatings from photoactive copper indium zinc sulfide nanocrystals and non-stoichiometric perovskite-structured lanthanum manganese oxide for preventing rapid and large-scale spread and outbreak of viral diseases. Influenza A virus was employed as the demonstration and more than 75% of virus would be disinfected within 15 min on both kinds of self-disinfecting coatings.3) The construction of virus-polymer nanoparticles with significantly enhanced stability for gene delivery and other biomedical applications. Non-covalent electrical interaction was engineered as anchors and acid-degradable polymer shell was conjugated onto adenovirus. As a result, more than 11 folds of enhanced stability has been observed after 12 days storage at 4 °C.
-------------------------------------

10135523_183 - 0.999997491042 - technology_and_computing
[graph, network, node, structure]

Acquisition of network graph structure
A network graph describes the web of connections between entities in a system. Network graphs are a flexible abstraction; they are equally useful in representing which neurons communicate in a flatworm's brain and which international terrorists collaborate. Understanding how humans learn the structure of network graphs will be useful both to maximize the efficiency of teaching natural networks and to minimize cognitive complexity when designing artificial networks. To this end, I conducted five experiments in which subjects learned which objects in a set were connected. For example, some learned "who is friends with whom" in a social network. These experiments yielded several results. Strong support was found for the hypothesis that the deep structure of a graph affects how quickly it will be learned. Scale-free graphs were acquired more readily than other graphs. Much less support was found for the hypothesis that the surface description given a graph affects learning. For example, learning a social network was no easier or harder than learning a transportation network. The manner in which the learning task was described also had no discernible effect on the rate of acquisition. Learning followed two patterns within each graph. First, if a node of strong salience was present (e.g. a person node labeled "You" in a social network), then edges involving that node were learned quickly. Second, learners responded to network centrality. They responded most accurately when queried on an edge that involved two central nodes. In the last experiment, I show that graphs of medium density are difficult to train, and care must be taken to match training technique to graph structure. I also demonstrate that visual depictions of graphs are generally better teaching material than verbal descriptions. Finally, I propose a general model for network graph acquisition. In this model, the learner initially relies on a frequency heuristic to identify which nodes have many connections and which have few. This allows for educated guessing regarding which edges are valid. Slowly, explicit knowledge is accrued until the subject has perfect knowledge and need no longer rely on a heuristic
-------------------------------------

10136501_183 - 0.999837902076 - technology_and_computing
[mosfet, high, gate, iii-v]

Non-classical MOSFETs : design, modeling, and characterization
Low power and high density requires scaling of MOSFETs in VLSI. As the Si based bulk MOSFETs scale down to the limit imposed by gate oxide tunneling induced gate leakage, short channel effects (SCEs) induced loss of control on electrostatic integrity, high body doping induced high Vt variation, and band-to-band tunneling induced high substrate leakage, etc., two categories of novel MOSFETs are being intensively investigated: Si multiple gate MOSFETs and high mobility III-V material based MOSFETs. Among all types of Si multiple gate MOSFETs, nanowire MOSFET is drawing quite a few attentions for its superior electrostatic control through all-around gate structure. High mobility III-V MOSFETs are considered as a principal candidate to achieve high speed without too aggressive scaling, which can keep good control of electrostatic integrity. This dissertation is primarily devoted to modeling and characterization of challenges and features which are becoming pronounced in aggressively scaled MOSFETs and high mobility material based MOSFETs. High-k dielectric on III-V MOS capacitors are intensively characterized and modeled with the focus on defects at insulator- semiconductor interface as well as inside the oxide, which are grand challenges for III-V MOSFETs. A distributed bulk-oxide trap model is developed to account for the commonly observed frequency dispersion of small signal capacitance-voltage and conductance-voltage data in accumulation and near flat band region. The observed C-V humps in depletion to strong inversion are modeled by interface states model. For III-V MOSFETs design, SCEs and raised source/drain issues are studied using TCAD simulation. Fabricated III-V MOSFETs are characterized and mobility is extracted through experimental current voltage data and multiple frequency gate to channel capacitance measurement and data. For multiple gate Si MOSFETs, this dissertation focuses on nanowire MOSFETs. SCEs based on generalized scale length theory are discussed and compact models are proposed and validated by TCAD simulation. Quantum confinement effects on Vt shift in nanowire MOSFETs with anisotropic effective mass are modeled. Scaling limit is projected for extremely scaled nanowire MOSFETs based on Vt shift sensitivity and scale length theory. Finally, inversion layer capacitance beyond the conventional bulk Si-based MOSFETs is investigated for III -V MOSFETs as well as two typical 3-D transistors, namely symmetric double-gate MOSFETs and nanowire MOSFETs
-------------------------------------

10133933_183 - 0.999997326968 - technology_and_computing
[system, underwater, device, localization, tracking]

Strategies in tracking and localization of distributed underwater systems
Distributed underwater systems, consisting of multiple sensing platforms can provide critical data to better understand complex and inter-related ocean processes. However, to correctly interpret data collected by such systems, we need to know when and where samples were obtained. Since GPS is not available underwater, the position of devices should be estimated with respect to certain known references that may reside on the surface or on the sea-bed. The main challenge is that devices are energy-constrained. Further, low-cost solutions to both the sensing platforms and the overall system design would be critical in making these systems more prevalent and available to scientists. Existing underwater tracking techniques are not well-suited to distributed systems because they are built around stand-alone platforms. They ignore vital relative information between devices and require long range communication which is both expensive and has high energy consumption. As a result, existing techniques do not scale well in multi-vehicle systems. To address these challenges, we take a systems perspective to underwater positioning. This means that instead of viewing each node as a separate entity, as in traditional systems, we consider the network as a whole. Therefore, we shift our focus to tracking a collective, rather than independently positioning a number of devices. This paradigm-shift allows us to use collaboration between devices to improve the performance and energy-scalability of mobile distributed systems. However, it makes the problem of tracking and localization more complex. Our proposed solutions draw on the framework of factor graphs to optimally and jointly estimate the trajectories of multiple nodes by combining information in 4 dimensions. This framework allows us to leverage both from network density and accurate motion information, if available. In addition, we have identified node mobility as a key factor that can both impede and improve performance. We show how mobility in combination with delays in medium access is an impairment to time-synchronization and localization and propose cross-layer and collaborative approaches to counter its effect. Our proposed strategies are essentially aimed at more efficient localization and tracking in underwater networks where resources are constrained. We believe that our techniques in combination with existing systems would address the localization requirements of a wide range of underwater applications
-------------------------------------

10135736_183 - 0.999640698397 - technology_and_computing
[bot]

The Lives of Bots
I describe the complex social and technical environment in which bots exist in Wikipedia, emphasizing not only how bots produce order and enforce rules, but also how humans produce bots and negotiate rules around their operation.
-------------------------------------

10133778_183 - 0.998710222156 - technology_and_computing
[software, system, rich, services, library, architecture]

An introduction to Rich Services/Erlang
Rich Services addresses the challenges of building and integrating distributed software systems. These include the need for managing multiple stake holder concerns, for integrating distributed software systems as a single entity into higher level distributed software systems behind interfaces, for adapting to changing requirements and for providing scalability. There is a need for software libraries that support the development of software systems designed using the Rich Services architecture. Rich Services/Erlang is the first software library that supports the creation of distributed software systems designed using the Rich Services architecture. Erlang is a functional programming language which explicitly supports the development of distributed, concurrent software. The library leverages the features of Erlang and the Rich Services architecture to empower developers to focus on the design and application logic of their software systems, rather than the implementation complexity of the integration and messaging system. We begin with an introduction of the challenges encountered in the creation of distributed software systems and with a discussion of the need for the Rich Services architecture. We continue with an overview of Erlang and then the introduction to Rich Services/Erlang. The following chapter includes a description of the process used to develop Rich Services software systems using the library. The final chapter about Rich Services/Erlang discusses the run-time view of systems implemented with it. The next few chapters present our enterprise integration patterns and chat system case studies. The thesis finishes with our evaluation and our conclusion
-------------------------------------

10138247_183 - 0.999907574834 - technology_and_computing
[system, lighting, led, fluorescent, kerosene, cost, house, chicken, higher]

Illuminating the Pecking Order in Off-Grid Lighting:
A Demonstration of LED Lighting for Saving Energy in the Poultry Sector
The Lumina Project and Lighting Africa conducted a full-scale field test involving a switch from kerosene to solar-LED lighting for commercial broiler chicken production at an off-grid farm in Kenya. The test achieved lower operating costs, produced substantially more light, improved the working environment, and had no adverse effect on yields. A strategy using conventional solar-fluorescent lighting also achieved comparable yields, but entailed a six-fold higher capital cost and significantly higher recurring battery replacement costs. Thanks to higher energy and optical efficiencies, the LED system provided approximately twice the illumination to the chicken-production area and yet drew less than half the power.
At the study farm, 3000 chickens were grown in each of three identical houses under kerosene, fluorescent, and LED lighting configurations. Under baseline conditions, a yearly expenditure of 1,200 USD is required to illuminate the three houses with kerosene. The LED system eliminates this fuel use and expense with a corresponding simple payback time of 1.5 years, while the solar-fluorescent system has a payback time of 9.3 years. The corresponding reduction in fuel expenditure in both cases represents a 15percent increase in after-tax net income (revenues minus expenses) across the entire business operation. The differential cost-effectiveness between the LED and fluorescent systems would be substantially greater if the fluorescent system were upsized to provide the same light as the LED system.
Providing light with the fluorescent or LED systems is also far more economical than connecting to the grid in this case. The estimated grid-connection cost at this facility is 1.7 million Kenya Schillings (approximately 21,250 USD), which is nearly six-times the cost of the fluorescent system and 35-times the cost of the LED system.
The LED system also confers various non-energy benefits. The relative uniformity of LED lighting, compared to the fluorescent or kerosene lighting, reduced crowding which in turn created a less stressful environment for the chickens. The far higher levels of illumination also created a better environment for the workers, while eliminating the time required for obtaining fuel and maintaining kerosene lanterns. An additional advantage of the LED system relative to the solar fluorescent system was that the former does not require a skilled technician to carry out the installation. The portable LED system lighting layout is also more easily adjusted than that of the hardwired fluorescent systems. Furthermore, switching to the LED system avoids over one metric ton of carbon dioxide emissions per house on an annual basis compared to kerosene. 
There is high potential for replication of this particular LED lighting strategy in the developing world. In order to estimate the scale of kerosene use and the potential for savings, more information is needed on the numbers of chickens produced off-grid, as well as lighting uses for other categories of poultry production (egg layers, indigenous broilers ). Our discovery that weight gain did not slow in the solar-fluorescent house after it experienced extended lighting outages beginning on day 14 of the 35-day study suggests that conventional farming practices in Kenyan broiler operations may call for more hours of lighting than is needed to achieve least-cost production.
-------------------------------------

10133605_183 - 0.999959922055 - technology_and_computing
[motion, method, algorithm, video, frame, architecture, true]

Method and architecture design for motion compensated frame interpolation in high-definition video processing
Digital displays such as Liquid Crystal Display (LCD) and plasma display televisions have become prevalent in recent years. Sports broadcasting and movies are two prime factors responsible for this popularity. However, motion blur and judder appear as objects move rapidly or color dramatically changes on a wide range of LCD devices because of slow response time and sample-and-hold drive nature. Frame Rate Up Conversion (FRUC) is a well-studied method that is used to minimize these detrimental effects. A novel, fast, and efficient method with a well-designed architecture is proposed for Motion Compensated Frame Interpolation (MCFI) or Frame Rate Up Conversion. Unlike previous works involving high complexity, time-consuming iterations, and higher complex architecture, the proposed method adopts a one-pass, low-complexity approach without any iteration and is capable of dealing with High Definition (HD) video processing. Rather than a conventional motion estimation in MCFI, our method employs a unique true motion engine that explores at most nine motion candidates with different motion directions and then determines one true motion by referring to neighboring spatial and temporal information. For the purpose of motion estimation, the proposed method introduces an adaptive overlapped block matching algorithm known as the Multi-Directional Enlarged Matching Algorithm (MDEMA), and considers different overlapped types based on the direction of the current motion vector in order to enhance searching accuracy and visual quality. For practical issues and real-time HD requirements, the proposed architecture employs a modified Multi-level Successive Eliminate Algorithm (MSEA), which is a Fast Full-Search Block Matching Algorithm (FFSBMA) and has the ability to reduce the heavy computation of full search while maintaining similar quality. According to analyzed temporal information, our method explores true motion candidates and refines the accuracy of true motions for blocks or sub-blocks. Experimental results show that the proposed algorithm provides better video quality than conventional methods and demonstrates excellent performance for 30fps HD1080p video (1920x1080 resolution) at 180MHz or 30fps 720p video (1280x720) at 83MHz
-------------------------------------

10133648_183 - 0.999987198896 - technology_and_computing
[mass, seismometer, sensor, signal, noise, interferometric]

Development and characterization of an observatory-class, broadband, non-fedback, leaf-spring interferometric seismometer
Motivated by the desire to have instruments whose performance rivals that of the best observatory sensors, two new seismometers have been developed that are designed to operate in borehole environments free of electronics and force-feedback. This is accomplished by the development of optical sensors consisting of leaf-spring suspended masses whose positions are monitored interferometrically. A digital signal processor (DSP) samples fringe signals created by the interferometric sensor and produces a displacement record of the seismometer mass with a resolution of less than 1 pm (10⁻¹² m). Maximum mass motion is limited by practical issues to about 15 mm, providing a dynamic range of 10¹⁰, equivalent to 33 bits. For a moderate-sized mass with readily achievable free period and damping, the mass- spring sensor's fundamental thermal noise is less than ambient noise at the seismically quietest sites. Hence, there is no need to abandon this basic, simple, mass- spring design. Elimination of electronics, however, means elimination of force feedback - the paradigm in seismometry for the past several decades. We have tested our non-fedback optical seismometers against standard fedback observatory-quality seismometers (STS-1) and found that they provide equivalent seismograms for signals ranging in frequency from tides to at least 10 Hz. We examine the effects of thermal, magnetic, and barometric noise observed on the interferometric sensors and compare with the STS-1
-------------------------------------

10139540_183 - 0.997048302706 - technology_and_computing
[ventilation, controller, residential, energy]

Development of a Residential Integrated  Ventilation Controller
The goal of this study was to develop a Residential Integrated Ventilation Controller (RIVEC) to reduce the energy impact of required mechanical ventilation by 20percent, maintain or improve indoor air quality and provide demand response benefits.   This represents potential energy savings of about 140 GWh of electricity and 83 million therms of natural gas as well as proportional peak savings in California.  The RIVEC controller is intended to meet the 2008 Title 24 requirements for residential ventilation as well as taking into account the issues of outdoor conditions, other ventilation devices (including economizers), peak demand concerns and occupant preferences.  The controller is designed to manage all the residential ventilation systems that are currently available. A key innovation in this controller is the ability to implement the concept of efficacy and intermittent ventilation which allows time shifting of ventilation.  Using this approach ventilation can be shifted away from times of high cost or high outdoor pollution towards times when it is cheaper and more effective.   Simulations, based on the ones used to develop the new residential ventilation requirements for the California Buildings Energy code, were used to further define the specific criteria and strategies needed for the controller.   These simulations provide estimates of the energy, peak power and contaminant improvement possible for different California climates for the various ventilation systems.  Results from a field test of the prototype controller corroborate the predicted performance.
-------------------------------------

10129836_178 - 0.717464850704 - technology_and_computing
[method, bibliographical, real, reference, application, dsm, design, development, estate, process]

Application of the Design Structure Matrix (DSM) to the real estate development process using modular construction methods
Includes bibliographical references (p. 111-112).
-------------------------------------

10132753_183 - 0.999527947374 - technology_and_computing
[model, datum, estimation, source, asphalt, parameter, rutting, concrete, pavement]

Why voters may prefer congested public clubs
The accurate prediction of rutting development is an essential element for the efficient management of pavement systems. In addition, progression models of highway pavement rutting can be used to study the effects of different loading levels, and thus in allocating cost responsibilities to various vehicle classes for their use of the highway system. Further, such models can be used for evaluating different strategies for design, maintenance and rehabilitation. Finally, if the models contain information about asphalt concrete mixes, they can also provide directions in the proportioning of aggregate, asphalt and air in the mix.
      The objective of this paper is to demonstrate the effectiveness of the estimation of rutting models by combining the information from two data sources. The data sources considered are the AASHO Road Test and the WesTrack Road Test. Combined estimation with both data sources is used to identify parameters that are not identifiable from one data source alone. In addition, this estimation approach also yields more efficient parameter estimates.
      The results presented in this paper demonstrate that joint estimation produces more realistic parameter estimates than those obtained by using either data set alone. Furthermore, joint estimation allows us to account for the effects of pavement structure, axle load configuration, asphalt concrete mix properties, freeze-thaw cycles and hot temperatures in a single model. Finally, it allows to us to predict the relative contributions of rutting originating both in the asphalt concrete and in the unbound layers in the same model.
-------------------------------------

10175488_189 - 0.88946950111 - technology_and_computing
[map, floor, residence, dunellen, william, estate, marsh, rice]

William Marsh Rice estate in Dunellen, NJ, first floor of residence
Color image of a hand-drawn map of the ground floor of the main residence, William Marsh Rice estate, Dunellen, New Jersey. The scale is 1” to 8’. “B. Fosgate, Architect, Plainfield, N.J.” is stamped on the right hand side of the map. There is a fragment of illegible writing on the lower portion of the map.
-------------------------------------

10129821_178 - 0.999724791627 - technology_and_computing
[technique, activity, design, computer, location]

Computer techniques for studying activity locations in a new design.
Bibliography: leaves 116-118.
-------------------------------------

10137339_183 - 0.999597863792 - technology_and_computing
[mapping, transversality, chapter, result, target, formal, dimension, generic, manifold]

Transversality of CR mappings between CR submanifolds of complex spaces
We investigate the geometric property of transversality of holomorphic, formal or CR mappings between real-analytic, formal or smooth generic submanifolds of complex spaces of equidimension as well as of different dimensions. In Chapter 3, we shall consider the CR transversality in equidimension case. The main purpose of this chapter is to show that a holomorphic, formal or smooth CR mapping sending a real-analytic, smooth or formal generic submanifold M into such another Mʹ is CR transversal to the target, provided that the source manifold is of finite bracket type and the mapping is of generic full rank. This result and its corollary completely resolve two questions posed by Peter Ebenfelt and Linda Preiss Rothschild in a paper from 2006. We also show that under a very mild assumption on the source manifold, the generic full rank condition imposed on the mapping is also necessary for the CR transversality to hold. This result confirms a conjecture in a paper by Bernhard Lamel and Nordine Mir. In Chapter 4, we consider the transversality of mappings when the target manifold is of higher dimension. We will restrict ourself to the situation in which both manifolds M and Mʹ are hypersurfaces in Cn⁺¹ and CN⁺¹ respectively, where 1 < n < N. A main result of this chapter implies that under certain restrictions on the dimensions n, N and the rank of the Levi-form of the target hypersurface, if the set of points at which the mapping H fails to be a local embedding has codimension at least 2, then the mapping must be transversal to the target at all points. Another result of this chapter implies that under some more restrictive assumptions, any finite holomorphic mapping sending M into Mʹ is transversal at all points, unless the source hypersurface is of infinite type. This result may be considered as a different dimension analogue of a theorem by M. Salah Baouendi and Linda Preiss Rothschild from 1990
-------------------------------------

10138949_183 - 0.999940627297 - technology_and_computing
[software, version, reliability, product, family]

Software Selection for Reliability Optimization using Time Series Analysis and Machine Learning
A software product family generally has multiple product lines or software versions, each with, in general, a different functionality. Also with new software versions, functionality usually measured by the number of features, in general, increases. The availability of several versions of a software family raises several important technical issues related to trade-off between software functionality and software reliability.In this thesis, we consider the following issue: given a family of different versions of the same software product, how does one select versions of the software that maximize reliability, are measured by the number of software bugs. To resolve this issue, we develop a framework integrating time series analysis and machine learning to classify software versions of a given family into clusters based on user-specified reliability metrics. This classification or separation then enables the user to optimize the selection of software for maximum reliability. Time series analysis, in particular is used to predict the future evolution of bugs. Machine learning methods, in particular Expectation-Maximization clustering and K-means clustering are applied to prediction-based metrics to classify software into clusters. We demonstrate the application of this framework to reliability prediction, classification and optimization of network IOS products.
-------------------------------------

10134201_183 - 0.995892535455 - technology_and_computing
[magnetic, storage, development, industry, bit, limit]

Protein-Based Disk Recording for Aerial Densities Beyond 10 Tbit/in <super>2</super>.
Technological developments in the magnetic disk storage industry have relied heavily on the scalability of magnetic bits to ever smaller dimensions in order to provide an increase in areal data densities. This progress has recently been stifled by the Superparamagnetic limit. The Superparamagnetic limit fundamentally restricts the size to which a magnetic bit can be decreased. This limit results from thermal instability of magnetic bits which result in randomization of the magnetic moments within the magnetic media itself as bit dimensions are decreased.  Presently, the Superparamagnetic limit promises to stifle further progress in the magnetic disk-based data storage industry. Therefore, new avenues for technological development in this sector must be pursued to maintain compliance with moor's law and propel future development in this sector at a pace which will keep the industry vital in the coming decades. In this thesis, a new methodology is proposed by which the technological developments of the traditional magnetic data storage industry can be utilized and the pitfalls of magnetic storage can be avoided. Herein a disk storage implementation involving photo-reactive protein films consisting of Bacteriorhodopsin is presented for extending aerial densities beyond 10 Tbit/in <super>2</Super>. System requirements of a commercial application utilizing this technology are presented in detail. The absorption properties of Bacteriorhodopsin (BR) monolayer films, deposited via Electrostatic Self-Assembly, have been studied for the first time to better understand the underlying read/write processes as applicable to this new technology.
-------------------------------------

10134683_183 - 0.721756019861 - technology_and_computing
[modulation, bidirectional, model, technique, noise, pon, optical, network, low, novel]

Improved techniques for passive optical networks
The work in this thesis introduces a variety of techniques that can be used to improve the performance of previous state of the art passive optical networks (PONs). These networks will be of increasing importance as increasing bandwidth demand drives the penetration of optical fiber into access networks. PONs require novel design techniques due to their bidirectional nature and tight cost requirements. In this thesis improvements are proposed in three categories: filtering of bidirectional noise; novel modulation techniques; and use of non-traditional low cost devices (specifically vertical cavity surface emitting lasers (VCSELs)). High pass filtering (HPF) is shown to reduce the impact of bidirectional Rayleigh noise, which is dominant in PONs. It is demonstrated that under certain circumstances Rayleigh noise immunity is improved by up to 10 dB with proper selection of an HPF. A model to predict the effects of baseline wander caused by HPF is developed and experimentally validated. This model is combined with an existing Rayleigh noise model, which is experimentally validated. This model is extended to predict the performance of a bidirectional on-off keyed (OOK) PON and experimentally verified. Three methods are investigated to improve the ONU modulator. First it is shown that transmitter pre-distortion can provide a doubling of the non-equalized datarate of a low modulation bandwidth ONU modulator. Next a four level intensity modulation format is demonstrated. It is shown that while this method is possible, it provides no improvement over OOK modulation. A novel method for modulating the orthogonal phases of an intensity modulated microwave subcarrier, similar to quadrature amplitude modulation (QAM), is demonstrated. Significant performance improvement is realized with this scheme. Finally the use of a VCSEL in PONs is evaluated. It is predicted that using a VCSEL as central office (CO) laser will offer minor benefits and some moderate challenges. Use of a vertical cavity semiconductor optical amplifier (VCSOA) as an ONU modulator is prevented by a dispersive nonlinearity. This nonlinearity leads to bistability. First demonstration of butterfly bistability in a 1550 nm VCSOA and record low power bistability are demonstrated
-------------------------------------

10135072_183 - 0.999685145445 - technology_and_computing
[system, thermal, experimental, heat, design, transfer, surface, scale]

Phase Change Materials for Thermal Energy Storage in Concentrated Solar Thermal Power Plants
Experimental studies are presented that aim to utilize phase change materials (PCM's) to enhance thermal energy storage systems for concentrated solar thermal power (CSP) systems. Both laboratory scale and prototype flow loop scale experiments were conducted. Background and motivation of the system is presented followed by experimental results and experimental system design.	Laboratory experiments were performed to determine the effectiveness of various surface treatments on changing the nucleation kinetics and enhancing heat transfer in the system. Experimental data is presented to show the effectiveness of both surface finish and surface material treatments.A large scale flow loop was designed and built to determine real heat transfer during solidification. Extensive design calculations were performed along with CAD design before the apparatus was constructed. Once the system was built, systematic testing and development led to a fully functional system ready for actual heat transfer testing.
-------------------------------------

10134739_183 - 0.999230422099 - technology_and_computing
[tool, analysis, water, rdna, workflow, microbial, sequence, ecology, software, ribosomal]

WATERS: a Workflow for the Alignment, Taxonomy, and Ecology of Ribosomal Sequences
Abstract
Background
For more than two decades microbiologists have used a highly conserved microbial gene as a phylogenetic marker for bacteria and archaea. The small-subunit ribosomal RNA gene, also known as 16 S rRNA, is encoded by ribosomal DNA, 16 S rDNA, and has provided a powerful comparative tool to microbial ecologists. Over time, the microbial ecology field has matured from small-scale studies in a select number of environments to massive collections of sequence data that are paired with dozens of corresponding collection variables. As the complexity of data and tool sets have grown, the need for flexible automation and maintenance of the core processes of 16 S rDNA sequence analysis has increased correspondingly.

Results
We present WATERS, an integrated approach for 16 S rDNA analysis that bundles a suite of publicly available 16 S rDNA analysis software tools into a single software package. The "toolkit" includes sequence alignment, chimera removal, OTU determination, taxonomy assignment, phylogentic tree construction as well as a host of ecological analysis and visualization tools. WATERS employs a flexible, collection-oriented 'workflow' approach using the open-source Kepler system as a platform.

Conclusions
By packaging available software tools into a single automated workflow, WATERS simplifies 16 S rDNA analyses, especially for those without specialized bioinformatics, programming expertise. In addition, WATERS, like some of the newer comprehensive rRNA analysis tools, allows researchers to minimize the time dedicated to carrying out tedious informatics steps and to focus their attention instead on the biological interpretation of the results. One advantage of WATERS over other comprehensive tools is that the use of the Kepler workflow system facilitates result interpretation and reproducibility via a data provenance sub-system. Furthermore, new "actors" can be added to the workflow as desired and we see WATERS as an initial seed for a sizeable and growing repository of interoperable, easy-to-combine tools for asking increasingly complex microbial ecology questions.
-------------------------------------

10130062_178 - 0.924487041743 - technology_and_computing
[visual, dynamic, reference, modeling, scene, bibliographical, generative]

Generative modeling of dynamic visual scenes
Includes bibliographical references (p. 301-312).
-------------------------------------

10134032_183 - 0.999855567875 - technology_and_computing
[execution, system, technique, adaptivity, multicore, resource, power, performance, uncertainty]

Tackling computation uncertainty through fine-grained and predictable execution adaptivity in multicore systems
The continued scaling of silicon fabrication technologies has enabled the integration of dozens of processing cores on a single chip in the next computer generation. Our ability to exploit such computational power, however, is checkmated not only by limitations of parallelism extraction techniques, but furthermore by increasing levels of execution uncertainty within the system. As device feature sizes scale below 45nm, reliability has rapidly moved to the forefront of concerns for leading semiconductor companies, with the main challenge being the scaling of system performance while meeting power and reliability budgets. To make things worse, such an unreliable computational fabric is used to concurrently execute an increasing number of applications that constantly vie for execution resources, thus furthermore making the execution environment more dynamic and unpredictable. The unreliability in the electronic fabric, in conjunction with the unpredictability in the execution process, has motivated the incorporation of execution adaptivity in future multicore systems, so that computational resources can be frequently renegotiated at run-time. The challenge, however, is to attain adaptivity in conjunction with the goals that designers already face, such as computation efficiency, power and thermal management, and predictability of worst-case performance. The traditional approaches of providing adaptivity at runtime dynamically will fail to scale as we move to systems of dozens of cores. Neither do static techniques that rely solely on compiler analysis deliver efficient adaptivity though. Instead, I have proposed a set of compiler-directed run-time optimization techniques that can combine the advantages of both, capable of reacting to unpredictable events while at the same time exploiting intensive program information to guide runtime decisions. Technically, this thesis addresses the increasing levels of execution uncertainty in future multicore systems induced by device failures, heat buildups, or resource competitions from three aspects. It presents several tightly-coupled techniques to either 1) maximally mitigate a source of uncertainty, such as thermal stress, or 2) precisely detect resource variations, especially the ones induced by device failures, and then 3) quickly reconfigure the execution in a predictable manner with no reliance on spare units. These techniques are developed with the considerations of minimizing power and performance impact, localizing communication and migration so as to satisfy interconnect constraints, and ensuring high predictability so as to meet worst-case performance constraints of mission-critical applications. The successful incorporation of these techniques in future multicore systems, I believe, will engender adaptive, scalable architectures that can seamlessly reshape execution paths and schedules in an amortizable, high- volume, fixed-silicon fabric
-------------------------------------

10133447_183 - 0.857268411122 - technology_and_computing
[magnetic, recording, media, mfm, characterization, fabrication, device, probe]

Media Fabrication and Characterization Systems for Multilevel Three-Dimensional Magnetic Recording
The data storage industry recently substituted its five-decade-old technology (longitudinal magnetic recording), which reached its fundamental limitations due to thermal instabilities in the recording media, with perpendicular magnetic recording. Nonetheless, the lifetime of this newly adopted technology and other next generation alternatives including heat-assisted magnetic recording (HAMR) and bit patterned media (BPM) is comparably short. In order to defer the superparamagnetic limit substantially beyond 1 Tbits/in^2, it may be necessary to stack information in a third (vertical) dimension. This vertical stacking underlies the concept of multilevel (ML) three-dimensional (3D) magnetic recording and memory. In 3D magnetic devices, information is recorded, not only on one surface (as in all modern 2D applications), but in the entire 3D bulk of the recording media. As a result, substantially larger amount of data could be recorded on the same surface area (as compared to any 2D alternative).  This study addresses design configuration, media fabrication, and characterization techniques for a particular ML 3D magnetic recording device. A newly developed ML 3D magnetic recording media enables selective reading and writing of up to 4 distinct signal levels, in comparison to only two signals in conventional 2D-based magnetic recording media. It is believed that at least over six layers (or over 2^6 = 64 signal levels) could be independently accessed in a ML 3D magnetic recording device. Gallium ion implantation as a potential method to alter the magnetic properties of 3D media is also discussed.  Furthermore, with every emerging technology, both fabrication systems and characterization techniques must be developed. One of the most crucial characterization tools, which enable the direct visualization of the smallest bit of magnetic information, is Magnetic Force Microscopy (MFM). MFM is thus an integral part of the development and characterization of ML 3D magnetic recording devices. Nevertheless, the best lateral resolution conventional state-of-the-art probes can produce under ambient conditions is limited to about ~ 25 nm. In this study, newly developed fabrication techniques for MFM probes are presented. Specifically, plateau probes to enhance the overall capabilities of MFM and high lateral resolution (below 10 nm in ambient conditions) multi-domain MFM probe. The enhancement in MFM capabilities has the potential to facilitate the development of HAMR, BPM, and ML 3D magnetic systems.
-------------------------------------

10132840_183 - 0.793106941019 - technology_and_computing
[traffic, theory]

Understanding and Modeling Driver Behavior in Dense Traffic Flow
We present in this report a new car-following theory that can reproduce both the so-called capacity drop and traffic hysteresis, two prominent features of multi-phase vehicular traffic flow. This is achieved through the introduction of a single variable, driver response time, that depends on both vehicle spacing and traffic motion. By specifying different functional forms of response time, one can obtain not only brand new theories but also some of the well-known old car-following theories, which is demonstrated in this report through both theoretical analyses and numerical simulation.
-------------------------------------

10129931_178 - 0.999996846606 - technology_and_computing
[system, modularity, behavior, signaling, retroactivity]

Modularity in signaling systems
Modularity is a property by which the behavior of a system does not change upon interconnection. It is crucial for understanding the behavior of a complex system from the behavior of the composing subsystems. Whether modularity holds in biology is an intriguing and largely debated question. In this paper, we discuss this question taking a control system theory view and focusing on signaling systems. In particular, we argue that, despite signaling systems being constituted of structural modules, such as covalent modification cycles, modularity does not hold in general. As in any engineering system, impedance-like effects, called retroactivity, appear at interconnections and alter the behavior of connected modules. We further argue that while signaling systems have evolved sophisticated ways to counter-act retroactivity and enforce modularity, retroactivity may also be exploited to finely control the information processing of signaling pathways. Testable predictions and experimental evidence are discussed with their implications.
-------------------------------------

10136107_183 - 0.943099512134 - technology_and_computing
[device, liquid, frequency, oscillation, material, model, reversible, interface, parameter, mathematical]

A Magnetomechanical Thermal Energy Harvester With A Reversible Liquid Interface
A device that has the potential to provide electrical power to wireless sensors is investigated in this thesis. The device uses a ferromagnetic material to exploit the temperature difference between a heat source and a heat sink to produce oscillating motions and temperature polarizations, which can then be converted into electrical energy by piezoelectric materials and pyroelectric materials, respectively. One advantage of the proposed device is that it can exploit the characteristics of both piezoelectric materials and pyroelectric materials to harvest waste heat energy; therefore, it is expected to have potential high power output and conversion efficiency. Furthermore, the advantages of the reversible liquid interface can be exploited in this device to improve its performance.    The work presented in this thesis uses a coupled thermal and mechanical mathematical model to optimize the design of the proposed device. Three important parameters used in the mathematical model, the spring constant, the capillary force and the magnetic force, are calculated and then validated with experimental results to ensure that the modeling predictions match the actual behaviors of the device.    The mathematical model is then solved, and the modeling results are validated with experimental results to confirm that the model is able to correctly predict the behaviors of the device with reasonable levels of accuracy. The oscillation frequency is an important parameter for the device operation because a higher oscillation frequency means that more electrical energy can be collected in a given amount of time. Thus, the oscillation frequency is used as an index to evaluate the performance of the device. A parametric study is conducted for some design parameters, including the liquid volume of the reversible liquid interface, the total gap distance and the cold surface temperature, to attempt to increase the oscillation frequency.    The mathematical model predicts that the device can achieve a relatively high oscillation frequency when the optimized design parameters are used.    A thermal energy harvester using a ferromagnetic material with a reversible liquid interface is then physically built. Experiments are performed to study the effect of the reversible liquid interface on the performance of the device, and the results indicate that the reversible liquid interface can effectively increase the oscillation frequency in the low hot surface temperature region. Finally, when the optimized design parameters predicted by the mathematical model are applied, the device achieves a maximum oscillation frequency of 8.3Hz at a hot surface temperature of 41.8oC.
-------------------------------------

10135788_183 - 0.981117645758 - technology_and_computing
[building, system, energy, exterior, design, comfort, control, window, occupant]

Impact of Fixed Exterior Shading on Daylighting: A Case Study of the David Brower Center
Commercial buildings in the U.S. consumed 18% of primary energy and 36% of the nation's electricity in 2006 (U.S. Department of Energy, 2011). According to the 2003 Commercial Buildings Energy Consumption Survey (CBECS), heating, cooling and lighting account for 36%, 8% and 21%, respectively, of the total energy consumed in the commercial building sector (Energy Information Administration EIA, 2008). In response to increasing concerns over global warming, a number of initiatives to reduce energy used by U.S. buildings have taken form over the course of the past decade, including the development of voluntary rating systems such as the Leadership in Energy and Environmental Design (LEED) rating system of the U.S. Green Building Council (USGBC) and the Energy Star rating system of the U.S. Environmental Protection Agency. Yet despite significant efforts on behalf of a range of public and government organizations progress has been slow (Scofield, 2009).
      One of the essential components in low-energy buildings is the building envelope. Window systems are critical to occupant comfort and well-being, but frequently bring a high level of complexity to the design process due to the inherent difficulty of striking a balance between occupant comfort needs, building energy use, project budget and a range of other considerations. While windows provide a way to introduce daylight and views, fenestration design must be carefully assessed in terms of daylighting, visual comfort, heat gain and heat loss. A number of studies suggest that without proper solar and lighting control, occupants are likely to draw shades or blinds when visual or thermal comfort thresholds are exceeded (Figure 1) and that blinds are likely to remain closed for extended periods of time, negating the potential benefits of having the window in the first place (Galasiu & Veitch, 2006; Inkarojrit, 2008). Automated controls provide a way to control facade systems, as is the case with automated shading, however they provide their own set of challenges – added operational complexity and cost, and the need for maintaining additional controls and components (Heschong Mahone Group HMG, 2008; Zelenay, Perepelitza & Lehrer, 2011). In contrast, fixed window elements, such as fixed exterior shading, may offer less opportunity for selective control of daylighting and solar heat gain, however the risk of faulty system operation, experienced with automated systems that are improperly commissioned or maintained, is eliminated.  
      While exterior shading systems offer significant benefit in terms of solar control and occupant thermal comfort and are quite common in Europe, they are not typically implemented on U.S. buildings (Perepelitza 2010 Zelenay et al., 2011). The prevalence of exterior shading systems in Europe can be explained by higher energy prices, stricter building codes, and higher expectations regarding the quality of the working environment and construction (Yudelson, 2009). Owner and design team concerns about operation and maintenance of and high cost of systems are the main factors impeding the widespread adoption of these systems in the U.S. (Lee, Selkowitz, Bazjanac, Inkarojrit, & Kohler, 2002; Lee & Selkowitz, 2005; HMG, 2008; Zelenay et al., 2011).
      In light of the fact that exterior shading is uncommon in the U.S., the question of why fixed exterior louvers were implemented at the David Brower Center, a four-story mixed-use building in Berkeley, California, is a compelling one (Figure 2). The building, situated in a dense urban neighborhood in downtown Berkeley (Figure 3), was designed by the San Francisco-based firm Daniel Solomon Design Partners (formerly Solomon E.T.C.) in collaboration with Tipping Mar + Associates (structural engineer), Integral Group (mechanical engineer formerly Rumsey Engineers), and Loisos + Ubbelohde (daylighting and facade consultant). 
-------------------------------------

10133062_183 - 0.983506448844 - technology_and_computing
[datum]

Preserving research data
Granting ownership rights to data, as if it were private property, only limits data access without ensuring the benefits of researcher precedence or the rewards for good data collection.
-------------------------------------

10134496_183 - 0.899331585785 - technology_and_computing
[noise, ambient, daylight, datum, acoustic, intensity, experimental]

Correlation time of ocean ambient noise intensity in San Diego Bay and target recognition in acoustic daylight images
A method for passively detecting and imaging underwater targets using ambient noise as the sole source of illumination (named acoustic daylight) was successfully implemented in the form of the Acoustic Daylight Ocean Noise Imaging System (ADONIS). In a series of imaging experiments conducted in San Diego Bay, where the dominant source of high-frequency ambient noise is snapping shrimp, a large quantity of ambient noise intensity data was collected with the ADONIS (Epifanio, 1997). In a subset of the experimental data sets, fluctuations of time-averaged ambient noise intensity exhibited a diurnal pattern consistent with the increase in frequency of shrimp snapping near dawn and dusk. The same subset of experimental data is revisited here and the correlation time is estimated and analysed for sequences of ambient noise data several minutes in length, with the aim of detecting possible periodicities or other trends in the fluctuation of the shrimp-dominated ambient noise field. Using videos formed from sequences of acoustic daylight images along with other experimental information, candidate segments of static-configuration ADONIS raw ambient noise data were isolated. For each segment, the normalized intensity auto-correlation closely resembled the delta function, the auto-correlation of white noise. No intensity fluctuation patterns at timescales smaller than a few minutes were discernible, suggesting that the shrimp do not communicate, synchronise, or exhibit any periodicities in their snapping. Also presented here is a ADONIS-specific target recognition algorithm based on principal component analysis, along with basic experimental results using a database of acoustic daylight images
-------------------------------------

10135250_183 - 0.989901770018 - technology_and_computing
[analysis, frame, model, concrete, masonry-infilled, rc, specimen, behavior, cyclic, material]

Numerical analysis of masonry-infilled reinforced concrete frames subjected to seismic loads and experimental evaluation of retrofit techniques
Masonry-infilled reinforced concrete (RC) frames constitute a significant part of the building inventory in earthquake-prone regions around the world. The development of accurate analysis tools and of retrofit techniques is important for enhancing the seismic safety of older construction of this type. Most of the existing analytical approaches have either adopted simplified models with limited predictive capabilities or been restricted to the monotonic loading regime. Additionally, many of the retrofit methods used in practice have not been experimentally validated. The present study aimed to establish and use refined computational tools for the reliable and robust analysis of masonry-infilled RC frames with and without retrofit, and to experimentally validate recently developed retrofit schemes. Constitutive models have been developed to allow the analysis of infilled frames. An existing smeared-crack model has been enhanced to accurately describe diffuse cracking and crushing in concrete and masonry under cyclic loading. Furthermore, a novel cohesive interface model has been formulated to describe strongly localized cracks in the concrete columns and the behavior of the mortar joints. The model can account for various aspects of the cyclic behavior, such as the cyclic normal unloading/reloading response, the frictional sliding and geometric dilatation and the irreversible crushing under large compressive stresses. A major challenge was the formulation of a robust stress update algorithm, which is also presented. The models are validated with experimental tests at the material and at the structural level. The analyses demonstrate the capability of the models to accurately capture the response of infilled frames. The results of large-scale, shake-table tests on a masonry-infilled RC frame are also presented. The purpose of the tests was to evaluate two retrofit methods for infills, using overlays of Engineered Cementitious Composite material (ECC) and of Glass Fiber Reinforced Polymer (GFRP). The material properties, specimen configuration, input ground motions and experimental observations are described in detail. Both retrofit techniques improved the performance of the specimen. A series of nonlinear analyses has been conducted to provide further insight on the behavior of the retrofitted specimen. Dynamic analyses have been conducted for infilled RC frames subjected to a collection of eight ground motions, scaled to match a target intensity level. The results of the analyses indicate that the addition of an ECC overlay can significantly improve the performance of an infilled frame
-------------------------------------

10136871_183 - 0.985098748718 - technology_and_computing
[hermitian, manifold, ricci, vector, second, geometry, algebraic]

Positivity and vanishing theorems in complex and algebraic geometry
In this thesis, we consider geometric properties of vector bundlesarising from algebraic and Hermitian geometry. On vector bundles in  algebraic geometry, such as ample, nef and globally generated vectorbundles,  we are able to construct positive Hermitian metrics indifferent senses(e.g. Griffiths-positive, Nakano-positive anddual-Nakano-positive) by $L^2$-method and deduce many new vanishingtheorems for them by analytic method instead of the Le Potier-Lerayspectral sequence method.On Hermitian manifolds, we find that the second Ricci curvaturetensors of various metric connections are closely related to thegeometry of Hermitian manifolds.  We can derive various vanishingtheorems for Hermitian manifolds and also for complex vector bundlesover Hermitian manifolds by their second Ricci curvature tensors. Wealso introduce a natural geometric flow on Hermitian manifolds byusing the second Ricci curvature tensor.
-------------------------------------

10133625_183 - 0.797712329435 - technology_and_computing
[processing, study, syntactic, erp, hemisphere, lh, chapter, agreement, rh]

Event-related brain potential investigations of left and right hemisphere contributions to syntactic processing
Syntactic processing is widely held to be a left hemisphere (LH) phenomenon, a view influenced by a large body of research showing lesions to certain LH areas are far more devastating than are lesions to corresponding right hemisphere (RH) areas. Although few studies have examined whether RH damage causes subtle syntactic processing deficits, there is evidence it does. This dissertation investigated the relative contribution of each hemisphere to syntactic processing in neurologically normal individuals using event-related potential (ERP) and behavioral studies in combination with the visual half- field paradigm. Central presentation ERP studies were conducted as a baseline against which to compare the results of the lateralized studies. The first experiment series (chapters two, three) examined processing of (in)correct grammatical number agreement marked either lexically or morphologically. Both behavioral and ERP results suggested the hemispheres are equally able to appreciate lexically marked agreement. In contrast, the RH appears to have greater difficulty than the left in processing morphologically marked agreement. The second experiment series (chapters four, five) investigated whether this LH advantage for morphologically-marked agreement errors reflects a language-specific difference in hemispheric processing or a low level, perceptually- based difference. Stimuli included both morphological and lexical conditions; salience of lexical markings was manipulated to adjudicate between these alternatives. Behavioral results suggested that the observed processing differences were based at the perceptual level. However, the ERP results obtained were not in accord with the predictions and did not lend themselves to any clear conclusions with respect to the hypothesis investigated. The central presentation studies in chapter two also investigate how aging affects syntactic processing. ERPs from elderly compared to young participants showed no evidence of an age-related delayed or diminished P600 effect, although there were changes in its scalp distribution, suggesting a qualitative, rather than any strictly quantitative, age-related change in speed of processing. Chapter four provides data relevant to the debate concerning the mental representation(s) of regular and irregular words, and the mental processes underlying the left anterior negativity component. Overall, we find that the RH is sensitive to certain grammatical manipulations, although not always in the same manner as the LH
-------------------------------------

10139767_183 - 0.999944259954 - technology_and_computing
[beam, ion, transport]

INITIAL COMMISSIONING OF NDCX-II
The Neutralized Drift Compression Experiment-II (NDCX-II) will generate ion beam pulses for studies of Warm Dense Matter and heavy-ion-driven Inertial Fusion Energy. The machine will accelerate 20-50 nC of Li+ to 1.2-3 MeV energy, starting from a 10.9-cm alumino-silicate ion source. At the end of the accelerator the ions are focused to a sub-mm spot size onto a thin foil (planar) target. The pulse duration is compressed from ~;;500 ns at the source to sub-ns at the target following beam transport in a neutralizing plasma. We first describe the injector, accelerator, transport, final focus and diagnostic facilities. We then report on the results of early commissioning studies that characterize beam quality and beam transport, acceleration waveform shaping and beam current evolution. We present simulation results to benchmark against the experimental measurements.
-------------------------------------

10137899_183 - 0.831947205958 - technology_and_computing
[complement, injury, system, activation]

Complement activation in the injured central nervous system: another dual-edged sword?
AbstractThe complement system, a major component of the innate immune system, is becoming increasingly recognised as a key participant in physiology and disease. The awareness that immunological mediators support various aspects of both normal central nervous system (CNS) function and pathology has led to a renaissance of complement research in neuroscience. Various studies have revealed particularly novel findings on the wide-ranging involvement of complement in neural development, synapse elimination and maturation of neural networks, as well as the progression of pathology in a range of chronic neurodegenerative disorders, and more recently, neurotraumatic events, where rapid disruption of neuronal homeostasis potently triggers complement activation. The purpose of this review is to summarise recent findings on complement activation and acquired brain or spinal cord injury, i.e. ischaemic-reperfusion injury or stroke, traumatic brain injury (TBI) and spinal cord injury (SCI), highlighting the potential for complement-targeted therapeutics to alleviate the devastating consequences of these neurological conditions.
-------------------------------------

10175460_189 - 0.818384627923 - technology_and_computing
[aerial, strategy]

Aerial Strategies and their Effect on Conflict Characteristics
This project asks the question of how different aerial strategies can affect the characteristics of aerial campaigns in conflict.  It begins by developing a new categorization of aerial strategies that distinguishes aerial strategies by how targeted thy are.  Data is collected on the type of strategies that were used in aerial campaigns from 1914 to 2003.  A preliminary analysis of aerial strategy choice is conducted, studying the effect of military doctrines on strategy choice.  The project also takes into consideration the role that ground forces, both those of the state carrying out the aerial attack and of its opponent, will play in determining the effect of aerial strategies on campaign duration and outcome.
-------------------------------------

10133551_183 - 0.999901108042 - technology_and_computing
[control, optimal, system, nonlinear, uncertain, controller]

Self-Organizing and Optimal Control for Nonlinear Systems
Vehicle formation control is one of important research topics in transportation. Control of uncertain nonlinear systems is one of fundamental problems in vehicle control. In this dissertation, we consider this fundamental control problem. Specially, we considered self-organizing based tracking control of uncertain nonaffine systems and optimal control of uncertain nonlinear systems. In tracking control of nonaffine systems, a self-organizing on-line approximation based controller is proposed to achieve aprespecified tracking accuracy, without using high-gain control nor large magnitude switching. For optimal control of uncertain nonlinear systems, we considered point-wise min-norm optimal control of uncertain nonlinear systems and approximately optimal control of uncertain nonlinear systems. In point-wise non-norm optimal control, optimal regulation and optimal tracking controllers were proposed with the aid of locally weighted learning observers. By introducing control Lyapunov functions and redefining the optimal criterions, analytic controllers were proposed and were optimal in the sense of min-norm. In approximately optimal control of uncertain nonlinear systems, adaptive optimal  controllers were proposed with the aid of iterative approximation techniques and adaptive control. By iteratively learning, the difficulty of solving Hamilton-Jacobian-Bellman (HJB) equation is overcome. The proposed adaptive optimal algorithms can be applied to solve optimal control problem of a large class of nonlinear systems. To show effectiveness of the proposed controllers for above problems, simulations were done  in computers.
-------------------------------------

10138250_183 - 0.830188067114 - technology_and_computing
[electricity, load, demand, building, method, response, facility]

Quantifying Changes in Building Electricity Use, with Application to Demand Response
We present methods for analyzing commercial and industrial facility 15-minute-interval electric load data. These methods allow building managers to better understand their facility's electricity consumption over time and to compare it to other buildings, helping them to ask the right questions to discover opportunities for demand response, energy efficiency, electricity waste elimination, and peak load management.
We primarily focus on demand response. Methods discussed include graphical representations of electric load data, a regression-based electricity load model that uses a time-of-week indicator variable and a piecewise linear and continuous outdoor air temperature dependence, and the definition of various parameters that characterize facility electricity loads and demand response behavior. In the future, these methods could be translated into easy-to-use tools for building managers.
-------------------------------------

10133438_183 - 0.988327264304 - technology_and_computing
[diagram, decision, rule, node, algorithm, state-space, model, generation, symbolic, variable]

Decision Diagram Algorithms for Logic and Timed Verification
Symbolic verification has received much attention from both academia and industry in the past two decades. In particular, techniques based on decision diagrams have been successfully applied to various asynchronous and synchronous models.Decision diagrams can compactly encode sets and relations, or vectors and matrices. For canonicity, variables associated to the nodes must be found in a predefined order on any path from the root, and duplicate nodes cannot be present. In addition, a <italic>reduction rule</italic> is enforced, the simplest being the <italic>quasi-reduced</italic> rule, where no variable is ever skipped. However, more efficient rules exist, where edges skip <italic>redundant</italic> nodes. With the <italic>fully-reduced</italic> rule, a node is redundant if all its outgoing edges point to the same node. With the <italic>zero-suppressed</italic> rule, a node is redundant if only its 0-edge is not pointing to a pre-defined default <italic>terminal</italic> node. None of these rules, however, is particularly effective when encoding transition relations of asynchronous systems or rate matrices of Markov models. We then introduce an <italic>identity-reduced</italic>rule, which generalizes <italic>Kronecker</italic> encodings to take advantage of state variables that remain unchanged after an event occurrence, and a <italic>c-reduced</italic>rule, which generalizes the zero-suppressed rule, and propose a new <italic>generally-reduced</italic> form of decision diagrams where each variable uses a specific reduction rule. We then illustrate the effectiveness of this new canonical form of decision diagrams with a wide set of applications.<italic>State-space generation</italic> is usually the first and fundamental step for symbolic verification. Generally-reduced decision diagrams allow a more efficient symbolic state-space generation for general asynchronous systems by allowing <italic>on-the-fly extension</italic> of the state variable domains.  After implementing both breadth-first and <italic>saturation-based</italic> state-space generation with this new data structure, we are able to exhibit substantial efficiency improvements with respect to traditional decision diagrams. Since previous works demonstrated that saturation outperforms breadth-first approaches, saturation with this new structure is now arguably thestate-of-the-art algorithm for symbolic state-space generation of asynchronous systems. When state-space generation completes, we also obtain the complete transition relation which can be used forfurther analysis.For synchronous systems, we study a type of  <italic>timed Petri nets</italic>, which extends the traditional Petri nets to explicitly include real time in the model. We consider two fundamental reachability problems for timed Petri nets with positive integer firing times: <italic>timed reachability</italic> (find all markings where the model can be at a given finite time) and <italic>earliest reachability</italic> (find the minimum time when each reachable marking is entered). For these two problems, we define efficient symbolic algorithms that make use of bothgenerally-reduced decision diagrams without edge value and <italic>edge-valued decision diagrams</italic>, which associate integer value to the edges of decision diagrams. Runtime results on an extensive suite of models are provided to show the effectiveness and capability of our algorithm to cope with large state spaces.Then, we study the use of decision diagrams in stochastic models. We present a new type of edge-valued decision diagrams which can be used to encode non-negative real-valued functions. We then utilize it in a new <italic>approximate numerical solution algorithm for general structured ergodic models</italic>. The approximation uses a state-space encoding provided by decision diagrams and a transition rate matrix encoding provided by these new edge-valued decision diagrams. The new method retains the favorable properties of a previously proposed Kronecker-based approximation, while eliminating the need for a Kronecker-consistent model decomposition. Removing this restriction allows for a greater utilization of event locality, which facilitates both state-space generation and transition rate matrix generation, thus extends the applicability of this algorithm to larger and more complex models.All these algorithms are implemented based on our newly-designed <italic>Decision Diagram Library (DDL)</italic>, which provides a user-friendly interface to create and manipulate generally-reduced decision diagrams with or without edge value It is the first library written for this purpose. This library is written in C++ and we adopt <italic>smart pointers</italic> for decision diagram node interface, similar to those in the Boost libraries, which automatically handle reference counts and garbage collection; this technique prevents memory leak and simplifies the interface, which greatly facilitate library users.All above algorithms are integrated into our verification tool Smart version 2.
-------------------------------------

10137082_183 - 0.990668422835 - technology_and_computing
[chapter, datum, method, model, variable, mixed, continuous, incomplete, approach, imputation]

Multiple Imputation of High-dimensional Mixed Incomplete Data
It is common in applied research to have large numbers of variables with mixed data types (continuous, binary, ordinal or nomial) measures on a modest number of cases. Also, even a simple imputation model can be overparameterized when the number of variables is moderatelylarge. Finding a joint model to accommodate multivariate data with mixed datatypes is challenging. Here we develop two joint multiple imputation models. One is using multivariate normal components for continuous variables and latent-normal components for categorical variables. Following the strategy of Boscardin and Weiss (2003) and using Parameter-expanded Metropolis-Hastings estimation (Boscardin,Zhang and Belin 2008), weuse a hierarchical prior for the covariance matrix centered around a parametric family. The second one is using a factor analysis model to impute missing items. It is an extension of Song and Belin (2004). The report is organized as follows: Chapter 1 gives a brief introduction of the research problem. Chapter 2 lists the review of the background knowledge related to our two new approaches. We introduce two existing methods of handling high-dimensional continuous incomplete data in Chapter 3 and another two methods of handling mixed incomplete data in Chapter 4. Our newly developed methods are outlined in Chapter 5. In Chapter 6, simulations under various conditions are carried out to compare the results based on our approaches with the results from the rounding method (Bernaards et al. 2007) as well as available-case analysis. In Chapter 7, our two approaches are applied to the California HealthInterview Survey (CHIS) 2009 data set. Several possible extensions and further directions of our methods are discussed in Chapter 8.
-------------------------------------

10133643_183 - 0.992084011269 - technology_and_computing
[chip, ghz, db, imbalance, sige, power, noise, imaging, lna, radiometer]

SiGe integrated circuits for millimeter-wave imaging and phased arrays
This dissertation presents work in two areas, the first of which is 35-44 GHz power dividers for phased-array transmit systems. A compact active 1:16 single-ended power divider in a 0.18 mum SiGe technology is presented, which achieves 0.8 dB rms gain imbalance and 6⁰ rms phase imbalance at 35 GHz. A second-generation divider is also presented, with 0.3 dB rms gain imbalance and 4⁰ rms phase imbalance at 40.5 GHz. The cascode-node power division approach is shown to be a useful and compact power division topology. A differential broadside-coupled stripline (BCS) structure integrated vertically in the 0.18 mum SiGe interconnect stackup is also developed, which can produce highly symmetric corporate-feed networks (tree structure). A 1:8 power divider is presented which incorporates the BCS structure and attains 0.4 dB rms gain imbalance and 3⁰ rms phase imbalance at 44 GHz. Finally, a sixteen-element 44 GHz beamforming chip with integrated phase shifters utilizing the BCS structure is also presented, and is the first example of a sixteen-element phased-array beamformer at any frequency. The second area of work is in W-Band (70-110 GHz) imaging systems, and several W-band RFICs are developed in the IBM8HP SiGe process (0.12 mum BiCMOS). A wideband 84-100 GHz LNA with 19 dB gain and 8 dB NF is first presented, along with a second-generation LNA achieving 8 dB more gain. An 80- 110 GHz SPDT switch with 2.3 dB insertion loss and 21 dB isolation is developed, and a biased power detector circuit with 14 kV/W responsivity, 40 nV/pHz output noise, and 2.5-3 pW/pHz NEP is presented along with noise and responsivity analysis. These circuits can replace current (and more expensive) III-V chips in many applications. Two passive imaging chips are developed using these RFICs. First, a total-power radiometer is presented (LNA + Detector) which can achieve 0.69 K temperature resolution when 1/f noise contributions are removed by electronic or mechanical chopping. Following this is a Dicke radiometer chip integrating a SPDT, LNA, and W-band detector. This chip can achieve 0.84 K temperature resolution by using electronic chopping, which is comparable to current III-V implementations and is the first SiGe or CMOS W-Band imaging chip. The thesis also contains a summary of the equations required for imaging systems, and an investigation of the additional 1/f noise found in the radiometer chips. The 1/f noise problem is solved using large-area resistors in the bias network layout. The thesis concludes with a presentation of differential SiGe LNA designs and radiometer chips which are designed to be compatible with planar differential antennas
-------------------------------------

10130145_178 - 0.981198622212 - technology_and_computing
[mobile, reference, evaluation, bibliographical, application, infrastructure]

Evaluation infrastructure for mobile distributed applications
Includes bibliographical references (p. 50-54).
-------------------------------------

10130219_178 - 0.998696143158 - technology_and_computing
[ship, in-water, navigation, grant, united, states, autonomous, inspection, office, advanced]

Advanced perception, navigation and planning for autonomous in-water ship hull inspection
United States. Office of Naval Research (Grant N00014-07-1-0791)
-------------------------------------

10139488_183 - 0.999894019657 - technology_and_computing
[mapreduce, environment, hpc, performance, implementation, hadoop, file, system, model, mariane]

MARIANE: MApReduce Implementation Adapted for HPC Environments
MapReduce is increasingly becoming a popular framework, and a potent programming model. The most popular open source implementation of MapReduce, Hadoop, is based
on the Hadoop Distributed File System (HDFS). However, as HDFS is not POSIX compliant, it cannot be fully leveraged by applications running on a majority of existing HPC environments such as Teragrid and NERSC. These HPC environments typically
support globally shared file systems such as NFS and GPFS. On such resourceful HPC infrastructures, the use of Hadoop not only creates compatibility issues, but also affects overall performance due to the added overhead of the HDFS. This paper not only presents a MapReduce implementation directly suitable for HPC environments, but also exposes the design choices for better performance gains in those settings. By leveraging
inherent distributed file systems? functions, and abstracting them away from its MapReduce framework, MARIANE (MApReduce Implementation Adapted for HPC Environments) not only allows for the use of the model in an expanding number of HPC
environments, but also allows for better performance in such settings. This paper shows the applicability and high performance of the MapReduce paradigm through MARIANE, an implementation designed for clustered and shared-disk file systems and as such not dedicated to a specific MapReduce solution. The paper identifies the components and trade-offs necessary for this model, and quantifies the performance gains exhibited by our approach in distributed environments over Apache Hadoop in a data intensive setting, on the Magellan testbed at the National Energy Research Scientific Computing Center (NERSC).
-------------------------------------

10133465_183 - 0.950054770745 - technology_and_computing
[access, open, circulation, anthropological]

ANTHROPOLOGY OF/IN CIRCULATION: The Future of Open Access and Scholarly Societies
In a conversation format, seven anthropologists with extensive expertise in new digital technologies, intellectual property, and journal publishing discuss issues related to open access, the anthropology of information circulation, and the future of scholarly societies. Among the topics discussed are current anthropological research on open source and open access; the effects of open access on traditional anthropological topics; the creation of community archives and new networking tools; potentially transformative uses of field notes and materials in new digital ecologies; the American Anthropological Association's recent history with these issues, from the development of AnthroSource to its new publishing arrangement with Wiley-Blackwell; and the political economies of knowledge circulation more generally.
-------------------------------------

10135626_183 - 0.999021937088 - technology_and_computing
[visualization, environment, visual, datum, interactive, scalable, analysis, analytic, interaction, technique]

Visual analytics in scalable visualization environments
Visual analytics is an interdisciplinary field that facilitates the analysis of the large volume of data through interactive visual interface. This dissertation focuses on the development of visual analytics techniques in scalable visualization environments. These scalable visualization environments offer a high-resolution, integrated virtual space, as well as a wide-open physical space that affords collaborative user interaction. At the same time, the sheer scale of these environments poses a number of challenges, including data management, visualization techniques, and interaction paradigms that support large-scale, interactive visual exploratory analysis. This dissertation addresses these challenges with the special attention on the large volume of very high-resolution image data sets. The presented core visualization approach can immediately address tens of terapixel worth of information by employing view-dependent, adaptive, out-of-core visualization techniques. Building on this approach, two domain-specific challenges are addressed. One is interactive image fusion, facilitating the visualization and analysis of high-resolution satellite imagery. The other is interactive visual exploratory analysis of the large volume of cultural data sets, in order to support the development and refinement of new insights and hypotheses into the data sets. Finally, a method towards creating a co-located, collaborative user interaction paradigm in scalable visualization environments is presented. This method provides a multiuser, user-centric graphical user interface (GUI) for these environments, controlled by multitouch mobile devices
-------------------------------------

10134953_183 - 0.997815809126 - technology_and_computing
[performance, application, hpc]

PMap : unlocking the performance genes of HPC applications
Performance modeling, the science of understanding and predicting application performance, is important but challenging. High Performance Computing (HPC) with large- scale applications and aggressive technologies, such as dynamic computational grids, hybrid computing platforms, and innovative storage systems, further complicates the task. This dissertation proposed and proved the hypothesis that a small number of performance primitives can be extracted from HPC applications and leveraged for fast application performance modeling and prediction even on large-scale dynamic systems. PMap : a set of methods and tools to extract, measure, and analyze performance primitives in HPC applications are proposed, implemented, and verified under these challenging environments. Two production computational grids, Teragrid and Geon, were monitored with periodically running benchmarks for about half a year. Their performance fluctuated in the 50% range. However, simple benchmarks that serve as performance primitives can be used to predict application performance with a relative error as low as 9%. To map program constructs to the best matched hardware components in hybrid computing platforms, an automatic idioms (performance primitives) recognition method was proposed and implemented based on the open source compiler Open64. With the NAS Parallel Benchmark (NPB) as a case study, the prototype system is about 90% accurate compared with idiom classfication by a human expert. The performance of the idiom benchmarks with their corresponding in- stances in the NPB codes on two different platforms were compared with different methods. The approximation accuracy is up to 97%. With the HPC data challenge and emerging storage technologies, a flash-based supercomputer DASH was designed, built, and tuned. A large parameter space was swept by fast and reliable measurements developed to investigate varying design options, and the results showed that performance can be improved by as much as 9x with appropriate existing technologies developed here. Finally, the PMaC framework was extended to model and predict application performance on flash storage systems. Results showed that the total I/O time can be predicted with reasonable error of 15%. The end result of this body of work is that the performance of applications on supercomputers can be understood by mapping their performance genetics
-------------------------------------

10135146_183 - 0.999916287975 - technology_and_computing
[swcnt, structure, network, nanotube]

Self-assembling functionalized single-walled carbon nanotubes
Single-walled carbon nanotubes (SWCNTs) are promising bottom-up building materials due to their superior properties. However, the lack of an effective method to arrange large quantities of SWCNTs poses an obstacle toward their applications. Existing studies to functionalize, disperse, position, and assemble SWCNTs provide a broad understandings regarding SWCNTs behavior, especially in aqueous electrolyte solution. Inspired by ionic polymer metal composite (IPMC) materials, this dissertation envisions fabrication of orderly SWCNTs network structure via their ionic clustering-mediated self -assembly. SWCNTs tend to bundle together due to inter- nanotube VDW attractions, which increase with nanotube length. The author seeks short SWCNTs with long chain molecules bearing ionic termini to facilitate debundling and self-assembly in aqueous electrolyte solution through end-clustering. First, a simple model was applied based on essential physical factors. The results indicated that SWCNTs must be shorter than 1̃00 nm to achieve stable network structures. Experiments were then carried out based upon the results. Short SWCNTs (50-100 nm) were end- functionalized with hexaethylene glycol (HEG) linkers bearing terminal carboxylate anions. Both 2D and 3D network structures were observed after placing the functionalized SWCNTs in aqueous electrolyte (sodium ion). The network structures were characterized by microscopic and spectroscopic methods. A novel approach was applied via electron tomography to study the 3D structures of SWCNTs structure in aqueous electrolyte. Free energy analysis of the SWCNTs network structure was implemented with the assistance of both analytical tools and molecular simulations. The results indicate that, when a cluster is formed by three functionalized SWCNTs ends, the resulting network structure is most stable. Indeed, 72% of the clusters/joints were formed by three nanotubes, as observed in experiments. Finally, Monte Carlo simulations of coarse-grained SWCNTs were implemented to understand the behavior of SWCNTs. SWCNTs clusters were observed as a result of this simulation
-------------------------------------

10133501_183 - 0.999991610539 - technology_and_computing
[interference, network, throughput, beamforming, scheme, wireless, pboa-orb, optimum, algorithm, fca]

Interference management in multiple-antenna wireless networks
This dissertation focuses on the topic of interference management in wireless networks with multiple-antenna nodes. Two network paradigms are considered, namely, time- division duplexing (TDD)/code-division multiple-access (CDMA) cellular and ad hoc. In TDD/CDMA cellular networks with asymmetric data traffic, dynamic channel allocation (DCA) enhances resource utilization compared to fixed channel allocation (FCA); however, it induces base-to-base and mobile-to-mobile crossed-slot intercell interference that can severely degrade network performance. To deal with this problem, a decentralized scheme is proposed, which combines an interference-aware DCA algorithm with space-time linear minimum-mean-square-error (LMMSE) joint detection at the base and mobile stations. The former assigns active links to timeslots in a way that crossed- slot interference is mitigated, while the latter suppresses the remaining intercell interference (along with intersymbol and intracell interference) by exploiting its spatio-temporal autocorrelation statistics. The performance of this scheme is evaluated in terms of SINR outage and average throughput via analytical approximations and Monte Carlo simulations, and it is compared with that of benchmark random DCA (RDCA) and FCA schemes. The cases of single- and dual-antenna reception with perfect and imperfect channel state information are examined. It is shown that the proposed scheme achieves higher average throughput than FCA (particularly for dual- antenna reception) as well as RDCA (for heavy traffic loads). These throughput gains are more significant in uplink than in downlink. In ad hoc networks, interference management via collision-avoidance medium access schemes results in poor spatial reuse and, thus, restricts network throughput. To address this shortcoming, two physical- medium-access-control cross-layer protocols are proposed. The first increases spatial reuse by integrating medium access, power control, and optimum receive beamforming in a distributed manner, and it is named progressive back-off algorithm with optimum receive beamforming (PBOA-ORB). The second additionally incorporates transmit beamforming, on the premise of centralized control, and it is named progressive back-off algorithm with transmit and optimum receive beamforming (PBOA-TORB). The performance of both protocols is evaluated in terms of aggregate throughput and energy efficiency via simulations over a single-hop network. It is shown that the throughput of PBOA-ORB increases linearly with the number of antennas per node thanks to interference suppression provided by optimum receive beamforming. PBOA-TORB achieves only an incremental throughput gain over PBOA-ORB despite its centralized nature. However, it is significantly more energy efficient than PBOA-ORB thanks to extra array gain provided by transmit beamforming. The research for this dissertation was conducted at the UCSD Center for Wireless Communication, under the "MIMO Wireless Communication Systems" project (CoRe research grant com04-10176) and the ̀̀Multiuser MIMO Systems'' project (CoRe research grant com07-10241)
-------------------------------------

10136634_183 - 0.999864479735 - technology_and_computing
[ftm, memory, flexible, transparent, substrate]

Flexible and Transparent Memory
In this thesis, we present a graphene channel transistor based flexible and transparent memory (FTM) fabricated on Poly-ethylene-naphtalate (PEN) substrate. FTM samples were successfully fabricated through low temperature processes preventing substrate deformation. The injection of electrons into the trap sites of a triple high-k dielectric stack resulted in a memory window of more than 9.0V.  The experimental results show great potential for FTM to be used as a memory cell for fully flexible and transparent electronics. Furthermore, FTM might enable making a breakthrough in innovative design for electronics that has been impossible when using stiff and opaque substrates.
-------------------------------------

10135955_183 - 0.978190865014 - technology_and_computing
[power, efficiency, implant, size, device]

Wireless power transfer for scaled electronic biomedical implants
Decreasing the physical size of electronic biomedical implants is an important objective in that it increases the number of realizable applications. For example, wireless pacemakers can be on the order of an inch in diameter while retinal and cochlear implants must be scaled to a centimeter to fit in their respective locations within the body. Perhaps the ultimate goal is devices that are no larger than a red blood cell, approximately five microns in diameter. At those dimensions devices reach compatibility with the circulatory system and can be injected into the blood stream. The chief impediment to such aggressive size reduction is the power supply. In this dissertation the use of electromagnetic (EM) radiation to wirelessly power miniature biomedical implants is investigated. Both inductive coupling using time varying magnetic fields and near-infrared (NIR) light collected by photodiodes are identified as viable solutions for transcutaneous power transmission. Minimum device size is limited by power radiation density at the implant and the efficiency with which the device can collect, convert and use this energy. Power density at the implant is restricted by safety regulations and tissue attenuation. Both are studied in this work, the results of which are used to build analytical models for power calculations. The use of ferrite rods in inductive implants can improve power transfer efficiency. In this work an analytical model is derived and supported experimentally which shows that mutual inductance can be improved by as much as a factor of 10 when NiZn ferrite rods are used within the implanted device. This improves efficiency and permits decreased implant size. Efficiently converting time-varying energy to usable DC power is essential for obtaining high implant efficiency. In this work a silicon-on-sapphire (SOS) CMOS rectifier that overcomes the dead-zone at low power characteristics of conventional rectifiers is developed. The design produces the targeted output power of 1&mu;W with a peak power conversion efficiency of 67% at 100MHz. Furthermore the rectifier achieves greater than 30% efficiency for input power levels as low as -40dBm. A larger rectifier designed with the same topology is used as part of an inductive power transfer system, which is able to illuminate an LED at distances greater than 5cm with input power levels to the primary coil of only 1W. For NIR light power transfer, a specialized photodiode designed using a fully depleted silicon on insulator CMOS process was demonstrated to collect ̃35muW of power from a 90mW laser. The insights gained from this work are used to predict the size scaling limits of implants powered using wireless techniques. It is estimated that to obtain 1&mu;W of power within the implant, inductive supplies will require a secondary coil radius of ̃15mum while photodiodes for NIR light power collection will require radii of at least ̃40mum
-------------------------------------

10130341_178 - 0.95321967331 - technology_and_computing
[house, system, impact, type, reference, bibliographical, leaf, generation, kinship]

The impact of kinship systems in the generation of house types
Includes bibliographical references (leaves 112-117).
-------------------------------------

10134446_183 - 0.999982650985 - technology_and_computing
[attack, system, network, performance]

Security and Performance Considerations in Wireless Networks
The open and shared nature of the wireless medium makes it easy for adversaries to launch simple, yet effective, denial of service attacks (DoS attacks).  As an example, jamming attacks, involve the uncoordinated transmission of electromagnetic energy on the medium.  In a carrier sensing network (e.g., 802.11), this attack strategy increases the number of collisions at the receiver side and/or blocks the medium access to legitimate nodes at the transmitting side.  Both of the above effects degrade the wireless network performance significantly.    Frequency hopping (FH) has been traditionally used to overcome jamming attacks.  However, we analytically and experimentally show that FH is inadequate to efficiently cope with jamming in today's networks.  Later we propose a suite of systems that aim at coping with jamming attacks at various levels (i.e., detection, localization and prevention).  We first identify two intelligent and effective jamming attacks that can be launched in 802.11 WLANs and we provide robust detection systems.  In particular, we design and implement (i) CMD, a system to detect active jamming attacks that exploit the carrier sensing functionality of 802.11 networks and (ii) FIJI, a cross-layer system for detecting (and mitigating) jamming attacks that exploit the performance anomaly of 802.11 WLANs.  Furthermore, given the importance of locating the jamming device in many deployment scenarios (e.g., battlefield), we propose a lightweight jamming localization scheme.  Our system utilizies ideas borrowed from the gradient descent optimization method.  The system's evaluations, show the potentials and applicability of our localization strategy.  The final step for coping with jamming attacks is jamming prevention.  Based on our initial measurement driven analysis, we do not rely on a FH scheme, that tries to simply avoid the jammer.  On the contrary, we design, implement and evaluate a prevention system, called ARES (Anti-jamming REinforcement System), to fight against the saboteur.   ARES is applicable to carrier sensing networks and tunes the parameters of rate adaptation and power control to improve the performance under the presence of an attack while ensuring that operations under benign conditions are unaffected. Our extensive evaluations, show that ARES improves the network throughput across all scenarios by up to 150%.
-------------------------------------

10137242_183 - 0.999998047306 - technology_and_computing
[communication, link, system, performance, uv, fso, author, path, model, woc]

Performance Limits of Outdoor Wireless Optical Communication Links through Scattering and Turbulent Channels
Scattering and turbulence are two major obstacles posed by the optical channel such that almost every outdoor wireless optical communication (WoC) system has to overcome. This thesis is devoted to studying their effects on the performance limits of communications in terms of the error probability and outage probability. Two typical WoC systems are considered in our scope of discussion. One is so-called non-line-of-sight (NLOS) ultraviolet (UV) communication system while the other has a line-of-sight (LOS) transmission path, working in the range of infrared (IR) and falls into the category of free-space-optical (FSO) links.With regard to an NLOS UV communication link, the author characterizes the scattering phenomena in terms of the channel path loss model and delay spread in the impulse response. By integrating an empirical model of path loss, NLOS UV communication link is comprehensively investigated for the first time to reveal its potential and fundamental tradeoffs of range versus rate are reported. Furthermore, in view of realistic implementation options of UV transceivers, the author effectively extends above results by coining typical device probabilistic models. This work presents more realistic insights on designing an NLOS UV system, weighing over device choices and tweaking the key parameters.For a FSO IR link over the range above 1km, the turbulence effect comes into play as a fading effect to overshadow the system performance. Traditionally, techniques such as error-control coding (ECC) and spatial diversity have been introduced to enhance the FSO link. By contrast, the author has proposed a novel method by employing an opportunistic cooperative relay to improve the performance of LOS FSO communications. It is shown the cooperative scheme indeed provides a more robust way of transmission through an information theoretic analysis.Along with the analytical study, the author adopt both numerical methods and Monte-Carlo simulation to illustrate the performance limits and advantages of proposed scheme. The results are expected to provide valuable reference to the link budget and system design of outdoor WoC links.
-------------------------------------

10137588_183 - 0.999821592703 - technology_and_computing
[performance, thread, system, high, application, technique, multithreaded, multicore, runtime, lock]

Runtime Support For Maximizing Performance on Multicore Systems
Since multicore systems offer greater performance via parallelism, future computing is progressing towards use of machines with large number of cores. However, due to the complex interaction among characteristics of multithreaded applications, operating system policies, and architectural characteristics of multicore systems, delivering high performance on multicore systems is a challenging task. This dissertation addresses the above challenge by developing runtime techniques to achieve high performance when running a single multithreaded application as well as high system utilization and fairness when running multiple multithreaded applications. The runtime techniques are based on a simple monitoring system that captures important application characteristics and relevant architectural factors with negligible overhead. To develop runtime techniques for achieving high performance when running a single multithreaded program on a multicore system, important performance limiting factors that impact the scalability of performance are identified. These factors include the threads configuration (i.e., the number of threads for a multithreaded program that provide the best speedup) and the thread scheduling and memory allocation polices employed. This dissertation presents two runtime techniques Thread Reinforcer, for dynamically determining appropriate threads configuration and Thread Tranquilizer, for dynamically selecting appropriate scheduling and memory allocation policies. By dynamically determining the appropriate threads configuration, scheduling policy, and memory allocation policy the performance of applications is maximized.Lock contention is an important performance limiting factor for multithreaded programs on a multicore system. The dissertation presents two techniques Thread Shuffling and FaithFul Scheduling to limit the performance impact due to locks. Thread Shuffling reduces high lock acquisition latencies, resulting from the NUMA nature of a multicore system, via inter-CPU thread migrations. FaithFul Scheduling reduces the durations for which threads hold locks by minimizing lock holder thread preemptions through adaptive time-quanta allocations. These techniques significantly enhance the performance of applications in the presence of high lock contention.Finally, this dissertation presents a coscheduling technique called ADAPT for achieving high system utilization and fairness when running multiple multithreaded applications on multicore systems. ADAPT uses supervised learning techniques for predicting the effects of interference between programs on their performance and adaptively schedules together programs that interfere with each other's performance minimally. It achieves high throughput, high system utilization, and fairness when running multiple multithreaded applications.
-------------------------------------

10137129_183 - 0.950521515759 - technology_and_computing
[fault, slip, model, northridge, event, earthquake]

A Renewed Look at the Coseismic Surface Deformation and Fault Slip of the 1994 Northridge Earthquake Using Space Geodesy
The January 17, 1994 M6.7 Northridge earthquake occurred in the densely populated suburbs northwest of Los Angeles, California, causing 33 deaths and ~$20 billion in damage. To quantify the influence, in terms of stress changes, of the Northridge event on surrounding faults, detailed knowledge of the location, orientation and amount of fault slip is important. Existing InSAR models of this earthquake typically were developed by fitting the pattern of displacements by trial and error, and were therefore somewhat subjective. In the 15 years since the original studies were published a number of new modeling tools and community data products have been developed that should enable us to produce more detailed, objective and robust results. We measure the coseismic deformation of this earthquake using InSAR data from the ERS-1 and JERS-1 satellites, combined with GPS measurements (Hudnut et al., 1996) that together show uplift of ~42 cm. Using these data, we first employ a nonlinear inversion to determine the parameters of a best-fitting model using rectangular, uniform slip dislocations. Our best-fitting fault solution contains two faults, a main fault with 2.3 m of slip and a secondary fault to the northwest with 0.8 m.In detail, however, the deformation pattern of the Northridge event is more complex than can be described by rectangular dislocations. To investigate this, we solve for a detailed slip distribution for the event using a non-planar triangular element fault mesh modified from the SCEC Community Fault Model (Plesch et al.,2007). This model shows a main asperity on a protrusion on the fault, with peak slip of ~2.7m, bounded at its western edge by a geometrical barrier, a steep down-dip parallel lateral ramp in the fault. Secondary slip of about 0.6m to the northwest of this feature is also present. These two slip patches together shows that the geometry of the fault strongly influences the slip pattern of the event.
-------------------------------------

10131390_183 - 0.99939456211 - technology_and_computing
[network, chaotic, recognition, neural, dynamics]

Pattern Recognition by a Distributed Neural Network: An Industrial Application
In this report, a distributed neural network of coupled oscillators is applied to an industrial pattern recognition problem. 1"he network s'tems j?om the study of the neurophysiology of the olfactory system. It is" shown that the network serves as an as'sociative memory, which possesses chaotic dynamics. The problem addres'sed is machine recognition of industrial screws, bolts, etc. in simulated real time in accordance with tolerated deviations/rom manufacturing specifications. AJ?er preprocessing, inputs are represented as 1 × 64 binary vectors'. We show that our chaotic neural network can accomplish this pattern recognition task better than a standard Bayesian statis'tical method, a neural network bins O' autoassociator, a three-layerJOedforward network under back propagation learning, attd our earlier o!factory bulb model that relies on a Hopf bifurcation from equilibrium to limit cycle. The existence of the chaotic dynamics provides the network with its"capability to suppress noise and irrelevant information with respect to the recognition task. The collective effectiveness of the "'cell-assemblies'" and the threshold function of each individual channel enhance the quali(v of the network as an associative memoo,. The network classifies"an uninterrupted sequence of objects"at 200 ms of simulated real time Jor each object. It reliably distinguishes the unacceptable objects (i.e., 100% correct classification), which is a crucial requirement ,for this speci.fic application. The effectiveness of the chaotic dynamics may depend on the broad spectrum of the oscillations, which may jorce classification by spatial rather than temporal characteristics of the operation. Further s'tudy of this biologically derived model is"needed to determine whether its"chaotic dynamics rather than other as yet unidentified attributes is responsible" for the superior performance, and, if so, how it contributes to that end. 
-------------------------------------

10133871_183 - 0.948323889574 - technology_and_computing
[packet, model, loss, visibility, frame, video, prioritization]

Video packet loss visibility models and their application to packet prioritization
In video transmission, packets can be lost for many reasons. Traditionally the impact of packet losses is measured by mean squared error induced by the loss in the pixel domain. However, mean squared error does not correlate with human perception well. In this dissertation, we aim to provide predictions of how human observers respond to different video packet losses. Based on their estimated visual importance, we can insert a prioritization bit for each video packet before sending it over a lossy network, or perform unequal channel protection on packets before transmission over a wireless channel. The models are developed from data collected from subjective tests. The models predict the packet loss visibility, that is, the probability of a given packet producing a glitch that will be observed by the end user if it is lost. We discuss the development and the application of encoder-based packet loss visibility models and network-based packet loss visibility models. We discuss an encoder-based packet loss visibility model using three subjective experiment data sets that span various encoding standards (H.264 and MPEG-2), group-of- picture structures, and decoder error concealment choices. The factors of scene cuts, camera motion, and reference distance are highly significant to the packet loss visibility. The encoder-based packet loss visibility model exploits factors in the pixel domain as well as reference frame information. The first application of the encoder- based packet loss visibility model is packet prioritization for a video stream. When a network gets congested at an intermediate router, the router is able to decide which packets to drop such that visual quality of the video is minimally impacted. Experiments are done to compare our perceptual-quality-based packet prioritization approach with existing Drop-Tail and cumulative-MSE-based prioritization methods. The result shows that our prioritization method produces videos of higher perceptual quality for different network conditions and group-of- picture structures. The second application of the encoder- based packet loss visibility model is unequal error protection. For an AWGN channel, we aim to minimize the end-to-end video quality degradation using rate-compatible punctured convolutional codes for a given channel rate budget. We solve the integer programming problem by the Branch and Bound method, K-means clustering, and the subgradient method. We also exploit the advantage of not sending or not coding packets of lower importance. The algorithm is compared to an existing method. In order to reduce the computational complexity of the encoder-based model so that a model can be implemented at the router, we aim to develop a network-based model that uses only information within one packet to predict the importance of that packet, requiring no frame-level reconstruction nor any information on the reference frame. We conduct subjective experiments for SDTV and HDTV resolutions on visual quality following packet loss. We design the model for SDTV and HDTV resolutions, and discuss the differences in the important factors between SDTV and HDTV models. We then use the model to measure the visual importance of incoming packets to the router. During network congestion, we drop the least visible frames and/or the least visible packets until the required bit reduction rate is achieved. Our algorithm performs better than dropping B packets/ frames. The way we estimated the frame importance is based on the summation of the visibility of all slices in a frame, which is an indirect approach. Therefore, we conduct subjective experiments and collect responses from human observers directly on whole frame losses. We develop a model which can predict the visibility of whole frame losses for B frames. This model could be useful for designing an intelligent frame dropping approach for use at a router during congestion
-------------------------------------

10134437_183 - 0.980406678383 - technology_and_computing
[cell, system, mufac, module, detection]

High-throughput piezoelectric-actuated micro-fluorescence- activated cell sorter (MuFACS)
In this thesis, I have developed a piezoelectric-actuated micro-fluorescence-activated cell sorter (MuFACS) and demonstrate its performances using various biological samples including mammalian cell and bacteria. Three major developments in this work included a high-sensitivity detection system, fast-response on-chip piezoelectric cell sorting module, and system integration. In my early work, optical arrayed waveguides combined with cross-correlation signal processing algorithm are implemented to achieve high-sensitivity scattering detection. The insight gained from the algorithm further allowed me to design and implement a spatial-filter based (space-time coding) fluorescent detection system. The system enables not only signal amplification (228}018 dB SNR enhancement) but also sorting even verification, allowing real-time optimization of sorting parameters. The first generation on-chip cell- sorting module involves flow-redirection using the principle of nozzle-diffuser, but due to the periodic flow and high fluid disturbance resulted from high-voltage piezoelectric actuation, the sorting module was redesigned, resulting in piezoelectric-actuated cell sorting module. The inexpensive module was able to manipulate single cells at high rate (> 1000 cells/s) under low powered actuation (< 10 mW and < 10 Vp-p). Integration of detection and sorting systems is achieved through the implementation of the preprogrammed FPGA-embedded external driver enables closed-loop control for triggering fluorescence-activated cell sorting. With the sorting event verification capability, sorting efficiency was found to be > 80%. Sample enriching experiments were done using beads and human mammalian cells, showing an enrichment factor > 200 fold (comparable to commercial FACS), which is the highest among MuFACS systems. The developed integrated MuFACS was also applied to address the challenges (detection sensitivity and cell-free DNA contamination) commonly encountered in single-cell genome sequencing. Flow cytometry-modified Tyramide Signal Amplification Fluorescence in situ Hybridization (TSA-FISH) and two-step optofluidic light confinement were implemented to enhance sensitivity. Also, dual-round cell-free DNA purification was performed and compared to commercial FACS, showing comparable results. Sorting of rare bacteria was achieved, showing 223-fold enrichment. I hope the work sets the benchmark for the future development of MuFACS systems. I believe the realization of a truly hand-held MuFACS that can be afforded by every research labs and clinics is not far off
-------------------------------------

10134633_183 - 0.999999390637 - technology_and_computing
[radio, system, frequency, software, front-end, hardware, high]

A wideband high dynamic range frequency hopping hardware front-end for the joint tactical radio system
The Joint Tactical Radio System project is the Department of Defense's effort to create and organize a communication network that links a variety of radio platforms (e.g. handheld, naval, and aircraft). This goal can be achieved by developing a family of interoperable software defined radios with its members optimized for specific platforms. Realistic software defined radios utilize two systems to enable flexible and interoperable communications: software and hardware. The software system is responsible for converting transduced real information (e.g. text, voice, and video) into digitally modulated information and performing data impairment correction and synchronization. The hardware system, also known as a radio frequency front end, performs the physical conversion between relatively low-frequency digital data and high frequency carriers for practical and realizable communication system implementations. This thesis describes the design and implementation of a wideband high dynamic range frequency hopping hardware front-end for the Joint Tactical Radio System software defined radio. The front-end realizes a transceiver that utilizes radio frequencies from 200 MHz to 3.2 GHz and provides a 78 dB maximum dynamic range. The frequency hopping local oscillator is achieved by direct digital synthesis and successive frequency multiplications. Additionally, a low-noise amplifier and band-pass filter bank in support of the radio frequency front-end implementation have been developed and tested. The system features a high level of discrete integration and has been implemented entirely through the use of commercially available integrated circuits and surface- mount devices. The complete system was constructed using manufacturer evaluation kits and custom designed boards
-------------------------------------

10138025_183 - 0.999774066125 - technology_and_computing
[feedback, loop, building, time, scale, performance, information]

Broken Information Feedback Loops Prevent Good Building Energy Performance—Integrated Technological and Sociological Fixes Are Needed
Information feedback loops for building performance range from the long-term— including university education of building designers and their experiential learning from past work on a time scale of years or decades; to the short term—including building occupants seeking to manage their environment with operable windows and thermostats, to building controls themselves on a time scale of seconds or minutes. In between are owners seeking to make informed renovation and retrofit decisions on a time scale of years, and operators looking for ongoing commissioning opportunities on a time scale of hours to months.
      Unfortunately all of these feedback loops are often broken, with meaningful convenient performance information typically unavailable for decision-making. Even automatic building controls often fail to perform as expected because of erroneous or missing data from sensors. We examine the current typical disconnects for each of the feedback loops, their interactions, and potential solutions. 
      Both improved technology and organizational change are needed to fully establish all the feedback loops for building performance, achieving the twin goals of building quality (e.g., comfort) and reduced resource use (e.g., energy). Currently research sometimes provides an intervention to temporarily close one or more of the feedback loops. However, closing of information feedback loops is often inhibited by perceptions of professional or business risk. Achieving the vision of ubiquitous deep efficiency for buildings will require research, development and demonstration integrating both technological and sociological issues to durably establish feedback at all time scales in building design and operation.
-------------------------------------

10132831_183 - 0.848700371678 - technology_and_computing
[transit, access, vision, travel, urban, blind, rias, audible, environment, people]

Towards an Accessible City: Empirical Measurement and Modeling of Access to Urban Opportunities for those with Vision Impairments, Using Remote Infrared Audible Signage
This paper examines the problems of defining and measuring access for blind travelers in an urban transit environment. Current accessibility measures do little to account for individual differences or the barriers faced by people with restricted mobility. Independent access to transit and activities in the urban environment are often denied or restricted for those with vision impairments. Their freedom of movement is not blocked by physical obstacles, but by information, signs, and spatial knowledge that are hard to access without vision. In this sense, services and facilities are considered inaccessible if people with limited or no vision lack the information necessary to adequately use them.
      Thirty legally blind people made five simulated transfers to different transit modes at a transportation terminal to identify specific barriers to successful travel. Regular blind mobility methods were tested against a Remote Infrared Audible Signage (RIAS) condition to determine if the devices offered a suitable replacement for typical visual cues and information needed for efficient travel and use of transit. The difficulties of accessing various transit locations and performing necessary tasks were measured, in both conditions, and the extra time penalties were compared and modeled showing the travel constraints of vision loss and the efficacy of using RIAS to increase access. RIAS provided superior travel times, increased independence, and decreased error production.
      A survey examined differences reported by blind travelers before and after exposure to RIAS. Many wayfinding tasks faced by transit users were shown to be quite difficult with normal navigation skills and aids, but presented little or no difficulty when using RIAS. High resistance to make mode transfers, especially in new environments, was reported. Inaccessible transit caused "non-trips" and also reduced travel and activity participation. RIAS revealed a hidden demand to travel more often, with greater safety, independence, and efficiency. These additional audible cues were perceived as enabling the users to access many more types of activities, such as education, employment, recreational, and entertainment. High monetary benefits were placed on the ability to travel independently and to gain access to urban opportunities, including increased income from employment.
-------------------------------------

10129785_178 - 0.999080483512 - technology_and_computing
[reference, transmission, bibliographical, impedance, directional]

Directional impedance of geared transmissions
Includes bibliographical references (p. 39).
-------------------------------------

10175424_189 - 0.999975588252 - technology_and_computing
[design, base, large-scale, beamforming, mubf, scalability, capacity, antenna, gain, station]

Argos: Practical Base Stations for Large-scale Beamforming
MU-MIMO theory predicts manyfold capacity gains by leveraging many antennas (e.g. M >> 10) on wireless base stations to serve many users simultaneously through multi-user beamforming (MUBF). However, realizing such a large-scale design is nontrivial, and has yet to be achieved in the real world. We present the design, realization, and evaluation of Argos, the first reported large-scale base station that is capable of serving many (e.g., 10s of) terminals simultaneously through MUBF. Designed with extreme flexibility and scalability in mind, Argos exploits hierarchical and modular design principles, properly partitions baseband processing, and holistically considers real-time requirements of MUBF. To achieve unprecedented scalability, we devise a novel, completely distributed, beamforming technique, as well as an internal calibration procedure to enable implicit beamforming across large arrays. We implement a prototype with 64 antennas, and demonstrate that it can achieve up to 6.7 fold capacity gains while using a mere 1/64th the transmission power.
-------------------------------------

10132673_183 - 0.955153756978 - technology_and_computing
[technology, partnership, pngv, goal]

Rethinking the Car of the Future
On September 29, 1993, President Clinton and the chief executive officers of Ford, Chrysler, and General Motors (the “Big Three”) announced the creation of what was to become known as the Partnership for a Generation of Vehicles (PNGV). The primary goal of the partnership was to develop a vehicle that achieves up to three times the fuel economy of today’s cars – about 80 miles per gallon (mpg) – with no sacrifice in performance, size, cost, emissions, or safety. The project would cost a billion dollars or more, split fifty-fifty between government and industry over a 10-year period. Engineers were to select the most promising technologies by 1997, create a concept prototype by 2000, and build a production prototype by 2004.  As the first deadline approaches, PNGV shows signs of falling short of its ambitious goals. Little new funding has been devoted to the project. More important, the organization structure that seemed appropriate in 1993 – its design goals, deadlines, and funding strategies – may prove to be counterproductive. The program designed to accelerate the commercialization of revolutionary new technologies has focused instead on incremental refinement of technologies that are relatively familiar and not particularly beneficial for the environment.  Major adjustments are needed in order to realize the full potential of this partnership. A reformed PNGV would be capable of efficiently directing funds toward the most promising technologies, the most aggressive companies, and the most innovative research centers. Now is the time to update the program by incorporating the lessons learned during its first few years.
-------------------------------------

10131023_183 - 0.999989358107 - technology_and_computing
[database]

The World Cultures Database
This article discusses the construction and uses of databases in comparative research.
-------------------------------------

10138081_183 - 0.731281599418 - technology_and_computing
[subject, space, interaction, mechanism, study, drug]

Individualized, discrete event, simulations provide insight into inter- and intra-subject variability of extended-release, drug products
Abstract
				
				
					
						Objective
					Develop and validate particular, concrete, and abstract yet plausible in silico mechanistic explanations for large intra- and interindividual variability observed for eleven bioequivalence study participants. Do so in the face of considerable uncertainty about mechanisms.
				
				
					
						Methods
					We constructed an object-oriented, discrete event model called subject (we use small caps to distinguish computational objects from their biological counterparts). It maps abstractly to a dissolution test system and study subject to whom product was administered orally. A subject comprises four interconnected grid spaces and event mechanisms that map to different physiological features and processes. Drugs move within and between spaces. We followed an established, Iterative Refinement Protocol. Individualized mechanisms were made sufficiently complicated to achieve prespecified Similarity Criteria, but no more so. Within subjects, the dissolution space is linked to both a product-subject Interaction Space and the GI tract. The GI tract and Interaction Space connect to plasma, from which drug is eliminated.
				
				
					
						Results
					We discovered parameterizations that enabled the eleven subject simulation results to achieve the most stringent Similarity Criteria. Simulated profiles closely resembled those with normal, odd, and double peaks. We observed important subject-by-formulation interactions within subjects.
				
				
					
						Conclusion
					We hypothesize that there were interactions within bioequivalence study participants corresponding to the subject-by-formulation interactions within subjects. Further progress requires methods to transition currently abstract subject mechanisms iteratively and parsimoniously to be more physiologically realistic. As that objective is achieved, the approach presented is expected to become beneficial to drug development (e.g., controlled release) and to a reduction in the number of subjects needed per study plus faster regulatory review.
-------------------------------------

10129972_178 - 0.996004809354 - technology_and_computing
[spatial, computational, reference, scale, response, temporal, disparate, analysis, immune, bibliographical]

Computational analyses of immune responses at disparate temporal and spatial scales
Includes bibliographical references (p. 139-147).
-------------------------------------

10137201_183 - 0.940717980719 - technology_and_computing
[kansl2, mtdna, repair, dna, ung, complex, study, mitochondrial]

Functional Characterization of Kansl2 and Its Role in Mitochondrial DNA  Repair and Maintenance
Mitochondrial DNA repair is a highly regulated, stress-responsive event that is crucial for maintaining mtDNA integrity. Oxidative damage and the mutation load of mtDNA are greater than nuclear DNA. The base excision repair (BER) machinery is important for repair of mtDNA as well as nuclear DNA; however, the pathway has not been as well characterized in mitochondria compared to the nucleus. Recent studies have shown that several repair factors are localized to mitochondria and form stable complex with mtDNA. In this study, I characterize a newly identified mtDNA protein, Kansl2, and its role in mtDNA repair. Kansl2 is conserved from flies to vertebrates, expressed ubiquitously in developing zebrafish and localizes to the mitochondrial matrix. Inactivation of Kansl2 in developing zebrafish caused abnormal heart and muscle development, with eventual death because Kansl2 is essential. Knockdown of Kansl2 enhanced apoptosis. Loss of Kansl2 resulted in aberrant mitochondrial cristae and compromised complex II and complex IV activity. Kansl2 binds to the mtDNA promiscuously via its Rad51-like domain. More importantly, I found that Kansl2 assembles in a 300 kDa molecular weight complex with uracil DNA glycosylase (Ung) in zebrafish mitochondria. I further characterized the molecular interaction of Kansl2 and Ung by determining the effect of Kansl2 on Ung activity in-vitro. These results affirm the hypothesis that Kansl2 serves as a scaffold for Ung and the interaction enhances the removal of uracil from the mtDNA by Ung. This study employs western blot analysis, animal studies, knockdown technology, native PAGE analysis and electron microscopy.
-------------------------------------

10135644_183 - 0.984180692661 - technology_and_computing
[method, search, line, step]

Projected-search methods for box-constrained optimization
Many algorithms used in unconstrained minimization are line-search methods. Given an initial point x and function f : Rn arrow R to be minimized, a line-search method repeatedly solves two subproblems : the first calculates a search direction p; the second performs a line search on the function phi(alpha) = f(x + alphap). Then, alphap is added to x and the process is repeated until a solution is located. Quasi-Newton methods are often used to calculate the search direction. A quasi-Newton method creates a quadratic model of f at x and defines the search direction p such that x + p is the minimizer of the model. After each iteration the model is updated to more closely resemble f near x. Line searches seek to satisfy conditions that ensure the convergence of the sequence of iterates. One step that decreases f "sufficiently" is called an Armijo step. A Wolfe step satisfies stronger conditions that impose bounds on phi(alpha). Quasi- Newton methods perform significantly better when using Wolfe steps. Recently Gill and Leonard proposed the reduced Hessian (RH) method, which is a new quasi-Newton method for unconstrained optimization. This method exploits key structures in the quadratic model so that the dimension of the search space is reduced. Placing box constraints x leads to more complex problems. One method for solving such problems is the projected-search method. This method performs an unconstrained minimization on a changing subset of the variables and projects points that violate the constraints back into the feasible region while performing the line search. To date, projected line- search methods have been restricted to using an Armijo- like line search. By modifying the line-search conditions, we create a new projected line search that uses a Wolfe- like step. This line search retains many of the benefits of a Wolfe line search for the unconstrained case. Projected-search methods and RH methods share a similar structure in solving for the search direction. We exploit this similarity and merge the two ideas to create a class of RH methods for box-constrained optimization. When combined with the new line search, this new family of algorithms minimizes problems in less than 74% of the time taken by the leading comparable alternative on a collection of standard test problems
-------------------------------------

10139148_183 - 0.88613948913 - technology_and_computing
[topograph, construction, eisenstein, geometry, arithmetic, gaussian]

An arithmetic construction of the Gaussian and Eisenstein topographs
We demonstrate a purely arithmetic construction of the Eisenstein and Gaussian topographs of Bestvina and Savin. Influenced by Conway's approach, we recover these topographs as incidence geometries over sets of ''generalized lax bases". We use the results of Johnson and Weiss to identify our constructions with the Coxeter geometries arising from projective general linear groups.
-------------------------------------

10131852_183 - 0.80978586768 - technology_and_computing
[press, microform, alternative, underground, source]

Preserving the U.S. Underground and Alternative Press of the 1960s and '70s: History, Prospects, and Microform Sources
This essay reviews the microfilming of underground and alternative press from the 1960s and 1970s and is based in part on research initially conducted at the University of Michigan.  Appendices lists microform sources. 
-------------------------------------

10134277_183 - 0.997069341902 - technology_and_computing
[solar, power, site, analysis, residential]

Statistical analysis of solar irradiation in a distributed microgrid
In recent decades, solar power has become increasingly more efficient and wide-spread in its use, particularly in residential applications. To allow residential solar power to continue its growth and become a larger percentage of energy production in the United States, the issues of solar intermittency must be understood. This project builds upon previous research in this field by analyzing 1 -second solar irradiation data gathered over 5 months by a unique array of 8 sites around the campus of UC San Diego. Correlations between the sites in both time and space, stratification of days by cloudiness, and ramp rate analysis are used to examine the variability of the solar power produced at the sites, while an aggregate ramp rate analysis is used to draw conclusions about the character of the net power produced from a distributed array of solar sites. These analyses have shown important characteristics of residential solar power generation that were previously not well described in the literature
-------------------------------------

10135543_183 - 0.965421279764 - technology_and_computing
[motion, home, suffix, base, speaker, p'urhepecha, social]

Motion deixis, home base and social indexicality among the P'urhepecha
In this thesis I analyze the category of deictic motion in the P'urhepecha language to better understand how language may provide insights into social relations and sense of belonging. I begin by outlining theoretical issues relating to deixis, focusing on the social implications. Next, I present the verbs and suffixes that speakers of P'urhepecha utilize to talk about coming and going. I then describe two suffixes that function to index the goal of motion as a home base. Through examples of home base suffix usage, I make the case that, in talking about motion, speakers index a range social factors that reveal their attitudes towards people and the spaces they inhabit. I show how the category of motion in P'urhepecha covers a wide semantic range through metaphor and semantic bleaching, revealing cultural schemas of motion and returning that underlie speakers' conceptualization of cosmological order and the unfolding of everyday events. Subsequently, I look at linguistic structures that emerge in changing contexts, specifically among people who have migrated. It is my thesis that speech practices in novel circumstances reflect migrants' subjective experiences of continuity and change. That is, the variable use of the home base suffixes in reference to motion towards goals that may or may not be conceptualized as ̀home' provides a running commentary on speakers' own shifting sense of place, identity and belonging. I conclude by proposing how this work may provide a point of departure for further research on the immigrant experience
-------------------------------------

10137832_183 - 0.889485760442 - technology_and_computing
[workflow, microarray, datum, pipeline, kepler, analysis, environment, processing, bioinformatic, development]

Workflows for microarray data processing in the Kepler 
environment
AbstractBackgroundMicroarray data analysis has been the subject of extensive and ongoing pipeline development due to its complexity, the availability of several options at each analysis step, and the development of new analysis demands, including integration with new data sources. Bioinformatics pipelines are usually custom built for different applications, making them typically difficult to modify, extend and repurpose. Scientific workflow systems are intended to address these issues by providing general-purpose frameworks in which to develop and execute such pipelines. The Kepler workflow environment is a well-established system under continual development that is employed in several areas of scientific research. Kepler provides a flexible graphical interface, featuring clear display of parameter values, for design and modification of workflows. It has capabilities for developing novel computational components in the R, Python, and Java programming languages, all of which are widely used for bioinformatics algorithm development, along with capabilities for invoking external applications and using web services.ResultsWe developed a series of fully functional bioinformatics pipelines addressing common tasks in microarray processing in the Kepler workflow environment. These pipelines consist of a set of tools for GFF file processing of NimbleGen chromatin immunoprecipitation on microarray (ChIP-chip) datasets and more comprehensive workflows for Affymetrix gene expression microarray bioinformatics and basic primer design for PCR experiments, which are often used to validate microarray results. Although functional in themselves, these workflows can be easily customized, extended, or repurposed to match the needs of specific projects and are designed to be a toolkit and starting point for specific applications. These workflows illustrate a workflow programming paradigm focusing on local resources (programs and data) and therefore are close to traditional shell scripting or R/BioConductor scripting approaches to pipeline design. Finally, we suggest that microarray data processing task workflows may provide a basis for future example-based comparison of different workflow systems.ConclusionsWe provide a set of tools and complete workflows for microarray data analysis in the Kepler environment, which has the advantages of offering graphical, clear display of conceptual steps and parameters and the ability to easily integrate other resources such as remote data and web services.
-------------------------------------

10134334_183 - 0.991654687963 - technology_and_computing
[optimization, ball, robot]

Simulation and optimization of a two-wheeled, ball- flinging robot
This paper presents a method for optimizing the throwing distance of a two-wheeled, self-balancing, remote controlled robot. The robot is maneuverable and comparable in size to a remote controlled toy car, but it moves around in an upright configuration using feedback control. In addition, it is capable of automatically picking up and throwing ping-pong balls. When throwing a ball, the body attached throwing arm is allowed to rotate quickly from a lay-down position to an upright position utilizing the principle of conservation of angular momentum. The equations of motions of a representative model of the dynamic system are derived with Lagrange equation and Rayleigh dissipation functions. The optimization consists of two parts. The shape optimization is based on the ball's exit condition from the arm, which wraps around an adjoint based motor input optimization. In simulation, this optimization scheme results in a significant increase in throwing distance while keeping the magnitude of the motor input within comparable range
-------------------------------------

10135114_183 - 0.997377459806 - technology_and_computing
[datum, clustering, co-clustering, matrix, spatial, method, pest]

Clustering: Algorithm, Optimization and Inference
Clustering is rapidly becoming a powerful data mining technique, and has been broadly applied to many domains. Usually data are arranged in a matrix with rows and columns, and each cell of this matrix is a real number. This dissertation aims at developing clustering algorithms with statistical inference incorporated in the following two scenarios.First, when each cell of the data matrix is not represented by a single numerical value and instead contains a scatter plot, the existing clustering methods are not applicable any more. In this dissertation, we develop both hierarchical clustering and co-clustering procedure to handle a data matrix of scatter plots. To more accurately reflect the nature of data, we introduce a dissimilarity statistic based on "data depth" to measure the discrepancy between two bivariate distributions without oversimplifying the nature of the underlying pattern. We also propose novel painting metrics and construct heat maps to allow visualization of the clusters. We demonstrate the utility and power of our proposed clustering methods through simulation studies and application to a microbe-host-interaction study.Second, when spatial information is embedded in the data matrix, the order of rows and columns can not be changed. Model-based spatial co-clustering has not been well studied. In this dissertation, we develop a co-clustering method using a Generalized Linear Mixed Model (GLMM) for spatial data. To avoid the high computational intensity associated with global optimization, we propose a heuristic optimization algorithm to search for a near optimal co-clustering. A sampling strategy is introduced to capture as much of the spatial information that is available from the sparse data as possible. For an application pertinent to Integrated Pest Management (IPM), we combine the spatial co-clustering technique with a statistical inference method to make assessment of pest density more accurate. We demonstrate the utility and power of our proposed pest assessment procedure through simulation studies and apply the procedure to a study of the persea mite (Oligonychus perseae), a pest of avocado trees.
-------------------------------------

10134538_183 - 0.999999087607 - technology_and_computing
[digital, transmitter, amplifier, novel, noise, algorithm, broadband, frequency, system, nonlinearity]

Digital signal generation for wireless communication systems
Radio Frequency (RF) transmitters have long been realized by analog circuitry particularly at the front-end that interfaces with the antenna. Migrating wireless transmitter functions from analog to digital circuitry can bring many benefits such as lower cost, lower power, and higher integration while eliminating some of the drawbacks of analog circuitry such as sensitivity to temperature and processing variation. The objective of this dissertation is to investigate and analyze techniques used to implement wireless transmitters with digital circuitry. Several different transmitter designs addressing different wireless standards are presented, which serve to show the advantages of the digital approach, highlight major obstacles to its success with emphasis on the issues unique to digital transmitters, and introduce novel analysis and simulation techniques leading to new designs addressing the obstacles identified. The dissertation begins by summarizing barriers to digital transmitter implementation, and introducing new metrics to facilitate comparative evaluation. In-band power ratio is discussed, a novel metric useful for the prediction of relative efficiency of switching amplifiers driven with different quantization algorithms. The transmitter system examples begin with a low-power standard with relatively low signal accuracy requirement (ZigBee) where a novel quantization algorithm driving a CMOS amplifier highlights the digital transmitter benefits of simplicity, high integration, and low total transmitter power. The dissertation then discusses more complex, higher-power systems (CDMA and WCDMA) to highlight the major obstacles to a cellular- handset-class digital transmitter. Novel optimizations of a delta-sigma modulator's digital implementation and noise transfer function design allow the modulator to operate quietly in the receive band, while retaining realistic clock frequencies and word lengths. Through laboratory measurements, analysis, and fixed-time-step simulations developed in this work, a set of nonlinearities is discovered that is inherent to types of switching amplifiers driven by aperiodic broadband signals. The cause of these nonlinearities is presented, and validated by an amplifier redesign reducing these nonlinearities by more than an order of magnitude. A variation on a well- known digital encoding algorithm, band-pass pulse width modulation, is shown to achieve amplifier efficiencies better than delta-sigma, without exciting broadband noise from amplifier nonlinearities. Novel digital-to-time circuit topologies and frequency domain simulations are presented to attain the high accuracy time resolution needed to achieve pulse width modulation at high frequencies. Laboratory measurements are provided showing the efficiency and noise advantages of this design. digital polar amplification system is presented highlighting novel digital drive algorithms. A general study on broadband noise generated by polar amplifier systems is undertaken with simulation and analysis. Unique time-domain properties of polar broadband noise, due to common impairments such as time misalignment, are presented and analyzed. A pre-distortion technique is presented that can reduce the broadband noise in polar amplifiers though the addition of simple DSP operations. Lastly, a novel all-digital frequency synthesis algorithm is presented along with hardware measurements
-------------------------------------

10134805_183 - 0.999996953573 - technology_and_computing
[network, technique, space, functional, neuronal, framework, activity, cell, dynamics, state]

Mapping functional connectivity in cellular networks
My thesis is a collection of theoretical and practical techniques for mapping functional or effective connectivity in cellular neuronal networks, at the cell scale. This is a challenging scale to work with, primarily because of the difficulty in labeling and measuring the activities of networks of cells. It is also important as it underlies behavior, function, and complex diseases. I present methods to measure and quantify the dynamic activities of cells using the optical flow technique, which can identify activity and directions of information processing using calcium fluorescence measurements. I present a unified framework for simulation and estimation of neuronal activity, tailored towards interpretation of experimental data, and implemented in a fully parallel fashion on graphics processor unit (GPU) cards. This framework permits experimenters to estimate hidden quantities in collected data, using any neuronal or astrocyte model. I introduce a technique for mapping functional connectivity in neuronal networks, using experimental data and an arbitrary state space model. The technique makes some simplifications that reduces the dimensionality of the estimation problem, and shows excellent performance for networks of up to 30 possible independent incoming connections. While the framework and mapping algorithms use a state space, parametric representation of individual cell dynamics, I've also developed a time-embedded, nonparametric technique for estimating input-output relationships, and applied it to estimating current from voltage measurements and spikes from fluorescent calcium. Without any knowledge of the underlying neuronal dynamics, this technique can reconstruct a current signal from measured voltage in mouse pyramidal neurons with an R-value of 0.9. Finally, I present my findings and theoretical perspectives acquired while developing the framework and methods. Optimization as a means of estimating functional weights is especially challenging due to the topology of the parameter space, with small perturbations in weights resulting in drastically different simulated dynamics. High-dimensional spaces are prone to the curse of dimensionality, and network states represented in such spaces are not likely to be stable or typical. Finally, the effects of the concentration of measure, as I believe I've observed when mapping large networks, makes it unlikely that real-world networks have more than about 7 independent functional inputs at any given time
-------------------------------------

101652_108 - 0.999505829197 - technology_and_computing
[x-ray, system, breast]

A Novel Detection System Consisting of a Large-Area Sensor and a Multicell Si-Pad Array Operated in Spectroscopic Mode for X-Ray Breast Imaging
<p>The ability of coherent X-ray scatter to provide the molecular structure of breast tissues could add a new dimension in X-ray breast imaging capable of tracking the molecular structural changes during disease progression and of improving the sensitivity to low-contrast lesions without increasing the radiation dose. Work is under way to build a laboratory prototype dual-sensor breast-imaging scanning system, which combines the diagnostic information from both the transmitted primary and the forward scattered X-rays. This required the design and development of a coherent X-ray scatter detection system based on a high-resistivity multielement two-dimensional (2-D) Si-pad array, a multichannel low-noise pulse processing front-end electronics chip, the XA1.3, and a new DAQ system. X-rays in the energy range of 17-45 keV can be detected with a FWHM energy resolution of 1-3 keV. Results on the characterization and optimization of the detector-readout electronics-DAQ system and its performance to measure diffraction signatures of most commonly used breast-equivalent materials of interest are presented.</p>
-------------------------------------

10139904_183 - 0.999991368183 - technology_and_computing
[quaternion, command, filter, tracking]

Quaternion-based Trajectory Tracking Control of VTOL-UAVs using Command Filtered Backstepping
This paper discusses trajectory tracking control for Vertical Take-Off and Landing (VTOL) Unmanned Aerial Vehicles (UAVs) using the command filtered backstepping technique. Quaternions are used to represent the attitude of the vehicle to ensure the global attitude tracking without singularities. Since the quaternions have their own unique algebra, they cannot be filtered by a vector-based command filter; therefore, a second-order quaternion filter is developed to filter the quaternion and automatically compute its derivative, which determines the commanded angular rate vector. A quadrotor vehicle is used as an example to show the performance of the proposed controller.
      Please cite the conference or journal version of the article, not this tech report.
-------------------------------------

10129791_178 - 0.711242198024 - technology_and_computing
[flow, reference, separation, control, bibliographical, cylinder]

Flow separation control with rotating cylinders
Includes bibliographical references (p. 61-62).
-------------------------------------

10134233_183 - 0.978751313514 - technology_and_computing
[model, continuum-scale, transport, hybrid, method, environment, pore-scale, cnt, part, flow]

Hybrid models of transport in crowded environments
This dissertation deals with multi-scale, multi-physics descriptions of flow and transport in crowded environments forming porous media. Such phenomena can be described by employing either pore-scale or continuum-scale (Darcy- scale) models. Continuum-scale formulations are largely phenomenological, but often provide accurate and efficient representations of flow and transport. In the first part of the dissertation, we employ such a model to describe fluid flow through carbon nanotube (CNT) forests placed in a turbulent ambient environment of a microscopic wind tunnel. This analysis leads to closed-form analytical formulae that enable one to predict elastic response of CNT forests to aerodynamic loading and to estimate elastic properties of individual CNTs, both of which were found to be in a close agreement with experimental data. The second part of this work explores the applicability range of continuum-scale models of transport of chemically active solutes undergoing nonlinear homogeneous and heterogeneous reactions with the porous matrix. We use two upscaling techniques (the volume averaging method and multiple-scale expansions) to formulate sufficient conditions for the validity of continuum-scale models in terms of dimensionless numbers characterizing key pore-scale transport mechanisms (e.g. Péclet and Damköhler numbers). When these conditions are not satisfied, standard continuum-scale models have to be replaced with upscaled equations that are nonlocal in space and time, effective parameters (e.g. dispersion tensors, effective reaction rates) do not generally exist, and pore- and continuum- scales cannot be decoupled. Such transport regimes necessitate the development of hybrid numerical methods that couple the pore- and continuum-scale models solved in different regions of the computational domain. Hybrid methods aim to combine the physical rigor of pore-scale modeling with the computational efficiency of its continuum-scale counterpart. In the third and final part of this dissertation, we use the volume averaging method to construct two hybrid algorithms, one intrusive and the other non-intrusive, that facilitate the coupling of pore- and continuum-scale models in a computationally efficient manner
-------------------------------------

10133030_183 - 0.979644814056 - technology_and_computing
[irrigation, water, algorithm, sensor, transport, groundwater]

The Performance of a Wireless Sensor Network for Structural Health Monitoring
An issue associated with agricultural irrigation using reclaimed wastewater is the potential threat to underlying groundwater quality. A prime example is nitrate, which serves as a fertilizing agent but has the potential to leach into groundwater. In order to balance water reuse and groundwater protection, intelligent irrigation management and monitoring systems are required for such water reuse systems. In this work, a nonlinear programming-based control algorithm is proposed to optimize irrigation scheduling subject to contaminant transport constraints. In support of the algorithmic developments, a networked sensor array is being designed for deployment at an agricultural research plot. This array will supply real-time field information about water infiltration and distribution, nitrate propagation, and heat transport, to the irrigation scheduling algorithm. The control scheme (measurement, decision, and action) will be continuously updated using on-line feedback from sensors. The simulator on which the management algorithm depends is a one-dimensional form of the Richards equation coupled to energy and solute transport mass balances.
-------------------------------------

10134651_183 - 0.999986224009 - technology_and_computing
[design, shm, gwshm, sensor, process, application, bayesian, system, framework]

A Bayesian experimental design approach to structural health monitoring with application to ultrasonic guided waves
The dissertation will present the application of a Bayesian experimental design framework to structural health monitoring (SHM). When applied to SHM, Bayesian experimental design (BED) is founded on the minimization of the expected loss, i.e., Bayes Risk, of the SHM process through the optimization of the detection algorithm and system hardware design parameters. This expected loss is a function of the detector and system design, the cost of decision/detection error, and the distribution of prior probabilities of damage. While the presented framework is general to all SHM applications, particular attention is paid to guided wave-based SHM (GWSHM). GWSHM is the process of exciting user-defined mechanical waves in plate or beam-like structures and sensing the response in order to identify damage, which manifests itself though scattering and attenuation of the traveling waves. Using the BED framework, both a detection-centric and a localization-centric optimal detector are derived for GWSHM based on likelihood tests. In order to objectively evaluate the performance in practical terms for the users of SHM systems, the dissertation will introduce three new statistics-based tools: the Bayesian combined receiver operating characteristic (BCROC) curve, the localization probability density (LPDF) estimate, and the localizer operating characteristic (LOC) curve. It will demonstrate the superior performance of the BED-based detectors over existing GWSHM algorithms through application to a geometrically complex test structure. Next, the BED framework is used to establish both a model-based and data -driven system design process for GWSHM to ascertain the optimal placement of both actuators and sensors according to application-specific decision error cost functions. This design process considers, among other things, non- uniform probabilities of damage, non-symmetric scatterers, the optimization of both sensor placement and sensor count, and robustness to sensor failure. The sensor placement design process is demonstrated and verified using several hypothetical and real-world design scenarios
-------------------------------------

10133479_183 - 0.993446158493 - technology_and_computing
[sinusoidal, signal, input, different, process, peak]

Time scale modication using a sinusoidal model
Time-scale modication of audio signals is a classic digital signal processing problem. While many solutions have been implemented over the years, none work perfectly without audible artifacts for all kinds of input. In most applications, trade-offs must be made between computational efficiency and quality, resulting in different solutions being favored in different contexts and for different classes of input signals. Sinusoidal modeling is an approach which has shown signicant promise for producing high-quality time-scaled audio. Leaving noise and transient signal components for future work in order to focus exclusively on sinusoidal components, this thesis describes a variation on traditional sinusoidal modeling which offers a unique combination of features, including separating the process of identifying individual sinusoidal peaks from the process of combining those peaks into sinusoidal tracks, special treatment of phase during the peak tracking process, and analysis enhancements for sinusoidal tracks with modulating frequency and multichannel input signals
-------------------------------------

10137464_183 - 0.999420530313 - technology_and_computing
[nanocapsule, system, synergy]

Enzyme Nanocapsules as Intracellular Scarvagers for Reactive Oxygen Species
We use superoxide and catalase synergy nanocapsule system to reduce the cytotoxicity caused by superoxide radicals, a typical reactive oxygen species (ROS). Enzymes are encapsulated inside positive shells via APS/TEMED initiated polymerization. The diameter and charge of the non-degradable nanocapsules are suitable for intracellular delivery and clear endocytosis has been observed via fluorescence microscope. Under induced radical environment, higher cell viability is observed when cells are incubated with both nanocapsules at the ratio of nSOD/nCatalase=5 compared with only nSOD or nCatalase system. We believe that the use of this synergy delivery providing a sound progress for eliminating ROS in vitro via synergy protein therapy. What's more, this bi-enzyme nanocapsule system can also be seen as solid foundation for multi-enzyme nanocapsule system to achieve other complex therapeutic applications.
-------------------------------------

10133700_183 - 0.986025029663 - technology_and_computing
[energy, radio, power, pc, processor, device, efficiency, technique, case, subsystem]

Building aggressively duty-cycled platforms to achieve energy efficiency
Managing power consumption and improving energy efficiency is a key driver in the design of computing devices today. This is true for both battery powered mobile devices as well as mains-powered desktop PCs and servers. In case of mobile devices, the focus of optimization is on energy efficiency to maximize battery lifetime. In case of mains- powered devices, we seek to optimize power consumption to reduce energy costs, thermal and environmental concerns. Traditionally, there are two main mechanisms to improve energy efficiency in systems: slowdown techniques that seek to reduce processor speed or radio power against the rate of work done, and shutdown techniques that seek to shut down specific components or subsystems - such as processor, radio, memory - to reduce power used by these components when not in use. The adverse effect of using these techniques is either reduced performance (e.g., increase in latency) and/or usability or loss of functionality. The thesis behind this dissertation is that improved energy efficiency can be achieved through system architectures that seek to design and exploit "collaboration" among heterogeneous but functionally similar subsystems. For instance, multiple radio interfaces with different power/performance characteristics can collaborate to provide an energy- efficient wireless communication subsystem. Furthermore, we show that in systems where such heterogeneity is not naturally present, we can introduce heterogeneous components to improve overall energy efficiency. We show that using collaboration, individual subsystems and even entire platforms can be shut down more aggressively to reduce energy consumption, while reducing adverse impacts on performance or usability. We have used collaboration to do energy efficient operation in several contexts. For battery powered mobile devices we show that wireless radios are the dominant power consumers, and then describe several techniques that use various heterogeneous radios present on these devices in a collaborative manner to improve their battery lifetime substantially, on average by two to three times and in some cases up to 8 times. First we present "Cell2Notify", a technique in which a lower power radio is used purely to wakeup a higher power radio. Next, we present "CoolSpots" and "SwitchR", systems that build a hierarchy of collaborative radios to choose the most appropriate radio interface, taking into account application characteristics as well as various energy and performance metrics. In the case of wall-powered desktop and laptop Personal Computers (PCs) we show that the dominant power consumers are the processors themselves. We then describe "Somniloquy", an architecture that augments a PC with a separate low power secondary processor that can perform some of the functions of the host PC on its behalf. We show that by using the primary processor (i.e. the PC) collaboratively with the secondary processor we can shut down PCs opportunistically, and as a result reduce the overall energy consumption by up to 80% in most cases
-------------------------------------

10175385_189 - 0.99698013431 - technology_and_computing
[optimization, problem, adaptive, pde, collocation, convergence, solution, numerical, region, trust]

An Approach for the Adaptive Solution of Optimization Problems Governed by Partial Diﬀerential Equations with Uncertain Coeﬃcients
Using derivative based numerical optimization routines to solve optimization problems governed by partial differential equations (PDEs) with uncertain coefficients is computationally expensive due to the large number of PDE solves required at each iteration. In this thesis, I present an adaptive stochastic collocation framework for the discretization and numerical solution of these PDE constrained optimization problems. This adaptive approach is based on dimension adaptive sparse grid interpolation and employs trust regions to manage the adapted stochastic collocation models. Furthermore, I prove the convergence of sparse grid collocation methods applied to these optimization problems as well as the global convergence of the retrospective trust region algorithm under weakened assumptions on gradient inexactness.  In fact, if one can bound the error between actual and modeled gradients using reliable and efficient a posteriori error estimators, then the global convergence of the proposed algorithm follows.  Moreover, I describe a high performance implementation of my adaptive collocation and trust region framework using the C++ programming language with the Message Passing interface (MPI). Many PDE solves are required to accurately quantify the uncertainty in such optimization problems, therefore it is essential to appropriately choose inexpensive approximate models and large-scale nonlinear programming techniques throughout the optimization routine. Numerical results for the adaptive solution of these optimization problems are presented.
-------------------------------------

10175474_189 - 0.999989920828 - technology_and_computing
[network, rain, arbitrary, architecture, man, datacenter, scalable]

Designing Scalable Networks for Future Large Datacenters
Modern datacenters require a network with high cross-section bandwidth, fine-grained security, support for virtualization, and simple management that can scale to hundreds of thousands of hosts at low cost.  This thesis first presents the firmware for Rain Man, a novel datacenter network architecture that meets these requirements, and then performs a general scalability study of the design space.

The firmware for Rain Man, a scalable Software-Defined Networking architecture, employs novel algorithms and uses previously unused forwarding hardware.  This allows Rain Man to scale at high performance to networks of forty thousand hosts on arbitrary network topologies.

In the general scalability study of the design space of SDN architectures, this thesis identifies three different architectural dimensions common among the networks: source versus hop-by-hop routing, the granularity at which flows are routed, and arbitrary versus restrictive routing and finds that a source-routed, host-pair granularity network with arbitrary routes is the most scalable.
-------------------------------------

10136308_183 - 0.937903596288 - technology_and_computing
[site, analysis, archaeological, standard, micro-debris, mm, dhiban]

Residue or Residon't? The Value of Archaeological Micro-debris in Unraveling Dhiban's Imperial Past
The archaeological site of Dhiban, Jordan is characterized by a complex legacy of colonial dominance by foreign empires, with occupation ranging from the Late Bronze Age (1500-1200 BCE) into the present. This turbulent history is reflected in its archaeological record, necessitating a much greater detailed system of analysis than ‘standard’ analytical practices for unraveling site use. Using archaeological data excavated from Dhiban's Middle Islamic occupation (1000-1450 CE), this paper introduces the initial steps of a larger project assessing the cost-benefits of conducting a more detailed analysis of excavated site material against the more ‘standard’ processes of analysis. The introduced methods focus on a ‘micro-debris’ analysis, using items ranging between <4 mm and 1 mm in size, to be used in conjunction with the more standard ‘heavy fraction’ analysis of items >4 mm in size for site interpretation. Although this paper provides only preliminary data and a speculative interpretation designed only to demonstrate the use of these methods, it appears that ‘micro-debris’ does not simply reflect the types and quantities of artifacts appearing in the ‘heavy-fraction’ materials, but rather represents different types of site activities then those found in ‘standard’ analyzed samples. The larger project works to expand on the pilot study introduced in this paper and to construct a solid narrative on greater social and economic trends, including the impact of state actions on local communities throughout this tumultuous, arid region, providing a much more comprehensive understanding of the daily lives of the people of Jordan’s past.
-------------------------------------

10137375_183 - 0.987036104277 - technology_and_computing
[energy, appliance, state, system, power, disaggregation, sensor]

Low-cost Appliance State Sensing for Energy Disaggregation
Fine-grained per appliance electrical energy consumption data is crucial to electrical energy conservation. However, energy meters are installed at few central points in buildings, providing only aggregated energy consumption data. Therefore, people are seeking ways to get disaggregated energy information. A key issue and most challenging problem in energy disaggregation is to know the power state of each appliance. We design a sensing system that can reliably keep track of the binary (on-off) states of appliances. In our system, the sensors are designed to be deployed at each appliance. However, we are able to minimize the hardware requirements so that the sensors are inherently low-cost. The whole system is totally affordable for large scale deployment. The evaluation shows that, despite the simplicity of hardware, the system can keep track of the power state of tens of appliances at 99.5% precision and recall with a single base station. We also propose an energy disaggregation approach, based on the information our sensor network provides combined with central power meter readings.
-------------------------------------

10134464_183 - 0.999053665486 - technology_and_computing
[sol-gel, xerogel, matrix, sensor, fiber-optic, diffusion, ph, glass, application, chemical]

Structure, Mechanism and Applications of  Sol-Gel Clad Fiber-Optic Sensors
Xerogels are porous glasses formed from the hydrolysis and polycondensation of metal alkoxides.  Xerogels are used as insulators, catalysts, hosts in electro-optical devices, as well as solid-state matrices in chemical sensors.  An important attribute of xerogels in chemical sensing applications is the porosity of the sol-gel matrix, which results from the formation of the matrix, allows analytes to diffuse into the glass and come in contact with the sensing element.  Our main goal, driven in part by the environmental applications, was directed toward incorporating sensing fluorescent chromophores into the sol-gel matrix and characterizing the effects of the matrix on the fluorophores' respective chemistry. The mechanism by which ions diffuse into the sol-gel matrix, illustrated by measuring of the proton diffusion into the glass, was elucidated by the time-dependent absorbance change of the pH indicator dye, fluorescein.  Small pore xerogel glasses were found to possess proton diffusion rates that depend on the direction of the pH change.  The fluorescein-doped xerogels showed a slower time response for a decrease in pH and a longer time response for an increase in pH.  The difference in these two rates could indicate that the sol-gel matrix provides a kinetic barrier to proton diffusion into the pores of the glass.Sol-gel clad fiber-optic waveguides were investigated as intrinsic distributed fiber-optic chemical sensors.  The porous sol-gel cladding allows diffusion of analytes into the evanescent field region close to the fiber-optic core.  Pulsed optical excitation (0.5 ns) and time-resolved emission detection were used to simultaneously monitor several sensor regions along a fiber optic waveguide.  Narrow band excitation and spectrally resolved emission provide additional means for discriminating between specific regions offering spatial sensitivity and kinetic-based sensing.  A fluorescein-doped silica xerogel clad pH sensor and an undoped xerogel clad lucigenin sensor were demonstrated as intrinsic sol-gel clad fiber-optic sensors.
-------------------------------------

10139163_183 - 0.981754121395 - technology_and_computing
[charge, transfer, anthranilamide, charge-transfer, photoinduced, dipole, electron, electret, intrinsic, bioinspired]

Design and Charge-Transfer Properties of Bioinspired Electrets
In order to develop and demonstrate fundamental strategies for improving the efficiency of photovoltaic devices that are commonly used for solar-energy capture and conversion, we introduced and studied anthranilamides as bioinspired electrets.     Charge transfer processes play a key role in chemical and biological systems. Photoinduced charge transfer represents the central paradigm of light-energy conversion of photosynthesis and photovoltaic devices. The Rehm-Weller equation allows for estimating the diving force of photoinduced charge transfer by employing readily measureable quantities such as the redox potentials and spectroscopic data of electron donors and acceptors. A significant part of my studies focused on the Born solvation term in the Rehm-Weller equation that introduces the electrostatic stabilization of the charge-transfer species by the surrounding media. Cyclic voltammetry, allowed me to demonstrate experimentally the effects of the supporting electrolyte on the redox potentials. These effects were especially pronounced for non-polar solvents. Most importantly, I devised an approach to address the discrepancies that the presence of electrolyte introduces to the charge-transfer analysis. Concurrently, my studies demonstrated that the Generalized Born model allows for addressing the deficiencies in the charge-transfer analysis involving redox species that are not spherical and that have heterogeneous charge distribution.     The other significant part of my studies focused on anthranilamides as bioinspired electrets that have the potential to accelerate charge separation and suppress the undesired charge recombination. My research provided the first experimental demonstration that the anthranilamides possess intrinsic dipoles. These amides with large intrinsic dipole moments (that is, electrets) can generate electric field, which enhances electron transfer from their N- to their C-termini and impedes it in the backward direction. To test the ability of the electrets to modify the direction of electron transfer, I incorporated an anthranilamide monomer in electron donor-acceptor (DA) dyads. Comparison between the charge-transfer kinetics of electret-acceptor dyads, revealed a faster initial photoinduced charge separation and a slower charge recombination when electron was transferred toward C-terminus. These findings were consistent with the orientation of the intrinsic dipole moment of the anthranilamide monomer. Aside from previous work employing polypeptides, this is the first demonstration of rectification of charge transfer by dipole moments of synthetic bioinspired derivatives.      In summary, the most important contributions from my doctoral work were: (1) developing methods for reliable interpretation of experimental results pertinent to charge-transfer kinetics and thermodynamics; (2) demonstrating rectification of photoinduced charge transfer induced by the anthranilamide intrinsic dipole; and (3) demonstrating that the photoinduced processes result in charges residing on the anthranilamides (i.e., radical ions) which is essential for attaining hopping mechanism for long-range charge transfer.
-------------------------------------

10135193_183 - 0.999995386698 - technology_and_computing
[circuit, algorithm, ac0, assignment, constant, satisfying, restriction]

A satisfiability algorithm for constant depth boolean circuits with unbounded fan-in gates
We consider the problem of efficiently enumerating the satisfying assignments to AC0 circuits. AC0 circuits are Boolean circuits with n inputs and their negations, one output, m = poly(n) total gates, and constant depth, and consist of unbounded fan-in AND and OR gates. The primary technical tool we use is a new algorithmic approach for efficiently simplifying restricted classes of circuits. This approach is based on a new extended version of Hastad's Switching Lemma. As the main result, we present a Las Vegas algorithm which takes an AC0 circuit as input and outputs a set of restrictions (assignments to subsets of the inputs) which partition {0; 1}n such that under each restriction the out-put of the circuit is constant. With high probability, the algorithm runs in time poly(m; n) 2n(1-mu) and outputs at most 2n(1-mu) restrictions, where = 1=O(lg m/n + d lg d)d-1. This is optimal up to the constants in the big-O for enumerating solutions with restrictions. This also implies the best known algorithm for AC0 circuit satisfiability and for counting satisfying assignments. Using similar techniques, we also give an algorithm for enumerating the solutions to a k-cnf, but where mu 1=O(k). Previously, algorithms with similar savings mu were known for finding a single satisfying assignment to a k-cnf, but not for counting or enumerating satisfying assignments. These results have some interesting applications to circuit lower bounds. We prove a new bound on the correlation of AC0 circuits with parity which is optimal up to constants, and show how several classic AC0 circuit lower bounds follow straightforwardly from our algorithm. Then, we use a powerful theorem due to Williams to show how a minor improvement in the running time for finding a single satisfying assignment for an AC0 circuit would imply NEXP 6 NC¹
-------------------------------------

10136139_183 - 0.990822876794 - technology_and_computing
[graphene, device, noise, communication, application]

Graphene Device Fabrication and Applications in Communication Systems
High carrier mobility, saturation velocity and thermal conductivity make graphene a promising material for high-frequency, analog and communication applications. The ambipolar properties of graphene provide opportunities for increased functionality in unconventional circuit architectures. In this dissertation, I describe the fabrication process of graphene devices, including the optical and Raman spectroscopic characterization and electron-beam lithography. The different electrical characteristics of the single-layer and bilayer graphene field-effect devices reflect differences in the electron band structures of the two systems. The fabricated graphene transistors have been used to design and experimentally demonstrate electronic circuits with communication functionalities such as phase-shift keying, frequency-shift keying and phase detection. Compared with conventional semiconductor electronic designs based on multiple unipolar transistors, the demonstrated graphene amplifiers and phase detectors have advantage of a simplified structure. An important issue for high-frequency and analog applications is the low-frequency noise, which up-converts and contributes to the phase noise of the systems. It was found that the low-frequency noise in graphene devices is dominated by 1/f noise in the frequency range from 1 Hz to 100 kHz (f is the frequency). The device exposure to different gases results in appearance of characteristic peaks in the noise spectral density. The latter can be utilized for selective gas sensing with graphene. The metal-graphene contact contributions to the 1/f noise can be strongly reduced via the use of the graded thickness graphene channels in the device structure. I have also investigated a possibility of tuning graphene properties via controllable exposure to the low-energy electron-beam irradiation. It was found that the charge neutrality point and resistivity can be tuned over a wide range of values. The obtained results are important for the proposed applications of graphene in analog electronics, communications and sensors.
-------------------------------------

10135031_183 - 0.999989404579 - technology_and_computing
[communication, channel, scattering, model, uv, modeling, nlos, path, loss, performance]

Modeling and Characterization of Ultraviolet Scattering Communication Channels
This thesis studies modeling of non-line-of-sight (NLOS) ultraviolet (UV) scattering channels and the corresponding communication link performance. The research focuses on the channel impulse response and path loss models based on extensive field measurements and theoretical characterization. In NLOS UV scattering environments, transmitted signals suffer from severe atmospheric attenuation and fading before arriving at a receiver, such as absorption, scattering, and turbulence. The thesis is devoted to development of analytical and experimental models to characterize NLOS UV communication channels.The author conducts comprehensive channel measurements for short communication ranges up to a few hundred meters and proposes an empirical path loss model. Meanwhile, an algorithm is developed to simulate the NLOS UV channel impulse response and path loss based on photons stochastic migration in the atmosphere. Effects of atmosphere conditions on single scattering and multiple scattering are investigated. An empirical curve-fitting model is developed to simplify the modeling work. Monte Carlo simulations provide good channel prediction for field tests in many scenarios. Then short range communication link performance is studied based on the theoretical models, and limitations by power and channel bandwidth are examined. Link budget results are also extended to long range communication links up to 5 kilometers. In this case, atmosphere turbulence becomes pronounced, and thus the intensity fluctuation at the receiver is mathematically modeled. These modeling results can provide insight into the performance tradeoffs and algorithm design for practical NLOS UV communication systems.
-------------------------------------

10136815_183 - 0.979507941085 - technology_and_computing
[underwater, sensor, swarm, sea, architecture, acoustic, surface, datum, current, monitoring]

MAC and Routing Protocols for Mobile Underwater Acoustic Sensor Swarms
Underwater Acoustic Sensor Networks (UW-ASNs) have recently been proposed as a way to explore and observe the ocean, which covers two-thirds of the Earth's surface. In particular, we consider a SEA Swarm (Sensor Equipped Aquatic Swarm) architecture for short-term ad hoc real-time aquatic exploration, such as oil and chemical spill monitoring, submarine detection, and surveillance, by deploying drifting sensor nodes (e.g., UCSD Drogues) to the venue of interest that form a swarm and move as a group with the ocean oceanic current. Each sensor monitors local underwater activities and reports critical data or events in real-time using acoustic multi-hop routing to a distant data collection center, e.g., surface buoys or Autonomous Underwater Vehicles (AUVs).As SEA Swarm architecture adopts acoustic links as a means of communication, it is accordingly confronted with long propagation delays, low bandwidth, and high transmission power consumption. To put SEA Swarm architecture into practical use and alleviate these limitations, we propose the Delay-aware Opportunistic Transmission Scheduling (DOTS) algorithm to increase channel utilization by harnessing both temporal and spatial reuse. Extensive simulation results show that DOTS outperforms existing solutions, S-FAMA, DACAP, and CS-ALOHA in a line topology, in a highly competitive medium access star topology, and in a random topology with an underwater mobility by harnessing temporal and spatial reuse. Furthermore, in a SEA Swarm architecture, a sensor cloud that drifts with water currents and enables 4D (space and time) monitoring of local underwater events such as contaminants, marine life and intruders, is escorted at the surface by drifting sonobuoys that collect the data from underwater sensors via acoustic modems and report it in real-time via radio to a monitoring center. Thus, to realize SEA Swarm architecture, designing an efficient anycast routing algorithm for reliable underwater sensor event reporting to any one of the surface sonobuoys is imperative. Major challenges are the ocean current and the limited resources (bandwidth and energy). We address these challenges and propose two hydraulic pressure based anycast routing protocols, namely HydroCast and VAPR, which exploit the measured pressure levels to route data to surface buoys. The proposed routing protocols are validated via extensive simulations.
-------------------------------------

10130110_178 - 0.951986422805 - technology_and_computing
[mobile, privacy, reference, preservation, auditing, bibliographical, service, application, ubiquitous]

Privacy preservation and auditing for ubiquitous mobile sensing applications and services
Includes bibliographical references (p. 77-79).
-------------------------------------

10135267_183 - 0.999786822288 - technology_and_computing
[system, spin, state, quantum, finite, magnetism, physics, size, pairing, cooper]

Exotic quantum magnetism and superfluidity in optical lattices
The progress of ultracold atoms renders numerous possibilities to investigate exotic magnetism and superfluidity, which are rarely observed in solid state systems. In this thesis, we will introduce two novel physical descriptions: "frustrated Cooper pairing" and "large-hyperfine spin physics". Geometric frustration in quantum magnetism refers to which magnetic interactions on different bonds cannot be simultaneously minimized, and usually Cooper pairing favors uniform phases among different lattice sites. Here, we introduce "frustration" in Cooper pairing in a fermionic p-orbital model. By mean- field calculations, we show that the system exhibits behavior analogous to frustrated magnetism, and an unconventional supersolid state with the f-wave symmetry. Next, we introduce large-spin physics. In usual condensed matter systems, large spin is not intriguing because large values of spin suppress quantum fluctuations. In contrast, in ultracold fermion systems, large-hyperfine spin enhances quantum fluctuations and brings exotic quantum magnetism. Here the simplest large-spin fermionic system, a spin-3/2 exchange system is proposed, which can be characterized by an Sp(4)/SO(5) symmetry. In one dimension, the ground states exhibit either a dimerized state with a finite spin gap or a gapless spin liquid state by means of the density matrix renormalization group method. In the latter case, the spin-spin correlation functions are identified to have 4-site periodicities, which behaves similarly to the SU(4) chain. In two dimension, we infer that there exist three competing phases: Neel ordering, columnar dimerization and 2 x 2 plaquette formation, in the thermodynamic limit by exact diagonalization calculation on small sizes. Finally we perform the projector Quantum Monte Carlo method to study another large-spin system: the half-filled SU(N) Hubbard model. We show that at half-filling there is no sign problem such that our simulations are accurate. By finite size scaling, it is clearly found that the magnetic Neel ordering can exist not only for N = 2 but also in the N = 4 case at strong interactions. For N ̲> 6 or N = 4 at small U, the numerical results do not have any prominent signal that the long-range ordering exists in the thermodynamic limit. Due to strong finite size effects and finite numerical accuracy, however, we are unable to make any conclusion to identify the physics in the regimes
-------------------------------------

10133968_183 - 0.991768647008 - technology_and_computing
[waveguide, silicon, device, photonic, circuit]

Modeling and characterization of strongly coupled silicon- on-insulator nanophotonic devices
Silicon photonics is technologically attractive because of the possibility of monolithically integrating multi- element photonic waveguide circuits with complex electronic circuits. To reduce the footprint of the photonic components, it is possible to fabricate strongly- coupled waveguides and resonators, e.g., with sub-100 nm separation gaps. The most insightful design tool used for photonic devices, coupled mode theory (CMT), is considered suspect for high-index contrast strongly coupled waveguides. Using a numerically assisted coupled mode theory (NA-CMT) developed for arrayed waveguides, it was shown how one may modify the basis parameters within CMT to calculate more accurate modal profiles and more accurate estimates of the value and the wavelength dependency (i.e., dispersion) of coupling coefficients. Traditional CMT inaccurately predicts both the field peak locations and the exponential decay rates of the field envelopes in the cladding regions. Examples of strongly- coupled silicon photonic devices based on waveguides and couplers include giant birefringence multi-slot waveguides, and large-bandwidth coupled-resonator optical waveguides (CROWS) consisting of several hundred coupled silicon microring resonators. Numerical techniques will be reported for accurately simulating the transmission properties of strongly coupled arrayed waveguides and disordered CROWs in excellent agreement with experimental measurements on fabricated devices. Experimental methods were developed for the accurate measurement of transmitted intensity and group delay of silicon nanophotonic waveguides and multi-resonator circuits including CROWs and side-coupled integrated spaced sequence of resonators (SCISSORS). The role of external amplification in reliably measuring waveguide transmission using the method of swept wavelength interferometry was studied in detail. Also, a technique of swept-wavelength infrared imaging was developed and applied for quantitative diagnostics of multi-resonator circuits which need not have accessible drop ports on every device
-------------------------------------

10134331_183 - 0.999922119468 - technology_and_computing
[datum, environment, multi-dimensional, display, technique, analysis]

Building a foundation for human centric multi-dimensional data analysis
This dissertation introduces a foundation for human centric, large-scale, multi-dimensional data analysis. This research enables collaborative workspaces by utilizing ultra-large, high-resolution display environments, distributed rendering techniques, and new interface modalities. Contributions include interactive visualization of ultra-large layered data sets, real-time distributed large-scale data acquisition, scalable distributed approaches for video playback in tiled display environments, natural exploratory techniques for multi- dimensional data, and multi-user interface technologies for distributed display environments. Presented is a technique for the interactive visualization and interrogation of multi-dimensional gigapixel imagery, allowing several users to simultaneously compare and contrast complex data layers in a collaborative environment. This system is augmented through a distributed data gathering and visualization component, which allows researchers to pull, construct, and interrogate geospatial information from remote servers. Multimedia content can also be configured interactively, and viewed in many side-by-side comparisons using various color and temporal filters. Techniques also allow for the scalable playback of video content through a distributed architecture. Additionally, multi-touch devices allow for hands-on analysis of massive, multi-dimensional data. The presented research introduces a set of natural metaphors, which allow for rapid analysis of global and local characteristics in the data set. The interface modalities can also be used for volumetric data, where position, gesture and pressure information are used for voxel density and depth specific operations. Finally, the combination of multi-touch devices and tiled display environments is presented, enabling multi-user collaborative environments
-------------------------------------

10136080_183 - 0.999186832213 - technology_and_computing
[model, time, datum, partner, subject, behavior, cluster, longitudinal, dpm, feature]

Hierarchical and semi-parametric Bayesian models for the study of longitudinal HIV behavior and tuberculosis incidence data
We propose and discuss two distinct and separate innovative Bayesian models.  In the first model, we propose a replacement for standard statistical methodologies for longitudinal sexual behavior data.  HIV intervention trials generally collect sexual behavior data repeatedly over time and involve multiple outcomes including the number of partners which are nested in subjects and the number of protected and unprotected sex acts with each partner which are inherently nested within partners.  The data is further complicated by characteristics of both partners and acts.   Partners can be HIV$^+$ or HIV$^-$ while sex acts can be protected or unprotected.  Properly modeling these outcomes and distinguishing these characteristics is critical.  Here we use a multilevel multivariate Bayesian model for modeling sexual behavior outcomes.  The proposed model accounts for the full complexity of sexual behavior allowing for simultaneous modeling of the number of partners and the number of sex acts with each partner, differentiation of behavior by partner serostatus, accounting for study eligibility criterions associated with the outcome of interest, and correlations between observations with the same subject, observations with the same partner, and observations across time.  We further show that the proposed model can be used to quantify and draw inference on seroadaptive behaviors.  Seroadaptive behaviors describe behaviors that vary based on the HIV status of partners with the goal of reducing the risk of transmission.  The model is used to analyze data from the Healthy Living Project.  In the second half of this thesis, we explore a novel extension to the Dirichlet process mixture (DPM) model to accommodate longitudinal data.  Longitudinal data is characterized by two features.  First, the data are a function of time implying dependence between sampling densities across time.  Second, the \emph{same} subjects are repeatedly measured over time.  The standard DPM model is a nonparametric Bayesian model that naturally clusters similar observations together and assigns a single value to each cluster.  It can be used to model an unknown density but addresses neither of these two features in longitudinal data.  A number of current extensions of the DPM model can accommodate dependent distributions which could be used to model the sampling distributions at each time point addressing the first feature.  However, assumptions in these extensions imply these models do not take advantage of the second feature of longitudinal data where the \emph{same} subjects are followed over time.  To account for both features, we propose the cluster memory Dirichlet process mixture (cmDPM) model extending the DPM model to properly accommodate longitudinal data.  In the cmDPM model, subjects are modeled as a DPM model at baseline.  Cluster assignments at future time points depend on where the subject was previously clustered.  Each subject may retain their cluster from the previous time point with some nonzero probability.  This implies that at later times, subjects are no longer exchangeable and their observed values depend on their previous clustering history.  Clusters that are retained over time evolve through a time dependent process.  The cmDPM model extends the DPM to use both the information of where the subject was previously clustered and the value assigned to that cluster to model subject data at the current time point.
-------------------------------------

10134903_183 - 0.99999790153 - technology_and_computing
[node, cost, repair, storage, amount, datum]

Distributed storage with communication costs
Distributed storage systems provide reliable storage of data by dispersing redundancy across multiple nodes. As individual nodes are unreliable this protects the integrity of the data against the failure of nodes. In order to maintain this reliability new nodes must be introduced into the system whenever nodes are lost which restore the redundancy. This process involves having a new node download information from remaining nodes and is known as the repair problem. In this thesis, we consider networks with communication costs associated to each link and explore means to minimize the cost of performing these repairs. We do this by considering a generalized method of repair wherein the amount of information downloaded to a new node varies amongst the other nodes in the network. We find that when nodes store the minimum amount of data that the minimum cost can be achieved by quasi-uniform repair, where the same amount of data is downloaded from each node communicated with. We also consider systems with the additional freedom that the amount of storage is allowed to vary from node to node and look at repair cost minimization there as well
-------------------------------------

10135095_183 - 0.908438833362 - technology_and_computing
[core, power, application, budget, transistor, system]

Configurable energy-efficient co-processors to scale the utilization wall
Transistor density continues to increase exponentially, but power dissipation per transistor improves only slightly with each generation of Moore's law. Given the constant chip-level power budgets, this exponentially decreases the fraction of the transistors that can be active simultaneously with each technology generation. Hence, while the area budget continues to increase exponentially, the power budget has become a first-order design constraint in current processors. In this regime, utilizing transistors to design specialized cores that optimize energy-per-computation becomes an effective approach to improve the system performance. To pursue this goal, this thesis focuses on specialized processors that reduce energy and energy-delay for general purpose computing. The focus on energy makes these specialized cores an excellent match for many of the commonly used programs that would be poor candidates for SIMD-style hardware acceleration (e.g. compression, scheduling). However, there are many challenges, such as lack of flexibility and limited computational power, that limit how effective these specialized cores are at targeting general purpose computing. Without addressing these concerns, these specialized cores would be limited in the scope of applications that they can effectively target. This thesis addresses these various challenges involved in making specialization a viable approach to optimize general-purpose computing. To this end, this thesis proposes Patchable Conservation Cores which are flexible, energy-efficient co-processors that contain the ability to be patched, enabling them to remain useful across versions of their target application. To demonstrate the effectiveness of these conservation cores in targeting a system workload, this thesis utilizes them to design a mobile application processor targeting the Android software stack. The results show that these specialized cores can cover significant fraction of the system execution while staying within a modest area budget. To further increase the fraction of the system execution that these specialized cores cover, this thesis proposes QASICs, specialized co-processors capable of executing multiple general-purpose computations. QASIC design flow exploits the similar code patterns present within and across applications to reduce redundancy across specialized cores as well as improve their computational power
-------------------------------------

10138342_183 - 0.999877203751 - technology_and_computing
[simulation, program, building, datum, environment, software, system, bcvtb, co-simulation, different]

Co-Simulation of Building Energy and Control Systems with the Building Controls Virtual Test Bed
This paper describes the implementation of the Building Controls Virtual Test Bed (BCVTB). The BCVTB is a software environment that allows connecting different simulation programs to
exchange data during the time integration, and that allows conducting hardware in the loop simulation. The software architecture is a modular design based on Ptolemy II, a software environment for
design and analysis of heterogeneous systems. Ptolemy II provides a graphical model building environment, synchronizes the exchanged data and visualizes the system evolution during run-time. The BCVTB
provides additions to Ptolemy II that allow the run-time coupling of different simulation programs for data exchange, including EnergyPlus, MATLAB, Simulink and the Modelica modeling and simulation
environment Dymola. The additions also allow executing system commands, such as a script that executes a Radiance simulation. In this paper, the software architecture is presented and the mathematical
model used to implement the co-simulation is discussed. The simulation program interface that the BCVTB provides is explained. The paper concludes by presenting applications in which different state
of the art simulation programs are linked for run-time data exchange. This link allows the use of the simulation program that is best suited for the particular problem to model building heat transfer,
HVAC system dynamics and control algorithms, and to compute a solution to the coupled problem using co-simulation.
-------------------------------------

10136577_183 - 0.98627774803 - technology_and_computing
[graph, value, layout]

Methods for Visually Exploring Large and Complex Networks
Most graph layout algorithms strive to present an uncluttered view of the graphthat reflects the structural relationship between nodes and edges comprising thegraph. Very few focus on providing a layout based on either node or edgeattribute values. This thesis presents a method that can reflectstructural information, be influenced primarily by attribute values of graphelements, or some combination of both. This is achieved using a force-basedgraph layout strategy and force transfer functions--a flexible graph layoutspecification that alters forces depending on attribute values or structuralinformation. An immediate benefit of this flexibility is theability to perform visual clustering via the resulting graph layouts.As graphs get larger and more complex, the flexibility for exploring differentrelational properties of graph elements will allow us to understand them better.As an example, this technique is used to group left and right blogs as well as detect outliersin a political blog dataset.
-------------------------------------

10133888_183 - 0.999261897668 - technology_and_computing
[policy, algorithm, throughput, optimal, delay, performance, orcd, backpressure, commodity, class]

Throughput optimal routing in wireless Ad-hoc networks
This dissertation considers the problem of routing multi- commodity data over a multi-hop wireless ad-hoc network. The few well-known throughput optimal routing algorithms in literature are all based on backpressure principle, which shows poor delay performance under many network topologies and traffic conditions. In contrast, heuristic routing algorithms which incorporated information of closeness to destination are either not throughout optimal or the thoughts optimality was unknown (e.g. opportunistic routing policy with congestion diversity aka. ORCD). The primary goal of this dissertation is to find routing policies beyond backpressure type that not only ensure throughput optimality but also maintain satisfactory average delay performance. In the single commodity scenario, by considering a class of continuous, differentiable, and piece-wise quadratic Lyapunov functions, we propose a large class of throughput optimal routing policies called K policies, which include both backpressure algorithm and ORCD as special cases. The proposed class of Lyapunov functions allow the routing policies to control the traffic along short paths for a large portion of state-space while ensuring a negative expected drift, hence, enabling the design of routing policies with much improved delay performances. We then extend K-policy to multi-commodity case by considering nonquadric Lyapunov functions. A multi-commodity version of ORCD algorithm is proposed based on the generalized K- policy and is shown to be throughput optimal under mild conditions. Interestingly, the algorithm selects the commodity that has the maximum backlogs ratio instead of the maximum difference of backlogs as in the original backpressure algorithm. Simulation results show that the proposed algorithms have better delay performances in all scenarios we considered
-------------------------------------

10134366_183 - 0.999995721933 - technology_and_computing
[network, system, time, model, intrusion, method, continuous, ctbn, attack, detection]

A Continuous Time Bayesian Network Approach for Intrusion Detection
Network attacks on computers have become a fact of life for network administrators. Detecting attacks accurately is important to limit their scope and destruction. Intrusion detection systems (IDSs) fall into two high-level categories: network-based systems (NIDS) that monitor network behaviors, and host-based systems (HIDS) that monitor system calls. In this work, we present a general technique for both systems.We consider the problem of detecting intrusions at the host level. We use anomaly detection, which identifies patterns not conforming to a historic norm. Our approach does not require expensive labeling or prior exposure to the attack type. In both types of systems, the rates of change vary dramatically over time (due to burstiness) and over components (due to service difference). To efficiently model such systems, we use continuous time Bayesian networks (CTBNs) and avoid specifying a fixed time interval. We build generative models from historic non-attack data, and flag future event sequences whose likelihood under this norm is below a threshold.As a NIDS, our method differs from previous approaches in explicitly modeling temporal dependencies in the network traffic. Our models are therefore more sensitive to subtle variations in the sequences of network events. We first construct a factored CTBN model for thenetwork packet traces. We present two simple extensions to CTBNs that allow for instantaneous events that do not result in state changes, and simultaneous transitions of two variables. We then extend this model to a connected one. We construct it in a hierarchical way and use Rao Blackwellized particle filtering for inference. We illustrate the power of our method through experiments on detecting real worms and identifying hosts on two publicly available network traces, the MAWI dataset and the LBNL dataset.For HIDS, we develop a novel learning method to deal with the finite resolution of system log file time stamps, without losing the benefits of our continuous time model. We demonstrate the method by detecting intrusions in the DARPA 1998 BSM dataset.
-------------------------------------

10136160_183 - 0.999998098628 - technology_and_computing
[coprocessor, application, domain-specific, acceleration, fpga, gpu, example, performance, domain, medical]

Coprocessor Acceleration for Domain-Specific Computing
There is a growing trend to use coprocessors to offload and accelerate domain-specific applications in order to obtain  significant performance improvement and energy/power reductions. Two important coprocessor components in the heterogeneous system are the GPU and FPGA. GPU (graphics processing unit) is increasingly used as a data-parallel coprocessor for general computations. The newest GPU has a much larger number of cores (compared to CPU) and very high peak FLOPS. FPGA (field programmable gate array), on the other hand, allows users to customize, at fine-grain level, the computational data path and memory hierarchy according to the exact need of the applications. FPGA excels in integer operations and bit-level operations. The thesis starts with several coprocessor acceleration examples for our focus application domains: the first domain is on VLSICAD algorithms and the second is on computational medical imaging. We detail application acceleration examples in the domains including lithography simulation for IC manufacturing, medical image reconstruction using compressive sensing, and  medical image registration using fluid models. Both GPU-accelerated versions and FPGA-accelerated versions have been implemented. Based on these implementations, we then analyze the performance and energy trade-offs, the interaction between the diverse application requirements and a spectrum of hardware systems, and how those domain-specific coprocessor acceleration case studies further bring us insights for domain-specific architecture innovations. In the end, we showcase an example for collaborative execution on the heterogeneous platform. Different scheduling policies are needed to optimize performance or energy. The thesis concludes as we present  reusable architecture templates and  realizations  for futuristic accelerator-rich CMPs.
-------------------------------------

10131991_183 - 0.762635435204 - technology_and_computing
[rail, report, alignment, corridor, angeles, analysis, san, revenue, service, high-speed]

Revenue and Ridership Potential for a High-Speed Rail Service in the San Francisco/Sacramento-Los Angeles Corridor
This study documents an investigation into the potential for high-speed rail (HSR) service in California between Los Angeles and San Francisco/Sacramento via a new alignment in the Central Valley. An earlier report, IURD Working Paper 564, reviewed technology and alignment issues, recommended a steel-wheel-on-steel rail option, and identified a preferred corridor for further analysis. This report presents the results of further analysis, including demand and revenue projections.
-------------------------------------

10138961_183 - 0.867399136006 - technology_and_computing
[laser, ao, system, plate, turbulence, part, simulation, adaptive, optics, measurement]

Advancing Adaptive Optics Technology:  Laboratory Turbulence Simulation and Optimization of Laser Guide Stars
Since Galileo's first telescope some 400 years ago, astronomers have been building ever-larger instruments.  Yet only within the last two decades has it become possible to realize the potential angular resolutions of large ground-based telescopes, by using adaptive optics (AO) technology to counter the blurring effects of Earth's atmosphere. And only within the past decade have the development of laser guide stars (LGS) extended AO capabilities to observe science targets nearly anywhere in the sky.  Improving turbulence simulation strategies and LGS are the two main topics of my research. In the first part of this thesis, I report on the development of a technique for manufacturing phase plates for simulating atmospheric turbulence in the laboratory.  The process involves strategic application of clear acrylic paint onto a transparent substrate.  Results of interferometric characterization of the plates are described and compared to Kolmogorov statistics.  The range of r0 (Fried's parameter) achieved thus far is 0.2 - 1.2 mm at 650 nm measurement wavelength, with a Kolmogorov power law.  These plates proved valuable at the Laboratory for Adaptive Optics at University of California, Santa Cruz, where they have been used in the Multi-Conjugate Adaptive Optics testbed, during integration and testing of the Gemini Planet Imager, and as part of the calibration system of the on-sky AO testbed named ViLLaGEs (Visible Light Laser Guidestar Experiments).  I present a comparison of measurements taken by ViLLaGEs of the power spectrum of a plate and the real sky turbulence. The plate is demonstrated to follow Kolmogorov theory well, while the sky power spectrum does so in a third of the data.  This method of fabricating phase plates has been established as an effective and low-cost means of creating simulated turbulence. Due to the demand for such devices, they are now being distributed to other members of the AO community. The second topic of this thesis pertains to understanding and optimizing the laser beacons used to bring AO correction to parts of the sky that lack a naturally bright light source for measuring atmospheric distortion.  Long pulse length laser guide stars (LGS) that use fluorescence from the D2 transition in mesospheric sodium are valuable both due to their high altitude, and because they permit Rayleigh blanking and fratricide avoidance in multiple LGS systems.  Bloch equation simulations of sodium-light interactions in Mathematica show that certain spectral formats and pulse lengths (on the order of 30 &#956;s), with high duty cycles (20-50%), should be able to achieve photon returns within 10% of what is seen from continuous wave (CW) excitation.  Utilizing this recently developed code (called LGSBloch), I investigated the time dependent characteristics of sodium fluorescence.  I then identified the optimal format for the new LGS that will be part of the upgrade to the AO system on the Shane 3 meter telescope at the Lick Observatory.  I discuss these results, along with their general applicability to other LGS systems, and provide a brief description of the potential benefits of uplink correction.  Predictions from the LGSBloch simulation package are compared to data from currently operating LGS systems.  For a CW LGS, the return flux measurements and theory show reasonable agreement, but for short pulse lasers, such as those at the Lick and Keck Observatories, the code seems to be overestimating the data by a factor of 2 - 3.  Several tactics to explicate this discrepancy are explored, such as verifying parameters involved in the measurements and including greater detail in the modeling.  Although these efforts were unsuccessful at removing the discrepancy, they illuminated other facets of the problem that deserve further consideration.  Use of the sophisticated LGSBloch model has allowed detailed study of the evolution of the energy level populations and other physical effects (e.g. Larmor precession, atomic recoil, and collisions).  This has determined formats that will have maximal coupling efficiency of the laser light to the sodium atoms in order to achieve the highest possible return signal per Watt of output power.  These quantitative findings allow the astronomical AO community to make rational, physics-based choices of which high-power (and unavoidably high-cost) lasers to procure for implementation in future LGS systems.
-------------------------------------

10158_7 - 0.781119209089 - technology_and_computing
[n-tuple, performance, result, method]

Benchmarking the n-tuple classifier with statlog dataset
The n-tuple recognition method was tested on 11 large real-world data sets and its performance compared to 23 other classification algorithms. On 7 of these, the results show no systematic performance gap between the n-tuple method and the others. Evidence was found to support a possible explanation for why the n-tuple method yields poor results for certain datasets. Preliminary empirical results of a study of the confidence interval (the difference between the two highest scores) are also reported. These suggest a counter-intuitive correlation between the confidence interval distribution and the overall classification performance of the system
-------------------------------------

10136488_183 - 0.999718028763 - technology_and_computing
[module, normalization, mirna, mrna, network, system, analysis, eigengene, method, luminex]

Challenges in High-throughput Data Analysis: Proteomic Data Pre-processing and Network Methods for Integrating Multiple Data Types
1)	Proteomic Data Pre-processing: Quantification and Normalization of Luminex Assay System      High through-put genomic and proteomic technologies allow rapid analysis of molecular targets of thousands of genes at a time, either at the DNA, RNA or protein level. In these type of experiments variations in expression measurements can occur from a variety of sources. Our goal was to examine measurement and normalization techniques to reduce the experimental variation in data derived from a bead-based multiplex Luminex assay system which allows simultaneous measurements of proteins. Normalization for the Luminex assay system requires a fundamentally different approach than the case of traditional microarrays. In the Luminex assay system, each experimental unit is a plate and each plate has results for multiple subjects and analytes. We quantified performance among different measurement systems (fluorescent intensity, background in fluorescent intensity, and observed concentration) in both high and standard scanning systems. Various normalization techniques (scale normalization, quantile normalization, lowess curve normalization) were adapted to the Luminex data scenario and their performance was compared in two datasets.       We used the coefficient of variation across plates to compare the performance of normalization methods.  Median and Lowess normalizations appeared to result in reducing plate- to-plate variation the most.  Quantile normalization does not appear to work well for these datasets. Our results suggest that simple normalizations such as scale and lowess curve normalizations perform better than complex methods such as quantile normalization.  Complex methods may add noise and bias to the normalized adjustment when the assumptions are not met. 2)	Integration of microRNA and mRNA by Weighted Gene Co-expression Network AnalysisWe focus on the step-by-step network construction and module detection of mRNAs by weighted gene co-expression network analysis (WGCNA), followed by identifying the strong correlation between miRNA and module eigengenes. We then evaluate whether the predicted mRNA targets are differentially present between a given module and other modules by using the Fisher's exact test. We retained miRNAs who are significant in the fisher exact test, and are strongly correlated with eigengenes in a module.Next we relate modules to disease status by using eigengene network methodology, we found that 11 out of 13 modules are significantly related with disease status. Enrichment analyses by DAVID software are implemented for the 11 modules.  We also run step-by-step network construction and module detection of miRNAs and found 6  modules. We used LASSO regression to explore the relationship between miRNA and mRNAs. The predictors are module eigengene of miRNA and the outcome is the eigengene from each mRNA module. We found that 1 miRNA "hsa_miR_25" is significantly anti-correlated with mRNA Magenta module. "hsa_miR_25" belongs to the miRNA module "blue" that is also predictive to Magenta mRNA module through LASSO regression. Its putative mRNA targets are found and integrated from the renal dataset.   In conclusion, the weighted co-expression network analysis provides a novel integrative view of miRNA and their putative genes. It also greatly alleviates the multiple testing problems that plague standard gene-centric methods.
-------------------------------------

10138276_183 - 0.999988055454 - technology_and_computing
[system, electricity, grid, paradigm, future, distribution, current, structure, generation, power]

Future Roles of Milli-, Micro-, and Nano- Grids
Although it has slowed considerably, consumption of electricity continues to grow in developed economies. Further, there are some unknowns which might accelerate this growth, such as electrification of vehicle fleets and geothermal heat pump space and water heating. Most analysts anticipate that distributed energy resources (DER) will provide a large share of the expanded generation capacity required to meet this seemingly inexorably increasing electricity demand. Further, given the urgency of tackling the climate change problem, most of the added assets must be carbonfree renewables or nuclear, end-use efficiency improvements, or highly efficient fossil-fired technologies. In developed economies worldwide, the current power delivery paradigm has been in place for more than a century, i.e. since the emergence of polyphase AC systems around the turn of the last century. A key feature of this structure is that, in principle, universal service is delivered at a consistent level of power quality and reliability (PQR) throughout large regions. This paper describes a future possible structure for the electricity generation and delivery system that leaves the existing high voltage meshed grid paradigm in place, but involves radical reorganization of parts of the distribution network and customer sites. Managing a much more diverse dispersed system poses major challenges to the current centralized grid paradigm, particularly since many of these assets are small to tiny by macrogrid standards and they may ultimately number in the millions. They are also not ones that centralized control can rely upon to function in traditionally dependable ways, e.g. renewable generation can be highly variable and changes in output of generators are not independent. Although most involved in the industry agree that a paradigm shift is both necessary and desirable to manage the new system, the nature of the future system remains quite unclear. In the possible structure described here, the traditional grid, or macrogrid, remains similar at the high voltage meshed level. Three new entities are added more locally: community grids or milligrids that operate a segment of the existing distribution system, microgrids which are akin to current customer sites but which have automonous control, and nanogrids, such as telecom or Ethernet networks that currently distribute power to many low-power devices. The latter exist currently in the local electrical systems but are not typically considered a part of the traditional electricity supply system. Because all these new entities exhibit some localized control, providing appropriate local heterogeneous PQR becomes a possibility. These new grid concepts enable a more "bottom-up" approach to electricity distribution, in contrast to the historic "top-down" model. The future will almost certainly include a mix of the two, but the balance among them and the interface (if any) between them is unclear.
-------------------------------------

10133773_183 - 0.862435588117 - technology_and_computing
[distance, method, classification]

Optimizing the distance function for nearest neighbors classification
When working with high dimensional data, it is often essential to calculate the difference or "distance" between two points. One of the most common distance functions is the Euclidean distance; while this method is easy to implement, it often works poorly. We explore some of its deficiencies, and discuss how they have been overcome in previous research by taking advantage of statistics of the data and by using a priori information about the problem space. We analyze two disparate methods that improve on Euclidean distance for classification. The first learns a Mahalanobis distance that is optimized on the statistics of the data to improve classification. The second incorporates a priori information using tangent distances to account for transformations that we intuitively know the classification should be invariant to. We combine these methods in a sequential manner that takes advantage of their unique strengths, improving the performance of either method by itself. Our combined method reduces the tangent distance's error on the USPS handwritten digit recognition dataset by 10.2% and the Mahalanobis distance method's error by 44.6%
-------------------------------------

10137340_183 - 0.9946333288 - technology_and_computing
[contact, beam, method, problem, constraint, element]

A Robust Method for Beam-to-Beam contact Problems Based on a Novel Tunneling Constraint
The need for better and more efficient computational methods to model the mechanics of contact has increasingly attracted the interest of researchers in this area over the last two decades. Contact problems--wherein two or more bodies meet through an interface--are extremely challenging from both mathematical and engineering points of view, because of the complex nonlinearities due to moving contact interfaces. Over the years, a significant effort has been devoted to address the resolution of continuum contact using the finite element method through several techniques including penetration functions, mortar methods, and domain decomposition. Nonetheless, a comparable effort to resolve beam contact problems is still lacking in the current literature.	Motivated by a number of deficiencies encountered in existing models for beam contact, this dissertation concerns the finite element modeling of contact among structural beams of <italic>zero thickness</italic> undergoing large deformations in space. The resulting methods and algorithms provide an unprecedented range of applicability in simulating not only contact of thin structural beams, but also collisions of cables (<italic>e.g.</italic>, knot tying and surgical thread simulations), and dislocation dynamics, among others.	The prevailing contact modeling techniques rely on a <italic>gap penetration constraint</italic> that is enforced by a <italic>penalty method</italic> to resolve overlapping between two or more contacting beams. Early attempts to resolve this problem, however, have displayed two major drawbacks: not only would the said constraint function be not extendable to planar beam contact problems, but also it would break down when dealing with zero thickness structural elements (<italic>e.g.</italic>, thin beams, cables, and ropes). It was also observed that the existing formulations are difficult to implement and, often, computationally inefficient. Moreover, the use of the penalty method, which dominates most of the existing literature available to date, leads to failures in preserving the exact resolution of contact and, sometimes, results in ill conditioned problems. Especially within a large scale system involving multiple contacting bodies, if the penalty approximation is too crude, then the simulations may predict non physical outcomes.	In this dissertation, a robust approach that is based on a novel contact constraint is proposed. This approach allows a simple and unified treatment of possible beam contact scenarios in two  and three dimensional problems involving large deformations; and is based on a <italic>Lagrange multiplier method</italic> to exactly enforce the contact constraint across the interfaces between beam element pairs. Simply based on the calculation of an oriented volume/area, the proposed approach possesses the ability to resolve any contact scenario exactly and efficiently, while being relatively easy to implement and more widely applicable than the few beam contact formulations available to date.	The primary focus of this work is the inner workings of the novel beam contact formulation proposed and, then, the delineation of the details of its numerical implementation in a finite element method setting. Application of the new approach is demonstrated through several benchmark problems, where specific consideration is given to the assessment of its robustness. The verification and assessment studies indicate that the proposed model can successfully resolve beam to beam contact with high fidelity for a broad spectrum of scenarios, wherein existing methods usually fail.	The lessons learned from this study may be applied to resolving line contact in a wide range of areas including structural mechanics, textile mechanics, and dislocation dynamics. They may serve as a useful tool for developing efficient and accurate numerical models to simulate anything from progressive collapse of structures under extreme event scenarios and crushing of foams, to modeling of knots or surgical threads, just to name a few. An incidental aim of this work has been to make the mechanics of beam contact a more accessible topic of learning.
-------------------------------------

10135463_183 - 0.999982184952 - technology_and_computing
[application, multicore, core, thread, technique, architecture, performance, hardware]

Efficient use of execution resources in multicore processor architectures
As the microprocessor industry embraces multicore architectures, inherently parallel applications benefit directly as they easily transform into sets of homogeneous parallel threads. However, many applications do not t this model. These applications include legacy binaries compiled for a single thread of execution and inherently serial applications. The inability of these two kinds of applications to exploit multicore architectures has created a crisis for the microprocessor industry : customers have come to expect significant performance improvements in all of their application every processor generation, but recent multicore architectures have failed to meet those expectations for many applications. This dissertation explores ways in which these applications can run efficiently on multi core platforms. The performance of legacy binaries compiled for a single thread of execution can be improved through automatic parallelization. We introduce a new technique to automatically parallelize binaries as they are executing. The parallelization technique leverages the benefits of hardware transactional memory, a synchronization mechanism enabling optimistic concurrency. Our technique exploits this to parallelize code that a traditional parallelizing compiler would be unable to transform due to potential memory aliasing. Applications with fundamentally serial code can benefits from core customization. The more heterogeneous the cores are, the more likely that a given application will nd a core on which it runs efficiently. We investigate two forms of heterogeneity : that created on homogeneous hardware by unbalanced resource assignment, and heterogeneity created by hardware asymmetry. We first consider a homogeneous multicore system composed of multithreading cores. Often the best schedules on such a system are unbalanced. We propose a set of novel scheduling algorithms that consider unbalanced schedules to nd good application-to-core assignments. We consider objective functions of both performance and energy. We also explore how applications can benefit from diverse Isms by considering heterogeneous-ISA multicore systems. We propose a new technique to rapidly migrate a thread among cores of different Isms, allowing applications to take advantage of hardware heterogeneity for performance gain or energy savings
-------------------------------------

10131529_183 - 0.999640949941 - technology_and_computing
[service, resource, multiple, network, throughput]

Throughput in multiple service, multiple resource communication networks
Communication networks that integrate multiple services using multiple resources are considered. In particular, the authors pose resource allocation problems, present a sensitivity analysis, and provide a glimpse of the possible behavior of such networks. The simplest discipline is assumed: a service request is accepted if the necessary resources are available; otherwise it is rejected. Two results are obtained. The first gives the sensitivity of throughput of service requests of type i with respect to offered traffic and service rates of type j. The second result is that the set of vectors of achievable throughput rates is a convex polyhedron given by an explicit set of linear inequalities.
-------------------------------------

10137227_183 - 0.999912523405 - technology_and_computing
[adc, pipeline, power, circuit, design, speed, specification]

Pipeline ADC Design Methodology
Demand for high-performance analog-to-digital converter (ADC) integrated circuits (ICs) with optimal combined specifications of resolution, sampling rate and power consumption becomes dominant due to emerging applications in wireless communications, broad band transceivers, digital-intermediate frequency (IF) receivers and countless of digital devices. This research is dedicated to develop a pipeline ADC design methodology with minimum power dissipation, while keeping relatively high speed and high resolution. Pipeline ADC is a mixed-signal system, which consists of sample and hold amplifier (SHA), sub-ADC, multiplying digital-to-analog Converter (MDAC) and bandgap voltage reference, comparator, switch-capacitor circuits and biasing circuits. This project set up a pipeline ADC design flow. It links all the specifications between the system levels and circuit levels together. With this design flow, if the overall ADC specifications are given, such as resolution, sampling rate, voltage supply and input signal range, all the sub-block circuitry specifications are achieved.This paper studies all the sub-block circuits of pipeline ADC first, and then come up with all the constraints and limitations for all the circuitry in term of speed and noises. Then a system level speed and power trade off consideration is explored in order to optimize the overall performance. As verification of the proposed design methodology, a 10-bit 40MHz pipeline analog-to-digital converter prototype is developed in commercial TSMC 90nm CMOS technology: using op-amp sharing, dynamic biasing methods, it works in two modes: pipelined ADCs for high speed, cyclic ADC for low speed (only last stage runs, other stages are power off to save power). For pipeline mode, the total power consumption decrease as the sampling frequency drops.
-------------------------------------

10134944_183 - 0.999993399361 - technology_and_computing
[algorithm, packing, optimization, sphere, derivative-free, chapter, application, labdog, convergence]

Applications on multi-dimensional sphere packings : derivative-free optimization
The field of n-dimensional sphere packings is elegant and mature in its mathematic development and characterization. However, practical application of this powerful body of work is lacking. The line of research presented in this work explores the application of sphere packings to the field of derivative-free optimization. Chapter 2 reviews the essential results available in this field, then extends these results by: (a) assembling a catalog of key properties of the principle dense and rare sphere packings and nets available, including hundreds of values not previously known; (b) introducing and characterizing several new families of regular rare sphere packings and nets; and (c) developing a new algorithm for efficient solution of discrete Thompson problems, restricted to nearest-neighbor points. These results are leveraged heavily in the applications addressed in Chapters 3 and 4. In particular, Chapter 3 builds from this presentation to develop a new algorithm for Lattice-Based Derivative-free Optimization via Global Surrogates (LABDOGS), leveraging dense sphere packings as an alternative to Cartesian grids to coordinate derivative-free searches. The LABDOGS algorithm provides a highly efficient, globally convergent optimization algorithm that requires nothing more than a definition of a feasible domain and a cost function handle. The LABDOGS algorithm demonstrates superior performance and convergence rates to its Cartesian-based competitors. Chapter 4 builds from the material of Chapter 2 and 3 to develop a highly efficient locally convergent derivative-free optimization algorithm called L-MADS, which builds from and improves upon the Mesh Adaptive Direct Search (MADS) class of optimization algorithms. The L-MADS algorithm offers an alternative to the Successive Polling substep of LABDOGS, providing a locally convergent pattern search algorithm that, unlike SP, offers good convergence behavior when challenging constraints on the feasible region are encountered. L-MADS inherits all the convergence characteristics of the best available MADS algorithms, while significantly improving convergence rates
-------------------------------------

10136202_183 - 0.824033229173 - technology_and_computing
[fabric, model, thread, appearance, light]

A measurement-based appearance model for fabrics
Accurate representations of real-world materials are a crucial prerequisite for realistic image synthesis. This thesis presents a method for rendering fabric based on a low-level simulation of light scattering from threads. Making use of reflectance acquisition techniques, we study the reflectance characteristics of a variety of threads extracted from real-world fabrics. We introduce a model for light scattering from threads which simulates both the surface and subsurface transport of light. The model's accuracy is assessed against physical measurements. In order to generate an image of fabric, we propose a weave pattern and thread curve encoding that enables us to mimic the structure of a wide variety of fabrics. We apply this appearance model in a path tracing framework to realistically reproduce the appearance of fabrics
-------------------------------------

10131389_183 - 0.999105758649 - technology_and_computing
[communication, transportation, telecommunications, mode, travel, relationship]

A Typology of Relationships Between Telecommunications And Transportation
This paper defines the relationship between telecommunications and transportation, by expanding on linkages already identified in the literature, by identifying additional relationships, and by putting these relationships into a robust conceptual framework. There are conceptual, physical, analytical, and regulatory parallels between telecommunications and transportation. Telecommunications affects the demand for, and supply of, transportation -- and vice versa. In the broadest sense, all communication requires transportation in order to occur: transportation either of people, or objects or of electronic impulses. In other words, communication takes place via one or more of those three modes. It is suggested that "communication breeds communication." That is, the easier it is to communicate (whether through travel or telecommunications), or the more that one or another form of communication takes place, the more that communication as a whole is stimulated. The relative shares of each of the three modes of communication may vary as one mode partially substitutes for another, but the absolute amounts of communication via each mode are likely to increase. Two empirical studies are summarized, one illustrating that teleconferencing increased travel, the other illustrating that telecommuting decreased travel. Other implications for transportation planning are highlighted.
-------------------------------------

10133879_183 - 0.714679786925 - technology_and_computing
[composite, layered, structure, shpb]

Performance of steel-polymer and ceramic-polymer layered composites and concrete under high strain rate loadings
In this thesis, experimental and numerical techniques are employed to investigate the possibility of improving the impact- and blast-resistance of composite structures. Finite element models are developed to study the dynamic response and large deformation of steel-polyurea bi- layers. A Split Hopkinson Pressure Bar (SHPB) setup is used to perform impact tests on steel-polyurea-steel sandwich structures. The SHPB experiments are simulated by FEM to explore the deformation history, failure, and fracture of these structures. Penetration tests are conducted to assess the ballistic performance of steel- and ceramic-polyurea layered composites. In addition, an approximate solution to the problem of wave propagation in cylindrical layered composites is analytically obtained. A semi-analytical method for dispersion correction of traveling waves in layered cylinders is developed and verified by FEM. The SHPB is also employed to characterize the high strain rate behavior of concrete in compression and tension. A finite element model is developed to address confusions about sample size and friction effects
-------------------------------------

10137064_183 - 0.99515205221 - technology_and_computing
[module, algebra, weyl, current, filtration, category]

Tilting Modules for the Current Algebra Associated to a Simple Lie Algebra
The category of graded level zero representations of current Lie algebra shares many properties with other well--known categories which appear in Lie theory and in algebraic groups in characteristic p.  In my thesis I explore further similarities.  In the first part of my thesis I show that a BGG style reciprocity holds for a category of graded modules for the current algebra.  The role of the standard and co--standard modules are played by the finite--dimensional local Weyl modules and the dual of the infinite--dimensional global Weyl modules respectively.  A key step in proving the reciprocity is defining a filtration on the projective modules.  Another is relating the characters of the local Weyl modules, which are known, to the characters of global Weyl modules.  In the second part, I define the canonical filtration of a graded module for the current algebra. In the case when g is of type A, I show that the  well--known homological condition for a canonical filtration to be a costandard filtration also holds in my situation. Finally, using these results, I construct the indecomposable tilting modules in this category and show that any tilting module is isomorphic to a direct sum of indecomposable tilting modules.
-------------------------------------

10130266_178 - 0.999882989703 - technology_and_computing
[system]

The Morgantown personal rapid transit system : the impact on future PRT systems.
Bibliography: leaves 143-144.
-------------------------------------

10134798_183 - 0.871055567269 - technology_and_computing
[communication, interaction, face-to-face, project, system, deaf]

Augmenting collocated interaction : the design of assistive technology to support face-to- face communication
Effective face-to-face communication with other people presents challenges for many reasons. We may be distracted, fatigued, or emotional and unable to communicate well or remember the details of a conversation. Certain contexts, such as medical and therapeutic interaction, present additional challenges to effective communication due to illness, stress, or disability. This thesis presents research on novel systems to support collocated interaction for populations who have communication challenges related to a hearing, speech, or developmental disability. I describe three research projects that involve understanding face-to-face communication needs, designing novel systems to augment interaction among collocated participants, and evaluating how these systems shape the nature of human-human interaction. First, the SIDES project introduces a cooperative tabletop computer game designed as a social skills therapy tool for children with Asperger's Syndrome, an Autism Spectrum Disorder. Children with AS often have difficulty understanding accepted social conventions, reading facial expressions, interpreting body language, and understanding social protocols. Findings indicate that cooperative tabletop computer games are a motivating and supportive tool for facilitating face-to-face interaction involving this population. Second, the Shared Speech Interface (SSI) project involves the design and evaluation of an application for an interactive multitouch tabletop display that facilitates medical communication between a Deaf patient and a hearing, non-signing physician. SSI provides Deaf individuals with a more private and independent alternative for medical communication. SSI also reshapes communication between the doctor and Deaf patient in important ways. Third, the Write-N-Speak project examines face-to-face communication for individuals with aphasia. Through a year-long field study, I understand the process of speech-language therapy for older adults with aphasia and introduce digital pen technology into this work environment. This project also involves the design and field deployment of Write-N-Speak, a programmable toolkit that allows non-technical end-users to independently create custom interactive paper materials. Through this thesis, I provide a deeper understanding of face-to-face human interaction involving critical user populations, such as children with autism, Deaf people, and older adults with aphasia. I introduce novel prototype systems that support face-to-face interaction among participants with varying abilities and examine how these systems augment collocated interaction
-------------------------------------

10136732_183 - 0.844522337412 - technology_and_computing
[dynamic, race, analysis, concurrency, error, trace, dissertation, program, tool, atomicity]

Dynamic Prediction of Concurrency Errors
Taking advantage of parallel processors often entails using concurrent software, where multiple threads work simultaneously. However, concurrent software suffers from a bevy of concurrency-specific errors such as data races and atomicity violations. Dynamic analysis, based on analyzing the sequence of operations (called a trace) from a source program as that program executes, is a powerful technique for finding concurrency errors in multithreaded programs. Unfortunately, dynamic analyses are confined to analyzing the observed trace.Nonetheless, there are situations where a concurrency error does not manifest on a particular trace although it is intuitively clear that the program that produced that particular trace contains a concurrency error. The central research hypothesis explored by this dissertation is that dynamic analysis can discover concurrency errors that do not manifest on the observed trace.This dissertation introduces a new relation, causally-precedes (CP), that enables precise predictive race detection, with no false positives. A single CP-based race detection run discovers several new races, unexposed by 10 independent runs of a traditional dynamic race detector. To further address dynamic predictive race detection, this dissertation introduces the must-before relation and accompanying dynamic analysis tool (Embracer) that is not precise but enables online prediction. Experimental results show that Embracer detects 20-23% more races than a traditional race detector alone for reactive programs.This dissertation also introduces SideTrack, a lightweight dynamic atomicity analysis tool that generalizes from the observed trace to predict additional atomicity violations. Experimental results show that this predictive ability increases the number of atomicity violations detected by SideTrack by 40%.When developing these tools, it became clear that it was difficult to test them. For example, test programs that contain data races may be non-deterministic. A methodology for deterministic testing for dynamic analysis tools using trace snippets, described in this dissertation, alleviates this difficulty.
-------------------------------------

10136602_183 - 0.990424930422 - technology_and_computing
[variable, distribution, coefficient, estimator, unobserved, linear, identification, nonparametric, chapter]

Essays on Nonparametric Identication
In Chapter 1, I extend the techniques in Li and Vuong (1998), Schennach (2004a), and Bonhomme and Robin (2010) to identify nonparametric distributions of unobserved variables in a system of linear equations with more unobserved variables than outcome variables and with subsets of statistically dependent unobserved variables. I construct estimators of the distributions of unobserved variables and derive their uniform convergence rates. In Chapter 2, I develop a method for identification and estimation of coefficients in a linear regression model with measurement error in all the variables. The method is extended to identification in a system of linear equations in which only some of the coefficients on the unobserved variables are known. The estimator uses an assumption that is testable in the data and is in the class of Extremum estimators. The asymptotic distribution of the estimator is derived. In Chapter 3, I identify the nonparametric joint distribution of random coefficients in a linear panel data regression model. The distributions of the coefficients can depend on covariates, coefficients can be statistically dependent or equal in distribution, and there can be more coefficients than the fixed number of time periods. I construct estimators from the identification proofs. In finite sample simulations all the estimators have tight confidence bands around their theoretical counterparts.
-------------------------------------

10137538_183 - 0.999983315038 - technology_and_computing
[simulation, protocol, scenario, network, manet, term, mobile, result]

A Benchmarking System for Mobile Ad Hoc Network Routing Protocols
Network simulations are heavily used in the networking community to evaluate the performance of computer networks and their protocols. Simulations are often chosen over alternatives such as live experiments due to limited resources in terms of scalability, as well as reproduceability of the experiments. Many of the routing protocols designed for Mobile Ad Hoc Networks (MANETs) are evaluated solely on their performance calculated by these simulations, but the simulation environments the routing protocols are exposed to are often limited in scope. Only certain aspects of the routing protocols are tested, so the protocols are only understood in terms of the fabricated scenarios that they are subjected to.We first investigate the current best practices in simulation-based multi-hop wireless ad-hoc network (MANET) protocol evaluation to examine how wide-spread this problem is in the networking community. We extend a prior characterization of the settings and parameters used in MANET simulations by studying the papers published in one of the premier mobile networking conferences between 2006 and 2010. We find that there are still several configuration pitfalls which many papers fall victim to, which in turn damages the integrity of the results as well as any research aimed at reproducing and extending these results. We then describe the simulation &ldquo;design space&rdquo; of MANET routing in terms of its basic dimensions and corresponding parameters. We then discuss the benchmark infrastructure that was created to provide an easy to use solution for testing these protocols in a wide range of scenarios. The following chapter looks extensively at the realistic scenarios provided with the benchmark that act as sample scenarios to promote modeling simulations after real-world situations, and to show the flexibility in adding new scenarios. We also propose four &ldquo;auxiliary&rdquo; metrics to increase simulation integrity. Next, we show results generated by the benchmarking tool and provide our concluding thoughts.
-------------------------------------

10129963_178 - 0.95090532921 - technology_and_computing
[change, model, detection, item, precision, memory]

No Evidence for an Item Limit in Change Detection
Change detection is a classic paradigm that has been used for decades to argue that working memory can hold no more than a fixed number of items (“item-limit models”). Recent findings force us to consider the alternative view that working memory is limited by the precision in stimulus encoding, with mean precision decreasing with increasing set size (“continuous-resource models”). Most previous studies that used the change detection paradigm have ignored effects of limited encoding precision by using highly discriminable stimuli and only large changes. We conducted two change detection experiments (orientation and color) in which change magnitudes were drawn from a wide range, including small changes. In a rigorous comparison of five models, we found no evidence of an item limit. Instead, human change detection performance was best explained by a continuous-resource model in which encoding precision is variable across items and trials even at a given set size. This model accounts for comparison errors in a principled, probabilistic manner. Our findings sharply challenge the theoretical basis for most neural studies of working memory capacity.
-------------------------------------

10135063_183 - 0.977603220294 - technology_and_computing
[zno, device, thin, film, p-type, emission, substrate]

Optoelectronics Devices Based on Zinc Oxide Thin Films and Nanostructures
Optoelectronics devices based on ZnO thin films and nanostructures are discussed in this dissertation. A ZnO homojunction LED was demonstrated. Sb-doped p-type ZnO and Ga-doped n-type ZnO on Si (100) substrate were used for the LED device. After achieving ohmic contacts on both types of ZnO, the device showed rectifying current-voltage (I-V) characteristics. Under forward bias, the device successfully showed ultraviolet emissions. The emission properties were analyzed and the emission was confirmed to come from ZnO near band edge emissions. Further analysis showed that the emission mainly comes from the p-type layer of the device. A ZnO ultraviolet laser diode was fabricated and demonstrated. The device consists of Sb-doped p-type ZnO layer and Ga-doped n-type ZnO layer. In between p-layer and n-layer, a thin MgZnO/ZnO/MgZnO quantum well structure was inserted. In this device, random lasing mechanism plays an important role. When the diode was biased, the generation of light was enhanced by the carrier localization effect from the quantum well. The light was scattered between the ZnO random grain boundaries. Since the scattering effect can be so intense that some of the light can return to its original place to form close travel loop, as "random laser cavity". As long as the gain can overcome loss from scattering and material loss, lasing action can be demonstrated. An improved ZnO LED device was grown and characterized. The device grown on c-plane sapphire substrate can favor ultimate device applications due to the improved crystal quality of ZnO and the possibility of getting single crystallinity. A double heterostructure (MgZnO/ZnO/MgZnO) was also inserted in between p-layer and n-layer of the device to enhance the light output. The device showed much enhanced output power of 457 nW, which is two orders stronger than the LED fabricated on Si substrate. The optimization of high quality ZnO thin film on c-plane sapphire substrate was discussed. The devices in chapter two, three and four utilized Si or sapphire substrate, and are all in polycrystalline nature. To solve this problem and get the basis of high output power LEDs and lasers, single crystalline, two dimensional surface ZnO thin films were grown in chapter five. MgO/ZnO double buffer layers were used to accommodate the lattice mismatch. MgO thickness was found to be very important in achieving good ZnO thin film. An optimized growth also yields low background electron concentration and high mobility, which can enable future high quality p-type ZnO engineering. Our research was also expanded from ZnO thin films to ZnO nanostructures. The purpose of chapter six is to demonstrate a ZnO nanowire laser. ZnO nanowires are an excellent cavity and itself is a great gain material. We expanded Sb-doped p-type ZnO from thin films to ZnO nanowires. A p-type ZnO nanowire/n-type ZnO thin film p-n junction was achieved. The device showed lasing action when injection current was larger than ~50 mA. The lasing mechanism and gain/feedback were also discussed in detail.
-------------------------------------

10133462_183 - 0.896948848302 - technology_and_computing
[roller, process, imprint, precision, roll, manufacturing, feature]

Precision Manufacturing of Imprint Rolls for the Roller Imprinting Process
The roller imprinting process is being developed for the efficient and accurate fabrication of microfluidic devices. As the precision of the imprinted features is dependent on the features of the imprint rolls used in the process, it is critical that the rolls are manufactured very accurately, conforming closely to their design. It is also important that imprint rolls are manufactured rapidly and cost-effectively to control the cost and lead-time of roller imprinting. This paper looks at the application of micromachining technology in the manufacturing of imprint rolls. Sources of error during the manufacturing process are identified, and their effect on the precision of the final imprinted feature is discussed. Toolpath planning strategies are presented for generating very smooth surfaces. The paper presents a framework of precision manufacturing requirements for the roller imprinting process.
-------------------------------------

10130079_178 - 0.99404709413 - technology_and_computing
[sketch-and-speech, reference, machine, bibliographical, description]

Understanding sketch-and-speech descriptions of machines
Includes bibliographical references (p. 85-86).
-------------------------------------

10175401_189 - 0.996168313679 - technology_and_computing
[monitoring, learning, accuracy, memory, item, jol, older, neural, performance, younger]

THE NEUROLOGICAL COMPONENTS OF METAMEMORY MONITORING: JOL ACCURACY IN YOUNGER AND OLDER ADULTS
Because maximizing the learning of new material is a relevant concern for most individuals, understanding the specific processes involved could be beneficial for people of all ages. Both encoding and monitoring occur during the learning acquisition phase, yet monitoring accuracy and subsequent neural activation have been relatively ignored in the literature. The current research adapts a common metacognitive paradigm using Judgments of Learning (JOLs) to explore the neural differences in monitoring between younger (18-25) and older (65+) adults.  Participants were asked to remember natural scenes and predict encoding success by providing a JOL response for each item. Participants were told to respond “will remember” if they believed they would remember that item on a later recognition memory test or “will forget” if they thought they would forget that item on a later recognition memory test. Actual memory performance was compared to predicted memory performance to provide a measure of monitoring accuracy. Individuals reported a JOL response for 150 intact (Easy) and 150 scrambled (Difficult) scenes while in a 3.0T fMRI scanner. Despite minimal differences in behavioral performance, there were several age-related neuroimaging findings of note. When compared to younger adults, older adults had decreases in medial temporal lobe (MTL) activation, as well as contralateral recruitment of the anterior cingulate. Most importantly, the present study also disambiguated structures related to encoding success (the right parahippocampus) and monitoring accuracy (the anterior cingulate). A novel account of neural structures that mediate monitoring is provided both across items varying in difficulty (Easy and Difficult) and across different age groups (Young and Old). Encoding and monitoring are important for learning acquisition and the present research provides the first account that successfully disambiguates the two processes. Results are discussed in reference to their educational implications on resource allocation during the learning of new material.
-------------------------------------

10135578_183 - 0.779546289897 - technology_and_computing
[optical, electrochemical, platform, novel, transducer, part, detection, fluorescent]

Novel Biosensing Platforms with Advanced Nanomaterials for Electrochemical and Optical Detection of Proteins and Viruses
Novel nanomaterials are actively sought by chemists, biologists and engineers for a variety of applications. Biosensor platforms used for diagnostic purposes would benefit greatly from the development of new electrochemical or optical transducer substrates. This dissertation is a detailed account of the research effort to develop novel nanotechnology platforms for advanced electrochemical and optical detection of biological molecules. Specifically, my goal was to design and fabricate operationally simple yet inexpensive new electrochemical and optical biosensors that are highly selective and sensitive. 	The first section will cover the early work in the development of a porous polyelectrolyte interfaces with well-defined structure and electrochemical behavior using electrochemical surface plasmon resonance (ESPR) spectroscopy. This early work will also cover the application of the unobstructed electron transfer on porous polyelectrolyte nanostructures for the detection of poliovirus type 1 (PV1) using electrochemical enzymatic amplification. The electrochemical behavior of this sensor for whole viral particles is characterized using an enzymatic sandwich based immunoassay, with the final antibodies tagged with an ultra sensitive electrochemically active enzyme. Two important factors that make this sensor design innovative is the high loading capacity within a 3-D nano-assembly and the unhindered fast electron transfer through the nanofilm to the electrode surface. The enzymatic amplification scheme is an alkaline phosphatase (ALP) system, which has shown superb properties to enhance the redox current of electroactive species in the presence of aminophenyl phosphate (APP).	The majority of the thesis will focus on the electrospinning technique and how electrospun nanofibers can be used to create novel optical transducer platforms. Electrospinning is a polymer processing technique used to create continuous fibers with diameters ranging from a few nanometers to micrometers, and is ideal for encapsulating functional units, especially fluorescent and optical receptors, for biosensing applications. This section of the thesis will be divided into three specific parts. The first part will focus on a solid-state reusable and porous nanofiber-based optical (fluorescent) transducer for detecting proteins using an array of fluorescent dendrimers. The second part will cover the details of an optical (colorimetric and fluorescent) transducer using electrospun nanofibers doped with the conjugated polymer polydiacetylene (PDA) for the detection of volatile organic compounds (VOCs) and proteins. Lastly, the third part will describe the development of 2D-silica nanofiber thin films doped with gold nanoparticles, their enhanced optical properties, and the potential use in future sensor technology.
-------------------------------------

10137268_183 - 0.996659493897 - technology_and_computing
[loop, analog, electrostatic, coreceptor, cell, interaction, compstatin, peptide, sequence, dynamics]

Studies of Protein Interactions and Knowledge-Based Drug Design: (A) The Electrostatic Nature of Recognition Between HIV-1 gp120 V3 Loop and Coreceptors CCR5/CXCR4, (B) Complement System Inhibition by Compstatin Family Peptides
Computational and experimental methods were used to understand (i) protein interactions involving the V3 loop of gp120 of HIV-1 with coreceptors in host cells and (ii) peptide analogs from the compstatin family to human C3. Computational methods, including molecular dynamics (MD) simulations and electrostatic calculations, provide quantitative predictions of dynamics and interactions at atomic resolution, while experimental methods, including surface plasmon resonance (SPR) and enzyme-linked immunosorbent assays (ELISA) are needed to confirm binding and inhibition. HIV-1 entry into host cells is mediated by the interaction of the V3 loop of gp120 and coreceptors CCR5 or CXCR4 on host cell surfaces, with assistance of viral protein gp41 and cell receptor CD4. The mechanism of coreceptor selectivity is not well understood, given the sequence variability and structural flexibility of the V3 loop. Positive net charge is a persistent physicochemical property throughout HIV subtypes and has been recognized as an influencing factor for cell entry. Electrostatic analyses of V3 loop structures with consensus sequences from HIV subtypes, show similar electrostatic potential characteristics, irrespective of sequence variability. Charge and other sequence-based criteria were combined to develop a scheme for determining coreceptor selection. In addition, MD simulations provide insight into loop dynamics, indicating that persistent salt bridges contribute in keeping the two loop strands in proximity, therefore providing a charged scaffold for electrostatic interactions with coreceptors, irrespective of structural variability. Compstatin family peptides are inhibitors of the complement system and potential drug candidates against autoimmune and inflammatory diseases. Compstatin analogs are cyclic peptides that inhibit cleavage of human C3, therefore preventing further complement system activation. Introduction of tryptophan residues at the termini resulted in potent analogs, but suffering from reduced solubility. To balance hydrophobicity (important for binding) and polarity (important for solubility), additional analogs were designed guided by MD simulation results of bound analogs to C3. New analogs with polar substitutions at the N-terminus, including dipeptide sequence extensions and use of methylated tryptophan residues, were experimentally tested with ELISAs, demonstrating comparable inhibition to that of known analogs, but with improved solubility.
-------------------------------------

10130192_178 - 0.999995983061 - technology_and_computing
[auv, system, accurate, oceanographic, experiment, method]

Accurate Geo-Referencing Method for AUVs for Oceanographic Sampling
The objective of the paper is to understand, characterize and enhance the achievable performance of the system of a state-of-the-art marine observation device, the Oceanserver IVER2 Autonomous Underwater Vehicle(AUV), in the Singapore coastal zone and with regard to accurate sampling of oceanographic properties. This paper discusses modifications made to the AUV, in order to make it useful for experiments in this region, which includes shallow water, strong currents, poor visibility, heavy traffic and a poor acoustic environment. These factors strongly influence the choice of navigation method and the system architecture which will enable the AUV to obtain accurate geo-referenced oceanographic properties, and to ensure its safe operation. Our science experiments usually involve sampling at various areas around the coast of Singapore within the same day; this calls for consistent positioning methods that allow ease in deploying and retrieving related equipment. This paper illustrates the use of ultra-short baseline (USBL) tracking system and ranges from an acoustic modem fused by a particle filter to aid the dead reckoning algorithm of the IVER2. The performance of the modified system is shown through simulations and field experiments.
-------------------------------------

10175459_189 - 0.994903660979 - technology_and_computing
[view, local, network, channel, interference, state]

A Matter of Perspective: Reliable Communication and Coping with Interference with Only Local Views
This dissertation studies interference in wireless networks. Interference results from multiple simultaneous attempts to communicate, often between unassociated sources and receivers, preventing extensive coordination. Moreover, in practical wireless networks, learning network state is inherently expensive, and nodes often have incomplete and mismatched views of the network. The fundamental communication limits of a network with such views is unknown.

To address this, we present a local view model which captures asymmetries in node knowledge. Our local view model does not rely on accurate knowledge of an underlying probability distribution governing network state. Therefore, we can make robust statements about the fundamental limits of communication when the channel is quasi-static or the actual distribution of state is unknown: commonly faced scenarios in modern commercial networks. For each local view, channel state parameters are either perfectly known or completely unknown. While we propose no mechanism for network learning, a local view represents the result of some such mechanism.

We apply the local view model to study the two-user Gaussian interference channel: the smallest building block of any interference network. All seven possible local views are studied, and we find that for five of the seven, there exists no policy or protocol that universally outperforms time-division multiplexing (TDM), justifying the orthogonalized approach of many deployed systems. For two of the seven views, TDM-beating performance is possible with use of opportunistic schemes where opportunities are revealed by the local view.

We then study how message cooperation --- either at transmitters or receivers --- increases capacity in the local view two-user Gaussian interference channel. The cooperative setup is particularly appropriate for modeling next-generation cellular networks, where costs to share message data among base stations is low relative to costs to learn channel coefficients. For the cooperative setting, we find: (1) opportunistic approaches are still needed to outperform TDM, but (2) opportunities are more abundant and revealed by more local views.

For all cases studied, we characterize the capacity region to within some known gap, enabling computation of the generalized degrees of freedom region, a visualization of spatial channel resource usage efficiency.
-------------------------------------

10136545_183 - 0.997461274914 - technology_and_computing
[technology, design, development, multiple-patterning, assessment, infrastructure]

Design Enablement and Design-Centric Assessment of Future Semiconductor Technologies
The semiconductor industry is likely to see several radical changes in the manufacturing, device and interconnect technologies in the next few years and decades. One of the most favorable options of manufacturing technologies is multiple-patterning lithography. This novel technology has serious implications on design, however, and its adoption will necessitate the application of "Design Enablement" methodologies to ensure the compatibility of design.This dissertation contributes to the design enablement of multiple-patterning technology. We propose a general methodology for the automated adaptation of layout to multiple-patterning masking the complexity in dealing with its manufacturing constraints. We also study the impact of this technology on design and show the benefits of bringing the design perspective into making manufacturing-process decisions. Lastly, we propose a novel technique for DP that reduces cost and improve overlay/Critical-Dimension (CD) control in multiple-patterning. Many technology choices are presented to achieve scaling to every next node and early technology assessment -- before the actual development of technologies -- has become more necessary than ever as a means to ensure faster adoption and manageable technology/design development costs. Technology assessment is currently a highly unsystematic procedure; it relies on small-scale experiments and manufacturing tests and much on speculations based on technologists/designers experience with previous technology generations. This dissertation also addresses the problem of increasing complexity in making technological decisions. It aims at the development of a computation infrastructure for the systematic and early assessment of technologies and their impact on circuit design. The infrastructure is the first of its kind and is expected to have a lasting impact on technology development. The infrastructure allows for true exploration of design and technology choices, thereby redirecting research and development efforts toward options that are more likely to eventually see adoption. Finally, the infrastructure is applied to evaluate multiple-patterning process decisions and study their implications on design.
-------------------------------------

10135235_183 - 0.999999438019 - technology_and_computing
[device, network, mobile, ubibot, computing, application, network-based, software, system]

UbiBot : a system for experimenting with mobile devices on a wireless network
Web 2.0 technologies have fueled a new generation of applications that come to the desktop from the network. The emerging field of mobile context-aware computing (MCAC) would benefit from network-based applications even more than desktop computing. With MCAC, there are many issues that a network application infrastructure needs to address beyond providing mere functionality, such as low network speed and robustness, small, battery-powered devices, and limitations in the software these devices are able to run. Furthermore, these devices offer unique sensing capabilities such as microphones, cameras, and GPS. Taking full advantage of these in network-based applications requires more flexibility than simply providing computing and network utilities. We introduce UbiBot, an extensible system for experimenting with network-based services for the mobile. UbiBot addresses many of the problems of mobile computing by employing a publish-subscribe architecture that enables dynamically reconfiguring the system to incorporate new services, delegate computation, and manage network performance issues, yet without having to modify the software on the mobile devices. Furthermore, the software for the mobile can be adapted to the evolving capabilities of new devices. We demonstrate the flexibility and ease of UbiBot through several case studies
-------------------------------------

10129865_178 - 0.999993141714 - technology_and_computing
[signal, representation, system]

Signals and Systems
6.003 covers the fundamentals of signal and system analysis, focusing on representations of discrete-time and continuous-time signals (singularity functions, complex exponentials and geometrics, Fourier representations, Laplace and Z transforms, sampling) and representations of linear, time-invariant systems (difference and differential equations, block diagrams, system functions, poles and zeros, convolution, impulse and step responses, frequency responses). Applications are drawn broadly from engineering and physics, including feedback and control, communications, and signal processing.
-------------------------------------

10133634_183 - 0.99999708098 - technology_and_computing
[network, mesh, protocol, modrate, order, performance, testbed, packet, system, rts-id]

Overhearing in 802.11 mesh networks
802.11-based mesh networks provide a useful and practical alternative to regular infrastructure-based wireless networks, but they have an intrinsic scaling limit due to their less efficient airtime utilization. Mesh networks forward packets multiple times, increasing airtime utilization and decreasing path throughput and useful channel capacity available to the clients. In this dissertation, we explore ways to optimize forwarding in order to decrease the number of packet transmissions and increase path throughput. The main technique that we explore in this dissertation is a phenomenon called 'overhearing'. Traditional mesh networks use only 'good' links with low packet loss rates in order to forward packets; overhearing allows utilization of the links with high losses in order to reduce the number of transmissions where possible. This dissertation proposes two methods that allow mesh networks to take full advantage of overhearing: 'RTS-id' is a backwards-compatible link-layer modification that allows adding overhearing support to traditional mesh networks without requiring changes to hardware or transport protocols. 'Modrate' is a new rate selection algorithm that can increase the amount of overhearing in bulk transfer systems that are already taking advantage of overhearing opportunities. In order to verify the operation of RTS-id, we implement the algorithm on a software-defined radio. We verify that RTS-id is compatible with existing, unmodified radios. We then develop a probabilistic transmission simulator and use it to quantify the potential gains from deploying RTS-id on existing large-scale wireless mesh networks. In order to verify the operation of modrate, we set up two wireless testbeds: a large, building-wide testbed operating in the 2.4-GHz range, and a smaller testbed operating in the 5- GHz range. We apply modrate to two existing overhearing- aware routing protocols, ExOR and MORE, and use our testbeds to measure the improvement provided by modrate in those systems. Finally, motivated by the somewhat unimpressive performance of modrate, we study the specific reasons for performance improvements in the ExOR and MORE protocols. We measure the performance of each protocol with various pieces of functionality disabled, and come to surprising conclusions: while systems such as ExOR and MORE have significantly better performance than traditional systems, a large fraction of these performance gains is caused not by overhearing, but by simpler protocol aspects like flow control and group acknowledgments
-------------------------------------

10134985_183 - 0.999936274546 - technology_and_computing
[system, inertia, pde, method, kkt, preconditioner, optimization, problem, matrix]

Methods for PDE-constrained optimization
The KKT systems arising in nonlinearly constrained optimization problems may not have correct inertia, and therefore must be modified to avoid convergence to nonoptimal KKT points. Matrix factorizations can determine the inertia of a general symmetric matrix but are too costly in the PDE contextIn PDE-constrained optimization, KKT systems are generally solved with preconditioned iterative methods that are unable to detect whether the current matrix has correct inertia. Moreover, the preconditioners assume the existence of a preconditioner for the underlying PDE. Methods are discussed that solve the constrained problem by minimizing a sequence of smooth primal-dual merit functions. The Newton equations are solved approximately with a variant of the preconditioned conjugate-gradient (PCG) method that naturally determines when the regularized KKT system for the constrained problem has incorrect inertia. Convergence is accelerated with a sparsity exploiting preconditioner that implicitly defines a positive-definite system. The preconditioning strategy is entirely algebraic and is based on an incomplete factorization of an equivalent symmetric indefinite system. It explicitly takes advantage of dual regularization, and in the PDE constrained context, does not require a preconditioner for the underlying PDE
-------------------------------------

10135007_183 - 0.961335395082 - technology_and_computing
[solar, concentrator, tracking, planar, system, tracker]

Automated micro-tracking planar solar concentrators
One aim of solar concentrators is to reduce the cost of a solar power systems by reducing the amount of expensive semiconductor used in exchange for optical concentrating components and tracking mechanics. Solar trackers rotate the solar panel or concentrator so that direct normal incidence is maintained throughout the day. As concentration increases, so does the required complexity and precision of the tracking mechanics. This increased precision results in a larger fraction of the total system cost in tracking. In this thesis I will present an alternative to large-scale two-axis mechanical trackers that relies on the unique geometry of the planar micro- optic solar concentrator. at the focal point of each lens. The facets reflect incoming light into guided modes of the waveguide towards edge-mounted photo-voltaic (PV) cells. This type of concentrator is compatible with traditional solar trackers but its unique geometry allows for more flexibility. By laterally translating the lens array with respect to the waveguide it is possible to couple off-axis light into the PV cell. In this work I evaluate some different implementations of the planar micro-optic solar concentrator optimized for this type of tracking, discuss a designed and implemented mechanical tracking platform to house the concentrator, and cover the electrical control and feedback used to maintain alignment. I then present some measurements from the system demonstrating functional tracking
-------------------------------------

10131846_183 - 0.848946462363 - technology_and_computing
[highway, toll, project, electronic]

How To Franchise Highways
Barcelona commuters receive a monthly highway bill, without ever having stopped at a tollbooth. Cars on the Autostrada, which connects Milan, Florence, Rome and Naples, whiz past roadside electronic readers that automatically deduct credit from prepaid smartcards which are similar to the copycards familiar to library users. Electronic toll collection is now used on the Esterel-Cote d'Azur; two toll-ring systems in Norway; the Dallas North Tollway; the Oklahoma Turnpikes; and two facilities in New Orleans. Reliability and accuracy rates run as high as 99.9 per cent. Unless there is successful labour resistance, by the year 2000 electronic toll collection will be operating on every major toll facility in the United States. Stopping at tollbooths will be obsolete for all but the infrequent traveller.
      The advance in technology is accompanied by a shift in policy. The franchising of highway services is now under way: California has four projects in progress; Virginia, one project; and planning is in hand in many other states. Furthermore, the Intermodal Surface Transportation Efficiency Act of 1991 will bring a tide of new projects, as it permits the commingling of federal and private funds. Different approaches to franchising have been used. This article investigates the alternatives and proposes a plan for highway franchising.
-------------------------------------

10129752_178 - 0.965949043074 - technology_and_computing
[production, system, area, reference, visual, floor, bibliographical, management, reduction, improvement]

Production system improvement : floor area reduction and visual management
Includes bibliographical references (p. 69-70).
-------------------------------------

10134450_183 - 0.92570849947 - technology_and_computing
[casimir, measurement, force, physics, au-coated, precision]

Dynamic Precision Measurement of the Casimir Force using Short Coherence Length Fiber-Based Interferometer
The Casimir effect has become very important in modern physics since its prediction in 1948. Not only is it the most accessible evidence of vacuum fluctuations in macroscopic systems but also has important applications in many areas such as condensed matter physics, atomic physics, cosmology and nano-technology. Therefore, the theoretical activity in the analysis of the Casimir effect has dramatically increased in the past 60 years. To provide deeper insights, precise Casimir force measurements are necessary.  Here, we develop a high precision dynamic Casimir force measurement system based on short coherence length fiber-optics interferometer in UHV. The frequency modulation (FM) technique and precise calibration are the key points to achieve a high precision in the Casimir force measurement. It is also the first time in our group, random errors are reduced to be smaller than systematic errors.  The Casimir force was measured between a Au-coated sphere and Au-coated plate using FM technique. A comparison to the theory where the description of the permittivity using the generalized Plasma and Drude model is done. In addition , Casimir Pressure measurement between a Au-coated sphere and Ito plate will be presented.
-------------------------------------

10139935_183 - 0.758347158012 - technology_and_computing
[energy, code, building, premium]

Energy Codes and the Landlord-Tenant Problem
This paper assesses whether commercial real estate participants are willing to pay a premium for an energy efficient building that has not received a green label. I utilize a unique dataset of detailed building-level observations and a spatial semiparametric matching framework that exploits quasi-experimental state-by-year variation in the implementation of mandatory building energy codes, to estimate selling price and rent premiums for a more stringent code. I find that buildings constructed under a more stringent energy code are associated with rent and selling price premiums of approximately 2.7% and 10%, respectively, compared to buildings constructed just before the code came into effect. When tenants pay directly for utilities, buildings constructed under an energy code are associated with 5.7% higher rents. These premiums are consistent with complete capitalization of estimated building-level savings, and therefore cast doubt on the existence of an energy efficiency gap resulting from adverse selection between landlords and tenants.
-------------------------------------

10139185_183 - 0.99999895396 - technology_and_computing
[hardware, general, query, user, purpose, dissertation, parallel, platform, application, integer]

Hardware Acceleration Of Database Applications
General purpose computing platforms have generally been favored over customized computational setups, due to the simplified usability and significant reduction of development time. These general purpose machines make use of the Von-Neumann architectural model which suffers from the sequential aspect of computing and heavy reliance on memory offloading. This dissertation proposes the use of hardware accelerators such as Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) as a substitute or co-processor to general purpose CPUs, with a focus on database applications. Here, large amounts of data are queried in a time-critical manner. This dissertation shows that using hardware platforms allows processing data in a streaming (single pass) and massively parallel manner, hence speeding up computation by several orders of magnitude when compared to general purpose CPUs. The complexity of programming these parallel platforms is abstracted from the developers, as hardware constructs are automatically generated from high-level application languages and/or specifications. This dissertation explores the hardware acceleration of XML path and twig filtering, using novel dynamic programming algorithms. Publish-subscribe systems present the state of the art in information dissemination to multiple users. Current XML-based publish-subscribe systems provide users with considerable flexibility allowing the formulation of complex queries on the content as well as the (tree) structure of the streaming messages. Messages that contain one or more matches for a given user profile (query) are forwarded to the user. This dissertation further studies FPGA-based architectures for processing expressive motion patterns on continuous spatio-temporal streams. Complex motion patterns are described as substantially flexible variable-enhanced regular expressions over a spatial alphabet that can be implicitly or explicitly anchored to the time domain. Using FPGAs, thousands of queries are matched in parallel. The challenges in handling several constructs of the assumed query language are explored, with a study on the tradeoffs between expressiveness, scalability and matching accuracy (eliminating false-positives).Finally, the first parallel Golomb-Rice (GR) integer decompression FPGA-based architecture is detailed, allowing the decoding of unmodified GR streams at the deterministic rate of several bytes (multiple integers) per hardware cycle. Integer decompression is a first step in the querying of inverted indexes.
-------------------------------------

10130070_178 - 0.996794534032 - technology_and_computing
[power]

Small-signal stability effects of frequency and voltage controllers on power systems with integration of wind power
Includes bibliographical references (p. 52-53).
-------------------------------------

10130209_178 - 0.998334736224 - technology_and_computing
[random, design, mission]

Mission Design for Compressive Sensing with Mobile Robots
This paper considers mission design strategies for mobile robots whose task is to perform spatial sampling of a static environmental field, in the framework of compressive sensing. According to this theory, we can reconstruct compressible fields using O(log n) nonadaptive measurements (where n is the number of sites of the spatial domain), in a basis that is "in coherent" to the representation basis 1; random uncorrelated measurements satisfy this incoherence requirement. Because an autonomous vehicle is kinematically constrained and has finite energy and communication resources, it is an open question how to best design missions for CS reconstruction. We compare a two-dimensional random walk, a TSP approximation to pass through random points, and a randomized boustrophedon (lawnmower) strategy. Not unexpectedly, all three approaches can yield comparable reconstruction performance if the planning horizons are long enough; if planning occurs only over short time scales, the random walk will have an advantage.
-------------------------------------

10134918_183 - 0.730792551203 - technology_and_computing
[memory, associative, region, stimulus, encoding, object, strategy, visual]

Associative encoding in episodic memory : binding items across time
As individuals navigate the world, they encounter a constant stream of stimuli, some of which are important to attend to, encode, and associate with other stimuli separated in time. How does the brain attempt to form and succeed in forming associative memories for temporally- discontiguous stimuli? Does using different associative strategies influence which brain regions are engaged in encoding, and could this have clinical implications for potential treatment approaches in patients with neurological damage and disease? The studies described in this dissertation were designed to address these questions using functional magnetic resonance imaging and behavioral memory testing. Using a novel memory paradigm, in which subjects encoded and formed associations between temporally-discontiguous sequentially-presented stimuli, the established involvement of prefrontal and medial temporal regions in associative encoding was disentangled, as these regions were found to subserve different functions in maintaining and binding visual stimuli. Successful associative encoding of object pairs involved coordination of frontoparietal working memory regions and the hippocampus. Frontoparietal regions were also engaged in visuospatial encoding, where the spatial cue preceded the centrally-presented object; however, parietal regions were modulated by attempted visuospatial binding while frontal responses predicted successful binding. Additionally, when examining the timing of associative memory formation with a temporal delay between visual objects, positive modulation of frontal, lateral occipital, and anterior medial temporal regions were found to predict success at binding and not during maintenance. Besides exploring the natural processes involved in associating and encoding temporally-discontiguous stimuli, another primary goal of this dissertation was to examine the influence of encoding strategy on regional memory effects. Encoding object pairs using a visual versus verbal strategy engaged similar regions as those reported in prior studies of visual- versus verbal-stimulus encoding. Such findings suggest a driving effect of strategy, not stimulus-type, on regional involvement in associative memory formation, which has implications for future development both in basic memory research and for potential clinical treatments. Together, the studies that comprise this dissertation addressed existing unknowns in the field of human memory formation and contributed to the understanding of how individuals with intact and impaired cognitive function form associative memories
-------------------------------------

10137889_183 - 0.845166348749 - technology_and_computing
[communication, contract, flexible, buyer, seller]

Let’s talk: How communication affects contract design
We study experimentally how the ability to communicate affects the frequency andeffectiveness of flexible and inflexible contracts in a bilateral trade context where sellers canadjust trade quality after observing a post-contractual cost shock and a discretionary buyertransfer. In the absence of communication, we find that rigid contracts are more frequent andlead to higher earnings for both buyer and seller. By contrast, in the presence of communication,flexible contracts are much more frequent and considerably more productive, both for buyers andsellers. Also, both buyer and seller earn considerably more from flexible with communicationthan rigid without communication. Our results show quite strongly that communication, a normalfeature in contracting, can remove the potential cost of flexibility (disagreements caused byconflicting perceptions). We offer an explanation based on social norms.
-------------------------------------

10129761_178 - 0.923861554208 - technology_and_computing
[resource, bibliographical, semiconductor, optimization, reference, management, inventory, capital, labor, operations]

Operations improvement in a semiconductor capital equipment manufacturing plant : resource optimization, labor flexibility, and inventory management
Includes bibliographical references (p. 143-145).
-------------------------------------

10135573_183 - 0.999994601761 - technology_and_computing
[network, uv, communication]

Ultraviolet Communication Network Modeling and Analysis
With recent advances in ultraviolet (UV) sources and detectors, UV communication and networking has received increasing interest for diverse applications. This thesis studies corresponding non-line-of-sight (NLOS) scattering channels, UV system and network performance, neighbor discovery and network connectivity issues.Based on NLOS communication link geometry and UV signal interaction with the atmosphere, the author develops two analytical channel models that describe the path loss in an integral form and closed-form respectively. Utilizing curve-fitting with field measurements, an easy-to-use empirical model is further developed. The results are then applied to study performance of a NLOS UV network, from outage probability to transmission throughput and network connectivity in a multi-user interference environment. Different UV transceiver structures and pointing geometry are incorporated, and fundamental relations of network k-connectivity with network parameters such as network scale and node density are studied. Subsequently, a neighbor discovery protocol for UV communication networks is proposed, using a novel handshake mechanism and direction synchronization technique. The protocol is further improved in terms of significantly reduced neighbor discovery time by assigning a leader node for node coordination. Meanwhile, the author also investigates the effects of channel delay spread on communication quality, i.e., data rate limitation from the induced self inter-symbol interference. These analyses and results provide valuable guidance for UV system and network design in a real environment.
-------------------------------------

10138974_183 - 0.999470789557 - technology_and_computing
[ppp, accountability, service]

An Overview of Accountability Mechanisms in Public - Private Partnerships in South Africa
Public-private partnerships (PPPs) are generally envisaged by countries around the world  as major innovative policy tools that will remedying the lack of dynamism in traditional public service delivery by increasing investment in infrastructure as well as improving the delivery of social services. To this end, the South African government since 1999 has adopted the use of PPPs as an integral strategy in its national and international developmental plan. However, with the growing trends in international best practice, noticeable loopholes and omissions have been observed in the existing PPP legal framework resulting in some accountability gaps. Therefore, if the government needs to compliment its developmental aspirations, there is a need to instill the confidence and competitiveness amongst bidders to use PPPs as coherent development-orientated best value tool to deliver services and infrastructures to taxpayers. This paper therefore draws attention to some of the accountability challenges resulting from the PPP legal framework and suggests some techniques that could serve as a platform for the possible review and amendment of core competencies
-------------------------------------

10134024_183 - 0.999603940141 - technology_and_computing
[randomness, encryption, security, scheme, random, pke, failure, public-key]

Public-key encryption secure in the presence of randomness failures
Public-key encryption (PKE) is a central tool for protecting the privacy of digital information. To achieve desirable strong notions of security like indistinguishability under chosen-plaintext attack (IND- CPA), it is essential for an encryption algorithm to have access to a source of fresh, uniform random bits. Further, these bits should never be revealed and never reused. In practice, our machines typically generate these random bits with software random number generators (RNGs). Unfortunately, RNGs are prone to problems. The resulting randomness failures can have disastrous consequences for the security of existing PKE schemes that rely on good randomness. In this dissertation we focus PKE security in the presence of three types of randomness failures: predictable randomness, repeated randomness, and revealed randomness. For predictable randomness, where the encryption algorithm is given random inputs that are predictable to an adversary, we argue that we want PKE schemes that are hedged against bad randomness: if the encryption scheme is given good randomness it provably meets traditional notions like IND-CPA, while if it is given poor randomness, it still provably provides some security. We formalize this security notion and give provably-secure constructions of hedged public-key encryption. Next, we show how repeated randomness failures, where the encryption algorithm is given random inputs that it was given previously, can occur in practice due to virtual machine snapshots. In particular, we show how many popular web browsers are vulnerable to these failures. We then turn to building PKE schemes that still provide provable security when given repeated randomness. We develop new models of security to capture this situation and prove that a simple and efficient modification to any existing secure scheme gives security under our new models. Finally, we study the strange effects revealed randomness failures, where the random inputs used for encryption are later revealed to an adversary, can have on public-key encryption security. Specifically, we focus on selective opening attacks. We show that a large class of PKE schemes, called lossy encryption schemes, provably resists selective opening attacks
-------------------------------------

10136117_183 - 0.999997809637 - technology_and_computing
[virtual, vdi, cache, desktop, storage, workload, datum, clone, hypervisor, optimization]

Hypervisor Side Cache for Virtual Desktop Infrastructure
Virtual Desktop Infrastructure (VDI) deployments run large numbers of desktops in a virtualized environment to increase flexibility and address cost. One of the major challenges VDI faces today is the cost of high bandwidth interconnection networks to shared storage. VDI storage workloads have a number of unique characteristics which make them a target for optimization. For example, VDI workloads exhibit high amount of redundant data transfers (from shared OS images), highly bursty behavior (from daily work patterns), and a common storage format (virtual disks).This thesis performs a detailed study of VDI workload and evaluates effectiveness of four hypervisor side optimization techniques. To eliminate network read requests and serve data from locally cached blocks, we evaluate two read caches, namely, location-addressed and content-addressed. We also compare these read cache with a simple mechanism which stores shared read-only virtual disks on hypervisor side local media. To eliminate transfer of redundant data that is written to the storage server, we evaluate the effectiveness of inline write deduplication. All the experiments are carried out in two setting, for full clone virtual desktops and linked clone virtual desktops.A detailed trace-driven simulation study of the mechanisms with a realistic VDI workload shows up to 75% reduction in the total network I/O traffic. We propose some recommended setting for choosing the right optimizations,  for example, for full clone virtual desktops content-addressed cache outperforms location-addressed cache by 50%.
-------------------------------------

10133277_183 - 0.951264526379 - technology_and_computing
[communication, work]

Travel, work, and telecommunications: a view of the electronics revolution and its potential impacts
Considerations of the impacts of electronic technologies on transportation usually focus on substitution of communications for travel, especially telecommuting. This topic is reviewed briefly, followed by consideration of electronic technology-induced changes in the structure of firms, work by individuals, and consumption. Today’s organization of the work place on the basis of time-at-a-place measurements dates from early in the Industrial Revolution; the communications control of production dates from the introduction of the telegraph. Recent and upcoming communications developments may relax time and place requirements while intensifying communications control. Resulting changes in production and consumption may challenge transportation developments in coming decades.
-------------------------------------

10132408_183 - 0.988630507456 - technology_and_computing
[use, characteristic, variable, land, neighbourhood, sociodemographic, travel]

Does Dissonance Between Desired and Current Residential Neighbourhood Type Affect Individual Travel Behaviour? An Empirical Assessment From the San Francisco Bay Area
In the USA and Europe land-use based solutions to transportation problems have rapidly gained in popularity over the past decade. It appears that the principles of New Urbanism (in the USA) or the Compact City (Europe) have found a solid place in the profession’s thinking. This popularity is not least the result of numerous empirical studies demonstrating that living in higher-density, mixed-use neighbourhoods is associated with less car use compared to living in low-density, suburban environments (Frank and Pivo, 1994; Meurs and Haaijer, 2001; Naess et al., 1995; Sun et al., 1998).  The academic literature is, however, equivocal about the effect of neighbourhood characteristics on reducing car use. Several ambiguities and criticisms can be discerned. First, there is disagreement about the importance of land use characteristics in explaining variations in travel behaviour. Opinions differ about the role of urban form vis-à-vis other sets of variables. Some authors claim, for instance, that factors, such as land-use mixing or density, are more important than factors related to travellers’ sociodemographic variables (Kockelman, 1997). Others are, however, more conservative and argue that sociodemographic variables explain a larger share of the variation in travel patterns than do land use characteristics (Crane and Crepeau, 1998; Snellen et al., 2001). Some studies claim that not only are sociodemographic variables more important than land use characteristics, but that this also applies to attitudes towards travelling, land use and the environment (Bagley and Mokhtarian, 2002; Kitamura et al., 1997). Part of the disagreement is no doubt attributable to differences in theoretical framework, research design, data, and geographical settings. However, the fact that the ambiguities persist calls for additional research.
-------------------------------------

10139167_183 - 0.999963495608 - technology_and_computing
[statistics, network, tracking, variant, likelihood]

Statistical Process Control Methods for Network Monitoring Using Generalized Linear Mixed Models
Network surveillance algorithms are becoming increasingly important as the ability to monitor a wide variety of data is rapidly expanding. Traffic metrics are usually count data that display a non-stationary pattern in their mean structure. We propose to model traffic counts using a generalized linear mixed model to capture these features. We then develop three tracking statistics proposed for anomaly detection. Two of the statistics are derived variants of a generalized likelihood ratio approach, which itself is not computationally tractable. The first of these variants is based on an approximation to the integrated likelihood while the second is based on the concept of h-likelihood. We also consider a tracking statistic that is an exponentially weighted moving average. We investigate the properties of the three tracking statistics from the point of view of false alarm rate and detection power, and compare the proposed tracking statistics with current literature. Our comparisons show that the two generalized likelihood ratio variants are preferred choices as SPC tools for network surveillance. Computational aspects of the three procedures are also discussed.
-------------------------------------

10133694_183 - 0.979775482562 - technology_and_computing
[material, turn-on, frequency, pitch, composite, coil, increase, spring]

Mechanically tunable plasma frequency
Composite materials with plasma turn-on frequencies in the microwave range can be used as electromagnetic filters. The permittivity of a material changes from negative to positive at the turn-on frequency. Previously, it had been theoretically determined that using non-magnetic metal wire coils could be used to create composites with turn-on frequencies that are dependent on the wire thickness, coil inner diameter, pitch, and coil spacing (Amirkhizi, 2002). This project focuses on creating a composite with a mechanically tunable turn-on frequency. A material is made out of an array of springs is placed within a non-metallic frame and the turn-on frequency of the material is altered through extending the springs and thereby altering the pitch. The springs are arranged with alternating chirality in order to create a non-chiral material. A vector network analyzer and horn antennas are used to send and receive microwave signals through the material. The measured scattering parameters are then used to calculate the permittivity of the material. The results show an increase in the turn-on frequency with an increase in pitch. Increasing the pitch by about from 3 mm to about 3.9 mm results in a corresponding increase in turn-on frequency of about 1.2 GHz for the non-chiral material setup
-------------------------------------

10131535_183 - 0.931379760688 - technology_and_computing
[model, probit]

Probit Model Estimation Revisited: Trinomial Models of Household Car Ownership
In this paper we revisit various important issues relating to practical estimation of the multinomial probit model, using an empirical analysis of car ownership as a test case. To provide context, a brief literature review of empirical probit studies is included. Estimates are obtained for a full range of model specifications, including models with random (uncorrelated and correlated) taste variation and/or a general random error structure, and issues of estimability, specification testing, and alternative normalizations for probit models are addressed. Three model trust region algorithms for finding maximum likelihood estimates are compared, and the superiority of a structured quasi-Newton method employing "model switching" over more traditional approaches (Broyden-Fletcher-Goldfarb-Shanno secant update, Berndt-Hall-Hall-Hausman) is demonstrated. The trust region algorithms have reliable convergence properties and provide useful diagnostic information. Finally, a comparison of some probit integral approximation schemes (Clark, and two variants of Mendell-Elston) versus numerical integration is included. There is additional evidence against using Clark's approximation, but a variant of the Mendell-Elston approach appears promising. Numerical problems with variable-ordering techniques (such as separated-split) are demonstrated.
-------------------------------------

10137763_183 - 0.995001130008 - technology_and_computing
[site, factor, linear, nonlinearity, nehrp]

Site response in NEHRP Provisions and NGA models
Site factors are used to modify ground motions from a reference rock site condition to reflect the influence of geologic conditions at the site of interest. Site factors typically have a small-strain (linear) site amplification that captures impedance and resonance effects coupled with nonlinear components. Site factors in current NEHRP Provisions are empirically-derived at relatively small ground motion levels and feature simulation-based nonlinearity. We show that NEHRP factors have discrepancies with respect to the site terms in the Next Generation Attenuation (NGA) ground motion prediction equations, both in the linear site amplification (especially for Classes B, C, D, and E) and the degree of nonlinearity (Classes C and D). The misfits are towards larger linear site factors and stronger nonlinearity in theNEHRP factors. The differences in linear site factors result largely from theirnormalization to a reference average shear wave velocity in the upper 30 m of about 1050 m/s, whereas the reference velocity for current application is 760 m/s. We show that the levels of nonlinearity in the NEHRP factors are generally stronger than recent simulation-based models as well as empirically-based models.
-------------------------------------

10134265_183 - 0.994682500076 - technology_and_computing
[detection, db, signal, tunable, rf, filter, radio, cognitive, tv, phase]

Detection technique and front-end RF tunable filter for cognitive radio systems
Cognitive radio (CR) can effectively increase the spectral efficiency. However, there is an inevitable interference problem between the primary licensed users and the unlicensed CR devices. A detection technique utilizing the local oscillator (LO) leakage signal coming from a nearby mobile terminal is proposed and evaluated for short-range wireless communication systems, such as an ultra-wide band (UWB) system. Due to the low-level LO leakage, the detection sensitivity is a great concern. Local oscillator phase noise of the detector can degrade the detection sensitivity. Spectrum broadening of the LO phase noise interaction with an adjacent channel signal through a multiplier is analyzed and the degradation in detection performance is evaluated. Spectrum broadening is applied to the cognitive radio (CR) device operation in UHF bands. A strong TV signal transmitted by a nearby TV station can interact with LO phase noise of a CR receiver, and induce serious in-band interference which possible degrades the receiver performance. In order to suppress strong TV signals, a wide dynamic range low-power CMOS-based RF tunable filter with image cancellation is presented using a balanced passive complex mixer and frequency dependent loads. The tunable RF notch filter with 17 dB attenuation and 1.6 dB insertion loss is achieved using parallel LC tank loads. A tunable RF bandpass filter is implemented with capacitive termination, and achieves 18 dB maximum attenuation and 1.8 dB insertion loss
-------------------------------------

10129646_178 - 0.976511445858 - technology_and_computing
[cooling, design, humid, reference, bibliographical, leaf, climate, residence, passive, multifamily]

Passive cooling design for multifamily residences in hot, humid climates
Includes bibliographical references (leaves 159-160).
-------------------------------------

10134344_183 - 0.912870629023 - technology_and_computing
[detection, technique, underwater, visibility, water, application, auv, vision, computer]

Computer vision techniques for underwater navigation
In the world of autonomous underwater vehicles (AUV) the prominent form of sensing has been sonar due to cloudy water conditions and dispersion of light. Although underwater conditions are highly suitable for sonar, this does not mean that vision techniques should be completely ignored. There are situations where visibility is high, such is in calm waters, and where light dispersion is not an issue, such as shallow water or near the surface. In addition, even when visibility is low, once a certain proximity to an object exists, visibility can increase. The focus of this project is this gap in capability for AUVs, with an emphasis on computer-aided detection through machine learning and computer vision techniques. All experimentation utilizes the Stingray AUV, a small and unique vehicle designed by San Diego iBotics. The first experiment is detection of an anchored buoy, which mimics the real world application of mine detection for the Navy. The second experiment is detection of a pipe, which mimics pipes in bays and harbors. The current algorithm for this application uses boosting machine learning on hue, saturation, value (HSV) to create a classifier followed by post processing techniques to clean the resulting binary image. There are many further applications for computer- aided detection and classification of objects underwater, from environmental to military
-------------------------------------

10135332_183 - 0.996608573458 - technology_and_computing
[method, lsi, collection, linear, idea, algebra, expansion, query]

Scoring methods iniInformation retrieval : a linear algebra perspective
Linear algebra based methods have a long, rich history in information retrieval (IR), starting with the vector space model (VSM) SM83. The VSM formalizes geometric intuitions, providing a simple and elegant framework. Latent semantic indexing (LSI) DDF⁺90, which builds on VSM, is another seminal idea in IR that is inspired by linear algebra. In LSI, the matrices resulting from singular value decomposition (SVD) are endowed with interpretations that enable us to handle language features such as synonymy and polysemy. However, there are methods motivated by ideas other than geometry that both have an intuitive appeal and perform well empirically. An example is the BM25 RW94 method, motivated by probability. Two recent ideas that enhance scoring methods are query expansion and score regularization. In this work, we describe a general linear algebra based framework for describing and understanding scoring methods. We show how all of BM25, query expansion and score regularization can be expressed in such a framework. Further, we consider particular scoring methods in two settings: unsupervised and supervised. In the unsupervised setting, we show how query expansion and score regularization, ideas much newer than LSI, have a close relationship with LSI. We also demonstrate how the available relevance judgments in standard collections can be used to cast query expansion in a supervised setting. We evaluate all methods on classical small collections, and all LSI methods on large TREC collections. A noteworthy feature of our work is that we use LSI on the full term-document matrix for large TREC collections for our results. Doing so reveals a surprising fact: LSI fails to work on large collections. We systematically explore this phenomenon in order to understand why LSI fails. Throughout, we use optimized BM25 as a strong baseline. While several methods demonstrate a small but consistent improvement ( 4%) over this baseline, some others show promise even with limited training data
-------------------------------------

10135253_183 - 0.809124085511 - technology_and_computing
[polydisc, function, result, analytic]

Bounded analytic functions on the polydisc
In the paper 'Distinguished Varieties' Agler and McCarthy proved several connections between the theory of bounded analytic functions on the bidisc and 1-dimensional algebraic varieties that exit the bidisc through the distinguished boundary. In this paper we extend several of their results to the theory of bounded analytic functions on the polydisc. We give sufficient conditions for a rational inner function on the polydisc to be uniquely determined in the Schur class of the polydisc by it's values on a finite set of points. This follows from giving sufficient conditions for a Pick problem on the polydisc to have a unique solution. We demonstrate that our results can be though of as a generalization to the polydisc of the Schwarz Lemma and Pick's Theorem on the disc. We establish our results by studying the Pick problem with Hilbert function space techniques
-------------------------------------

10133365_183 - 0.999700322643 - technology_and_computing
[bridge, model, maintenance, optimization, deterioration, condition]

History-Dependent Optimization of Bridge Maintenance and Replacement Decisions Using Markov Decision Process
Bridge maintenance and replacement optimization methods use deterioration models to predict the future condition of bridge components. The purpose of this paper is to develop a framework for bridge maintenance optimization using a deterioration model that takes into account aspects of the history of the bridge condition and maintenance, while allowing the use of efficient optimization techniques. Markovian models are widely used to represent bridge component deterioration. In existing Markovian models, the state is the bridge component condition, and the history of the condition is not taken into account, which is seen as a limitation. This paper describes a method to formulate a realistic history-dependent model of bridge deck deterioration as a Markov chain, while retaining aspects of the history of deterioration and maintenance as part of the model. This model is then used to formulate and solve a reliability-based bridge maintenance optimization problem as a Markov decision process. A parametric study is conducted to compare the policies obtained in this research with policies derived using a simpler Markovian model.
-------------------------------------

10134488_183 - 0.98399476441 - technology_and_computing
[cell, device, response, feedback, datum, environment, dynamic, negative, concentration, gal]

Synthetic biology in yeast : reconstructing the galactose network to probe the role of feedback induction in response to metabolic stimuli
With the expanding interest in cellular responses to dynamic environments, microfluidic devices have become important experimental platforms for biological research. Microfluidic "microchemostat" devices enable precise environmental control while capturing high quality, single cell gene expression data. For studies of population heterogeneity and gene expression noise, these abilities are crucial. I have developed a microchemostat device optimized for capturing data from thousands of cells in multiple sub-experiments. The device is robust, easy to use and capable of generating precisely controlled dynamic environments. The device uses an integrated fluidic junction, coupled to linear actuators, to modulate the external port pressures as a function of time. In this way the concentration of an inducer compound can be tightly controlled without the use of mechanical mixing devices. To analyze the large amounts of data generated, I have developed a method for automated cell tracking, focusing on the special problems presented by Saccharomyces cerveisiae cells. I have used these tools to probe the response of a natural genetic circuit, the Gal system of S. cerevisiae. I have altered the regulation of the native Gal system, replacing the transcriptional positive and negative feedback loops with artificial promoters. Looking at these modified strains, I have determined that induced negative feedback is essential for tuning the cell's genetic response to the external galactose concentration. Moreover, without induced negative feedback, the system exhibits bistability, with subpopulations of responding and non-responding cells. When observing these cells in a dynamic environment, I have found that the Gal network is optimized to respond sharply to changes in the inducer concentration, regardless of the network's original induction state
-------------------------------------

10131398_183 - 0.746568297973 - technology_and_computing
[emission, electric, vehicle, electricity]

Emission Impacts of Electric Vehicles
Alternative vehicular fuels are proposed as a strategy to reduce urban air pollution. In this paper, we analyze the emission impacts of electric vehicles in California for two target years, 1995 and 2010. We consider a range of assumptions regarding electricity consumption of electric vehicles, emission control technologies for power plants, and the mix of primary energy sources for electricity generation. We find that, relative to continued use of gasoline-powered vehicles, the use of electric vehicles would dramatically and unequivocally reduce carbon monoxide and hydrocarbons. Under most conditions, nitrogen oxide emissions would decrease moderately. Sulfur oxide and particulate emissions would increase or slightly decrease. Because other areas of the United States tend to use more coal in electricity generation and have less stringent emission controls on power plants, electric vehicles may have less emission reduction benefits outside California.
-------------------------------------

10130052_178 - 0.996311427675 - technology_and_computing
[bibliographical, platform, reference, high-throughput, total, vertebrate]

High-throughput vertebrate total analysis/screening platform
Includes bibliographical references.
-------------------------------------

10136205_183 - 0.948965552128 - technology_and_computing
[ordinary, modularity, 2-adic, real, place, representation, galois, field, dihedral]

Modularity of nearly ordinary 2-adic residually dihedral Galois representations
We prove modularity of some two dimensional 2-adic Galois representations over a totally real field that are nearly ordinary at all places above 2 and that are residually dihedral. We do this by employing the strategy of Skinner and Wiles using Hida families together with the 2-adic patching method of Khare and Wintenberger. As an application we deduce modularity of some elliptic curves over totally real fields that have good ordinary or multiplicative reduction at places above 2.
-------------------------------------

10133036_183 - 0.989498494938 - technology_and_computing
[bottleneck, theory]

Moving Bottlenecks: A Theory Grounded on Experimental Observation
This paper presents the most complete picture yet of moving bottlenecks on freeways, including experimental observations and a theory. The experimental observations include the “fingerprint” of a moving bottleneck on a series of loop detectors, and a set of controlled experiments in which moving bottlenecks were artificially introduced in the traffic stream. The paper also contrasts this evidence with current theories and describes a new one that is consistent with the data.
      High-resolution oblique plots of loop detector data from freeway I-880 in Oakland (California) are used to analyze the aforementioned fingerprint. They clearly display the presence of the bottleneck and its evolution in time and space, including the precise location in space-time where it appeared. The data also reveal a fleeting but real change in the drivers’ car-following attitude shortly after the bottleneck’s appearance.
      The controlled experiments reveal that the flow downstream of the bottleneck increases with the speed of the bottleneck when the bottleneck holds back a queue—in contradiction with two previous theories (Gazis and Herman, 1992, and Newell, 1993).
      The new theory includes these as special cases. It treats the moving bottleneck as a boundary condition that can be integrated with kinematic wave (KW) theory and also with variants of this theory that account for multiple vehicle types and changes in driver psychology. The empirical evidence suggests that the lengths of queues upstream of moving bottlenecks and the ensuing vehicle delays can now be predicted with good accuracy.
-------------------------------------

10133550_183 - 0.99987354918 - technology_and_computing
[feature, online, visual, system, efficient, classification, algorithm, extraction, coding]

An efficient online feature extraction algorithm for neural networks
Finding optimal feature sets for classification tasks is still a fundamental and challenging problem in the area of machine learning. The human visual system performs classification tasks effortlessly using its hierarchical features and efficient coding in its visual pathway. It is shown that early in the visual system the information is encoded using distributed coding schemes and later in the visual system the sparse coding is utilized. We propose a biologically motivated method to extract features that encode the information according to a specific activation profile. We show how our model much like the visual system, can learn distributed coding in lower layers and sparse coding in higher layers in an online manner. Online feature extraction is used in biometrics, machine vision, and pattern recognition. Methods that can dynamically extract features and perform online classification are especially important for real-world applications. We introduce online algorithms that are fast and efficient in extracting features for encoding and discriminating the input space. We also show a supervised version of this algorithm that performs feature selection and extraction in alternating steps to achieve a fast convergence and high accuracy
-------------------------------------

10133939_183 - 0.999489261169 - technology_and_computing
[c-core, transistor, power, challenge, energy, conservation, core, memory, code, design]

Design and architecture of automatically-generated energy- reducing coprocessors
For many years, improvements to CMOS process technologies fueled rapid growth in processor performance and throughput. Each process generation brought exponentially more transistors and exponentially reduced the per- transistor switching power. However, concerns over leakage currents have moved us out of the classical CMOS scaling regime. Although the number of available transistors continues to rise, their switching power no longer declines. In contrast to transistor counts, power budgets remain fixed due to limitations on cooling or battery life. Thus, with each new process generation, an exponentially decreasing fraction of the available transistors can be simultaneously switched. The growing divide between available transistors and utilizable transistors leads to a utilization wall. This dissertation characterizes the utilization wall and proposes conservation cores as a means of surmounting its most pressing challenges. Conservation cores, or C-Cores, are application-specific hardware circuits created to reduce energy consumption on computationally-intensive applications with complex control logic and irregular memory access patterns. C-Cores are drop-in replacements for existing source code, and make use of limited reconfigurability to adapt to software changes over time. The design and implementation of these specialized execution engines pose challenges with respect to code selection, automatic synthesis, choice of programming model, longevity/robustness, and system integration. This dissertation addresses many of these challenges through the development of an automated conservation core toolchain. The toolchain automatically extracts the key kernels from a target workload and uses a custom C-to- silicon infrastructure to generate 45ñm implementations of the C-Cores. C-Cores employ a new pipeline design technique called pipeline splitting, or pipesplitting. This technique reduces clock power, increases memory parallelism, and further exploits operation-level parallelism. C-Cores also incorporate specialized energy- efficient per-instruction data caches called cachelets into the datapath, which allow for sub-cycle cache- coherent memory accesses. An evaluation of C-Cores against an efficient in-order processor shows that C-Cores speed up the code they target by 1.5x, improve EDP by 6.9x and accelerate the whole application by 1.33x on average, while reducing application energy-delay by 57%
-------------------------------------

10133009_183 - 0.996482107617 - technology_and_computing
[library, application, language, sensor, service, network, construction]

A Sensor Network Application Construction Kit
We propose a new configuration language, component and service library, and compiler that make it easier to develop efficient sensor network applications. Our goal is the construction of smart application service libraries: high-level libraries that implement concepts like routing trees and periodic sensing, and that combine automatically into efficient programs. Important language features include flexible control over component sharing and transitive arrow connections, which let independently-implemented services knit themselves into integrated control flow paths. Our language, library, and compiler are collectively called SNACK (Sensor Network Application Construction Kit).
-------------------------------------

10138895_183 - 0.995147699221 - technology_and_computing
[dice, implementation, time, model]

A 4-stated DICE: quantitatively addressing uncertainty effects in climate change
We introduce a version of the DICE-2007 model designed for uncertaintyanalysis. DICE is a wide-spread deterministic integrated assessment model of climatechange. However, climate change, long-term economic development, and theirinteractions are highly uncertain. A thorough empirical analysis of the effects ofuncertainty requires a recursive dynamic programming implementation of integratedassessment models. Such implementations are subject to the curse of dimensionality.Every increase in the dimension of the state space is paid for by a combinationof (exponentially) increasing processor time, lower quality of the value function andcontrol rules approximations, and reductions of the uncertainty domain. The paperpromotes a four stated recursive dynamic programming implementation of the DICEmodel. Our implementation solves the infinite planning horizon problem for an arbitrarytime step. Moreover, we present a closed form continuous time approximationto the exogenous (discretely and inductively defined) processes in DICE and presenta Bellman equation for DICE that disentangles risk attitude from the propensity tosmooth consumption over time.
-------------------------------------

10136038_183 - 0.893700328843 - technology_and_computing
[liquid-based, mechanical, bridge, liquid, micro, structure, force]

Study of Characteristics of Liquid-Based Bridge Structure as Mechanical Elements
The intriguing mechanical characteristics of liquid bridge have drawn significant attentions in micro scale applications.  In the present work, we first study liquid-based bridges as reversible elements in multifunctional mechanical micro devices. The dependence of the liquid shape and rupture distance on the intrinsic contact angles, liquid volumes, pattern radius ratios, as well as the capillary force is characterized numerically. A comprehensive investigation is also conducted about liquid-based rings as bearing structures in electrostatically driven micro rotary stages. The numerical results of vertical force and viscous force help optimize the geometry and operation design of the liquid-based ring structure, while the horizontal displacement and tilting analysis contribute to prevent potential mechanical failures in fabrication and assembling. The modified numerical models and the capillary performance characterization methods can be extensively applied to liquid-based bridge studies in other specific applications.
-------------------------------------

10130284_178 - 0.999995861996 - technology_and_computing
[mapping, transparency, interactive, digital, bibliography]

Digital transparency applied to interactive mapping
Bibliography: p. 81-84.
-------------------------------------

10138267_183 - 0.999905063652 - technology_and_computing
[beam, electron, laser-plasma, transport]

Transport and Non-Invasive Position Detection of Electron Beams from Laser-Plasma Accelerators
The controlled imaging and transport of ultra-relativistic electrons from laser-plasma accelerators is of crucial importance to further use of these beams, e.g. in high peak-brightness light sources. We present our plans to realize beam transport with miniature permanent quadrupole magnets from the electron source through our THUNDER undulator. Simulation results demonstrate the importance of beam imaging by investigating the generated XUV-photon flux. In addition, first experimental findings of utilizing cavity-based monitors for non-invasive beam-position measurements in a noisy electromagnetic laser-plasma environment are discussed.
-------------------------------------

10135915_183 - 0.996045880632 - technology_and_computing
[current, model, device, gain, field, different, electron, hole, feedback, positive]

Investigation and modeling of impact ionization spatial-transient effects in silicon devices
Impact ionization (II) has played an important role in semiconductor devices; yet the understanding of II has not been mature. Abnormal behaviors related to II in deep sub-micrometer devices were observed and have not been fully explained. Existing models are not rigorously applicable to predicting II in different device structures and different operational regimes. Monte Carlo (MC) programs simulating transport of both electrons and holes are developed to investigate II in homogeneous electric field and in scaled devices. The programs' accuracy is verified by accurately producing many different transport parameters obtained from both experiments and previous MC simulations' results. Impact ionization, for the first time, is modeled as a positive feedback loop in which electrons create holes, and the secondary holes feed back secondary electrons. This model is analytically proven to be valid for short devices due to the existence of the II dead-space. This model is also numerically proven to be accurate by producing a good fit to the experimental data. It is easy to conclude from the positive feedback model that the breakdown voltage is the same for both the electron-initiating and hole-initiating II processes in a high field region. In addition, the positive feedback model also shows that the current gain from the electron-initiating II process is always higher than the current gain from the hole-initiating II process within the same high field region. More importantly, the positive feedback loop enables successful simulations of the II process in which both electrons and holes participate simultaneously. This is particularly important at high current gain. An efficient algorithm is also developed to speed up spatial transient simulations by implementing temporal meshes rather than the traditional spatial meshes. The II current gain in short p-i-n diodes is studied. The calculated results fit well to the experimental data of diodes with different lengths. Various physical insights are learned from the simulations. The minimum breakdown voltage for highly doped junctions is extrapolated to be at least 4.41V. Franz-Keldysh effect plays a significant role at low bias, especially for short devices. For the first time, Franz-Keldysh effect is invoked to explain the experimental current gain. II threshold energy is not constant with respect to the electric field, which partly explains various values of the reported threshold energy. II threshold energy is higher for holes than for electrons. Both electron and hole II coefficients come to equilibrium with the electric field after a dead-space distance. This spatial transient effect is a major cause for the disagreements among the experimentally-extracted values of the II coefficients. The values extracted from the double drift p-n junction experiments are more reliable in terms of accounting for the II spatial-transient effect. The II spatial-transient effect is identified to be the main cause for the failures of different well-known II models for semiconductor devices. A pseudo-local electric field model and the positive feedback model are proposed and proven to be sufficient in predicting the II current gain in short devices.MC simulations are conducted to study mixed tunneling and II process in short p-n diodes, which are potential terahertz source devices. Tunneling current is treated as generation current, which is also subject to the tunneling dead-space distance. Another gain stage is added on top of the positive feedback model to account for the tunneling dead-space. II is less important for more heavily doped p-n junctions. The contribution of the diffusion current and its II is negligible compared to the tunneling counterparts. Abnormal behaviors of II in deep sub-micrometer MOSFETs are investigated and explained. Channel carrier distribution functions are generated by MC simulations employing the rare-state algorithm. The thermal tail of the distribution function is Maxwellian with the lattice temperature as the effective temperature. By formulating the thermal tails as functions of position and bias voltage, an analytical formula of the substrate current is successfully derived for the first time. The formula is then used to explain experimental results of the substrate current in a sub-micrometer pMOSFET. The newly-developed formula is able to explain different abnormal behaviors of the substrate current that cannot be explained by the conventional formulas.
-------------------------------------

10129716_178 - 0.999395298081 - technology_and_computing
[reference, model, simulation, bibliographical, expenditure, town]

A simulation model of town expenditures.
Includes bibliographical references.
-------------------------------------

10137903_183 - 0.999968841202 - technology_and_computing
[software, simulation, geometry, stochastic, urdme, model, biological, complex, modeling, algorithm]

URDME: a modular framework for stochastic
simulation of reaction-transport processes in
complex geometries
Abstract
            
            
               
                  Background
               
               Experiments in silico using stochastic reaction-diffusion models have emerged as an important tool in molecular systems biology. Designing computational software for such applications poses several challenges. Firstly, realistic lattice-based modeling for biological applications requires a consistent way of handling complex geometries, including curved inner- and outer boundaries. Secondly, spatiotemporal stochastic simulations are computationally expensive due to the fast time scales of individual reaction- and diffusion events when compared to the biological phenomena of actual interest. We therefore argue that simulation software needs to be both computationally efficient, employing sophisticated algorithms, yet in the same time flexible in order to meet present and future needs of increasingly complex biological modeling.
            
            
               
                  Results
               
               We have developed URDME, a flexible software framework for general stochastic reaction-transport modeling and simulation. URDME uses Unstructured triangular and tetrahedral meshes to resolve general geometries, and relies on the Reaction-Diffusion Master Equation formalism to model the processes under study. An interface to a mature geometry and mesh handling external software (Comsol Multiphysics) provides for a stable and interactive environment for model construction. The core simulation routines are logically separated from the model building interface and written in a low-level language for computational efficiency. The connection to the geometry handling software is realized via a Matlab interface which facilitates script computing, data management, and post-processing. For practitioners, the software therefore behaves much as an interactive Matlab toolbox. At the same time, it is possible to modify and extend URDME with newly developed simulation routines. Since the overall design effectively hides the complexity of managing the geometry and meshes, this means that newly developed methods may be tested in a realistic setting already at an early stage of development.
            
            
               
                  Conclusions
               
               In this paper we demonstrate, in a series of examples with high relevance to the molecular systems biology community, that the proposed software framework is a useful tool for both practitioners and developers of spatial stochastic simulation algorithms. Through the combined efforts of algorithm development and improved modeling accuracy, increasingly complex biological models become feasible to study through computational methods. URDME is freely available at http://www.urdme.org.
-------------------------------------

10133743_183 - 0.999297471563 - technology_and_computing
[device, one-dimensional, nanostructure, application, nanoscale, property, electrical]

Fabrication, Device Assembly, and Application of One-Dimensional Chalcogenides Nanostructures
Nanotechnology has received a tremendous amount of research interests ever since the first discovery of carbon nanotubes.  One-dimensional nanostructures, such as nanorods, nanowires, nanobelts as well as nanotubes, are of significant interest because of their potential application as interconnects and functional units in nanoscale electrical, optoelectronic, electrochemical, electromechanical, thermoelectric, spintronic, photovoltaic, and sensory devices.  Nanoscale one-dimensional devices promise to deliver improved performance, to miniaturize bulky devices, to enable higher density nanoscale devices, and to lower energy consumption.  As the radius of these one-dimensional nanostructures fall below the exciton Bohr radius of their respective materials, the structural morphology and size effectively modulates the fundamental electrical, optical, and magnetic properties due to quantum confinement effect.  In addition, the high surface to volume ratio of one-dimensional nanostructures enables the device properties to be extremely sensitivity to the environment which is particularly attractive for sensing application.Currently, the focuses of nanotechnology research are 1) the fabrication technique with control over the composition, crystal structure, morphology, and size, 2) the device assembly of nanostructures into complex functional devices, and 3) the characterization and application of these nanoscale devices.  There are a multitude of fabrication techniques for one-dimensional nanostructures, including but not exclusively, vapor-solid, vapor-liquid-solid, colloidal, solution-liquid-solid, self-assembly, and template directed electrodeposition.  As one-dimensional nanostructures are produced, several techniques are available to assemble them into functional complex nanoscale devices, including but not exclusively, electron beam lithography, focus ion beam, magnetic assembly, and AC dielectrophoretic alignment.In this work, one-dimensional cadmium telluride (CdTe) nanostructures are fabricated via the template directed electrodeposition.  Fundamental properties, such as composition, crystal structure, morphology, size, electrical, and optoelectronic properties, are examined.  The tuning of electrical and optoelectronic properties by the modulation of various material characteristics are demonstrated for potential photodetection application.  To demonstrate biosensing application of one-dimensional nanostructure, label-free DNA recognition and sensing application capable of femtomolar detection is achieved with a single bismuth telluride (Bi<sub>2</sub>Te<sub>3</sub>) nanoribbon biosensing device.
-------------------------------------

10131679_183 - 0.847021669126 - technology_and_computing
[train, datum, california, high-speed, cost, study, physical, estimate, economic, report]

High Speed Trains for California (Volume II: Detailed Segment Descriptions, Cost Estimates, and Travel Time Calculations)
This report represents that conclusions of the first year of IURD's study of the potential for a high-speed passenger train service in California. Seven previous studies have each dealt with a specific high-speed train technology; each attempted an evaluation, standardized so far as data permitted, of its technical and economic viability.
      The present report first summarizes and synthesizes these seven studies, attempting a systematic point-by-point comparison. Then it goes on to develop a possible high-speed network for California in the light of known facts about the state's physical and economic geography. It develops physical profiles for such a route, and uses available cost data to produce an estimate of total construction cost. It gives simulations of timings between the major urban areas. These data will be used as basic inputs to the second stage of the work, now under way, which will analyze the market prospects for such a system and the ways in which it may be financed.
-------------------------------------

10135402_183 - 0.99998601148 - technology_and_computing
[robot, eod, controller, lyapunov, navigation, algorithm]

Robot Trac School : improving autonomous navigation in EOD robots
Advancements in the autonomous navigation of robots increases the range of behaviors that can be implemented, consequently increasing the utility of the robots to end users. To achieve these advancements, the state estimation and controls algorithms for Explosives Ordinance Disposal (EOD) robots have been studied and improved. In this work, I integrated a high precision, differential GPS system to measure ground truth positions, which were then used to find more accurate system and measurement noise covariance values. The more accurate noise models improved the state estimate of an extended Kalman filter. Independently, a model-based control law was implemented for a vehicle with nonholonomic unicycle constraints kinematics using a Lyapunov method. The Lyapunov controller was implemented on several different EOD robots and is compared to the previously existing PID controller with respect to navigation near simulated obstacles and in open space. Practical considerations for tuning the Lyapunov controller design variables are explored, and recommendations are given for several operating scenarios. The improved algorithms were implemented using multiple different robots. The algorithms are currently running on EOD robots used in the field. This work will accelerate development of advanced maneuvers, such as retroverse over long distances as well as obstacle avoidance
-------------------------------------

10133869_183 - 0.923138713227 - technology_and_computing
[veneer, seismic, design, performance, response, anchor, experimental]

Experimental and analytical investigation of the seismic performance of low-rise masonry veneer buildings
This dissertation presents an experimental and analytical evaluation of the seismic performance of clay masonry veneer in wood-stud buildings. The experimental program involved the shaking-table testing of eleven wall assemblies as well as a full-scale one-story building. Walls were subjected to separate in- and out-of-plane seismic excitation. The specimens had different anchor types, anchor spacing, aspect ratio, presence and absence of joint reinforcement and window openings. All the specimens were designed and constructed in accordance with the prescriptive requirements of the MSJC for masonry veneer for Seismic Design Categories D and E. The shaking- table tests showed that veneer complying with the current MSJC provisions can sustain ground motions far in excess of representative Design Basis and Maximum Considered Earthquakes for Seismic Design Categories D and E. The out -of-plane response was governed by the anchor axial strength. The in-plane response was characterized by veneer sliding for the squat panels and a combination of rocking and sliding for the slender ones. Experiments showed the possible reduction of the extraction capacities of the nails due to high moisture content in wood studs. Test data showed that veneers oriented parallel to the direction of shaking could restrain the wood structure. However, slender rocking veneer panels would induce additional seismic force to the wood structure under severe excitation. In the analytical phase of the research, numerical models were developed and calibrated by the experimental results. The models were capable of capturing the displacement and acceleration responses of the tested specimens as well as the failure mechanisms. The models were used to conduct parametric studies to examine the influence of different design variables on veneer response, including the effect of in-plane veneer on the seismic performance of wood-stud shear walls, and the behavior of two-story veneers under out-of-plane seismic excitation. The dissertation finally proposes a set of recommendations to improve current design provisions. Results highlighted the need for a minimum anchor strength requirement to assure a satisfactory level of performance under the Design Basis and Maximum Considered Earthquakes. The mass of the squat veneers oriented parallel to the direction of motion should not be treated as merely added mass
-------------------------------------

10137772_183 - 0.995535192297 - technology_and_computing
[time, continuous, communication]

Continuous Time and Communication in a Public-goods Experiment
We investigate the nature of continuous-time strategic interactions in public-goodsgames. In one set of treatments, four subjects make contribution decisions in continuous timewhile in another they make them only at discrete points of time. The effect of continuous timeis muted in public-goods games compared to simpler social dilemmas; the data suggest thatwidespread coordination problems are to blame. With a rich communication protocol, thesecoordination problems disappear and the median subject contributes fully to the public good,with no time decay. At the median, the same communication protocol is less than half aseffective in discrete time.
-------------------------------------

10134963_183 - 0.99999925986 - technology_and_computing
[filter, mhz, system, antenna, db, silicon, diode, high, single, bandwidth]

High performance channelizers, tunable notch filters, and silicon-based antennas for RF to millimeter-wave communication systems
This thesis first presents a 26-channel channelizer based on the mammalian cochlea and covering the 20-90 MHz band. Each channel has a 6-pole frequency response with a constant absolute bandwidth of 1.4 MHz at 20-30 MHz, and a constant fractional bandwidth of 4.5±0.6% at 30-90 MHz, and is built entirely using lumped elements. Measurements show an S₁₁ < -12 dB at 20-90 MHz, a loss of 4-7 dB, > 40 dB isolation between the channels, and agree well with simulations. The applications areas are in communication systems with very high levels of interferes and in defense systems. In another project, tunable lumped-element bandstop filters for the UHF-band cognitive radio systems are presented. The 2-pole filters are implemented using lumped elements with both single- and back-to-back silicon varactor diodes. The single diode filter tunes from 470 to 730 MHz with a 16-dB rejection bandwidth of 5 MHz and a filter quality factor of 52-65. The back-to-back diode filter tunes from 511 to 745 MHz also with a 16-dB rejection bandwidth of 5 MHz and a quality factor of 68- 75. Both filters show a low insertion loss of 0.3-0.4 dB. Nonlinear measurements at the filter null with deltaf = 2 MHz show that the back-to-back diode filter results in 12-dBm higher third order intermodulation intercept point (IIP3) than the single diode filter. A scaling series capacitor is used in the resonator arm of the back-to-back diode filter and allows a power handling of 25 dBm at the 16 dB rejection null. The cascaded response of two tunable filters is also presented for multi-band rejection applications, or for a deeper rejection null (> 36 dB with 0.6 dB loss at 600 MHz). The topology can be easily extended to higher-order filters and design equations are presented. The third project presents on-chip slot-ring and horn antennas for wafer-scale silicon systems. A high efficiency is achieved using a 100 mum quartz superstrate on top of the silicon chip, and a low loss microstrip transformer using the silicon backend metalization. A finite ground plane is also used to reduce the power coupled to the TEM mode. The slot-ring and 1- lambda²/₀ horn achieve a measured gain of 0-2 dBi and 6- 8 dBi at 90-96 GHz, respectively, and a radiation efficiency of ̃50%. The horns achieve a high antenna gain without occupying a large area on the silicon wafer, thus resulting in a low cost system. The designs are compatible with either single or two-antenna transceivers, or and with wafer scale imaging systems and power-combining arrays. To our knowledge, this is the highest gain on-chip antenna developed to-date. Finally, differential on-chip microstrip and slot-ring antennas for wafer-scale silicon systems are presented. The antennas are fed at the non- radiating edge which is compatible with differential coupled-lines, and are built on a 0.13-mum CMOS process with a layout which meets all the metal density rules. A high radiation efficiency is achieved using a 100 mum quartz superstrate placed on top of the silicon chip. Both antennas achieve a measured gain of ̃3 dBi at 91-94 GHz, with a -10 dB S₁₁ bandwidth of 7-8 GHz and a radiation efficiency of >50%. The designs are compatible with single and multi-element transceivers, and with wafer-scale imaging systems and power combining arrays. To our knowledge, this is the first demonstration of high- efficiency on-chip differential antennas at millimeter- wave frequencies
-------------------------------------

10133027_183 - 0.998021593815 - technology_and_computing
[information, side, network, explicit]

Correlated Data Routing in Sensor Networks
We study the correlated information routing with source coding using explicit side information in wireless sensor networks. This problem belongs to the difficult joint compression/routing optimization, and has its connections to the network information theory. We present our network and rate models based on explicit side information available at both the encoder and decoder, then introduce the integer programming formulation with a few simplifying assumptions. A two-stage DPCM coding scheme that can take advantage of the explicit side information and report the additional spatial is also included.
-------------------------------------

10132688_183 - 0.999061807098 - technology_and_computing
[ivh, vehicle, technology, scenario, current, goal, vision, highway, system]

Intelligent and Environmentally-Sensible Transportation System: An Alternative Vision
A recent US DOT plan guiding IVHS research correctly notes that, "Over the next 20 years, a national
      IVHS program could have a greater societal impact than even the Interstate Highway System".  But what will those impacts be? What could they be?
      The primary thrust of current IVHS initiatives is to accommodate more vehicles more safely using existing roadspace. The principal focus is on two sets of technologies: 1) real-time information to manage traffic flows better; and 2) automated controls to pack vehicles closer together. A variety of other applications are also being pursued, including transit and goods movement, but are receiving much less attention and government resources. The benefits of current IVHS initiatives are coming under increasing scrutiny. It appears unlikely that deployment of IVHS technologies, other than automated vehicle controls, will lead to major congestion reductions or road capacity expansions (e.g., Hall, 1993; AI-Deek et al, 1989). Highway automation could provide large capacity improvements, but perhaps at a huge economic, environmental, and social cost (Burwell, 1993, Gordon, 1992, Johnston and Ceerla, 1994).
      The current thrust of IVHS activities, as indicated above, has its historical origins in the highway engineering community, it is described in detail in the 1993 Draft National Program Plan for IVHS prepared by IVHS AMERICA. One might extrapolate these unfolding IVHS initiatives into the future and treat them as one potential IVHS scenario. It is a scenario that could be described as a pragmatic attempt to guide the development and deployment of information and control technologies or, less charitably, as a reductionist engineering approach to the problem of congestion and safety.
      An alternative IVHS vision is proposed here. The overarching goal inspiring this vision is increased accessibility -- not mobility, that is, improved access to goods and services, but with little or no increase in vehicle travel. Three complementary goals, suppressed or ignored in current IVHS activities, are also fundamental to this alternative vision greater consideration of the less privileged, enhanced environmental quality, and community liability.
      Pursuit of these goals would lead to a very different transportation future than in the first scenario. Many of the same IVHS products would be commercialized and promoted in both scenarios, with the difference being that in this second scenarios government more actively supports products and activities that benefit lower income classes and the environment. Government marshals its R&D resources, infrastructure investments, and rulemaking authority in such a way that goals of accessibility, equity, and environmental quality dominate the design of the overall system architecture. The many effects of IVHS technologies on travel behavior, land use patterns, vehicle acquisition decisions of households and businesses, and corporate logistical and facility location decisions are treated as primary impacts. The power of IVHS technologies to transform the urban and social landscape, similar to that of the Interstate Highway System, is acknowledged and harnessed.
-------------------------------------

10133426_183 - 0.999968395779 - technology_and_computing
[processing, stream, application, system, resource]

Rate Allocation in Distributed Stream Processing Systems
In today's world, stream processing systems have become important, as applications like media broadcasting, sensor network monitoring and on-line data analysis increasingly rely on real-time stream processing. At the same time, service overlays that support distributed stream processing applications are increasingly being deployed in wide-area environments. The inherent heterogeneous, dynamic and large-scale nature of these systems makes it difficult to meet the Quality of Service (QoS) requirements of distributed stream processing applications. This has necessitated the investigation of mechanisms that improve their scalability, efficiency and performance. In the first part of this work we consider the problem of composing stream processing applications in a distributed stream processing system. First, we propose a distributed stream processing system that composes stream processing applications dynamically, while meeting their rate demands. Second, we address the load balancing problem for distributed stream processing applications and present a decentralized and adaptive algorithm that allows the composition of distributed stream processing applications on the fly across a large-scale system, while satisfying their QoS demands. The algorithm fairly distributes the load on the resources and adapts dynamically to changes in the resource utilization or the QoS requirements of the applications.In the second part, we present BARRE (Burst Accommodation through Rate REconfiguration), a method to address the problem of burst accommodation in a distributed stream processing system. Upon the emergence of a burst, BARRE dynamically reserves the resources dispersed across the nodes of a distributed stream processing system, based on the requirements of each application as well as the resources available on each node. Our experimental results over a real distributed stream processing testbed demonstrate the efficiency of our approach.
-------------------------------------

10133799_183 - 0.99999765463 - technology_and_computing
[capability, datum, time, location, cluster, singular, decomposition, combination, value, false]

Machine learning for context-aware reminders and suggestions
People rapidly learn the capabilities of a new location, without observing every service and product. Instead they map a few observations to familiar clusters of capabilities, and assume the availability of other capabilities in the cluster. This dissertation proposes a similar approach to computer-based discovery of routine location capabilities, applying singular value decomposition to predict unobserved capabilities based on a combination of a small body of local observations and a larger body of data that is not specific to the location. I propose using the time and place of deleting items from a to-do list application to provide the local data. I also examined the effect of feedback on false positive errors, combined with a weighted singular value decomposition. For reminder purposes, an area within easy walking distance is a single location, but may contain many different shops and services, collectively offering its own combination of capabilities. A simple clustering algorithm would treat each combination as an independent cluster. Truncated singular value decomposition maps the observations to combinations of features, rather than to a single cluster. Simulations, using distributions derived from real world data, demonstrate the feasibility of this approach.The robustness of the technique was further tested by adding two difficulties, convenience stores and false training data. The convenience-store workload included some locations that provided only the thousand most frequently used capabilities, regardless of other cluster data. False positive feedback and feature weighting both allowed use of a larger truncation rank, improving convenience store results, and reduced errors due to false training data. The technique extends to estimate whether a capability is available at a given time. Data for short time intervals was "folded-in" to the singular value decomposition to obtain projections for those time intervals. The projections, interpreted as Poisson distribution arrival rates, were used to compare posterior probabilities for various time intervals given the observed data. The time extension was tested with workloads that included 24 hour supermarkets and early opening for a subset of capabilities at one location
-------------------------------------

10132676_183 - 0.998929674846 - technology_and_computing
[problem, dynamic, algorithm, salesman, stochastic]

Dynamic anad Stochastic Routing Optimization: Algorithm Development Analysis
The last several years has witnessed a sharp increase in interest in stochastic and dynamic routing and scheduling. Because many systems contain inherently stochastic factors, decisions must often be made before all necessary information is available. To a certain degree, algorithm development has lagged behind implementation. In order to fully leverage advances in information technologies, algorithms which explicitly consider dynamic and stochastic factors should be examined. Or, if static algorithms are to be applied in these dynamic environments, proper attention should be given to examining the conditions under which these perform well. This is the primary theme of this research.
      This dissertation examines several key dynamic and stochastic routing and scheduling problems: the probabilistic traveling salesman problem, the dynamic traveling salesman problem and the dynamic traveling repair problem. In addition, as part of our research on the dynamic traveling salesman problem, we examine a related M/G/l queuing problem with switching costs. These problems arise in pickup and and delivery options, repair fleet operations, and emergency vehicle and policy operations in addition to many computing, telecommunications and manufacturing applications.
      As part of our research, we demonstrate that heuristics which rely on partitioning the service region into smaller regions can be very effective for dynamic routing problems. Using a partitioning scheme we show that if a constant guarantee algorithm exists for the k- capacitated median problem, then a constant guarantee algorithm exists for the probabilistic traveling salesman problem. For the DTRP, we show that a partitioning algorithm is asymptotically optimal when the traffic intensity is high.
      We show that robust a priori algorithms can be developed for dynamic routing problems. For the M/G/l with switchover cost, we show that an a priori cyclic polling algorithm works very well using both theoretical and simulation analysis. Cyclic polling algorithm also works well for dynamic traveling salesman problem. For these both problems, we identify certain conditions under which the a priori (cyclic polluting) solution is close to optimal. We demonstrate that the existence of the connection between the static and dynamic vehicle routing and scheduling problem that have been observed by earlier researchers.
-------------------------------------

10136065_183 - 0.721423866015 - technology_and_computing
[material, property, structure, framework, structural, crystalline, function]

Crystalline Open Framework Materials: from Structure to Function
The open framework materials have emerged as a highly active and rapidly developing domain of research because their rich host-guest chemistry could be utilized to develop promising materials such as absorbents, catalysts, sensors and substrates for mechanistic studies into fundamental surface interactions and catalytic mechanisms.To better understand, predict and design materials with desired properties and functions, it is often essential to know the detailed structures, crystalline materials with well-defined periodic arrangements that allow for structural studies at the atomic resolution are of special importance. In addition, some properties such as uniform porosity and associated shape- or size-selectivity can be best designed with crystalline materials.Unlike some other materials such as semiconductors in which properties can be dramatically altered by simple doping, properties of open-framework materials are closely correlated with their general chemical compositions, and structural and geometrical features. As such, a great emphasis has been placed on creating materials with new crystal structures, in contrast with doping of well-known structure types. Hence the focus of this study is on the introduction of novel chemical-physical functions to the crystalline porous solids by fully exploring and understanding fundamental structural features and their possible correlation to properties and functions.The first part of this work presented here is on creating pure inorganic framework structures, specifically metal-chalcogenide supertetrahedral-cluster-based frameworks, together with a discussion on their structure-property relationships. A series of such molecular architectures have been built by directed assembly of supertetrahedral clusters that can be viewed as artificial atoms with tunable radii. A new class of "hollowed-out bulk semiconductor" materials has been achieved. Both experimental evaluations and theoretical simulations on some pure phase compounds have been performed. Based on these new structures, the relationship between materials' structural features and their optoelectronic properties is discussed.The later parts of the work presented here deal with hybrid porous solids, specifically synthesis, characterizations and novel functions of metal-organic framework materials (MOF). By designed synthesis, one MOF material with unusual 4-level structural hierarchy was assembled. Its magnetic property was evaluated, revealing an interesting geometry-induced magnetic frustration mechanism. Also, a catalytically active MOF was synthesized and characterized. Most importantly, single-crystal X-ray crystallography has been employed to capture the detailed molecular conformations of reaction intermediates on the active sites, which represents a powerful example of using a MOF-based novel methodology to elucidate catalytic mechanisms.
-------------------------------------

10137171_183 - 0.995638842253 - technology_and_computing
[datum, spike, wireless, spike-sorting, system]

Neural Spike Sorting in Hardware: From Theory to Practice
Brain-machine interfaces require real-time, wireless signal acquisition systems. However, wireless transmission of raw data is impossible for high-channel-count systems given the power constraints. Data rates could be reduced, thereby enabling wireless data transmission, by performing spike sorting--mapping each recorded action potential to the neuron that generated it--on a DSP at the recording site and transmitting only the sorting results. Our first objective was to design such a DSP. We first developed a standardized dataset and methodology in order to perform an extensive, unbiased comparison of published spike-sorting algorithms to determine which would be most appropriate for hardware implementation. We then considered various implementation issues, such as whether analog or digital spike detection is more efficient and how best to quantize neural signals. This work led to two low-power digital spike-sorting chips. Our second objective was to provide an offline solution for the research setting that would accelerate the processing of data that has already been recorded using conventional data-acquisition systems. Here, we present an FPGA-based spike-sorting platform that can  increase the speed of offline spike sorting by at least 25 times, effectively reducing the time required to sort data from long experiments from several hours to just a few minutes. We attempted to preserve the flexibility of software by implementing several different algorithms in the design, and by providing user control over parameters such as spike detection thresholds.
-------------------------------------

10133628_183 - 0.9999997108 - technology_and_computing
[predistortion, system, uwb, transmitter, power]

The predistortion system for an ultrawideband transmitter
The UWB standard became increasingly popular platform for implementing short range high data rate communication in various types of consumer electronic products. The UWB standard offers an economically viable solution for the personal area multimedia communication networks providing ultra low power consumption and minimal interference with the existing wireless networks (such as WiFi, Blue Tooth, Cellular). The need to maintain low power consumption while providing a reliable link justifies the development of a RF Transmitter Predistortion Linearization System, especially at the upper part of the useful frequency band of 6-10GHz, where larger propagation path loss and antenna interconnect losses are anticipated. The proposed Tx Predistortion System allows an increase in the maximum usable linear power of the UWB transmitter on the order of 2-3dB. The digitally assisted analog baseband predistortion approach was chosen as cost and power efficient method suitable for the commercial UWB chipset. In this thesis named " The Predistortion System for an Ultrawideband Transmitter " Victor Korol reports design and performance of the transmitter linearization system using a baseband analog predistortion approach. The system is implemented as part of the commercial UWB transceiver chip in a 0.13um CMOS process
-------------------------------------

10131680_183 - 0.997866115398 - technology_and_computing
[air, standard, area]

Marketable Credits for Light-Duty Vehicle Emission Control in California
Recent data indicate that many U.S. areas still fail to meet national ambient air quality standards (NAAQS) for one or more criteria pollutants. In 1990, ninety-six U.S. metropolitan areas violated the federal ambient ozone standard, and forty-one areas violated the standard for carbon monoxide (CO), affecting over 100 million people (U.S. EPA, 1990a). The federal Clean Air Act (CAA) Amendments of 1990 established more stringent control measures to further reduce air pollutants (U.S. EPA, 1990b) in an effort to attain air quality standards.
-------------------------------------

10137504_183 - 0.999935449259 - technology_and_computing
[model, system, control, finite, state, physical, software, plant, embedded, cpss]

Control of cyber-physical systems using incremental properties of physical systems
Cyber-Physical Systems (CPSs) are complex systems resulting from intricate interaction of digital computational devices with the physical systems. With the recent dazzling advances in computational devices, CPSs have become ubiquitous in modern technology. The increasing presence of CPSs on one hand and the incapability of current methods to analyze them on the other hand, impel the development of novel approaches for analysis and design. In CPSs, embedded computers have the responsibility of monitoring and controlling the physical plants using feedback loops using which physical plants affect computations and vice versa. In these closed-loop fashions, controllers implemented in software are termed embedded control software. Increasing use of embedded control software in life critical applications, such as aircraft flight control systems and automotive engine control systems, demands lots of efforts on software verifications and validations which are very costly. On the other hand, by changing the center of gravity from verification to design, it is possible to synthesize correct-by-design embedded control software while providing formal guarantees of correctness. The foundation of this proposed approach relies on some technical results showing how to construct equivalent finite state models for differential equation models describing physical plant. These finite state models are simpler descriptions of physical plant in whicheach state of the finite model represents a collection or aggregate of states in the physical plant. Similar finite state models are used in software and hardware modeling, which enable the composition of such models with the finite models of the physical systems. The results of this composition are finite models capturing the behavior of the physical systems interacting with the digital computation devices. Once such models areavailable, the methodologies and tools developed in computer science for verification and control synthesis purposes can be easily employed to physical systems, via these models. In the first part of this thesis I take an important step in my quest to synthesize correct-by-design embedded control software for CPSs by constructing finite state models for control systems. I propose a novel technique to compute bisimilar finite state models of incrementally stable nonlinear control systems. I show on practical examples that the finite state models computed by my procedure can be several orders of magnitude smaller than existing approaches. Moreover, I propose another technique to compute (not necessarily bisimilar) finite state models of any nonlinear control system as long as I am interested in its behavior in a compact set. In the second part of this thesis I will show some incremental properties under which nonlinear control systems admit finite state models. I propose some analysis tools to check those properties. Moreover, I provide some design techniques providing controllers enforcing those incremental properties for some special classes of nonlinear control systems.
-------------------------------------

10129743_178 - 0.999611625297 - technology_and_computing
[transportation, system, reference, mile, bibliographical, performance, last]

Approximating the performance of a last mile transportation system
Includes bibliographical references (p. 113).
-------------------------------------

10175438_189 - 0.985722665436 - technology_and_computing
[charge, transport, model, behavior, graphene, contact, osc, electrode, platinum, material]

Charge Transport and Transfer at the Nanoscale Between Metals and Novel Conjugated Materials
Abstract Organic semiconductors (OSCs) and graphene are two classes of conjugated materials that hold promise to create flexible electronic displays, high speed transistors, and low-cost solar cells. Crucial to understanding the behavior of these materials is understanding the effects metallic contacts have on the local charge environment. Additionally, characterizing the charge carrier transport behavior within these materials sheds light on the physical mechanisms behind transport. The first part of this thesis examines the origin of the low-temperature, high electric field transport behavior of OSCs. Two chemically distinct OSCs are used, poly-3(hexylthiophene) (P3HT) and 6,13- bis(triisopropyl-silylethynyl) (TIPS) pentacene. Several models explaining the low-temperature behavior are presented, with one using the Tomonaga-Luttinger liquid (TLL) insulator-to-metal transition model and one using a field-emission hopping model. While the TLL model is only valid for 1-dimensional systems, it is shown to work for both P3HT (1D) and TIPS-pentacene (2D), suggesting the TLL model is not an appropriate description of these systems. Instead, a cross-over from thermally-activated hopping to field-emission hopping is shown to explain the data well. The second part of this thesis focuses on the interaction between gold and platinum contacts and graphene using suspended graphene over sub-100 nanometer channels. Contacts to graphene can strongly dominate charge transport and mobility as well as significantly modify the charge environment local to the contacts. Platinum electrodes are discovered to be strong dopants to graphene at short length scales while gold electrodes do not have the same effect. By increasing the separation distance between the electrodes, this discrepancy is shown to disappear, suggesting an upper limit on charge diffusion from the contacts. Finally, this thesis will discuss a novel technique to observe the high-frequency behavior in OSCs using two microwave sources and an organic transistor as a mixer. A theoretical model motivating this technique is presented which suggests the possibility of retrieving gigahertz charge transport phenomena at kilohertz detection frequencies. The current state of the project is presented and discrepancies between devices made with gold and platinum electrodes measured in the GHz regime are discussed.
-------------------------------------

10137346_183 - 0.999667333773 - technology_and_computing
[memory, performance, storage, nv-heap, atomic, persistent, disk, support]

Providing fast and safe access to next-generation, non- volatile memories
Emerging non-volatile memory technologies such as phase change memory, spin-torque transfer memory, and the memristor, will provide many orders of magnitude decrease in latency compared to disk and flash memory, dramatic increases in bandwidth, and a byte-addressable interface similar to DRAM. These new memories will offer enormous performance gains and intuitive abstractions for storage, but fully realizing these benefits requires us to rid software of disk-centric optimizations, design decisions, and architectures that limit performance and ignore bottlenecks previously hidden by the poor performance of disk. The algorithms that storage and database systems use to enforce strong consistency guarantees are critical to performance, and current solutions are deeply tied to conventional disk technology. This dissertation addresses the problem of providing transactional support for fast, non-volatile memories that exploits their raw performance and makes programming easy. First, we present a prototype PCIe-based storage array that targets fast, non-volatile memories and provides hardware support for multi-part atomic write operations. Multi-part atomic writes atomically and durably commit groups of writes to storage. Unlike previous approaches for flash-based SSDs, multi- part atomic write support makes logging scalable and transparent, providing a strong foundation for flexible ACID transactions. Using multi-part atomic writes, existing transactions mechanisms such as ARIES-style write -ahead logging can be redesigned to make optimal use of these memories, providing up to 3.7x the performance of the baseline version of ARIES. Second, we address the problem of providing strong consistency guarantees for storage that is directly accessible via the processor's memory bus. We present NV-heaps, a persistent object store which provides a familiar programming interface and protects against application and system failures by avoiding familiar programmer errors as well as new errors that only arise with persistent objects. Compared to Berkeley DB and Stasis, two persistent object stores designed for disk, NV-heaps improves performance by 32x and 244x, respectively, for operations on a variety of persistent data structures. To further improve safety, we present programming language support for NV-heaps. We introduce a Java-like language that provides the features NV-heaps require, along with a new static dependent type system that enforces the invariants that make NV-heaps safe
-------------------------------------

10134691_183 - 0.973262890774 - technology_and_computing
[cloth, model, reflectance, datum, material, brdf, analysis]

A BRDF analysis of cloth
This thesis presents documentation and analysis of the Bidirectional Reflectance Distribution Function (BRDF) of cloth through actual measured data. Although significant cloth appearance modeling research exists, light reflectance has not been a focus. For many graphics applications, replicating the motion and/or texture of cloth coupled with a general reflectance model has been sufficient. For physical accuracy, however, correctly modeling how light interacts with a material is a necessity. Part of the difficulty in developing and verifying physically accurate models is scarcity of data. This thesis presents and examines reflectance data covering a breadth of cloth materials acquired with the UCSD Spherical Gantry. Over 30 different types of cloth were measured in total, made of some of the most common fibers such as cotton, linen, rayon, silk, wool, nylon, and polyester. Included in the analysis is an assessment of several well-known BRDF models that could be easily assumed to work for a material such as cloth. The performance of each model is evaluated with a non-linear constrained SQP algorithm that fits the models to the measured data. Through numerical errors and function plots, the conclusion is that while some models are sufficient for some cloth materials, most textiles exhibit complex reflectance properties that are not accounted for. Furthermore, BRDF data can vary greatly within the family of textiles depending on numerous variables such as fiber class, thread structure, fabrication method, and finishing
-------------------------------------

10133984_183 - 0.999854690519 - technology_and_computing
[use, ambient, control, building, conditioning, energy, bulb, humidity, air, campus]

Effects of ambient humidity on the energy use of air conditioning equipment
This paper addresses the real-time use of ambient wet bulb temperature measurements in the optimization of building air conditioning system control as a means to save energy. This is not currently a common industry practice due to the maintenance requirements and perceived unreliability of humidity sensors. However, research shows that ambient wet bulb temperature is of central importance in determining optimal supervisory control of these systems. Since the UCSD campus operates its own weather stations, a case study was performed here as a first step in the implementation of their use in campus building control
-------------------------------------

10130500_183 - 0.986405429639 - technology_and_computing
[contrast, optical, air, distribution, square, state, root, atmosphere, object]

The reduction of contrast by atmospheric boil
SIO Reference 58-35. It is shown that the probability of receiving light from an object viewed through a turbulent atmosphere follows a normal Gaussian distribution. Furthermore the root mean square angular deflection of the points of any object \\dll be proportional to the square root of the object-to-observer distance.From relations of the type described in the examples, it is possible to predict the apparent contrast throughout a given scene, provided the inherent contrast distribution, the Optical Air State, and the range of the target is known. The Optical Air State for a given condition of atmosphere can be measured using a telephotometer and a series of long thin black bars of varying widths.
-------------------------------------

10137188_183 - 0.999994104951 - technology_and_computing
[tpc, power, wireless, transmission, wban, mobility, device, scheme, state, insole]

Transmission Power Management for Wireless Health Applications
The proliferation of ubiquitous sensing devices along with advances in low power wireless communication technology have resulted in the extensive use of wireless body area networks (WBANs) as the building blocks of the emerging field of wireless health.  In these battery-operated WBANs, the sensor devices are strategically placed in/on the human body and the short/mid/long wireless communications are conducted on/off the surface of the body. As the battery energy does not follow Moore's law, energy-efficiency is always one of the design challenges of wireless health-monitoring systems, impacting usability, security, and cost. The idea of transmission power control (TPC) is to automatically reduce the radio amplifier's output power when the transmission power is more than required. Reduced transmission power translates into more energy savings and reduced interference problems. TPC techniques have been used in abundance in cellular networks and wireless LANs. TPC schemes for WBANs, however, are still in their infancy. For example, current IEEE 802.15.4 specifications do not differentiate between mobile and static settings, thus leaving WBAN transmitters in the dark as to what transmission power level they should utilize.In this dissertation, we have investigated the potential benefits and limitations of TPC as a means to extend the battery lifetime in WBANs at the first three abstraction levels. Physical and MAC layers' approach to TPC perform a local optimization, whereas network layer TPC is capable of a global optimization. At the network layer, we analytically solve an optimization problem whose solution determines an important parameter, i.e., energy-efficient cluster size, for a class of routing/MAC protocols in WBANs. Assuming that the routes are established in an energy-efficient manner, we then experimentally profile the 2.4 GHz on/off-body radio channel under several scenarios regarding mobility states and environments, and we showed that fixed transmission power either wastes energy or hinders reliability. Finally, we devote our attention to an ambulatory medical monitoring WBAN system, which is tied up with different characteristics in terms of mobility, periodicity, and `unforgivingness' of the wireless channel as a result of proximity to the ground as well as to human's body. The target ambulatory WBAN system encompasses a pair of wireless instrumented insoles (known as smart insoles) for gait data collection, plantar pressure monitoring, and gait analysis. We design a sensor-assisted TPC scheme that augments in-network information with information from built-in sensors. To this end, multiple mobility states are defined for the smart insoles and the mobility states are incorporated into transmission power control policies. Available sensor information is leveraged to detect the mobility states, based on which the TPC scheme switches strategies.We validate this new idea of switching transmission power control strategies by implementing and evaluating the sensor-assisted scheme and comparing it against a frame-based TPC scheme, which adjusts the transmit power solely based on recent information about packet transmission successes and failures. Our testbed experiments involving mixed mobility scenarios show that our TPC scheme obtains up to 50% increase in the battery lifetime, enabling the smart insoles to be used in uncontrolled environments. Such an improvement in battery longevity (from 4.0 hours to 7.8 hours) is made by reducing the average energy consumed for communication of a single packet from 4.51 mJ/pkt to 2.27 mJ/pkt.Although designed for the smart insoles as a severely energy-constrained device, the sensor-assisted TPC technique is readily deployable on a variety of today's commodity devices to make a connection between the sensing subsystem and the communication subsystem of such devices. In addition, as the underlying mobility state detection methods place relaxed requirements on how the device should be worn in terms of orientation and position, they can be used for a variety of purposes, such as improving the patient's compliance with medical treatments and therapies.
-------------------------------------

10132145_183 - 0.888764662606 - technology_and_computing
[variable, constraint, model, preference, attitudinal, drive]

Modeling the Preference for Telecommuting: Measuring Attitudes and Other Variables
This paper begins to operationalize a previously published conceptual model of the individual decision to telecommute. Using survey data from 628 employees of the City of San Diego, hypothesized drives to telecommute and constraints on / facilitators of telecommuting are measured. A binary logit model of the preference to telecommute from home is estimated, having a p2 of 0.68. The explanatory variables include attitudinal and factual information. Factor analysis is performed on two groups of attitudinal questions, identifying a total of 17 (oblique) factors which can be classified as drives and constraints. Additional measures are created from other data in the survey, usually objective sociodemographic characteristics. Variables representing at least four of the five hypothesized drives (work, family, independence/leisure, and travel) are significant in the final model. Variables from four of the 10 groups of constraints (job suitability, social/professional and household interaction concerns, and a perceived benefit of commuting) are significant, primarily representing internal rather than external constraints. The results clearly demonstrate the importance of attitudinal measures over sociodemographic ones, as the same demographic characteristics (such as the presence of children, commute time) will have different effects on preference for different people.
-------------------------------------

10133755_183 - 0.999998244266 - technology_and_computing
[communication, network, multi-hop, array, antenna, design, mimo-stbc, signal, performance, link]

Cross-Layer Design for Wireless Networks Using Antenna Arrays
One of the major problems experienced by wireless multi-hop networks is the intermittent network connectivity, which is a consequence of fluctuating link quality due to signal fading.Antenna array technology has been proposed to alleviate the problem of signal fading,and it provides significant performance increase on a single link. However, translating this link-level performance increase to an end-to-end gain in multi-hop networks is not straightforward; a cross-layer design is necessary to efficiently facilitate this translation. In this dissertation, we present cross-layer design approaches for providing end-to-end performance increase in multi-hop networks using antenna arrays. Each approach is designed to utilize a special capability with antenna arrays. Using antenna arrays, nodes can increase the signal strength in a specific direction;i.e., perform directional communications.  Using directional communications in a multi-hop network requires nodes to periodically update the directions of their neighbors, which introduces an overhead. We propose topology control algorithms that enable the use of directional communications in multi-hop networks with bounded overhead. The bounds provided by our Low Degree Spanner (LDS) and Distributed LDS (D-LDS) algorithms are near-optimal. Space-Time Block Coding (STBC) with antenna arrays (referred to as MIMO-STBC) offers significant robustness to fading without an overhead at the higher layers.Robust MIMO-STBC links can also provide performance improvements at the higher layersby the design of proper protocols. Such a design necessitates an accurate representation of the MIMO-STBC link behavior. To date, simplistic representations have been used. We design an accurate representation of MIMO-STBC communications, which  we show to have a high fidelity to the MIMO-STBC communications in practice.Antenna arrays also facilitate the spatial multiplexing of signals, allowing a node to transmit and receive multiple signals simultaneously. In a multi-hop network, spatial multiplexing enables receptions from multiple concurrent transmitters. However, such a reception is successful only ifboth the number and the strength of concurrent transmissions is controlled by a higher-layer mechanism. We design topology control algorithms for activating a maximal number of communications simultaneously, while ensuring that every communication is successful with high probability.
-------------------------------------

10137531_183 - 0.90749831558 - technology_and_computing
[inductor, inductance]

Gigahertz Non-Volatile Voltage Tuned Magnetic Film Inductors using a Ni/NiFe Core
Magnetic film inductors show promise for increasing the inductance density, and thus shrinking the size, of integrated inductors. With a higher inductance density, integrated inductors can improve RF circuit performance. The ability to individually tune the integrated inductors using an applied voltage has been researched in recent years, often using MEMS type devices. This study utilized the properties of multiferroics to achieve voltage tuned inductance for microstrip inductors. The microstrip inductors demonstrated inductance enhancement up to 75%, a maximum tuning of 25%, and to have operating frequency up to 2.4 GHz, important for microwave applications. The quality factor of the inductor was enhanced by 170% from the air-core inductor. Taking advantage of the properties of the two-phase multiferroic, the inductance change was also demonstrated to be non-volatile.  The unique properties of this inductor allow for both discrete non-volatile, as well as continuous, tuning of the inductance at GHz frequencies.
-------------------------------------

10130201_178 - 0.998532314407 - technology_and_computing
[system, language, layer, human, syntax, emergence, type]

The emergence of hierarchical structure in human language
We propose a novel account for the emergence of human language syntax. Like many evolutionary innovations, language arose from the adventitious combination of two pre-existing, simpler systems that had been evolved for other functional tasks. The first system, Type E(xpression), is found in birdsong, where the same song marks territory, mating availability, and similar “expressive” functions. The second system, Type L(exical), has been suggestively found in non-human primate calls and in honeybee waggle dances, where it demarcates predicates with one or more “arguments,” such as combinations of calls in monkeys or compass headings set to sun position in honeybees. We show that human language syntax is composed of two layers that parallel these two independently evolved systems: an “E” layer resembling the Type E system of birdsong and an “L” layer providing words. The existence of the “E” and “L” layers can be confirmed using standard linguistic methodology. Each layer, E and L, when considered separately, is characterizable as a finite state system, as observed in several non-human species. When the two systems are put together they interact, yielding the unbounded, non-finite state, hierarchical structure that serves as the hallmark of full-fledged human language syntax. In this way, we account for the appearance of a novel function, language, within a conventional Darwinian framework, along with its apparently unique emergence in a single species.
-------------------------------------

10136244_183 - 0.99997507451 - technology_and_computing
[graph, pagerank, model, network, clustering, vector, algorithm, vertex, problem, cluster]

Diffusion and clustering on large graphs
This dissertation studies two important algorithmic problems on networks : graph diffusion and clustering. These problems are closely related: bottlenecks that diffusive processes find difficult to cross make good cluster boundaries and vice versa. First, we give an efficient global partitioning algorithm specifically tailored for a graph-theoretic setting. It uses sampling with PageRank vectors as probability distributions, optimizing new PageRank-distance metrics that depend only on the jumping constant alpha. Once k centers are found, we give a graph drawing algorithm that uses a force-based layout, with spring constants determined by PageRank vectors, that highlights clustered structure. We study graph clustering in two additional contexts. The first is with subsets of communication networks that have imposed boundaries when the topologies reach private subnetworks. We empirically show that spectral clustering using Dirichlet eigenvectors instead of the usual eigenvectors can be more suitable. Second, we study the extended planted partition model, a random-graph model that starts with a predetermined partitioning of the vertices with arbitrary expected degrees. We give a spectral clustering algorithm to recover the planted partition under certain conditions. Our algorithm uses a new matrix, the degree- corrected random-walk Laplacian, and unlike some prior work, does not assume knowledge of any graph generation parameters. Next, we propose and study Dirichlet PageRank, or PageRank vectors with arbitrary boundary conditions, to address vertex ranking problems resulting from the propagation of trust and distrust in networks. Using Dirichlet PageRank, we can compute vertex rankings in the presence of known spammers and negatively-weighted edges. We also study a diffusive network epidemic model : the contact process. We show that network epidemics can by stopped by finding a local cluster around the infection's starting points and inoculating the vertices in that cluster with antidote proportional to a personalized PageRank vector. This is more efficient than prior work which required distributing antidote widely to the entire graph. Finally, we propose a new voter model for the evolution of electoral opinions in social networks. Unlike the traditional voter model, our model operates on hypergraphs, allowing for more complex interactions, and it yields a wider range of outcomes
-------------------------------------

10134449_183 - 0.999987211575 - technology_and_computing
[bridge, seismic, design, system, displacement, experimental, construction]

Improving the design and performance of concrete bridges in seismic regions
There are a large number of bridges throughout the United States in need of repair or replacement to resolve structural or operational deficiencies. In replacing these structures, and constructing new bridges, engineers should utilize these construction opportunities to improve the design, construction and performance of new structures. This dissertation focuses on three distinct means for improving bridge systems throughout the U.S. First, improve the seismic response of bridge systems using controlled rocking of bridge piers. Second, improve the constructability of bridges in high seismic regions by development of a validated superstructure system for accelerated construction. Third, improve seismic design through the implementation of a simplified displacement based design approach. Controlled rocking in bridge piers can provide a method of seismic resistance that results in significantly less damage following strong ground shaking. Simplified and detailed design and response prediction methods are presented in this dissertation. These methods were validated against three beam-column experiments. Results from experimental work and analytical prediction models were used to perform a series of nonlinear time history analyses. These results indicate the use of controlled rocking will result in similar ultimate displacement demands with appreciably less residual displacement as compared to conventional systems. Use of precast concrete systems in high seismic regions has been limited due to uncertainties in the performance of connections between elements. A conceptual bridge system was developed, designed and validated through experimental testing to determine the adequacy of the details for seismic applications. This experimental work included the consideration of potential superstructure inelasticity due to relative settlement or vertical motion. Results indicated the capacity of the connection could be adequately predicted using strain-compatibility approaches considering effects of construction staging. Experimental testing validated the system has adequate rotation capacity to accommodate potential demands from relative settlement and vertical motion from relative settlement and vertical motion. A displacement based design procedure is presented which is aimed at the efficient allocation of reinforcing steel in concrete columns for seismic design. This procedure uses established displacement modification factors in combination with mechanics based displacement capacities to derive minimum design resistance require for a column under seismic actions
-------------------------------------

10135951_183 - 0.999993061844 - technology_and_computing
[interface, misorientation, conductivity, relative, wafer, iii-v, substrate, effect, electrical, miscut]

The Effect of Offcut Angle on Electrical Conductivity of Direct Wafer-Bonded n-GaAs/n-GaAs Structures for Wafer-Bonded Tandem Solar Cells
III-V compound semiconductors possess advantageous materials properties, such as direct bandgap and high carrier mobility, which make them attractive in optoelectronic, fiber optical communications and high-speed digital circuit applications.  Integration of III-V heterostructures using direct wafer bonding has the added benefit of avoiding constraints in lattice parameter mismatch compared to epitaxially grown devices.  As a result, wafer bonding is significant in its ability to integrate mismatched materials and circumvent the issues of inferior device performance due to a high density of threading dislocations.In the solar industry, off-axis substrates are commonly used in the growth of III-V epitaxial layers to avoid the formation of antiphase boundaries, which act as deep level non-radiative recombination centers.  Previously published studies from our research group showed that an additional sulfur-based passivation technique can reduce the density of surface charge states and improve the conductivity across the interface.  However, a research topic that has not been investigated is the effect that miscut substrates have on the performance of direct-bonded III-V devices.  In this study, the effect of the wafer offcut angle on the electrical conductivity of III-V solar cell devices is investigated using n-GaAs/n-GaAs wafer-bonded structures.GaAs (001) wafers misoriented towards <111>A are chosen and compared to nominal on-axis substrates.  Prior to bonding, the surfaces are treated with either an oxide etch or additional 5-min treatment in aqueous (NH<sub>4</sub>)<sub>2</sub>S.  Off-axis wafers are positioned face-to-face with a non-zero relative surface misorientation between the tilted (001) planes and low force bonding is initiated at room temperature.  The highest degree of misorientation in this study is produced using 6° off-axis wafers and can be represented as a grain boundary with a twin defect at the (1 1 13) plane and a tilt angle of 12° about the common 110 direction.  The samples are annealed at 400 °C for two hours to strengthen the bond.  It is observed that the electrical conductivity improves considerably with a short rapid thermal processing (RTP) at 600 °C for 2 mins.  However, the degree of miscut has a detrimental effect, as both the oxide-etched and sulfur-treated samples exhibit increasingly non-ohmic behavior with greater relative misorientations.  A theoretical model that describes the electron tunneling across a grain boundary between semiconductor bicrystals is used to represent the bonded interface and estimate the barrier conduction height.  Fitting the zero-bias conductance data over a range of temperatures reveals an increased barrier height for greater offcut angles, with 4° and 6° miscut sulfur-passivated wafers producing a 0.4 eV increase.  When compared to on-axis structures, the interface resistance at room temperature increases from 0.01 ohm&middot;cm<super>2</super> to 3.4 ohm&middot;cm<super>2</super>.High resolution transmission electron microscopy (HRTEM) and scanning transmission electron microscopy (STEM) are used to compare the interface morphology across the range of relative misorientations after the 600 °C RTP.  The ratio of well-bonded crystalline regions to amorphous oxide inclusions is consistent across all bonded samples, indicating that the degree of misorientation does not affect the level of interface recrystallization.  It is also observed that regions adjacent to the interface undergo a process of atomic redistribution and recrystallize into the same lattice arrangement as the bulk semiconductor.The effect of relative surface misorientation on conductivity is further investigated by fabricating zero-degree relative (001) misoriented bonded samples using 6° miscut substrates and subjecting to the same thermal and sulfur passivation treatment.  This can be described as a grain boundary at the (1 1 13) plane without any twin defects about the 110 direction.  Current-voltage (I-V) characteristics are comparable to nominal on-axis specimen with the interface resistance measured as 0.02 ohm&middot;cm<super>2</super>.  It is concluded that the degree of relative misorientation of (001) planes across the bonded interface has a significant impact on the electrical properties, as illustrated by the two orders of magnitude difference in conductance.Non-ohmic behavior has previously been discovered in direct-bonded n-type GaAs/GaAs structures.  In order to pinpoint the source of this inferior conductance, several lines are physically cut into the surface with a diamond scribe to electrically isolate a portion of the metal contacts.  I-V measurements are taken at various test points to compare the resistance across the different interfaces.  The measured resistance across the metal-metal and metal-semiconductor-metal regions is found to be 0.8 ohms.  However, the inclusion of the bonded interface results in a significant increase in resistance to 770 ohms at zero-bias conditions.  The non-ohmic behavior is confirmed to be solely attributed to the bonded interface.These results demonstrate the potential usefulness of using off-axis substrates in the fabrication of direct wafer-bonded III-V heterostructures.  More importantly, the primary effect on conductivity does not originate from the miscut substrates themselves.  Instead, the out-of-plane relative misorientation of the tilted (001) planes is the critical parameter that needs to be controlled within a manufacturing environment in order to achieve acceptable electrical performance in multijunction solar applications.
-------------------------------------

10136003_183 - 0.800896596539 - technology_and_computing
[gillespie, #occupywallstreet, twitter, algorithmic, wrong, controversy, trends, algorithm, tarleton, censorship]

Can an Algorithm be Wrong?
How do we know if we are where it’s at?  Tarleton Gillespie explores the controversy over Twitter Trends and the algorithmic ‘censorship’ of #occupywallstreet.
-------------------------------------

10136968_183 - 0.977922637187 - technology_and_computing
[traffic, tor, delay, interactive]

Torchestra : reducing interactive trac delays over Tor
Tor is an onion routing network that protects users' privacy by relaying traffic through a series of nodes that run Tor software. As a consequence of the anonymity that it provides, Tor is used for many purposes on the internet including interactive traffic as well as for bulk file downloads. Such bulk downloads cause delays for interactive traffic as all traffic between a pair of Tor nodes goes over a single connection. The resulting delays discourage people from using Tor for normal web activity. We propose a potential solution to this problem called Torchestra which separates interactive and bulk traffic onto two separate TCP connections between any pair of nodes. We classify a circuit as carrying either type of traffic based on the Exponentially Weighted Moving Average of its number of cells. We evaluate our proposal by simulating traffic using several methods and show that Torchestra provides up to 32% reduction in delays for interactive traffic compared to the Tor traffic prioritization scheme of Tang and Goldberg and up to 40% decrease in delays when compared to unprioritized Tor
-------------------------------------

10135371_183 - 0.999988246459 - technology_and_computing
[ic, wireless, ir-uwb, esd, protection, design, technology]

Full-Band Impulse-Radio Ultra Wideband Transceivers With Integrated ESD Protection
The past decade has witnessed rapid proliferation of wireless communications, which continues to enjoy a booming growth driven by unprecedented technology advances and strong consumer demands. New wireless technologies are being developed to provide people with high-speed low-cost multi-mode multi-task wireless communication environments with high quality of service (QoS). Of all the proposed wireless techniques, ultra wideband (UWB) is a promising  technology, which becomes a front contender for various extremely high data throughput wireless applications, particularly for wireless video streaming and wireless sea-volume data transformation typically requiring a data speed up to several giga bit per second (Gbps).This dissertation describes research and integrated circuit (IC) implementation of a single-full-band carrier-free impulse-radio ultra wideband (IR-UWB) system. The IR-UWB transceiver adopts a simple-most-digital architecture with low design complexity, aiming to achieve the whole IR-UWB system-on-a-chip (SoC) in standard complementary metal-oxide-semiconductor (CMOS) process. Detail analysis for the IR-UWB system architecture is provided. Critical circuit building blocks, such as pulse generator (PG), BPSK modulation, receiver front-end low-noise amplifier (LNA) and correlator, are described both theoretically and experimentally. Adequate on-chip electrostatic discharge (ESD) protection is required for all IC chips and ESD protection design for radio-frequency (RF) IC emerges as a challenging design task as semiconductor IC technologies continue to advance into the very-deep-sub-micron (VDSM) regime. ESD protection for IR-UWB ICs is more challenging compared with narrow band IC designs. In this thesis, a novel ESD-RFIC co-design technique for UWB ICs was developed and experimentally verified. The interactions between ESD protection unit and core UWB IC were thoroughly investigated. The IR-UWB transmitter, front-end LNA and correlator ICs were designed with full ESD protection using the new ESD-RFIC co-design technique in this work.
-------------------------------------

10136074_183 - 0.947519957628 - technology_and_computing
[nanocrystal, memory, nc, device, novel, core-shell, layer, performance, tisi2]

Novel Nanocrystal Floating Gate Memory
This work is devoted to investigating the feasibility of engineering nanocrystals and tunnel oxide layer with a novel structure. Several novel devices are demonstrated to improve the performance of the novel nanocrystal memories.A novel TiSi2 nanocrystal memory was demonstrated. TiSi2 nanocrystals were synthesized on SiO2 by annealing Ti covered Si nanocrystals. Compared to the reference Si nanocrystal memory, both experiment and simulation results show that TiSi2 nanocrystal memory exhibits larger memory window, faster writing and erasing, and longer retention lifetime as a result of the metallic property of the silicide nanocrystals. Due to thermally stable, CMOS compatible properties, TiSi2 nanocrystals are highly promising for nonvolatile memory device application. Metal/high-k dielectric core-shell nanocrystal memory capacitors were proposed. This kind of MOS memory shows good performance in charge storage capacity, programming and erasing speed. A self-assembled di-block co-polymer is used to align the NCs to improve the scalability of the overall sample. An ordered Co/Al2O3 core-shell nanocrystal (NC) nonvolatile memory device was also fabricated. Self-assembled di-block co-polymer process aligned the NCs with uniform size. Co/Al2O3 core-shell NCs were formed using atomic layer deposition of Al2O3 before and after the ordered Co NC formation. Compared to Co NC memory, Co/Al2O3 core-shell NC memory shows improved retention performance without sacrificing writing and erasing speeds.Another new discrete NiSi nanocrystals (NCs) were synthesized by rapid thermal oxygen annealing (RTO) of very thin Si/Ni/Si films on SiO2 tunneling layer. The RTO process resulted in smooth surface of the NC floating layer, in turn, uniform thickness of subsequent control oxide layer. Metal-oxide-semiconductor capacitor memory was fabricated. Electrical properties of the memory device such as programming, erasing and retention were characterized and good performance was achieved, which is due to the reduction of the leakage paths in the smooth device structure. Therefore, it is concluded that metallic nanocrystal with aligned core-shell structure memory is a very promising candidate to replace Si nanocrystal for future generation nonvolatile flash memory devices.
-------------------------------------

10133737_183 - 0.95935652284 - technology_and_computing
[operator, pattern, representation, activity, neural]

Brain neural activity patterns yielding numbers are operators, not representations
We contrapose computational models using representations of numbers in parietal cortical activity patterns (abstract or not) with dynamic models, whereby prefrontal cortex (PFC) orchestrates neural operators. The neural operators under PFC control are activity patterns that mobilize synaptic matrices formed by learning into textured oscillations we observe through the electroencephalogram from the scalp (EEG) and the electrocorticogram from the cortical surface (ECoG). We postulate that specialized operators produce symbolic representations existing only outside of brains. 
-------------------------------------

10137609_183 - 0.973443824257 - technology_and_computing
[nanocrystal, memory, device, scaling, gate, process, flash, deposition, oxide, work]

Engineer Nanocrystal Floating Gate Memory Scaling
Flash memory is the dominant nonvolatile memory technology that has been experiencing fastest market growth driven by the booming of portable electronic devices. Since its invention in 1980s, it has been through aggressive scaling. Leading semiconductor memory manufacturers such as Samsung, Intel/Micron and SanDisk/Toshiba have unveiled their 19/20nm NAND flash technology in production. However, how long the fast scaling pace of flash can be maintained remains a question mark since this device using continuous polycrystalline Si floating gate faces increasing challenge brought by poor immunity to charge leakage and process difficulty due to large vertical gate stack dimension. To enable the further scaling of flash technology, change needs to be made at the cell level. Devices using discrete charge storage units have been recognized as potential alternatives to conventional flash cells, including charge trapping type device and nanocrystal floating gate device.	Nanocrystal floating gate memory is considered a promising future nonvolatile memory candidate because of its immunity to weak-point leakage in tunnel oxide and thus its superior scalability in terms of tunnel oxide thickness and power consumption. However, no scaling is easy. Problem of nano-dot density fluctuation has arisen for this type of device as scaling process proceeds. The increasing sensitivity of chip-level device performance to dot distribution in scaled cells requires that nanocrystal deposition, which is the key step for device fabrication, should be fully understood, and that this device should start evolving to incorporate material and structure innovations. This work is devoted to propelling the scaling process of nanocrystal memory through nanocrystal deposition behavior investigation and device structure engineering. In chapter 2, Si nanocrystal growth on patterned oxide substrate by chemical vapor deposition is studied both experimentally and theoretically and directed self-assembly behavior of Si nanocrystals is identified as due to the effect of substrate morphology. This deepens our understanding on nanocrystal nucleation and growth and provides a general guidance to deposition process for memory applications. Chapter 3 includes the work on a gate stack-engineered nanocrystal MOS capacitor memory device. An Al2O3-SiO2 double-barrier structure is utilized for dielectric layers and process-induced oxide degradation issue is well solved, resulting in improved memory performance. Chapter 4 and chapter 5 focus on the demonstration of non-planarity concept for nanocrystal memory device. Non-planar nanocrystal memories with multiple and single triangular-shaped Si nanowire channel are introduced. This new concept of nanocrystal memory device is aimed at alleviating dot density variation issue at the scaled technology nodes and helps extend the scaling limit of planar device. 	Based on all the theoretical and experimental work in this dissertation, it is concluded that nanocrystal memory scaling lies in good understanding and control of the key process as well as continued cell architecture engineering. This work serves as a step toward the scaling limit of nanocrystal floating gate device for next generation nonvolatile memory development.
-------------------------------------

10131386_183 - 0.999899914646 - technology_and_computing
[model, datum, variable, analysis, discrete, simultaneous, equation, dynamic, golob, choice]

Simultaneous Equation Systems Involving Binary Choice Variables
In this paper a simultaneous modeling system for dichotomous endogenous variables is developed and applied empirically to longitudinal travel demand data of modal choice. The reported research is motivated by three factors. First, the analysis of discrete data has become standard practice among geographers, sociologists, and economists. In the seventies a number of new tools were developed to handle multivariate discrete data (Bishop, et al., 1975; Fienberg, 1980; Goodman, 1972). However, while these methods are invaluable in studying empirical relationships among sets of discrete variables, they have a limited ability to reveal the underlying causal structure that generated the data.
      Second, in travel demand analysis and housing market modeling, attention has been focused largely on single-equation models. It can be argued that this scope is too limited. Human decisions are usually not taken in isolation but in conjunction with other decisions and events. There may be complex feedback relations, recursive, sequential, and simultaneous decision structures that cannot be adequately described in a single equation. This has been a major motivation in the seventies in sociology for the development of a new modeling approach: linear structural equations with latent variables. Such models combine the classical simultaneous equation system model with a linear measurement model. Original developments, particularly the LISREL model (Joreskog, 1973, 1977), did not allow for discrete dependent variables. More recently, Muthen (1983, 1984, 1987) and others (e.g., Bentler, 1983, 1985) developed models that incorporate various types of non-normal endogenous variables, including censored/truncated polytomous and dummy variables. This paper explores the possibilities of this method for simultaneous equation models in dynamic analysis of mobility.
      A third motivation for the present research is the rapid growth of longitudinal data sets. In recent years many longitudinal surveys have become available for geographical, economic, and transportation analyses. In labor and housing market analysis the Panel Study of Income Dynamics (PSID, 1984) has played an important role (Heckman and Singer, 1985; Davies and Crouchley, 1984, 1985). In consumer behavior, the Cardiff Consumer Panel has been a major motivation for the development and testing of dynamic discrete choice models (Wrigley, et al., 1985; Wrigley and Dunn, 1984a, 1984b, 1984c, 1985; Dunn and Wrigley, 1985; Uncles, 1987). In the Netherlands a large general mobility panel has been conducted annually since 1984 (J. Golob, et al., 1985; van Wissen and Meurs, 1989). Here analyses have focused on discrete data on modal choice (T. Golob, et al., 1986), as well as on dynamic structural modeling (Golob and Meurs, 1987, 1988; Kitamura, 1987; Golob and van Wissen, 1988; Golob, 1988). The present paper is an extension of this line of research to incorporate dynamic structural models of modal choice, using data from the Dutch Mobility Panel.
      This paper is organized as follows: In Section 2 the basic methodology is developed. In Section 3 the simultaneous equation system of dummy variables is compared with the conditional logistic model, which is derived from, and equivalent to, the familiar log-linear model. In the fourth section, both models are applied to a dynamic data set of train and bus usage. Some major conclusions regarding the above are drawn in the final section.
-------------------------------------

10135177_183 - 0.949545064599 - technology_and_computing
[noise, field, ambient, information, source, correlation, opportunity]

Using ambient noise to extract coherent environmental information
The ambient noise recorded in the ocean has traditionally been treated solely as the noise that a desired acoustic signal must overcome. Nevertheless, it contains environmental information relating to the physical characteristics of the ocean where the noise was recorded. Different types of ambient noise (i.e. diffuse fields vs. independent sources of opportunity) provide the signals for extracting various types of information from the environment. By cross-correlating time-aligned recordings of ambient noise, it is possible to extract time-of- arrival information of the acoustic ray path between the sensors which recorded the data - even with no a priori knowledge of the sound field which contributed to the noise recording. When correlation processing fails due to anisotropy in the ambient noise field, information about the physical environment may still be extracted given trackable sources of opportunity. Due to the nature of the ocean noise field and the fluctuations of the environment itself, difficulties arise in extracting the time-of- arrival information quickly and accurately before environmental changes corrupt the result. The use of arrays of sensors incorporating standard array processing techniques into the noise correlation function is shown to dramatically improve the correlation results both theoretically and when applied to experimental data. In addition, the effects of the directionality of the noise field and the relationship between the directionality of the incoming energy and the signal-to-noise ratio defined from the noise correlation function are examined both theoretically and using ambient noise recordings from experiments. When the ambient noise field is dominated by loud source events, these sources of opportunity may be used directly to invert for environmental characteristics. A model is presented for the use of independent, low frequency, loud sources of opportunity (ice calving events) to invert for the propagating sound speed over very long distances
-------------------------------------

10133514_183 - 0.939983128685 - technology_and_computing
[protocol, production, frame]

The media frame : the theory and practice of integrating a variety of production protocol in modern experimental temporal art
Temporal art production protocol since Wagner has seen a narrowing of performance practices and audience expectations. This narrowing has created numerous obstacles for integrating media with distinct protocol into a single frame of presentation. This paper sites numerous examples of pieces since 1950 that attempt to integrate a variety of production protocol with varying degrees of success. It also proposes a vocabulary for analyzing the frame of presentation and its manipulation
-------------------------------------

10134822_183 - 0.99310946021 - technology_and_computing
[optical, fiber-optic, modulation, phase, nonlinear, receiver, efficiency]

Nonlinear Transmission Impairments in High-Spectral Efficiency Fiber-Optic Communications
This dissertation is designed to focus on the field of nonlinear impairments generated along the fiber-optic transmission links in high spectral efficiency optical modulation systems, while also extracting the performance trade-offs of conventional and novel receivers. Fundamental knowledge of optical fiber, major phase modulation nonlinearities, and high spectral efficiency optical modulation and reception are discussed on an introductory level. Further, balanced detection schemes for optical duobinary with a comparison of differential phase shift keying (DPSK) are demonstrated and analyzed, both in simulations and experiments. Frequency discriminator receiver structures for quadrature phase shift keying (QPSK) as well as their linear and nonlinear noise properties are exhibited. At the end, polarization multiplexing QPSK with digital coherent receiver well known as future vision of fiber-optic communications is also simulatedly studied through the RZ- and NRZ- pulse shape impact in a 9-channel 100G wavelength division multiplexing (WDM) systems.
-------------------------------------

10132036_183 - 0.855752446774 - technology_and_computing
[air, motor, car, use, pollution, vehicle]

On the Costs of Air Pollution from Motor Vehicles
Air pollution is frequently the stated reason for special measures aimed at controlling motor vehicles. In the United States, motor vehicle emission standards are set explicitly in clean air legislation, while policies at several levels of government are designed to reduce the use of cars for particular purposes like commuting. In Europe, high fuel taxes and subsidies to urban mass transit and intercity rail travel in large part aim to reduce car use.
-------------------------------------

10134411_183 - 0.999238495065 - technology_and_computing
[performance, circuit, kbp, throughput, tor, average, router, anonymity, distance, hop]

Improving Tor performance through better path selection
Tor, the Onion Router, is a popular anonymity overlay network. Its goal is to provide anonymity at little cost to performance. However, Tor network performance is still fairly slow, due to the fact that the bandwidth it provides is limited by how much the volunteers running the Tor routers wish to give. There are a few studies that investigate changing the structure of the Tor network to improve performance. We go in a different direction and investigate different path selection strategies; specifically, that of varying the number of hops in a circuit, varying the performance flags in a circuit, and varying the geographic distance between routers in a circuit. We show how much improvement can be had by reducing the path length, which gives the user guidance on how to trade off anonymity for performance. Our throughput results are as follows: a two hop circuit had an average throughput of 9.6Kbps, and a three hop circuit had an average throughput of 6.7Kbps, and a four hop circuit had an average throughput of 5.2Kbps. We found that there was not much difference between having "stable", "fast", or s̀t̀able and fast" flags, but found that for preserving anonymity the latter was preferable. These were also reasonably fast, with average throughputs between 7.2Kbps and 7.4Kbps. We also found that some improvement could be had by keeping the distance between routers small, with distances between 200-600 miles and 3800-4200 miles providing throughput around 6.5Kbps, and distances greater than that dropping by about 2Kbps
-------------------------------------

10135534_183 - 0.939314240698 - technology_and_computing
[turbine, wind, operational, seismic, model, experimental, state]

An experimental and numerical study of wind turbine seismic behavior
This dissertation presents an experimental and numerical investigation into the seismic response of modern wind turbines. Currently, no consensus exists in the industry and there is significant interest in improving prediction of the behavior of wind turbines simultaneously subjected to wind, earthquake, and operational excitation. To this end, an experimental program was planned in order to evaluate seismic loading of wind turbines. In 2004, a preliminary shake table test of a 65-kW utility scale wind turbine was conducted that provided an experimental basis to begin the work discussed herein. A monitoring campaign was performed at Oak Creek Energy Systems in Mojave, California to assess variability of in-situ dynamic characteristics of two modern wind turbines (900-kW and 1.5-MW rated power) under different operational states and wind conditions. A second shake table experiment with a more extensive test program and improved instrumentation was executed, in which orientation of shaking and operational state were found to significantly influence response. Using the finite element program OpenSees, beam- column models of the tested specimens were constructed and calibrated. Collected data provided a basis to show that such a model could reproduce salient characteristics including natural frequencies, mode shapes, and dynamic response time histories for a parked turbine. In-situ results were used to guide construction of full turbine- foundation-soil models that provided insight into soil- structure interaction phenomena. An existing tool to simulate turbine dynamics, the FAST code, was extended to include seismic loading to allow simulation of operational turbines subjected to base shaking and validated based on shake table results. Using a calibrated model of the tested 900-kW turbine it is shown that neglecting aerodynamics results in significant over estimation of the tower bending demand. An investigation of turbines ranging from 65-kW to 5-MW concluded that consideration of aerodynamics and operational state becomes increasingly important with size. The updated FAST code was demonstrated to accurately reproduce observed dynamics of operating turbines, providing a validated tool for seismic design of turbines. These contributions clarify that operational state and orientation of shaking are important considerations and enable the development of a new generation of turbines that appropriately consider seismic loads
-------------------------------------

10133969_183 - 0.999999057554 - technology_and_computing
[network, approach, sensor, wsn, energy, node, connectivity, multi-hop, communication, wireless]

Energy efficient strategies for wireless sensor networks with varying connectivity properties
Wireless sensor networks (WSNs) are envisioned as an approach to a wide variety of monitoring applications. From environmental monitoring to military surveillance, the range of scenarios is diverse, as are the constraints on the networking problem. In most cases, these wireless systems are energy-constrained, and it is impractical or infeasible to replace or recharge the batteries. These systems must be designed with the goal of energy efficiency. In this work, we investigate energy efficient approaches for WSNs that exhibit a variety of connectivity properties. We study three categories of WSNs in particular. In the traditional view of multi-hop sensor networks, there is a large number of sensor nodes with end -to-end connectivity, and they use multi-hop communication. We also consider dense, small-scale sensor networks, in which there are only a few nodes, and communication is primarily single-hop. Lastly, we look at sparse sensor networks, known as a delay- or disruption- tolerant network (DTN). These networks lack end-to-end connectivity, and must rely on mobility to make connections to route data. With these three categories of WSNs, we consider approaches that are specifically applied to each unique scenario. For multi-hop WSNs, we use a distributed approach to Kalman filtering for target tracking. By moving the signal processing onto the local nodes, the RF communication, which typically consumes the most power, is reduced. Through simulation, we evaluate the tracking performance. For our approach to dense, small -scale networks, we exploit the fact that multi-hop communication is rarely needed. We implement a hybrid MAC/ routing protocol that assumes that routing is only needed on rare occasions, thus reducing the idle listening energy consumption. For sparse DTNs, we developed a power management scheme that uses the local information available to determine a node's likelihood of delivery, which is used to decide if a node should sleep or listen/ beacon. We evaluate these two categories of WSNs through simulation and some experimental work (for the small-scale case), and we show significant gains in energy efficiency over existing approaches
-------------------------------------

10133520_183 - 0.999994125859 - technology_and_computing
[power, network, analysis, noise, voltage, domain, optimization, worst, algorithm, drop]

Power network analysis and optimization
Power networks supply power from the P/G pads on a chip to the circuit modules. With the rapid increase of working frequency and continuous scaling of VLSI technology, the power supply network is experiencing unprecedented noise, which causes significant delay variation of devices, or even logic failure. Therefore, robust and reliable power supply network has increasing importance for high-speed circuit performance. In this dissertation, we study the methodologies and algorithms to perform the power networks analysis and optimization. We design an efficient circuit simulation flow based on frequency domain computation, which serves as a helpful tool for analysis and optimization. Then, we explore approaches to make the worst case noise analysis considering clock gating with multiple domains. The worst case voltage drop and violation area are studied in this analysis work. After power network analysis, the optimization is to confine the voltage fluctuation to meet with a target noise tolerance. The power-up sequencing problem and the noise minimization with decoupling capacitors (decap) and controlled-ESRs are studied. In the circuit simulation work, a frequency domain based simulation method is proposed to obtain the time domain voltage response. With the vector fitting technique, the frequency-domain responses are approximated by a partial fraction expression, which can be easily converted to time-domain waveform. Numerical results show that the proposed simulation method is up to several hundred times faster than commercial fast simulators, like HSPICE and MSPICE. And, the proposed method is able to analyze large-scale power networks that the commercial tools are not able to afford. The worst case voltage drop and violation area analysis are both studied in a multi- domain clock gated power network. We describe a linear time complexity algorithm to find the worst case voltage drop and the corresponding clock gating pattern. An efficient integer linear programming (ILP) based approach is proposed to find the worst voltage violation area. Leakage current is taken into consideration to accurately estimate the violation noise. The optimization work covers two pars. Firstly, an efficient heuristic algorithm is introduced to arrange the power-up sequence in a multi- domain power network to minimize the noise. Secondly, we propose a sequential quadratic programming (SQP) based algorithm to optimize power network with both decap and controlled-ESR. A revised sensitivity computation is derived to consider both voltage drop and overshoot. Experimental results shows the controlled-ESR reduces the noise by 25% with the same decap budget
-------------------------------------

10134063_183 - 0.863183350086 - technology_and_computing
[arbor, parameter, function, adf, deviation, standard, cell, density, type, dendrite]

Characterizing the spatial density functions of neural arbors
Recently, it has been proposed that a universal function describes the way in which all arbors (axons and dendrites) spread their branches over space. Data from fish retinal ganglion cells as well as cortical and hippocampal arbors from mouse, rat, cat, monkey and human provide evidence that all arbor density functions (adfs) can be described by a Gaussian function truncated at approximately two standard deviations. A Gaussian density function implies that there is a minimal set of parameters needed to describe an adf : two or three standard deviations (depending on the dimensionality of the arbor) and an amplitude. However, the parameters needed to completely describe an adf could be further constrained by a scaling law found between the product of the standard deviations and the amplitude of the function. In the following document, I examine the scaling law relationship in order to determine the minimal set of parameters needed to describe an adf. First, I find that the flat, two- dimensional arbors of fish retinal ganglion cells require only two out of the three fundamental parameters to completely describe their density functions. Second, the three-dimensional, volume filling, cortical arbors require four fundamental parameters: three standard deviations and the total length of an arbor (which corresponds to the amplitude of the function). Next, I characterize the shape of arbors in the context of the fundamental parameters. I show that the parameter distributions of the fish retinal ganglion cells are largely homogenous. In general, axons are bigger and less dense than dendrites; however, they are similarly shaped. The parameter distributions of these two arbor types overlap and, therefore, can only be differentiated from one another probabilistically based on their adfs. Despite artifacts in the cortical arbor data, different types of arbors (apical dendrites, non-apical dendrites, and axons) can generally be differentiated based on their adfs. In addition, within arbor type, there is evidence of different neuron classes (such as interneurons and pyramidal cells). How well different types and classes of arbors can be differentiated is quantified using the Random ForestTM supervised learning algorithm
-------------------------------------

10134467_183 - 0.999992319934 - technology_and_computing
[robot, mode, control, rover, system]

Design and control of a passively stabilized multimodal hopping robot
This paper presents an original design for a multimodal hopping robot. The robot harnesses passive stability to reduce the complexity of the control problem required to keep upright during the hopping motion. The nominal mode is an upright rover mode, which behaves as an inverted pendulum. When the rover encounters an obstacle, it changes modes. The wheels are gimballed perpendicular to the robot body, and spin up to generate a stabilizing gyroscopic reaction torque. In this mode, the robot behaves like a spinning top. The robot enters a passively stable regime of steady precession at a constant nutation angle. By triggering a hop at a chosen point in the precession, the robot can traverse obstacles larger than its wheel diameter. The paper also examines the theoretical underpinnings of the dynamic system in both the rover and top modes. Lagrange equations and Routh's method are used to obtain the equations of motion for the top mode. The stability of the steady precession is examined, and a control scheme for maintaining a constant rotor speed is found. The equations and controls for the rover mode are found via state space analysis. The design process and current mechanical system of a physical prototype are explained. The control scheme is implemented using a minimal set of sensors. The robot is controlled by a Texas Instruments microprocessor, which allows for untethered control. All systems are powered by an onboard battery pack, allowing the robot to function as a standalone system
-------------------------------------

10136068_183 - 0.988112899334 - technology_and_computing
[camera, selection, approach, game, problem, handoff]

Camera Selection, Handoff and Control in Video Networks
Due to the broad coverage of an environment and the possibility of coordination among different cameras, video sensor networks have attracted much interest in recent years. Although the field-of-view (FOV) of a single camera is limited and cameras may have overlapping/non-overlapping FOVs, seamless tracking of moving objects is desired. As the increasing of the video network complexity, there are more and more camera nodes in a network. This makes it hard for human observers to take care of the entire system and brings the emergence of the camera selection, handoff and control technologies.In this study, we introduce a series of economics frameworks into the camera selection, handoff and control problem. This starts with two game theoretical approaches - the potential game approach and the weakly acyclic game approach. With these two methods, we can model the camera selection and handoff problem as a multiplayer game. Existing learning algorithms in the game theory literature make it efficient to find an optimal as well as stable solution to this problem. As camera selection and handoff largely depend on the accuracy of the applied trackers, we develop a technique to jointly consider the tracking problem and the camera selection problem. In this work, fusion of multiple trackers is integrated with the camera selection process in a closed-loop manner. Finally, active camera controls are considered by using the auction protocol. Unlike previous work, the bid price is formulated to have a vector representation, such that when a camera is available to follow multiple objects, we consider the "willingness" of this camera to track a particular object. Meanwhile, the potentially available cameras can also be considered to follow an object after some panning or tilting operations. Most of the computation is decentralized by computing the bid price locally while the final assignment is made by a virtual auctioneer based on all the available bids, which is analogous to a real auction in economics. Thus, we can take the advantage of distributed/centralized computation and avoid their pitfalls. All these approaches are evaluated with real-world data under the VideoWeb 45 camera network environment. These proposed approaches are also compared with each other and other approaches. The experimental results show the robustness and efficacy of this study.
-------------------------------------

10138986_183 - 0.999999478847 - technology_and_computing
[mobile, interface, theory]

Review: Mobile Interface Theory: Embodied Space and Locative Media by Jason Farman
Mobile Interface Theory makes an important step toward a fuller reckoning with the social consequences of mobile technology.
-------------------------------------

10134939_183 - 0.999988448757 - technology_and_computing
[model, geometry, system, kinect]

Capturing geometry in real-time using a tracked Microsoft Kinect
We propose a new geometry scanning system that permits the user to obtain detailed triangle models of real-world places with a tracked Microsoft Kinect device. The system generates a texture map for the triangle mesh using video frames from the Kinect's color camera and displays a continually-updated preview of the textured model in real- time, allowing the user to re-scan the scene from any direction to fill holes or increase the texture resolution. We also present filtering methods to maintain a high-quality model of reasonable size by removing overlapping or low-precision range scans. Our approach works well in the presence of degenerate geometry or when closing loops about the scanned subject, making it especially suited for scanning large areas. We demonstrate the ability of our system to acquire 3D models at human scale with a prototype implementation in the StarCAVE, a holographic virtual-reality environment at the University of California, San Diego
-------------------------------------

10136827_183 - 0.992117163987 - technology_and_computing
[noise, oscillator, phase, amplifier, frequency, rtwo, operation]

A Low Phase Noise Oscillator: Rotary Traveling Wave Oscillator
Increasing demand for bandwidth in the digital communications, wired or wireless, requires integrated circuits operating at ever higher frequencies. Design and fabrication of low cost transmitter/receiver circuits remains increasingly challenging. With scaling, advances in complementary metal-oxide-semiconductor (CMOS) technologies have proven a serious competitor to the traditional SiGe, GaAs and bipolar technologies which have proven to be high power and expensive with a relatively low yield.Voltage-controlled oscillators (VCOs) are essential building blocks for frequency synthesizers and clock-and-data recovery loops. Monolithic ring and LC oscillators have been commonly used as VCOs in such systems. Digital ring oscillators have inferior noise performance but can readily generate multiphase signals, while LC VCOs offer better phase noise for a given power dissipation. It remains challenging to achieve all the desired VCO specifications simultaneously as the frequency of operation approaches the self-resonance frequency of the on-chip inductors and the cutoff frequency of transistors. The Rotary Traveling Wave Oscillator (RTWO) proposed by John wood in 20019 is a distributed oscillator that can be viewed as originating from distributed amplifiers. The RTWO operates by creating a rotating traveling wave within a closed-loop differential transmission line that serves as the resonant structure. Distributed CMOS inverters act as amplifiers to compensate line loss, and latches, to reduce amplitude variation. Amplifier gate and drain capacitance is absorbed into the transmission-line enabling energy to be recirculated adiabatically and results in a maximum frequency limited only by Fmax of the integrated circuit technology used. These oscillators were initially proposed for distributed clock generation in digital systems. Subsequently it has been realized that they are not only capable of very high frequency operation but also of multiphase operation ,with lithographically defined phase precision, and with lower phase noise compared to alternate approaches.Here the fundamental physics and phase noise of RTWO are studied. First we investigate the connection between the distributed amplifier and the RTWO and the implications of lossy and periodically loaded transmission lines on its operation. This leads to the consideration of the inherently multi-mode nature of the resonator, which provides additional degrees of freedom relative to traditional designs and can be use to significantly decrease the rise and fall times of the oscillations leading to reduced phase noise. Operation is investigated in three different regions of operation: quasi-linear, critical oscillation and strongly nonlinear. In the weakly nonlinear region, a Duffing-van der Pol differential equation is developed to describe the oscillators' behavior. By using the elliptic perturbation method, a Jacobian Elliptic function type solution is derived that provides us with a more accurate calculation of the oscillating frequency relative to the first order estimates to date. Subsequently and more importantly the novel aspects of phase noise in the RTWO is investigated. Two separate but closely related models of oscillator phase noise are introduced. The first is proposed by Leeson11 and the other one is by Lee and Hajimiri12 are investigated as relevant to these oscillators. For thermally induced white phase noise using Abidi's sampling mechanism13 a theoretical frame work is developed for the effect of broadband white noise on RTWOs. Following this and most importantly the upconversion of flicker noise is studied on its impact on close-in phase noise. Several upconversion mechanisms are introduced and investigated that are unique to RTWOs. The interaction of line dispersion with amplifier nonlinearity is shown to lead to the upconversion of 1/f noise to close-in phase noise in the weakly nonlinear regime.  Dispersion is implicated more generally in the creation of drive transistor asymmetries that can also lead to device 1/f noise upconversion in the strongly nonlinear large signal regime. Asymmetries in the integrated contribution of transistor gm variations to 1/f noise are further shown to arise from the latency of the amplifier response. Most importantly we develop a novel technique we term "Gate Offset" to provide a compensatory phase alteration at the amplifiers that can not only remove the 1/f contribution due to amplifier delay but also compensate for dispersion induced delays.Lastly, a widely tunable monolithic integrated rotary traveling wave oscillator based on the design intuition developed from this theory is designed, fabricated and measured. Results are presented that show a doubling of performance relative to the state of the art.
-------------------------------------

10136657_183 - 0.849874553334 - technology_and_computing
[energy, storage, high, material, structure, architecture, electrode, cnt, performance]

Rational Material Architecture Design for Better Energy Storage
Human civilization relies on an abundant and sustainable supply of energy.  Rapidly increasing energy consumption in past decades has resulted in a fossil-fuel shortage and ecological deterioration.  Facing these challenges, humankind has been diligently seeking clean, safe and renewable energy sources, such as solar, wind, waves and tides, to offset the diminishing availability or to take place of fossil fuels.  At the same time, the search for strategies to reduce fossil-fuel consumption and decrease CO2 emission, such as to replace tradition vehicles by electrical vehicles (EVs), is demanded.  However, the energy harvested from renewable sources must be stored prior to its connection to electric grids or delivery to customers, and EVs need sufficient on-board power sources.  These essential needs have made energy storage a critical component in the creation of sustainable society.	Among all energy storage technologies, electrochemical energy storage within batteries or electrochemical capacitors (ECs) is the most promising approach, since as-stored chemical energy can be effectively delivered as electrical energy with high energy density and power density, high efficiency, long service life and effective cost.  However, the performance of current batteries and ECs are constrained by poor material properties, though great effort has been made to improve materials during the past few years.  The objective of this dissertation is to address the limitation of current energy storage materials by rational architecture design according to the well-recognized principles and criteria.  To achieve this goal, the research strategy is to design and fabricate multifunctional architectures by integrating distinct material structures and properties to address the limitation of traditional materials and create a new family of high-performance energy storage materials with desired properties.	Different types of energy storage architectures were investigated and compared with conventional structures to demonstrate such design concepts.  First, hierarchically porous carbon particles with graphitized structures were designed and synthesized by an efficient aerosol-spray process.  By comparison with commercially available activated carbon and CNTs, it was found that hierarchical pore architecture is important for providing high surface area and fast ion transport, which leads to high capacitance and high power EDLC materials.  Secondly, MnO2/mesoporous carbon nanocomposites were designed.  MnO2 layers with different thicknesses were deposited on mesoporous carbon scaffolds with hierarchical pore structure and the charge storage performance of the composites was correlated to MnO2 layer thickness.  It was determined that a suitable thickness is critical to ensure good electronic conductivity, sufficient electrolyte diffusion and high capacitance.  Thirdly, interpenetrating oxide nanowire/CNT network structures were designed and fabricated by an in situ hydrothermal reaction.  The composition, CNT length, pore structure, V2O5 structure, electrode thickness and architecture are critical factors.  Synergistic effects obtained between V2O5 nanowires and CNTs resulted in an optimal composition with the highest storage performance.  Long CNTs led to robust flexible electrodes, while a hierarchical V2O5 structure enabled storage of both lithium and sodium ions at high rates.  Thus, electrode architectures can be engineered to achieve high-rate, thick electrodes for bulk energy storage.  Last, various architectures obtained through integrating nanocrystals and CNTs were designed and fabricated using ultrafine TiO2 nanocrystals as a model system.  Electrodes were fabricated by directly coating thin film TiO2 on conductive Indium-Tin-Oxide (ITO) glass, by conformably coating nanocrystals on pre-formed CNT papers, or by solvation-induced assembly between nanocrystals and CNTs.  It was demonstrated that thick electrodes with high charge capacity, high rate performance and cycling stability rely on functional architecture that simultaneously provides high electronic conductivity, easy ion diffusion, abundant surface actives sites and robust structure and interfaces.	The general conclusion derived from these studies is that the energy storage performance of electrode materials can be significantly improved by constructing rational architectures that provide effective ion diffusion, good electronic conductivity, fast electrode reaction, robust structure and a stable interface, which normally cannot be obtained with conventional materials.  This strategy also can be extended to other devices, such as batteries and fuel cells, providing a general design platform for high performance energy materials.  Further exploration in this research direction will ultimately lead to high energy, high power, and long life energy storage devices for many applications, including portable electronics, EVs and grid-scale energy storage.
-------------------------------------

10132142_183 - 0.946192843473 - technology_and_computing
[command, regulation, control]

Regulation by Prices and by Command
Standard economic theory states that regulation by price is more efficient than regulation by command and control. Exceptions may arise of regulators have good knowledge of the supply curve. In practice, though, governments usually regulate by command and control, and do so when there is uncertainty about the technology of supply. We show that government may prefer to regulate by command and control when it cares about the investment decisions of a firm.
-------------------------------------

10131779_183 - 0.999958586152 - technology_and_computing
[tool, koehler, use, orang-utan]

Koehler and Tool Use in Orang-Utans
Wewill comment on some specific behaviors observed by Koehler, on the basis of our studies of semi-wild orang-utans conducted in 1991 and 1992 at Sepilok/Sandakan and Semengoh/Kuching Orang-utan Rehabilitation Centers in East Malaysia. Most of our comments refer to tool use,although these may be connected with other observations  
-------------------------------------

10134875_183 - 0.992560373209 - technology_and_computing
[estimator, density]

Topics in nonparametric statistics
This thesis is concerned with nonparametric techniques for inferring properties of time series. First, we consider finite-order moving average and nonlinear autoregressive processes with no parametric assumption on the innovation distribution, and present a kernel density estimator of a bootstrap series that estimates their marginal densities root-$n$ consistently. This is equal to the rate of the best known convolution estimators, and faster than the standard kernel density estimator. We also conduct simulations to check the finite sample properties of our estimator, and the results are generally better than corresponding results for the standard kernel density estimator. Next, given stationary time series data, we study the problem of finding the best linear combination of a set of lag window spectral density estimators with respect to the mean squared risk. We present an aggregation procedure and prove a sharp oracle inequality for its risk. We also provide simulations demonstrating the performance of our aggregation procedure, given Bartlett and other estimators of varying bandwidths as input. This extends work by Rigollet and Tsybakov on aggregation of density estimators. The last part of this thesis introduces a class of robust autocorrelation estimators based on interpreting the sample autocorrelation function as a linear regression. We investigate the efficiency and robustness properties of the estimators that result from plugging on three common robust regression techniques. Construction of robust autocovariance and positive definite autocorrelation estimates is discussed, as well as application of the estimators to AR model fitting. We finish with simulations, which suggest that the estimators are especially well suited for AR model fitting
-------------------------------------

10135247_183 - 0.956061194958 - technology_and_computing
[algorithm, problem, svp, cvp, time, lattice, vector, complexity, result]

Algorithms for the closest and shortest vector problems on general lattices
The shortest vector problem (SVP) and closest vector problem (CVP) are the most widely known problems on point lattices. SVP and CVP have been extensively studied both as purely mathematical problems, being central in the study of the geometry of numbers and as algorithmic problems, having numerous applications in communication theory and computer science. There are two main algorithmic techniques for solving exact SVP and CVP: enumeration and sieving. The best enumeration algorithm was given by Kannan in 1983 and solves both problems in n̂{O(n)} time, where n is the dimensionality of the lattice. Sieving was introduced by Ajtai, Kumar and Sivakumar in 2001 and lowered the time complexity of SVP to 2̂{O(n)} , but required $2̂{O(n)}$ space and randomness. This result posed a number of important questions: Could we get a \emph{deterministic} 2̂{O(n)} algorithm for SVP? Is it possible to acquire a 2̂{O(n)} algorithm for CVP?. In this dissertation we give new algorithms for SVP and CVP and resolve these questions in the affirmative. Our main result is a \emph{deterministic} Õ(2̂{2n}) time and Õ(2n̂) space that solves both SVP and CVP and by reductions of (Micciancio, 2008) most other lattice problems in NP considered in the literature. In the case of CVP the algorithm improves the time complexity from n̂{O(n)} to 2̂{O(n)}, while for SVP we achieve single exponential time as sieving, but without using randomization and improving the constant in the exponent from 2.465 (Pujol and Stehl\ 'e, 2010) to 2. The core of the algorithm is a new method to solve the closest vector problem with preprocessing (CVPP) that uses the Voronoi cell of the lattice (described as intersection of half-spaces) as the result of the preprocessing function. We also present our earlier results on sieving algorithms. Although the theoretical analysis of the proposed sieving algorithm gives worse complexity bounds than our new Voronoi based approach, we show that in practice sieving can be much more efficient. We propose a new heuristic sieving algorithm that performed quite well in practice, with estimated running time 2̂{0.52n} and space complexity 2̂{0.2n}
-------------------------------------

10134954_183 - 0.999999096227 - technology_and_computing
[mobile, space, communication, interaction, project, media, social, phone, part, design]

Lightweight social communication using visual media and mobile phones
When traditional social media are brought off the desktop into the mobile milieu, the resulting interactions may become cumbersome. Yet, the mobile milieu demands even lighter-weight interactions than those on the desktop. We posit that mobile tools can support expressive, lightweight communication by leveraging users' existing tools and practices, streamlining essential interactions, and exploiting the affordances of visual media. To evaluate our ideas, we explore the design space of tools for supporting lightweight social communication using visual media and mobile phones. We present four projects representing points in this space, covering it in the dimensions of proximity (proximate or remote) and interaction focus (input or output). In our analysis, we consider why and how people communicate using these tools, the social consequences of using them, and the implications for design. In the remote-output part of the design space, the Emotipix project aims to reduce demands on attention by embedding an ambient display of friends' photos in the background of the mobile workflow. In the remote-input part of the space, the UbiSketch project aims to enrich communication by supporting ubiquitous sketching, enabling users to easily publish paper-based information to social media channels in real time. In the proximate- output part of the space, the projector phone project explores the use of personal, mobile projection to transcend the limitations of phones' small screens, enabling the use of arbitrary elements of the physical world as (potentially large) display surfaces. In the proximate-input part of the space, the ShadowPuppets project looks at employing hand shadows as input to support collocated interaction with projector phones. We implemented functional prototype systems and conducted user studies, with quantitative and qualitative measures, which validated our techniques. Our design devices were largely successful in supporting expressive, lightweight mobile communication, and some surprising results point to areas for future work
-------------------------------------

10139689_183 - 0.99996401877 - technology_and_computing
[building, bcvtb, bacnet, interface]

BacNet and Analog/Digital Interfaces of the Building Controls Virtual Testbed
This paper gives an overview of recent developments in the Building Controls Virtual Test Bed (BCVTB), a framework for co-simulation and hardware-in-the- loop. First, a general overview of the BCVTB is presented. Second, we describe the BACnet interface, a link which has been implemented to couple BACnet devices  to  the  BCVTB.  We  present  a  case study where the interface was used to couple a whole building simulation program to a building control system to assess in real-time the performance of a real  building.  Third,  we  present the ADInterfaceMCC, an analog/digital interface that allows a USB-based analog/digital converter to be linked to the BCVTB. In a case study, we show how the link was used to couple the analog/digital converter to a building simulation model  for local
loop control.
-------------------------------------

10133265_183 - 0.71878781907 - technology_and_computing
[lot, user, transit, park-and-ride, bay, parking, area]

A Study of Park and Ride Facilities and Their Use in the San Francisco Bay Area
Park-and-ride lots are important support facilities for transit and ride-sharing in the San Francisco Bay Area of California. The authors designed and carried out the region’s first large-scale, detailed study of park-and-ride facilities and users. Three Bay Area Rapid Transit (rail) station parking lots were also surveyed. The user survey results showed that almost all the parking users were commuters; at the freeway lots, half were transit users and the remainder were organized and casual car-poolers. Most drove alone to the park-and-ride lot and made long trips to work, more than 30 mi one way. Users had concerns about lot security, the lack of lighting, and the quality of transit services offered. Analysis of focus group data determined that schedule adherence rather than frequency was the cause of most concerns. Participants expressed a willingness to pay for parking that was fenced, security patrolled, and lighted, with shelters for waiting. Together, the surveys and focus groups have provided insights into ways to improve the park-and-ride lots and the services offered there, as well as on how travelers view transit and carpooling options. The results provide a sound basis for planning improvements.
-------------------------------------

10130999_183 - 0.99997808879 - technology_and_computing
[code]

STDS04.COD:  Political Organization
This file describes codes for variables v81-v98 of the SCCS data set. These codes are from Arthur Tuden and Catherine Marshall. 1972. Political Organization: Cross Cultural Codes 4. ETHNOLOGY 11:436-464.
-------------------------------------

10139216_183 - 0.999013830563 - technology_and_computing
[ecm, soc, dfus, device, cell, hypothesis, care, devoid, efficacy, trial]

Cellular versus acellular matrix devices in treatment of diabetic foot ulcers: study protocol for a comparative efficacy randomized controlled trial
Abstract
				
				
					
						Background
					Diabetic foot ulcers (DFUs) represent a significant source of morbidity and an enormous financial burden. Standard care for DFUs involves systemic glucose control, ensuring adequate perfusion, debridement of nonviable tissue, off-loading, control of infection, local wound care and patient education, all administered by a multidisciplinary team. Unfortunately, even with the best standard of care (SOC) available, only 24% or 30% of DFUs will heal at weeks 12 or 20, respectively.The extracellular matrix (ECM) in DFUs is abnormal and its impairment has been proposed as a key target for new therapeutic devices. These devices intend to replace the aberrant ECM by implanting a matrix, either devoid of cells or enhanced with fibroblasts, keratinocytes or both as well as various growth factors. These new bioengineered skin substitutes are proposed to encourage angiogenesis and in-growth of new tissue, and to utilize living cells to generate cytokines needed for wound repair.To date, the efficacy of bioengineered ECM containing live cellular elements for improving healing above that of a SOC control group has not been compared with the efficacy of an ECM devoid of cells relative to the same SOC. Our hypothesis is that there is no difference in the improved healing effected by either of these two product types relative to SOC.
				
				
					
						Methods/Design
					To test this hypothesis we propose a randomized, single-blind, clinical trial with three arms: SOC, SOC plus Dermagraft&#174; (bioengineered ECM containing living fibroblasts) and SOC plus Oasis&#174; (ECM devoid of living cells) in patients with nonhealing DFUs. The primary outcome is the percentage of subjects that achieved complete wound closure by week 12.
				
				
					
						Discussion
					If our hypothesis is correct, then immense cost savings could be realized by using the orders-of-magnitude less expensive acellular ECM device without compromising patient health outcomes. The article describes the protocol proposed to test our hypothesis.
				
				
					
						Trial registration
					ClinicalTrials.gov: NCT01450943. Registered: 7 October 2011
-------------------------------------

10129804_178 - 0.997572339283 - technology_and_computing
[reference, complex, structure, sampling-based, bibliographical, coverage, planning, path]

Sampling-based coverage path planning for complex 3D structures
Includes bibliographical references (p. 173-186).
-------------------------------------

10135030_183 - 0.817113159962 - technology_and_computing
[ghz, db, sp4t, isolation, power, output, loss, cmo, switch, spdt]

Advanced CMOS circuits for microwave and millimeter-wave communications
The thesis presents advanced circuits in CMOS for microwave and millimeter-wave communications. First, low- noise W-band amplifiers and mW-level 170-200 GHz output doublers in 45-nm Semiconductor-On-Insulator (SOI) CMOS technology are presented. The transistors are modeled using R/C extraction and full electromagnetic modeling. The measured ft of a 30x1-mum transistor is 200-210 GHz at a bias current of 0.3-0.5 mA/mum. A 3-stage W-band amplifier shows a record noise figure of 6.0 dB and a saturated output power of 7.5-8.0 dBm with a power added efficiency of 9%, all at 95 GHz. The G-band balanced doubler results in an output power of 1 mW at 180 GHz. A W -band amplifier/G-band doubler chip is also demonstrated, with a peak output power of 0.5-1 mW at 170-195 GHz and a conversion gain of -2 - -1 dB. This work shows that 45-nm SOI CMOS, built for digital and mixed-signal applications, results in state-of-the-art performance at W-band and G- band. Next, a miniature DC-70 GHz single-pole four-throw (SP4T), 50-70 GHz single-pole double-throw (SPDT), and single-pole four-throw (SP4T) switches built in a low-cost 0.13-mum CMOS process are presented. The DC-70 GHz SP4T switch is based on a series-shunt design with input and output matching circuits. Deep n-well (also called triple- well) CMOS transistors are used to minimize the substrate coupling. Also, deep trench isolation is used between the different ports to minimize the port-to-port coupling. The SP4T results in a measured insertion loss of less than 3.5 dB up to 67 GHz with an isolation of greater than 25 dB. The measured port-to-port coupling is less than 28 dB up to 67 GHz. The measured P1dB and IIP3 are independent of frequency and are 9-10 dBm and 20-21 dBm, respectively. The active chip area is 0.24x0.23 mm². When this work was published, it represented the widest bandwidth SP4T switch in any CMOS technology to-date. The 50-70 GHz single-pole double-throw (SPDT) and single-pole four-throw (SP4T) switches are based on tuned lambda/4 designs with output matching networks. High substrate resistance together with deep trenches and isolation moats are used for low insertion loss. The SPDT and SP4T switches result in a measured insertion loss of 2.0 and 2.3 dB at 60 GHz, with an isolation of > 32 dB and > 22 dB, respectively. The measured output port-to-port isolation is > 27 dB for both designs. The P1dB is 13-14 dBm with a measured IIP3 of > 23 dBm for both switches. Both designs have a return loss better than -10 dB at all ports from 50 to 70 GHz. The active chip area is 0.39x0.32 mm² (SPDT) and 0.59x0.45 mm² (SP4T). When this work was published, it presented the lowest loss 60 GHz SPDT and SP4T switches and also the highest isolation SPDT switch in any CMOS technology to- date. In another project, 5-6 GHz 8x8 and Ku-band 4x4 Butler matrices are presented in a 0.13-mum CMOS implementation. The 8x8 design results in an insertion loss of 3.5 dB at 5.5 GHz, with a bandwidth of 5-6 GHz and no power consumption. The chip area is 2.5x1.9 mm² including all pads. The 8x8 matrix is mounted on a Teflon board with 8-antennas, and the measured patterns agree well with theory and show an isolation of > 12 dB at 5-6 GHz. The 4x4 design results in an insertion loss of 2.4 dB at 12 GHz with a bandwidth of 11-13 GHz. The chip area is only 0.85x0.65 mm² including all pads, and the power consumption is 0̃ mA from a 1.5 V power supply. The 4x4 matrix is mounted on a Teflon board with 4-antennas, and the measured patterns agree well with theory and show an isolation of > 11 dB at 11-13 GHz. CMOS Butler matrices are an excellent candidate for MIMO systems, and can also replace small-element phased-array systems for high-gain transceivers. The applications areas are in high data-rate communications. Finally, a contactless, microwave-based gamma ray detector for detecting low-energy gamma ray photons is presented as an appendix. The detection is based on a microwave cavity perturbation method, and uses a reflection-type cavity resonator with a detector-grade CZT crystal. The reflected power from the cavity is measured and this monitors the changes in the photoconductivity of the CZT crystal in the presence of a gamma-ray. A gamma ray detection sensitivity of < 100 keV is achieved at room temperature. The proposed system can be used in homeland security or radioactive material characterization applications
-------------------------------------

10133289_183 - 0.978438019915 - technology_and_computing
[model, deterioration, optimization, bridge]

Dynamic Programing based Maintenance and Replacement Optimization for Bridge Decks using History-Dependent Deterioration Models
In this research, a reliability-based optimization model of bridge maintenance and replacement decisions is developed. Bridge maintenance optimization models use deterioration models to predict the future condition of bridges. Some current optimization models use physically-based models taking into account the history of deterioration. However, due to the complexity of the deterioration models, the number of decision variables in these optimization models is limited. Some other optimization models consist of a full set of decision variables; however, they use simpler deterioration models. Namely, these deterioration models are Markovian, and the state of the Markov chain is limited to the condition of the facility.
-------------------------------------

10132957_183 - 0.965658918157 - technology_and_computing
[failure, pavement, datum, model, event, time, survey, duration, analysis]

Analysis of Experimental Pavement Failure Data Using Duration Models
Predicting pavement performance under the combined action of traffic and the environment provides valuable information to a highway agency. The estimation of the time at which the pavement conditions will fall below an acceptable level (failure) is essential to program maintenance and rehabilitation works and for budgetary purposes. However, the failure time of a pavement is a highly variable event; terminal conditions will be reached at different times at various locations along a homogeneous pavement section. A common problem in modeling event duration is caused by unobserved failure events in a typical data set. Data collection surveys are usually of limited length. Thus, some pavement sections will have already failed by the day the survey starts, others will reach terminal conditions during the survey period, while others will only fail after the survey is concluded. If only the failure events observed during the survey are included in the statistical analysis (disregarding the information on the after and before events), the model developed will suffer from truncation bias. If the censoring of the failure events is not accounted for properly, the model may suffer from censoring bias.
      In this paper, an analysis of the data collected during the AASHO Road Test is presented. The analysis is based on the use of probabilistic duration modeling techniques. Duration models enable the stochastic nature of pavement failure time to be evaluated as well as censored data to be incorporated in the statistical estimation of the model parameters. The results, based on sound statistical principles, show that the failure times predicted with the model match the observed pavement failure data better than the original AASHO equation.
-------------------------------------

10175430_189 - 0.937794954737 - technology_and_computing
[tower, base, retail]

Sponge, Table, Pads: Exchange Space
This thesis takes the typical one-to-one relationship between a tower and a lobby and asks what if a three-layer base that produces a higher density and a scale exceeding the physical boundary of a single office tower replaces the lobby. Combing retail, conference facilities and recreation the base at once consolidates the programmatic needs of office workers and thickens the singular exchange of lobby-office-lobby to one of recreation-lobby-retail-office-lobby. 

Spanning the length of a ten-acre site in London, the base establishes a large horizontal floor plate and introduces big-box retail and an economy privileging lower prices onto a site surrounded by boutiques and mom-and-pop stores. Instead of planar adjacencies, retail and towers overlay in section. Vertical, horizontal and transverse circulations intersect, turn and unite, forming new programmatic possibilities and proliferating the cultural, economic and social life of a tower onto the city.
-------------------------------------

10139681_183 - 0.999557725361 - technology_and_computing
[resource, workflow, cloud, platform, model, scientist, selection]

Comparison of Resource Platform Selection Approaches for Scientific Workflows
Cloud computing is increasingly considered as an additional computational resource platform for scientific workflows. The cloud offers opportunity to scale-out applications from desktops and local cluster resources. At the same time, it can eliminate the
challenges of restricted software environments and queue delays in shared high performance computing environments. Choosing from these diverse resource platforms for a workflow execution poses a challenge for many scientists. Scientists are often faced with deciding resource platform selection trade-offs with limited information on the actual workflows. While many workflow planning methods have explored task scheduling onto different resources, these methods often require fine-scale characterization of the workflow that is onerous for a scientist. In this position paper, we describe our early exploratory work into using blackbox characteristics to do a cost-benefit analysis across of using cloud platforms. We use only very limited high-level information on the workflow length, width, and data sizes. The length and width are indicative of the workflow duration and parallelism. The data size characterizes the IO requirements. We compare the effectiveness of this approach to other resource
selection models using two exemplar scientific workflows scheduled on desktops, local clusters, HPC centers, and clouds. Early results suggest that the blackbox model often makes the same resource selections as a more fine-grained whitebox model.
We believe the simplicity of the blackbox model can help inform a scientist on the applicability of cloud computing resources even before porting an existing workflow.
-------------------------------------

10134984_183 - 0.999973241342 - technology_and_computing
[amplifier, ghz, bandwidth, wave, gain, constructive, circuit, technique, transimpedance, 3-db]

RFIC design for high-speed optical and multigigabit wireless communication systems
In this dissertation, high-speed and high-frequency millimeter-wave circuit techniques are introduced for silicon integrated circuit processes. A transimpedance limit for multistage transimpedance amplifiers (TIAs) is derived and applied to a bandwidth enhancement technique using inductive-series pi networks. A 40-Gb/s TIA is demonstrated in a 0.13 mum CMOS process and achieves a transimpedance gain of 50 dBomega with a 3-dB bandwidth of 29 GHz. A low-power optical front-end is implemented in a 45-nm silicon-on-insulator (SOI) CMOS process. The modulator driver uses floating body devices to generate a voltage swing of more than 2 Volts. The optical receiver exhibits a transimpedance exceeding 55 dBomega over 30 GHz and consumes only 9 mW from a 1 V supply. Next, a 160- Gb/s amplifier is realized with stagger-tuned stages that are equalized for high bandwidth and low gain ripple. The staggered response is demonstrated with a Darlington feedback amplifier and a constructive wave amplifier. The broadband amplifier is implemented in a 0.12-mum Silion- Germanium (SiGe) BiCMOS process and achieves a gain of 10 dB and 3-dB bandwidth of 102 GHz. In contrast, a 45-nm SOI CMOS, cascode distributed amplifier exhibits 9-dB gain over a 3-dB bandwidth of 92 GHz. In the second part of this dissertation, millimeter-wave circuit design techniques for wireless communication systems are presented. Constructive wave amplification is shown to amplify forward traveling waves along a single transmission line. A 0.12-mum SiGe BiCMOS constructive wave amplifier achieves more than 37.5-dB gain with a 3-dB bandwidth of 14.6 GHz and, consequently, demonstrates a gain-bandwidth product as high as 1,095 GHz. A Q-band (40̃45 GHz) bidirectional transceiver is demonstrated that eliminates the need for transmit/receive switches with a novel PA/LNA circuit. The transmitter Psat is 9.5 dBm while the receiver noise figure is 4.7 dB. Finally, using the constructive wave amplifier technique, a W-band, bidirectional constructive wave amplifier is demonstrated to allow amplification in one of two directions. The amplifier has a peak gain of 16 dB and tuned between 77 and 94 GHz
-------------------------------------

10134713_183 - 0.927540837053 - technology_and_computing
[power, spectacle]

Games of Power: Spectacle in the Aztec and Roman Worlds
In a cross-cultural, cross-temporal comparison, Ana Mitrovici juxtaposes the ritual games and spectacles of the Romans and the Aztecs. The display of conquered peoples by imperial powers is commonly examined in the light of post-colonial theories about power struggles and identity. In addition, with a spectacle like those examined here, the deliberate injection of peripheral images into the center of power shows the centers construction of power and self-identity very clearly. Using her comparison to show the functions of spectacle in imperial identity formation, Mitrovici highlights some of the uses of violence and display within a borderlands complex.
-------------------------------------

10132136_183 - 0.701983122735 - technology_and_computing
[residential, location, distribution, alternative, worksite, similar, spatial, worker, effect, transport]

Worker and Workplace Heterogeneity, Transport Access, and Residential Location: A Historical Perspective on Stockholm
Most analyses of urban transportation and residential location ignore the effects of labor force experience or individual skills upon the location of the worksite; they also ignore the potential effect of these factors upon the tradeoff between housing and community costs.
      This paper, in contrast, analyzes the spatial distribution of worksites by industry, occupation, and educational requirements within a large metropolitan area. In a parallel fashion, we investigate the spatial distribution of the residential sites of workers, differentiated in a similar manner.
      We use this spatially disaggregated information to analyze regularities in commuting and transport behavior. We also develop alternative measures of regional homogeneity, more descriptive alternatives to ratios of jobs to income and similar summary statistics measuring regional "balance."
-------------------------------------

10131834_183 - 0.999683050495 - technology_and_computing
[work, highway, means, transport, access, air, driving, issue, spurt, tool]

Access Magazine Spring 1993
The first issue of ACCESS seems to have been well received, so we're pleased to continue these summaries of our research. Paralleling the spurt of work on new transportation technology, there's been renewed attention to institutional means for improving the nation's transport system. We focus here on several such fiscal and organizational tools for decreasing solo driving, increasing transit riding, and thereby reducing highway congestion, air pollution, and energy consumption.
-------------------------------------

10129910_178 - 0.999429174656 - technology_and_computing
[model, system]

Analysis of various all-electric-ship electrical distribution system topologies
As advances in technology mature, the need is evident for a coherent simulation of the total electric-drive ship to model the effect of new systems on the overall performance of the vessel. Our laboratory has been developing an integrated architectural model in a physics-based environment which analyzes ship variants using a standard set of metrics, including weight, volume, fuel usage and survivability. This paper discusses advances in the model including the use of operational scenarios, incorporation of a survivability metric, and streamlining the performance of model. The model is employed herein to compare two possible distribution system topologies: a ring bus and a breaker-and-a-half. The ring bus is heavier and larger but more survivable. Fuel usage is equivalent in the two variants.
-------------------------------------

10134186_183 - 0.724096601393 - technology_and_computing
[design, run, factorial, class, fractional, optimum]

Robust and Optimum Fractional Factorial Designs
This thesis is devoted to the study of robust and optimum fractional factorial designs. We consider models that contain the general mean, main effects, and <italic> k </italic> two-factor interactions for 2<super>m</super> fractional factorial experiments. We define S<sub>i</sub> to be the set of all (1 &times m) vectors, with elements 1 and -1 of weight i, where the weight of a vector is the number of nonzero elements in it. We present the robustness property of two classes of designs D={S<sub>0</sub>, S<sub>1</sub>, S<sub>m-1</sub>, S<sub>m</sub>} and D<sub>1</sub>={S<sub>0</sub>, S<sub>1</sub>, S<sub>2</sub>, S<sub>m</sub>} with respect to any t runs as well as a specific set of t runs in the sense that the full estimation capacity of the designs remain when we delete any t runs as well as specific t runs from the designs D and  D<sub>1</sub>. The number of runs are (2+m) and 2+m+.5(m)(m-1) in D and D<sub>1</sub> respectively.     We introduce a general structure <bold>M</bold> for the information matrices of a class of models possibly describing the data from a fractional factorial experiment with m factors each at two levels and n runs.  We characterize all the eigenvalues and eigenvectors for such matrices <bold>M</bold>. For m=4 we establish the robustness property of the design D<sub>7</sub>={S<sub>0</sub>, S<sub>1</sub>, S<sub>3</sub>}. The runs of D<sub>7</sub> are contained in design D when m=4. We show all the information matrices from design D<sub>7</sub> and designs obtained from D<sub>7</sub> by deleting some runs are special cases of <bold>M</bold>.     Let D<sub>T</sub> be the class of designs with n runs for estimating the main effects only and let F<sub>T</sub> be the class of foldover designs with 2n runs, n runs from T in  D<sub>T</sub> and another n runs from -T, having full estimation capacity for k=1. We prove that if T*  in D<sub>T</sub>  is E-optimum, the foldover design T*, -T* is optimum design with respect to AMCR and GMCR in  F<sub>T</sub>. Furthermore, if T* is D- and A- optimum with a special structure for <bold>X'<sub>1T*</sub>X<sub>1T*</sub></bold> we prove T*, -T*  is GD, AD, GT, and AT optimal in   F<sub>T</sub>.
-------------------------------------

10138847_183 - 0.700398497584 - technology_and_computing
[correlation, measure, co-expression, module, relationship, information, gene, mutual, network]

Comparison of co-expression measures: mutual information, correlation, and model based indices
Abstract
				
				
					
						Background
					Co-expression measures are often used to define networks among genes. Mutual information (MI) is often used as a generalized correlation measure. It is not clear how much MI adds beyond standard (robust) correlation measures or regression model based association measures. Further, it is important to assess what transformations of these and other co-expression measures lead to biologically meaningful modules (clusters of genes).
				
				
					
						Results
					We provide a comprehensive comparison between mutual information and several correlation measures in 8 empirical data sets and in simulations. We also study different approaches for transforming an adjacency matrix, e.g. using the topological overlap measure. Overall, we confirm close relationships between MI and correlation in all data sets which reflects the fact that most gene pairs satisfy linear or monotonic relationships. We discuss rare situations when the two measures disagree. We also compare correlation and MI based approaches when it comes to defining co-expression network modules. We show that a robust measure of correlation (the biweight midcorrelation transformed via the topological overlap transformation) leads to modules that are superior to MI based modules and maximal information coefficient (MIC) based modules in terms of gene ontology enrichment. We present a function that relates correlation to mutual information which can be used to approximate the mutual information from the corresponding correlation coefficient. We propose the use of polynomial or spline regression models as an alternative to MI for capturing non-linear relationships between quantitative variables.
				
				
					
						Conclusion
					The biweight midcorrelation outperforms MI in terms of elucidating gene pairwise relationships. Coupled with the topological overlap matrix transformation, it often leads to more significantly enriched co-expression modules. Spline and polynomial networks form attractive alternatives to MI in case of non-linear relationships. Our results indicate that MI networks can safely be replaced by correlation networks when it comes to measuring co-expression relationships in stationary data.
-------------------------------------

10137026_183 - 0.999997054497 - technology_and_computing
[tcp, scheme, network, coding, pipeline, application]

TCP in Error-Prone, Intermittent MANETs: Exploiting Codes and Multipath
Wireless communications over error-prone, intermittent networks are challenging, and prevent many applications from working properly. Network coding has been successful in providing low-cost robust communications in such challenged wireless networks. Nevertheless, existing schemes do not take into account constraints from applications, such as delay for streaming applications and for TCP timeouts, fairness, and data reordering. This study aims to bridge the gap between coding protocols and applications. We consider two types of important applications, multicast streams and unicast TCP. In the context of multicast streams, we propose a novel coding scheme, Pipeline Coding. Compared to the conventional batch coding scheme, our analysis shows that Pipeline Coding improves both packet delivery ratio (PDR) and end-to-end delay. Pipeline Coding is also shown via simulations and testbed experiments to improve the throughput, and significantly reduce coding delay. In addition, Pipeline Coding is tested in a heterogeneous VANET testbed, where vehicles upload streams through WiMAX and WiFi links via roadside access points (AP) and base stations (BS) to a static client. The experiment results demonstrate that Pipeline Coding not only achieves better PDR but also utilizes heterogeneous links without explicit scheduling.For TCP over static, error-prone environments, a combined intra-/inter-flow coding scheme, ComboCoding, is proposed. ComboCoding combines inter-flow and intra-flow coding and features an adaptive redundancy control to provide robust, fair TCP communication in challenged wireless networks. Simulation results show that TCP with ComboCoding delivers higher throughput than other coding options in high loss scenarios. Moreover, we study the behavior of multiple TCP sessions intersecting and interfering with each other in the same ad hoc network. The results show that ComboCoding consistently provides better fairness among multiple co-existing TCP sessions when compared with TCP without coding.For TCP over challenged mobile ad-hoc networks (MANETs), we propose a network coded multipath scheme for conventional TCP--CodeMP. CodeMP adapts to frequent link changes in MANET and requires no explicit control messages. The scheme exploits multiple-path redundancy and maintains total transparency to transport layer protocols. The proposed coding scheme is based on three components: (1) random linear coding scheme with adjustable redundancy, (2) multipath routing, (3) ACK Piggy coding. Simulation results show that in an extreme MANET scenario where two TCP sessions co-exists and nodes move as fast as 25 m/s with up to 40% packet error rate (an environment in which regular TCP collapses completely), CodeMP achieves at least 700Kbps aggregate TCP goodput, with a Jain's fairness index of 0.99.
-------------------------------------

10133691_183 - 0.991851558127 - technology_and_computing
[algorithm, ratio, problem, maximum, covering, approximation, online, cover, constraint]

Approximation Algorithms for Covering Problems
In this thesis we present sequential and distributed approximation algorithms for covering problems. First, we give a sequential $\ratio$-approximation algorithm for Monotone Covering, a generalization of many fundamental NP-hard covering problems. The approximation ratio $\ratio$ is the maximum number of variables on which any constraint depends. (For example, for vertex cover, $\ratio$ is 2.) The algorithm unifies, generalizes, and improvesmany previous algorithms for fundamental covering problems  such as Vertex Cover, Set Cover, Facilities Location, and Covering Mixed-Integer Linear Programs with upper bound on the variables.The algorithm is also a $\ratio$-competitive algorithm for online Monotone Covering, which generalizes online versions of the above-mentioned covering problems as well as many fundamental online paging and caching problems. As such it also generalizes many classical online algorithms, including LRU, FIFO, FWF, Balance, Greedy-Dual, Greedy-Dual Size (a.k.a.  Landlord), and algorithms for connection caching, where $\ratio$ is the cache size. It also gives new $\ratio$-competitive algorithms for upgradable variants of these problems, which model choosing the caching strategy andan appropriate hardware configuration (cache size, CPU, bus, network, etc.).Then  we show distributed versions of the sequential algorithm. For Weighted Vertex Cover, we give a distributed 2-approximation algorithm taking $O(\log n)$ rounds. The algorithm generalizes to covering mixed integer linear programs (CMIP) with two variables per constraint ($\ratio=2$). For any Monotone Covering problem, we showa distributed $\ratio$-approximation algorithm taking$O(\log^2 |\calC|)$ rounds, where $|\calC|$ is the number of constraints. Last, we extend the distributed algorithms for covering to compute $\ratio$-approximate solutions for Fractional Packing and Maximum Weighted b-Matching in hypergraphs, where $\ratio$ is the maximum number of packing constraints in which a variable appears (for Maximum Weighted b-Matching $\ratio$ is the maximum edge degree --- for graphs $\ratio=2$).
-------------------------------------

10136570_183 - 0.999593422945 - technology_and_computing
[model, system, casimir, effect, critical, boundary]

The Critical Casimir Effect in Model Physical Systems
The Casimir effect is an interaction between the boundaries of a finite system when fluctuations in that system correlate on length scales comparable to the system size. In particular, the critical Casimir effect is that which arises from the long-ranged thermal fluctuation of the order parameter in a system near criticality. Recent experiments on the Casimir force in binary liquids near critical points and 4He near the superfluid transition have redoubled theoretical interest in the topic. It is an unfortunate fact that exact models of the experimental systems are mathematically intractable in general. However, there is often insight to be gained by studying approximations and toy models, or doing numerical computations. In this work, we present a brief motivation and overview of the field, followed by explications of the O(2) model with twisted boundary conditions and the O(n->infinity) model with free boundary conditions. New results, both analytical and numerical, are presented.
-------------------------------------

101642_108 - 0.997815939484 - technology_and_computing
[agreement, language, database, system]

A typological database of agreement
This paper discusses the construction of a typological database of agreement on the basis of fifteen languages taken from different language families so as tomaximise diversity. For each of these languages, the database will contain detailed information about agreement controllers, targets, domains, categories, and conditions. Thus the database is designed to help us to develop a general typology of agreement systems which predicts what is, and what is not a possible agreement system in natural language. This is primarily a theoretical aim, but the database may also have practical applications in that agreement has implications for the design of parsers in natural language systems.
-------------------------------------

10130293_178 - 0.999973524363 - technology_and_computing
[design, vertical, reference, residential, neighborhood, bibliographical, leaf, high-rise, exploration]

Vertical neighborhoods : a residential high-rise design exploration
Includes bibliographical references (leaves 212-216).
-------------------------------------

10130061_178 - 0.999997027183 - technology_and_computing
[system, large-scale, traffic, reference, agent, bibliographical, mobile, congestion-aware]

Congestion-aware traffic routing for large-scale mobile agent systems
Includes bibliographical references (p. 191-201).
-------------------------------------

10138188_183 - 0.995850124335 - technology_and_computing
[amr, field, vector, application, curve, visualization, problem, integral, simulation]

On the Computation of Integral Curves in Adaptive Mesh Refinement Vector Fields
Integral curves, such as streamlines, streaklines, pathlines, and timelines, are an essential tool in the analysis of vector field structures, offering straightforward and intuitive interpretation of visualization results. While such curves have a long-standing tradition in vector field visualization, their application to Adaptive Mesh Refinement (AMR) simulation results poses unique problems. AMR is a highly effective discretization method for a variety of physical simulation problems and has recently been applied to the study of vector fields in flow and magnetohydrodynamic applications. The cell-centered nature of AMR data and discontinuities in the vector field representation arising from AMR level boundaries complicate the application of numerical integration methods to compute integral curves. In this paper, we propose a novel approach to alleviate these problems and show its application to streamline visualization in an AMR model of the magnetic field of the solar system as well as to a simulation of two incompressible viscous vortex rings merging.
-------------------------------------

10136731_183 - 0.996635080863 - technology_and_computing
[spad, detector, avalanche, single, photon, circuit, self-quenching, self-recovering, external, model]

Physics of self-quenching and self-recovering Single Photon Avalanche Detectors (SPADs)
Single photon avalanche detectors (SPADs) have attracted increasing attention because of their critical roles in many important applications such as quantum cryptography, optical time-domain reflectometry, time-resolved spectroscopy, non-line-of-sight optical communication, space communication and light detection and ranging. Although Silicon SPADs are limited to wavelength below 1.1um, III-V based SPADs with Separate Absorption and Multiplication (SAM) structure are of special interest in the telecommunication wavelengths, exhibiting superior performance with high single photon detection efficiency and low dark count rate. Conventionally, due to the self- sustaining nature of avalanche, operation of SPADs requires integration of external quenching circuit to achieve both quenching and recovering capabilities. Integration of external quenching circuit with III-V SPADs, however, adds substantial complexity to device fabrication, especially for array detectors when each individual detector requires an external quenching circuit. Here, we present single photon avalanche detectors featuring a Transient Carrier Buffer (TCB) layer to form an energy barrier which can tentatively stop avalanche-generated carriers, demonstrating self-quenching and self-recovering capabilities. For this self-quenching and self-recovering SPAD, the escape rate of those stopped avalanche carriers from the barrier determines the self-recovery time and thus count rate of the single-photon detector. A physical model has been developed to simulate the dynamic characteristics of the detector. The simulation results agree well with the experimental data, and the self- recovery time is found to be reduced with increasing temperature, the magnitude of overbias, the dosage in the charge layer and the barrier height. In addition, thermionic emission shows a stronger dependence on temperature and a weaker dependence on device bias and charge layer dosage than tunneling. The model contains no fitting parameters and therefore can be used to model and predict the device behaviors
-------------------------------------

10132166_183 - 0.925293394611 - technology_and_computing
[design]

Multiple Roadway Boulevards: Case Studies, Designs, and Design Guidelines
This study and report was born of experience with boulevards and -- following research on the safety characteristics of such roads -- driver and pedestrian behavior on them, their physical design quantities, and existing standards and norms that effectively govern their construction, develops a comprehensive set of design guidelines for their future construction and use.
-------------------------------------

10133018_183 - 0.998788142264 - technology_and_computing
[visual, human, system, object, detection, model, assumption, observer, study, part]

Dynamic Estimation of Oncoming Vehicle Range and Range Rate: An Assessment of the Human Visual System's Capabilities and Performance
The detection of impending collisions and the subsequent choice and regulation of maneuvers to deal with them are general problems of locomotor control that arise in many situations, both human and non-human. When an object moves towards an observer, the size of the image that it projects onto the retina of the observer’s eyes increases, providing a powerful sensation of motion. Physiological and psychophysical research into this “looming” effect provides strong evidence for the existence of neural “looming detectors” that are used by humans and non-humans alike to detect and respond to oncoming objects. Automotive applications constitute an important context for the study of the visual perception of looming. To date, however, this aspect of the driver’s performance has largely been neglected, and human driver models typically incorporate representations of the visual system that are based upon idealized behavior and in some cases questionable assumptions.
      In this three part study we begin to address the deficiency by quantifying the visual system’s ability to detect and track an object’s approach, as represented by the rate of change of the angle θ that its image subtends on the retina of the eye. In the first part we tested a long-standing assumption of an absolute threshold in the human’s ability to detect dθ/dt, below which humans are unable to discern that θ is changing (and thus that a collision is imminent). The results provide evidence contradicting the threshold assumption, and indicate instead that the detection task is more accurately described as one of signal detection (detection of the signal dθ/dt in the presence of noise) with no threshold limitation. Collision avoidance requires that an observer accurately and continuously track an approaching object’s distance and closing speed. In the second part of this study we investigated the dynamic response of the visual system to changes in θ, employing both psychophysical and classical frequency response techniques. We found that the visual system exhibits a band-pass characteristic in this task that is well described by a linear, minimum phase, second order transfer function. Further analysis revealed that this aspect of the visual system exhibits a biphasic impulse response, which is the focus for the third part of our study. According to the model, certain pairs of “impulsive” stimuli presented in the proper sequence will reinforce one another, and thus be more easily detected, while others will cancel each other and be less so. This final series of experiments provided evidence consistent with this hypothesis.
      The shortcomings of human driver models based upon current assumptions are discussed, and the development of improved models based the dynamic response characteristics of the visual system and the principles of signal detection are described. To focus our efforts we have assumed a fairly constrained driving scenario (the “Lead Vehicle Braking” scenario), but these results are applicable to any scenario (automotive or not) in which the observer has an unobstructed view of the approach of an object or stationary obstacle.
-------------------------------------

10132151_183 - 0.741945491318 - technology_and_computing
[line, metrolink, commuter, air, system]

An Automobile/Transit Emissions Evaluation of Southern California's Metrolink
In order to alleviate traffic congestion and obtain better air quality in the South Coast Air Basin, the Southern California Regional Rail Authority (SCRRA) began constructing a new commuter train system called Metrolink in October 1992. There are currently five lines in operation: the Riverside Line, San Bernardino Line, Santo Clarita Line, Ventura County Line, and the Fullerton Line. The system is still expanding and when complete, Metrolink will form the nation’s sixth largest commuter rail system, with construction costs exceeding $500 million. Metrolink commuter trains connect suburban communities with centers of business, such as Burbank, Glendale, Industry, and downtown Los Angeles. The SCRRA monitors the passenger ridership counts of each line closely, from which the amount of congestion mitigation can be determined. However, there have been no detailed studies on the direct air quality impact of Metrolink since its inception.
-------------------------------------

10131702_183 - 0.92067076678 - technology_and_computing
[traffic]

Commuter Stress
Everyone knows that being stuck in traffic is stressful, and few look forward to long commutes on congested roadways. But the negative effects of traffic congestion go beyond frustration and fatigue. Our research at UC Irvine since the late 1970s has discovered adverse health impacts of what we call "high impedance" commuting.
-------------------------------------

10134054_183 - 0.999964245535 - technology_and_computing
[channel, detection, datum, symbol, deterministic, time-varying, probabilistic, estimation, frequency, receiver]

Time varying channels : characterization, estimation, and detection
This dissertation is concerned with digital communications systems operating over channels that vary in time due to mobility of the transmitter or receiver. Velocity of the transmitter or receiver causes a Doppler shift or smearing in frequency which hinders parameter estimation and degrades data detection performance. To aid in our goal of improved estimation and detection performance, we discuss several characterization methods, both probabilistic and deterministic, that can accurately model the time-varying channel and that enable efficient parameter estimation and data detection. The first topic we address is that of carrier frequency offset estimation in the presence of a time-varying frequency-selective channel. The scheme is data-aided, meaning that a block of pilot data known to the receiver is first transmitted through the channel. Joint estimators of both the CFO and the time-varying frequency-selective channel are developed using both probabilistic and deterministic channel models. The probabilistic CFO estimator is shown to achieve the Cramer -Rao Lower Bound, and the deterministic estimator can perform well, but depends heavily on the choice of channel parameterization in the deterministic model. The second topic we focus on is concerned with blind data detection over time-varying channels. The problem we consider is recovery of a short block of time-domain data symbols transmitted over a channel with time-varying multiplicative distortion. The receiver does not know the data, the channel, or any statistics that could parameterize a probabilistic description of the channel. We design data detection algorithms based on a deterministic parameterization of the channel that can be implemented efficiently with the sphere decoding algorithm. Additionally we provide techniques for a priori selection of the channel parameterization that result in near-optimal performance over a wide range of channel conditions. In our final topic we consider blind detection over a frequency-selective time-varying channel. A short block of symbols is transmitted in the frequency domain, as either an OFDMA frequency allocation or as a subset of a larger OFDM symbol. The multiplicative distortion across the symbols is varying, due to frequency-selectivity. In addition, the transmitted symbols interfere with one another due to the channel time-variation, manifested as intercarrier interference. We introduce a novel method of correlatively coding the symbols across frequency before transmission that reduces the power of the ICI at the receiver. The scheme results in transmitted symbols that are constant in magnitude, which enables the application of efficient detection algorithms also based either probabilistic or deterministic parameterization of the channel. The resulting blind detector is efficient and can outperform an OFDM system which knows the channel perfectly
-------------------------------------

10135664_183 - 0.999784821878 - technology_and_computing
[datum, format, triumf, translator, analysis]

A Translator for ISIS and TRIUMF MuSR Data
There are relatively few &mu;SR labs, and each of them interprets their data with in-house software. Inevitably, the various analysis suites come with their own strengths and weaknesses; for example, some lend themselves to batch analysis, while others are more flexible with respect to fitting functions. It is then no surprise that several translators are included by default with the TRIUMF mud_util toolset.  Among these are tools to convert &mu;SR data from obsolete TRIUMF data formats to the modern specification, as well as between Paul Scherrer Institute (PSI) and TRIUMF formats. mud_util also provides the ability to convert data from an outdated format used by the Rutherford Appleton Laboratories' ISIS muon source, but a tool which understands data in the contemporary Nexus format is conspicuously absent. Such a &ldquo;translator&rdquo; would provide a welcome chance to check that the results of data analysis are not affected by the quirks of a particular program.
-------------------------------------

10136818_183 - 0.999447012284 - technology_and_computing
[model, linear, test, glmm, regression]

Goodness-of-Fit Tests for Autoregressive Logistic Regression Models and Generalized Linear Mixed Models
Goodness-of-fit is a very important concept in data analysis, as most statistical models make some underlying assumptions. When these assumptions are violated, any model inference can be suspect. Thus, a goodness-of-fit check is necessary in order to trust any conclusions drawn from the model. Herein we propose two goodness-of-fit tests, one that addresses autoregressive logistic regression (ALR) models and another that is appropriate for generalized linear mixed models (GLMMs).Both GLMMs and ALR models are extensions of generalized linear models, a broad class of models that includes logistic regression and Poisson regression. ALR models go a step beyond typical generalized linear models by regressing upon past observations. In contrast, GLMMs go beyond the scope of generalized linear models by incorporating random effects. For the ALR model, a chi-square test is proposed and the asymptotic distribution of the statistic is derived. General guidelines for a two-dimensional, dynamic binning strategy are provided, which make use of two types of maximum likelihood parameter estimates. For smaller sample sizes, a bootstrap p-value procedure is discussed. Simulation studies indicate that the procedure has the correct size and is sensitive to model misspecification. In particular, the test is very good at detecting the need for an additional lag. An application to a dataset relating to late-onset Alzheimer's disease is provided. For GLMMs, we propose a Cramer-von-Mises omnibus test statistic, which extends upon a procedure applied to Poisson regression. Here, predictors of the random effects are plugged into the model to approximate a simpler, generalized linear model. The statistic is then calculated by making use of a probability integral transformation. Simulation studies indicate that the test has good size and power for a Poisson GLMM. Some ideas for future research are also proposed.
-------------------------------------

10137367_183 - 0.979162221208 - technology_and_computing
[problem, algorithm, dimension, multiple, result, bin, vector, optimal]

Optimal Vector Packing in Multiple Dimensions with Heuristic Search
We study an extension of the well-known bin-packing problem to multiple dimensions, resulting in the "vector packing" problem. The problem is to find the minimum number of multidimensional bins to pack a set of vectors into without exceeding the bin capacity in any dimension of any bin. While we use approximate methods to inform our search, we ultimately return optimal solutions. This work is an attempt to extend the results of Kor03 to multiple dimensions and combine it with some new techniques. A hybrid DFBnB algorithm which searches in two separate problem spaces is used as the final algorithm. Our hybrid algorithm also uses a heuristic to assist the search, and an approximate algorithm to initially bound the cost of an optimal solution. The resulting algorithm can be run on vector-packing problems with any number of dimensions, and good results are shown for up to five-dimensional problems. An attempt is also made to define a method to compare these results with future algorithms, as consensus on appropriate test suites for vector-packing problems seems to be lacking.
-------------------------------------

10175417_189 - 0.902106912358 - technology_and_computing
[dna, complex, dynamic, imaging, strand, probe, analysis, ability, fluorescent, sample]

Development of Dynamic DNA Probes for High-Content in situ Proteomic Analyses
Dynamic DNA complexes are able to undergo multiple hybridization and dissociation events through a process called strand displacement.  This unique property has facilitated the creation of programmable molecular detection systems and chemical logic gates encoded by nucleotide sequence.  This work examines whether the ability to selective exchange oligonucleotides among different thermodynamically-stable DNA complexes can be harnessed to create a new class of imaging probes that permit fluorescent reporters to be sequentially activated (“turned on”) and erased (“turned off”).  Here, dynamic DNA complexes detect a specific DNA-conjugated antibody and undergo strand displacement to liberate a quencher strand and activate a fluorescent reporter.  Subsequently, incubation with an erasing complex allows the fluorophore to be stripped from the target strand, quenched, and washed away.  This simple capability therefore allows the same fluorescent dyes to be used multiple times to detect different markers within the same sample via sequential rounds of fluorescence imaging.
We evaluated and optimized several DNA complex designs to function efficiently for in situ molecular analyses.  We also applied our DNA probes to immunofluorescence imaging using DNA-conjugated antibodies and demonstrated the ability to at least double the number of detectable markers on a single sample.  Finally, the probe complexes were reconfigured to act as AND-gates for the detection of co-localized proteins.  Given the ability to visualize large numbers of cellular markers using dynamic DNA probe complexes, high-content proteomic analyses can be performed on a single sample, enhancing the power of fluorescence imaging techniques.  Furthermore, dynamic DNA complexes offer new avenues to incorporate DNA-based computations and logic for in situ molecular imaging and analyses.
-------------------------------------

10135479_183 - 0.998509200821 - technology_and_computing
[envelope, amplifier, high, power, tracking, efficiency, voltage]

High efficiency wideband envelope tracking power amplifier for next-generation wireless communications
The latest generation of smart devices deployed in cellular networks has created explosive growth in network data traffic, and the increasing demand for broadband services with higher data rates , require higher peak to average power ratio (PAPR) with wider bandwidth. One of the challenges in the conventional power amplifiers (PAs) with fixed supply voltage, is the degraded efficiency and generated heats at a large back-off to meet tight linearity requirements. This dissertation presents high efficiency wideband envelope tracking power amplifiers for 2.1 GHz micro base-stations and 2.5 GHz wireless mobile applications. By superimposing the envelope signal at the drain such that the RF amplifier operates consistently closer to saturation, the overall efficiency is improved and the generated heat is reduced dramatically. In the first part of the dissertation, a high performance BiCMOS DMOS monolithic envelope amplifier for micro-base station power amplifiers is presented. Due to the low breakdown voltage of the CMOS transistors, the high voltage envelope amplifier has been implemented with discrete components with high voltage process. Compared to these discrete solutions, an integrated circuits implementation for the envelope amplifier brings many benefits. The design of monolithic envelope amplifiers for high voltage (VDD = 15 V) envelope tracking applications, and the design techniques to solve the reliability issues with thin gate oxide is described. The overall envelope tracking system employing a GaN-HEMT RF transistor, and fully integrated high voltage envelope amplifier with a 0.35mum BiCMOS DMOS process, is demonstrated. In the second part, a high- efficiency wideband envelope tracking power amplifier for mobile LTE applications will be presented. The CMOS envelope amplifier with hybrid linear and switcher is designed in a 150 nm CMOS process. The envelope amplifier employs direct sensing of the linear stage current to reduce the propagation delay in the switcher. The strategy is demonstrated to improve the efficiency of the complete envelope tracking power amplifier system. The resulting performance of envelope tracking system employing a GaAs HBT-based RF PA with a 5 MHz LTE signal input demonstrated state-of-the-art efficiency while meet the linearity requirement
-------------------------------------

10133785_183 - 0.999803958698 - technology_and_computing
[electronic, contact, semiconductor, effect, structure, nanoscale, device, microscopy, nitride, chapter]

Scanned probe characterization of semiconductor nanostructures
Advances in the synthesis of materials and device structures have accentuated the need to understand nanoscale electronic structure and its implications. Scanning probe microscopy offers a rich variety of highly spatially accurate techniques that can further our understanding of the interactions that occur in nanoscale semiconductor materials and devices. The promising nitride semiconductor materials system suffers from perturbations in local electronic structure due to crystallographic defects. Understanding the electronic properties and physical origin of these defects can be invaluable in mitigating their impacts or eliminating them all together. In the second chapter of this dissertation, scanning capacitance microscopy (SCM) is used to characterize local electronic structure in a-plane n-type gallium nitride. Analysis reveals the presence of a linear, positively charged feature aligned along the direction which likely corresponds to a partial dislocation at the edge of a stacking fault. In the third chapter, conductive atomic force microscopy is used to determine the effects of Ga/N flux on the conductive behavior of reverse-bias leakage paths in gallium nitride grown by molecular beam epitaxy (MBE). Our data reveal a band of fluxes near Ga/N ̃ ̃1 for which these pathways cease to be observable. These observations suggest a method for controlling the primary source of reverse-bias Schottky contact leakage in n-type GaN grown by MBE. A deeper understanding of the interaction between macro-scale objects and nanoscale electronic properties is required to bring the exciting new possibilities that semiconductor nanowires offer to fruition. In the fourth chapter, SCM is used to examine the effects of micron-scale metal contacts on carrier modulation and electrostatic behavior in indium arsenide semiconductor nanowires. We interpret a pronounced dependence of capacitance spectra on distance between the probe tip and nanowire contact as a consequence of electrostatic screening of the tip-nanowire potential difference by the large metal contact. These results provide direct experimental verification of contact screening effects on the electronic behavior of nanowire devices and are indicative of the importance of accounting for the effect of large-scale contact and circuit elements on the characteristics of nanoscale electronic devices
-------------------------------------

10137440_183 - 0.999961376531 - technology_and_computing
[anti-, dll, datum, hyperon, spin, sample, proton, gev]

Longitudinal Spin Transfer to Lambda and Anti-Lambda Hyperons Produced In Polarized Proton-Proton Collisions at sqrt{s} = 200 GeV
Studies on the spin structure of the proton have been an active area of research; after the EMC experiment and subsequent experiments found that only about 30% of the total proton spin is carried by quark spins. The Relativistic Heavy Ion Collider (RHIC) is the world's first and only polarized proton collider. The Solenoidal Tracker At RHIC (STAR) has full azimuthal acceptance and is ideally suited to advance studies of the proton spin. The longitudinal spin transfer, D<sub>LL</sub>, of &lambda; and anti-&lambda; hyperons in longitudinally polarized proton-proton collisions is sensitive to quark and anti-quark polarization in the polarized proton; as well as to polarized fragmentation; and has been proposed as a possible probe of (anti-)strange quark polarization.The STAR collaboration has previously reported an initial proof-of-concept measurement of D<sub>LL</sub> of &lambda; and anti-&lambda; hyperons from a data sample obtained at sqr(s)=200 GeV in 2005.  The data correspond to an integrated luminosity of 2 pb<super>-</super> with 50% beam polarization.  Considerably larger data samples corresponding to 6.5 pb<super>-</super> and 25 pb<super>-</super> with beam polarizations of 57% at sqr(s)=200 GeV were obtained in 2006 and 2009 using an upgraded instrument.  Improvements were made on the analysis procedure to reduce background contribution to the &lambda; + anti-&lambda; measurements. These new measurements of D<sub>LL</sub> form the main topic of this dissertation.  The sample of hyperons residing within a jet that triggered the experiment are classified as near-side hyperons, and are analyzed separately from an away-side sample that has similar precision.  In addition to D<sub>LL</sub>, the double longitudinal spin asymmetry, A<sub>LL</sub>, for the production of &lambda; and anti-&lambda; hyperons has been extracted.  The dependences of D<sub>LL</sub> on pseudo-rapidity, p<sub>T</sub>, and the fragmentation ratio, <italic>z</italic>, are studied.  The stated D<sub>LL</sub> from &lambda; and anti-&lambda; each disfavor one of the published model predictions for D<sub>LL</sub> for a combined &lambda; and anti-&lambda; sample, and are consistent with other predictions as well as with the preceding data.  The disfavored model prediction is based on the assumption that up, down, and strange (anti-)quark spins contribute equally to polarized fragmentation into  &lambda; + anti-&lambda; hyperons.  Future data with improved statistical precision, including data at sqr(s)=500 GeV, are needed to discriminate between the other models.
-------------------------------------

10136929_183 - 0.999997322214 - technology_and_computing
[system, solution, datum, density, regularity, initial, case, time, space, non-constant]

Nematic Liquid Crystal Systems and Magneto-Hydrodynamics System:The Properties of Their Solutions
We study a simplified system for the flow of Nematic Liquid Crystals (LCD) in the cases of non-constant density and constant density with certain initial and boundary conditions. This is a coupled non-parabolic dissipative dynamic system. For the system with non-constant density in the whole space $\mathbb{R}^3$, we establish the global existence of weak solutions. For the system with non-constant density on a bounded domain in two dimensional and three dimensional cases, we establish the regularity and uniqueness of solutions. We obtain that, in 2D, the global regularity with general data; in 3D, the global regularity with small initial data  and a local (short time) regularity with large data. In addition, with more smoothness assumption on initial data, we obtain the uniqueness both for dimension 2 and 3 cases. The main tools are higher order energy estimates (so called Ladyzhenskaya estimate), Gagliardo-Nirenberg interpolation inequalities and the frozen coefficient method.For the system with constant density in the whole space $\mathbb{R}^3$ with small initial data,we study the large time behavior of  solutions and obtain algebraic decay rate for the solutions inenergy space. The main ingredient to derive decay is Fourier splitting method which was originally introduced by M. Schonbek to study the large time behavior of solutions to Navier-Stokes equations.We also study the magneto-hydrodynamics system (MHD). Wedemonstrate that the solutions to the Cauchy problem for the threedimensional incompressible MHD system candevelop different types of norm inflations in $\dot{B}_{\infty}^{-1, \infty}$.Particularly the magnetic field can develop norm inflation in shorttime even when the velocity remains small and vice verse.
-------------------------------------

10136179_183 - 0.999966451957 - technology_and_computing
[code, variable, multivariate, algorithm, cumulant]

Multivariate Cumulates in R
We give algorithms and R code to compute all multivariate cumulants up to order p of m variables.
-------------------------------------

10130055_178 - 0.72872323758 - technology_and_computing
[bibliographical, circuit, process, fabrication, modeling, planarization, advanced, reference]

Advanced modeling of planarization processes for integrated circuit fabrication
Includes bibliographical references (p. 215-225).
-------------------------------------

10131676_183 - 0.969785476316 - technology_and_computing
[method, model, limit, case, bl, datum, binary, choice, analysis, hl]

Analysis of Binary Choice Frequencies With Limit Cases
An extensive evaluation of alternative estimation methods for logistic binary choice probabilities when applied to binary frequency data with limit cases is presented in this paper. The methods examined are: binomial-logistic (BL) model, Berkson's (BK) method, and Haldane's (HL) method. These models are applied to weekly household mode choice data that contained a substantial number of limit cases in which one of the alternatives was never chosen. The results obtained indicate that the BL model is a practical tool that outperforms all the other methods examined in this study. The BL models accommodate limit cases without requiring any additional assumptions or approximations. The BK and HL methods have been shown to offer coefficient estimates similar to, and fits that are somewhat worse than, those obtained by the BL models. These methods remain to be useful tools for the analysis of binary frequency data, especially in initial phases of analysis. In this paper it is also shown that coefficient estimates of HL method are sensitive to the value of the adjustment constant, 6, used to incorporate limit cases and its optimal value may depend on the data at hand.
-------------------------------------

10137030_183 - 0.999884379611 - technology_and_computing
[control, system, predictive, model, economic, process, method]

Economic and Distributed Model Predictive Control of Nonlinear Systems
Maximizing profit has been and will always be the primary purpose of optimal process operation. Within process control, the economic optimization considerations of a plant are usually addressed via a two-layer architecture. In general, this architecture includes: the upper layer that optimizes process operation set-points taking into account economic considerations using steady-state system models, and the lower layer (i.e., process control layer) whose primary objective is to employ feedback control systems to force the process to track the set-points. Optimizing closed-loop performance with respect to general economic considerations for nonlinear systems in a unified framework has recently become a subject of increasing theoretical interest and practical importance. In addition to a tighter integration of economics and control, advances in communication technologies have motivated augmentation of traditional point-to-point and wired local control systems with additional cheap and easy-to-install networked sensors and actuators and control systems. Networked distributedcontrol systems can substantially improve the efficiency, flexibility, robustness and fault tolerance of an industrial control system while reducing the installation, reconfiguration and maintenance expenses at the cost of coordination and design/redesign of different control systems in the new architecture.This dissertation presents rigorous, yet practical, methods for the design of economic and distributed predictive control systems. Beginning with a review of recent results on the subject, the dissertation presents the design of Lyapunov-based economic model predictive control scheme for a broad class of nonlinear systems using state and output feedback. Then, the dissertation focuses on the development of an economic model predictive control method with guaranteed improvement in closed-loop performance compared to conventional Lyapunov-based model predictive control designs. Subsequently, the dissertation focuses on the design of a networked distributed model predictive control method for multirate uncertain systems subject to communication disruptions and measurement noise and distributed model predictive control method for switched systems to compute optimal manipulated input trajectories that achieve desired stability, performance and robustness specifications. The control methods are applied to nonlinear chemical process networks and their effectiveness and performance are evaluated through extensive computer simulations.
-------------------------------------

10137158_183 - 0.706958536292 - technology_and_computing
[method, reference, genome]

Genome reassembly with high-throughput sequencing data
Recent studies in genomics have highlighted the significance of structural variation in deter-mining individual variation. Current methods, however, are predominantly focused on eitherassembling whole genomes from scratch, or identifying the relatively small changes betweena genome and a reference sequence. While significant progress has been made in recent yearson both de novo assembly and resequencing (read mapping) methods, few attempts havebeen made to bridge the gap between them.In this paper, we present a computational method for incorporating a reference sequenceinto an assembly algorithm. We propose a novel graph construction that builds upon thewell-known de Bruijn graph to incorporate the reference, and describe a simple algorithm,based on iterative message passing, which uses this information to significantly improveassembly results. We validate our method by applying it to a series of 5Mb simulationgenomes derived from both mammalian and bacterial references. The results of applyingour method to this simulation data are presented along with a discussion of the benefits anddrawbacks of this technique.
-------------------------------------

10134987_183 - 0.983370929857 - technology_and_computing
[channel, ppbar, result, top, pb, quark, tqb]

Measurement of The Single Top Quark Production Cross Section at sqrt(s)=1.96 TeV
Within the standard model top quarks are predicted to be most often produced in pairsvia the strong interaction. However they can also be produced singly through the weakinteraction. This is a rarer process with many experimental challenges. It is interestingbecause it provides a new window to search for evidence of physics beyond the standardmodel picture, such as a fourth generation of quarks or to search for insight into the HiggsMechanism. Single top production also provides a direct way to calculate the CKM matrixelement Vtb.       This thesis presents new measurements for single top quark production in the s+t, s andt channels using 5.4 fb-1 of data collected at the DØ detector at Fermilab in Batavia, IL.  The analysis was performed using Boosted decision trees to separate signal from background and Bayesian statistcs to calculate all the cross sections. The results obtained are:s channel :  &sigma; (ppbar &rarr; tb + X) = 0.68 +0.41 - 0.39 pb,t channel :  &sigma; (ppbar &rarr; tqb + X) = 3.03 +0.78 - 0.66 pb,Combined s+t channels :   &sigma;(ppbar &rarr; tb + X, tqb + X) = 3.01 +0.80 - 0.75 pb.     The s channel has a Gaussian significance of 1.6&sigma;, the t channel 5.5&sigma;, and the s+t 4.3&sigma;.  The results are consistent with the standard model predictions within one standard deviation. By combining these results with the results for two other analyses (using different MVA techniques) improved results of:Combined s+t channels :   &sigma;(ppbar &rarr; tb + X, tqb + X) = 3.43 +0.73 - 0.74 pb,t channel :  &sigma;(ppbar &rarr; tqb + X) = 2.90 +0.59 - 0.59 pb.were obtained. These give a 5.6&sigma; significance for the combined s + t channel and 5.5&sigma; for the t channel.
-------------------------------------

10132148_183 - 0.886847124791 - technology_and_computing
[function, cost, profit]

Restricted Least Squares Subject to Monotonicity and Concavity Constraints
Economists have devoted a large research effort to the estimation of cost functions and profit functions. Since the popularization of the dual approach to such functions, econometricians have focussed particularly on methods for fitting functions that satisfy the restrictions implied by optimizing behavior, minimizing costs or maximizing profits. For the most part, researchers have sought simple parametric functional forms that are sufficiently flexible to approximate well all possible functions in the families of cost and profit functions. Given such parametric functions, much of the estimation has followed the method of least squares.
-------------------------------------

10135065_183 - 0.946831552821 - technology_and_computing
[graphene, device, property, electrical, mechanical, trilayer]

Electrical and Mechanical Properties of Graphene
Graphene is an exciting new atomically-thin two-dimensional (2D) system of carbon atoms organized in a hexagonal lattice structure. This "wonder material" has been extensively studied in the last few years since it's first isolation in 2004. Its rapid rise to popularity in scientific and technological communities can be attributed to a number of its exceptional propertiess. In this thesis I will present several topics including fabrication of graphene devices, electrical and mechanical properties of graphene.I will start with a brief introduction of electronic transport in nanosclae system including quantum Hall effect, followed by a discussion of fundamental electrical and mechanical properties of graphene. Next I will describe how graphene devices are produced: from the famous "mechnical exfoliation" to our innovative "scratching exfoliation" method, together with the traditional lithography fabrication for graphene devices. We also developed a lithography-free technique for making electrical contacts to suspended graphene devices. Most of the suspended devices presented in this thesis are fabricated by this technique. Graphene has remarkable electrical properties thanks to its crystal and band structures. In Chapter 3, I will first focus on proximity-induced superconductivity in graphene Josephson transistors. In this section we investigate electronic transport in single layer graphene coupled to superconducting electrodes. We observe significant suppression in the critical current Ic and large variation in the product <italic>IcRn</italic> in comparison to theoretic prediction; both phenomena can be satisfactorily accounted for by premature switching in underdamped Josephson junctions.  Another focus of our studies is quantum Hall effect and many body physics in graphene in suspended bilayer and trilayer graphene. We demenstrate that symmetry breaking of the first 3 Landau levels and fractional quantum Hall states are observed in both bilayer and trilayer suspended graphene devices. A surprising finding in these systems is the observation of insulating states in both suspended bilayer and trilayer graphene devices, which arises from electronic interactions. In bilayer graphene, we observe a phase transition between the single-particle metallic state and the interaction-induced insulating state in ultra-clean BLG, which can be tuned by temperature, disorder, charge density n and perpendicular electric field <italic>E</italic>. In trilayer graphene we demonstrate dramatically different transport properties arising from the different stacking orders, and an unexpected spontaneous gap opening in charge neutral ABC-stacked trilayer graphene. One of graphene's unique properties is that it is nature's thinnest elastic membrane with exceptional mechanical properties. In chapter 7 I will describe the first direct observation and controlled creation of one- and two-dimensional periodic ripples in suspended graphene sheets, using both spontaneously and thermally generated strains. We are able to control ripple orientation, wavelength and amplitude by controlling boundary conditions and exploiting graphene's negative thermal expansion coefficient, which we measure to be much larger than that of graphite. In addition, we also study the morphological change of suspended graphene sheets by apply gate voltages, which is a simple and direct method to strain and buckle graphene.Our experimental results contribute to the fundamental understanding of electrical and mechanical properties of graphene, and may have important implications for future graphene based applications.
-------------------------------------

10139515_183 - 0.999990533141 - technology_and_computing
[datum, network, large, on-demand, scientific, transfer, overlay]

"On-demand Overlay Networks for Large Scientific Data Transfers"
Abstract. Large scale scientific data transfers are central to scientific processes. Data from large experimental facilities have to be moved to local institutions for analysis or often data needs to be moved between local clusters and large supercomputing
centers. In this paper, we propose and evaluate a network overlay architecture to enable highthroughput, on-demand, coordinated data transfers over wide-area networks. Our work leverages Phoebus and On-demand Secure Circuits and Advance
Reservation System (OSCARS) to provide high performance wide-area network connections. OSCARS enables dynamic provisioning of network paths with
guaranteed bandwidth and Phoebus enables the coordination and effective utilization of the OSCARS network paths. Our evaluation shows that this approach leads to improved end-to-end data transfer throughput with minimal overheads. The achieved
throughput using our overlay was limited only by the ability of the end hosts to sink the data.
-------------------------------------

10138572_183 - 0.76293878474 - technology_and_computing
[callose, plug, tube, pollen, deposition, ecotype, species, pattern, grain, arabidopsis]

Callose plug deposition patterns vary in pollen tubes of Arabidopsis thaliana ecotypes and tomato species
Abstract
				
				
					
						Background
					
					The pollen grain contains the male gametophyte that extends a pollen tube that grows through female tissues in order to deliver sperm to the embryo sac for double fertilization. Growing pollen tubes form periodic callose plugs that are thought to block off the older parts of the tube and maintain the cytoplasm near the growing tip. The morphology of callose plugs and the patterns of their deposition were previously shown to vary among species, but variation within a species had not been examined. We therefore systematically examined callose plug deposition in Arabidopsis thaliana ecotypes, tested for heritability using reciprocal crosses between ecotypes that had differing deposition patterns, and investigated the relationship between callose plugs and pollen tube growth rate. We also surveyed callose plug deposition patterns in different species of tomato.
				
				
					
						Results
					
					We used in vitro grown pollen tubes of 14 different A. thaliana ecotypes and measured the distance from the pollen grain pore to the first callose plug (termed first interval). This distance varied among Arabidopsis ecotypes and in some cases even within an ecotype. Pollen tubes without a callose plug were shorter than those with a callose plug, and tubes with a callose plug near the grain were, on average, longer than those with the first callose plug farther from the grain. Variations in the first callose plug position were also observed between different species of tomato.
				
				
					
						Conclusions
					
					We showed that the position of the first callose plug varied among Arabidopsis ecotypes and in tomato species, and that callose plug deposition patterns were heritable. These findings lay a foundation for mapping genes that regulate callose plug deposition or that determine pollen tube length or growth rate.
-------------------------------------

10134203_183 - 0.999995937063 - technology_and_computing
[amplifier, dynamic, linear, wideband, rf, envelope, linear-in-db, sige, detector, hbt]

SiGe HBT linear-in-dB high dynamic range RF envelope detectors and wideband high linearity amplifiers
This research work aims on exploiting SiGe HBT technologies in high dynamic range wideband RF linear-in- dB envelope detectors and linear amplifiers. First, an improved all-npn broadband highly linear SiGe HBT differential amplifier is presented based on a variation of Caprio's Quad. A broadband linear amplifier with 46dBm OIP₃ at 20MHz, 34dBm OIP₃ at 1GHz, 6dB noise figure and 10.3dBm P₁dB is demonstrated. Second, an improved exact dynamic model of a fast-settling linear-in-dB Automatic Gain Control (AGC) circuit is developed. Based on this, a wideband linear-in-dB RF envelope detector is implemented in TowerJazz Semiconductor 0.18 Mum SBC BiCMOS process with 2.5GHz bandwidth and 50dB dynamic range
-------------------------------------

10134602_183 - 0.854333455098 - technology_and_computing
[model, multinomial, bernoulli, factor, latent, user, item, rating, accuracy]

Simultaneous regression and clustering to predict movie ratings
A recommender system uses information from a user's past behavior to present items of interest to him. A fundamental problem in recommender systems is approximating a full user-item matrix where most of the entries are missing. The rows of the matrix represent the users and the columns represent the items. The entries indicate the plausibility that the user will enjoy the item. In this thesis the items are movies and the entries ratings. In this thesis I compare three statistical models that how a user will rate a movie. The first two are Bernoulli models that predict whether a rating is greater than three out of five. The first Bernoulli model uses logistic regression. The second Bernoulli model is a latent factor model. The third model extends the latent factor model to use a five class multinomial. A five class multinomial is chosen to predict a rating on a scale of one to five. The results show that latent factor model that uses a Bernoulli distribution has a better accuracy than a model trained by logistic regression. The latent factor model is extended to use a multinomial. The accuracy of variants of the multinomial model are evaluated. A technique to initialize the multinomial model is shown to improve the accuracy. However the accuracy is lower than other models used in the Netflix competition. The Bernoulli and multinomial latent factor models are compared against each other. The Bernoulli model is more accurate
-------------------------------------

10138815_183 - 0.838293188714 - technology_and_computing
[mobility]

Toward an integrated mobility agenda
Mobility affects the health and well-being of a growing aging population. Although this is an important area of research, most of the work focuses on one type of mobility at a time, e.g., walking or driving or the use of passenger transport. This presentation outlines an integrated mobility agenda, which is based on the lived experiences of older adults, and examines the health effects of combinations of different modes of mobility as part of everyday life, e.g., walking and driving. The research and practice implications of this integrated approach are addressed.
-------------------------------------

10134030_183 - 0.984060947781 - technology_and_computing
[npc, complex, interphase, assembly, mitosis]

Cell cycle dependent differences in nuclear pore complex assembly
In metazoa, nuclear pore complexes (NPCs) assemble from disassembled precursors into a reforming nuclear envelope (NE) at the end of mitosis, and into growing intact NEs during interphase. Whether there are differences in the mechanism of NPC assembly in these two scenarios is a long standing question in the field that has not been addressed. Experiments described in this dissertation, show that ELYS, a nucleoporin critical for the recruitment of the essential Nup107/160 complex to chromatin, is crucial for NPC assembly at the end of mitosis, but is not required in interphase. Conversely, the transmembrane nucleoporin POM121 is critical for the incorporation of the Nup107/160 complex into new assembly sites specifically during interphase and plays a role in fusing the two leaflets of the NE. We show that in contrast to post-mitosis, where the Nup107/160 complex is targeted to chromatin via ELYS, during interphase this NPC sub-complex assembles at sites of forming pores. These results indicate that, in organisms with open mitosis, NPCs assemble by two distinct mechanisms to accommodate cell cycle-dependent differences in NE topology
-------------------------------------

10138162_183 - 0.999866822189 - technology_and_computing
[interface, silicon, chemical, amorphous]

Impact of solid-phase crystallization of amorphous silicon on the chemical structure of the buried Si/ZnO thin film solar cell interface
The chemical interface structure between phosphorus-doped hydrogenated amorphous silicon and aluminum-doped zinc oxide thin films is investigated with soft x-ray emission spectroscopy (XES) before and after solid-phase crystallization (SPC) at 600C. In addition to the expected SPC-induced phase transition from amorphous to polycrystalline silicon, our XES data indicates a pronounced chemical interaction at the buried Si/ZnO interface. In particular, we find an SPC-enhanced formation of Si-O bonds and the accumulation of Zn in close proximity to the interface. For an assumed closed and homogeneous SiO2 interlayer, an effective thickness of (5+2)nm after SPC could be estimated.
-------------------------------------

10137690_183 - 0.999984001764 - technology_and_computing
[metric, space, manifold, curvature, volume, deformation, map]

Average Distance Functions and Their Applications
This thesis explores families of metric spaces.  It has two parts.  First, the Kuratowski embedding is an isometric embedding of a metric space, M, into the space of bound, Borel functions over M, equipped with the sup norm.  We extend this map to a family of maps by averaging over metric r--balls.  The image of M under this map can be regarded as a deformation.  After restricting our metric spaces to Riemannian manifolds, we explore how curvature affects this deformation.  Furthermore, we give a complete description of the deformation of the n-sphere.  Second, we prove a diffeomorphsim stability theorem.  The smallest r so that a metric r--ball covers a metric space M is called the radius of M. The volume of a metric r-ball in the space form of constant curvature k is an upper bound for the volume of any Riemannian manifold with sectional curvature &#8805; k and radius &#8804; r. We show that when such a manifold has volume almost equal to this upper bound, it is diffeomorphic to a sphere or a real projective space.
-------------------------------------

10129719_178 - 0.996915496803 - technology_and_computing
[location]

Location decision: a study of inter-city location variables and trade-offs for a group of Iranian managers and professionals.
Bibliography: leaves 122-126.
-------------------------------------

10133873_183 - 0.999940663628 - technology_and_computing
[measurement, error, model, endogeneity, estimation, variable, endogenous, nonlinear, chapter]

Endogeneity and measurement error in nonparametric and semiparametric models
It has long been an area of interest to consider a consistent estimation of nonlinear models with measurement error or endogeneity in the explanatory variables. Contrast to linear parametric models, both topics in nonlinear models are difficult to correct for. As a result, many of studies have addressed only one of them in nonlinear models, although controlling for only one mostly fails to identify economically meaningful structural parameters. Thus, this dissertation presents solutions to simultaneously control for both endogeneity and measurement error in general nonlinear regression models. Chapter one of this dissertation studies the identification and estimation of covariate-conditioned average marginal effects of endogenous regressors in nonseparable models when the regressors are mismeasured. Endogeneity is controlled for by making use of covariates as conditioning instruments; this ensures independence between the endogenous causes and other unobservable drivers of the dependent variable. Moreover, distributions of the underlying true causes from their error-laden measurements are recovered. Specifically, it is shown that two error-laden measurements of the unobserved true causes are sufficient to identify objects of interest and to deliver consistent estimators. Chapter two develops semiparametric estimation of models defined by conditional moment restrictions, where the unknown functions depend on endogenous variables which are contaminated by nonclassical measurement errors. A two-stage estimation procedure is proposed to recover the true conditional density of endogenous variables given conditioning variables masked by measurement errors, and to rectify the difficulty associated with endogeneity of the unknown functions. Chapter three investigates empirical importance of endogeneity and measurement error in economic examples. The proposed methods in chapter one and two are applied to topics of interest, the impact of family income on children's achievement and the estimation of Engel curves, respectively. The first application finds that the effects of family income on both math and reading scores from the proposed estimator are positive and that the magnitudes of the income effects are substantially larger than previously recognized. From the second application, findings indicate that correcting for both endogeneity and measurement error obtains significantly different shapes of Engel curves, compared to the method which ignores measurement error on total expenditure
-------------------------------------

10136060_183 - 0.999997124691 - technology_and_computing
[method, system, linear, identification, realization, constraint, response, datum, behavior, experiment]

Realization-based system identification with applications
The identification of dynamic system behavior from experimentally measured or computationally simulated data is fundamental to the fields of control system design, modal analysis, and defect detection. In this dissertation, methods for system identification are developed based on classical linear system realization theory. The common methods of state-space realization from a measured, discrete-time impulse response are generalized to the following additional types of experiments: measured step responses, arbitrary sets of input-output data, and estimated cross-covariance functions of input-output data. The methods are particularly well suited to systems with large input and/or output dimension, for which classical system identification methods based on maximum likelihood estimation may fail due to their reliance on non-convex optimizations. The realization-based methods by themselves require a finite number of linear algebraic operations. Because these methods implicitly optimize cost functions that are linear in state-space parameters, they may be augmented with convex constraints to form convex optimization problems. Several common behavioral constraints are translated into eigenvalue constraints stated as linear matrix inequalities, and the realization- based methods are converted into semidefinite programming problems. Some additional constraints on transient and steady-state behavior are derived and incorporated into a quadratic program, which is solved following the semidefinite program. The newly developed realization- based methods are applied to two experiments: the aeroelastic response of a fighter aircraft and the transient thermal behavior of a light-emitting diode. The algorithms for each experiment are implemented in two freely available software packages
-------------------------------------

10133972_183 - 0.999990133204 - technology_and_computing
[light, optical, system, microfluidic, lens, scatter, chip, collection]

Optical systems for integration with microfluidics
My thesis research has focused on means of integrating optical systems into microfluidic chips, specifically for the creation of lab-on-a-chip flow cytometers. The benefits of microfluidics are perhaps most often applied to biological assays, which frequently employ optical readout of fluorescence or light scatter. By integrating the optical system onto the microfluidic chip, we can facilitate chip interfacing while ensuring optical alignment to a tiny sample. Integrated optical systems also offer the ability to collect light from a localized area, allowing for the collection of true angular light scatter (which carries much information about cells) and can furthermore significantly improve the signal to noise ratio (SNR) relative to simple fiber or waveguide based approaches to integrated light collection. This work explores both the unique challenges and advantages encountered when creating optical systems integrated with mold-replicated microfluidic devices. The first contribution presented is the demonstration of fluid- filled lenses integrated alongside microfluidic channels using a slab waveguiding structure. The use of fluid represents an important tradeoff between lens power and Fresnel reflections. The creation of a slab waveguiding structure is critically important to control light losses when utilizing lens systems for light collection. The second contribution in this work is the demonstration of a microfluidic chip emplying a number of lenses to perform both localized excitation of the samples as well as light collection from localized areas defined by a specific angular range. Sample coefficients of variation (CVs) ranged from 9-16% for a single bead population, far exceeding previously-published CVs of 25-35%. The last contribution is an atypical approach to optical systems based on the unique advantages offered by microfabricated architectures, namely small sizes and close proximities to the sample. Using only custom-shaped total internal reflection (TIR) based components and light blocking elements, we create a device that can achieve forward light scatter CVs of 8-28%. The device is able to clearly distinguish 5 @mu;m, 10 @mu;m, and 15 @mu;m beads based on forward and side scatter. The results from this vastly simplified optical system show great promise towards reaching the performance metrics of the commercial cytometer
-------------------------------------

10134960_183 - 0.806316959079 - technology_and_computing
[transport, density, particle, magnetic, contribution, plasma, change, diii-d, different, pump-out]

Particle transport as a result of resonant magnetic perturbations
This thesis makes contributes to field of plasma physics with a particular focus on particle transport as a result of resonant magnetic perturbations (RPMs) in magnetic confinement devices (Tokamaks). RPMs have proven to be a useful technique to suppress edge localized modes (ELMs) that under certain conditions can damage the confinement device. In order to suppress ELMs, these magnetic perturbations are created to be be resonant at the edge of the plasma (i.e., by selecting an n=3 spectrum and a q₉₅ = 3.6). However, RMPs lead to a changes in the density profile, not only in the pedestal area, but also deeper in the plasma core, limiting plasma performance. As a first contribution in this thesis we carefully investigate density pump-out, and show that it is the result of a change in particle transport (as opposed to a change in neutral fueling). A second contribution of this work is the introduction of a weighted magnetic diffusion coefficient (D/OFL) that allows us to make quantitative comparisons between experimental datasets from different Tokamak devices. By comparing D/OFL for MAST L-modes and DIII-D H-modes, we find that both machines exhibit a very different density pump-out for similar D/OFL values. Since turbulent particle transport is very different for L and H -modes, we investigate, as a third contribution of this work, the influence of RMPs on turbulent particle transport in both MAST and DIII-D. We find that while an increase in turbulent transport on MAST correlates well with density pump-out, no meaningful correlation was found for pedestal density changes in DIII-D. Therefore, as a final contribution in this thesis, we investigate how convective particle transport parallel to the magnetic field alters the density profiles. We compare the increase in convective parallel particle transport and find good agreement with experimental density profiles
-------------------------------------

10134494_183 - 0.998768258301 - technology_and_computing
[analysis, dataflow, program, concurrent, detection, radar, engine, race]

Dataflow analysis for concurrent programs using data-race detection
Dataflow analyses are a critical part of many optimizing compilers as well as bug-finding and program-understanding tools. However, many dataflow analyses are not designed for concurrent programs. Dataflow analyses for concurrent programs differ from their single-threaded counterparts in that they must account for shared memory locations being overwritten by concurrent threads. Existing dataflow analysis techniques for concurrent programs typically fall at either end of a spectrum: at one end, the analysis conservatively kills facts about all data that might possibly be shared by multiple threads; at the other end, a precise thread-interleaving analysis determines which data may be shared, and thus which dataflow facts must be invalidated. The former approach can suffer from imprecision, whereas the latter does not scale. This dissertation presents a framework called RADAR, which automatically converts a dataflow analysis for sequential programs into one that is correct for concurrent programs. With RADAR, the vast body of work in designing dataflow analyses for sequential programs can be reused in the concurrent setting. RADAR uses a race detection engine to kill the dataflow facts -- generated and propagated by the sequential analysis -- that become invalid due to concurrent writes. This approach of factoring all reasoning about concurrency into a race detection engine yields two benefits. First, to obtain analyses for code using new concurrency constructs, there is no need to update each individual analysis; instead, one need only design a suitable race detection engine for the constructs. Second, it gives analysis designers an easy way to tune the scalability and precision of the overall analysis by only modifying the race detection engine or swapping in a different one. This dissertation describes the RADAR framework and its implementation using a race detection engine called RELAY. RELAY is designed to be a push-button solution and is designed to analyze independent portions of programs in parallel. Combined with RELAY, RADAR is capable of analyzing large C programs including the Apache web server and a subset of the Linux kernel in a reasonable time budget. We compare the results of RADAR to reasonable upper and lower bounds, and show that it is effective in generating concurrent versions of a null-pointer dereference analysis as well as several traditional compiler-oriented dataflow analyses
-------------------------------------

10130152_178 - 0.998197873157 - technology_and_computing
[rapid, reference, measurement, image, pair, bibliographical, algorithm, aberration, out-of-focus]

An algorithm for rapid measurement of aberrations in pairs of out-of-focus images
Includes bibliographical references (p. 57).
-------------------------------------

10137992_183 - 0.999996708637 - technology_and_computing
[database, bayesian, network, search, problem, probabilistic, individual, model, approach, graphical]

A Bayesian network approach to the database search problem in criminal proceedings
Abstract
				
				
					
						Background
					The &#8216;database search problem&#8217;, that is, the strengthening of a case - in terms of probative value - against an individual who is found as a result of a database search, has been approached during the last two decades with substantial mathematical analyses, accompanied by lively debate and centrally opposing conclusions. This represents a challenging obstacle in teaching but also hinders a balanced and coherent discussion of the topic within the wider scientific and legal community. This paper revisits and tracks the associated mathematical analyses in terms of Bayesian networks. Their derivation and discussion for capturing probabilistic arguments that explain the database search problem are outlined in detail. The resulting Bayesian networks offer a distinct view on the main debated issues, along with further clarity.
				
				
					
						Methods
					As a general framework for representing and analyzing formal arguments in probabilistic reasoning about uncertain target propositions (that is, whether or not a given individual is the source of a crime stain), this paper relies on graphical probability models, in particular, Bayesian networks. This graphical probability modeling approach is used to capture, within a single model, a series of key variables, such as the number of individuals in a database, the size of the population of potential crime stain sources, and the rarity of the corresponding analytical characteristics in a relevant population.
				
				
					
						Results
					This paper demonstrates the feasibility of deriving Bayesian network structures for analyzing, representing, and tracking the database search problem. The output of the proposed models can be shown to agree with existing but exclusively formulaic approaches.
				
				
					
						Conclusions
					The proposed Bayesian networks allow one to capture and analyze the currently most well-supported but reputedly counter-intuitive and difficult solution to the database search problem in a way that goes beyond the traditional, purely formulaic expressions. The method&#8217;s graphical environment, along with its computational and probabilistic architectures, represents a rich package that offers analysts and discussants with additional modes of interaction, concise representation, and coherent communication.
-------------------------------------

10130125_178 - 0.940506146227 - technology_and_computing
[bibliographical, kepler, reference, binary, catalog, database]

A catalog of new eclipsing binaries in the Kepler database
Includes bibliographical references (p. 75-76).
-------------------------------------

10136197_183 - 0.977310428417 - technology_and_computing
[application, nervous, system, nanotechnology, silica, calcium, development]

Applications of nanotechnology to the central nervous system
Nanotechnology and nanomaterials, in general, have become prominent areas of academic research. The ability to engineer at the nano scale is critical to the advancement of the physical and medical sciences. In the realm of physical sciences, the applications are clear: smaller circuitry, more powerful computers, higher resolution instruments. However, the potential impact in the fields of biology and medicine are perhaps even grander. The implementation of novel nanodevices is of paramount importance to the advancement of drug delivery, molecular detection, and cellular manipulation. The work presented in this thesis focuses on the development of nanotechnology for applications in neuroscience. The nervous system provides unique challenges and opportunities for nanoscale research. This thesis discusses some background in nanotechnological applications to the central nervous system and details : (1) The development of a novel calcium nanosenser for use in neurons and astrocytes. We implemented the calcium responsive component of Dr. Roger Tsien's Cameleon sensor, a calmodulin-M13 fusion, in the first quantum dot-based calcium sensor. (2) The exploration of cell-penetrating peptides as a delivery mechanism for nanoparticles to cells of the nervous system. We investigated the application of polyarginine sequences to rat primary cortical astrocytes in order to assess their efficacy in a terminally differentiated neural cell line. (3) The development of a cheap, biocompatible alternative to quantum dots for nanosensor and imaging applications. We utilized a positively charged co-matrix to promote the encapsulation of free sulforhodamine B in silica nanoparticles, a departure from conventional reactive dye coupling to silica matrices. While other methods have been invoked to trap dye not directly coupled to silica, they rely on positively charged dyes that typically have a low quantum yield and are not extensively tested biologically, or they implement reactive dyes bound to larger encapsulated molecules
-------------------------------------

10133623_183 - 0.999257728921 - technology_and_computing
[performance, power, wire, consumption, on-chip, design, scheme]

Low power high performance interconnect design and optimization
As technology scales, interconnect planning has been widely regarded as one of the most critical factors in determining the system performance and total power consumption. As the result of shrinking dimension, on-chip wires are getting more resistive, and the delay is becoming larger comparing to gate delay. On the other hand, the self capacitance of wires does not scale with feature size, and as wiring density grows, the total coupling capacitance increases, which results in substantial increment of interconnect power consumption. Meanwhile, off-chip interconnect is also becoming a limiting factor for system performance since the growth of chip's I/O bandwidth has been outpaced by the growth of communication. To meet the performance challenge, the per- pin interconnect bandwidth must be further improved with given power budget. For on-chip interconnect, buffer insertion has been adopted to reduce the signal delay. However, the added buffers require extra power consumption and increase routing complexity. In this dissertation, we investigate a set of interconnect performance metrics, optimize the repeated on-chip wires under different design goals and compare the performance metrics of optimum results. The quantitative delay-energy trade-offs for different design goals are demonstrated. Even with repeaters, nominal on-chip global wires still can not keep up with the pace of gate scaling. We propose a high speed signaling scheme using transmission line properties to address the performance issue. The transmission line allows the signal to travel at the speed of light in the medium. The signal toggles as wave instead of enforced electronic charges and thus saves power. However, the inter-symbol interference (ISI) limits the communication bandwidth. We use passive compensation to alleviate the ISI and develop an optimization flow for a given technology and wire dimension. We compare the nominal repeated wires with the transmission lines under different design goals. For off-chip serial links, we propose a set of passive equalization schemes to enhance the performance with low power consumption. We apply the schemes to the CPU-memory links using IBM POWER6 system as a test vehicle. An optimization flow is devised to optimize the parameters of the equalizers. We derive the performance improvement and power consumption of the proposed schemes. We also demonstrate the sensitivities to the variations of RLC parameters and noise
-------------------------------------

10129969_178 - 0.999991167991 - technology_and_computing
[scheduler, nondeterministic, task, framework, probabilistic, local, choice, information, pioa]

Task-Structured Probabilistic I/O Automata
Modeling frameworks such as Probabilistic I/O Automata (PIOA) and Markov Decision Processes permit both probabilistic and nondeterministic choices. In order to use these frameworks to express claims about probabilities of events, one needs mechanisms for resolving nondeterministic choices. For PIOAs, nondeterministic choices have traditionally been resolved by schedulers that have perfect information about the past execution. However, these schedulers are too powerful for certain settings, such as cryptographic protocol analysis, where information must sometimes be hidden. Here, we propose a new, less powerful nondeterminism-resolution mechanism for PIOAs, consisting of tasks and local schedulers. Tasks are equivalence classes of system actions that are scheduled by oblivious, global task sequences. Local schedulers resolve nondeterminism within system components, based on local information only. The resulting task-PIOA framework yields simple notions of external behavior and implementation, and supports simple compositionality results. We also define a new kind of simulation relation, and show it to be sound for proving implementation. We illustrate the potential of the task-PIOAframework by outlining its use in verifying an Oblivious Transfer protocol.
-------------------------------------

10134825_183 - 0.999989952279 - technology_and_computing
[cloud, system, storage, application, file, cost, computing]

Migrating enterprise storage applications to the cloud
Cloud computing has emerged as a model for hosting computing infrastructure and outsourcing management of that infrastructure. It offers the promise of simplified provisioning and management, lower costs, and access to resources that scale up and down with demand. Cloud computing has seen growing use for Web site hosting, large batch processing jobs, and similar tasks. Despite potential advantages, however, cloud computing is not much used for enterprise applications such as backup, shared file systems, and other internal systems. Many challenges need to be overcome to make cloud suitable for these applications, among them cost, performance, security, and interface mismatch. In this dissertation, I investigate how cloud infrastructure can be used for internal services in an organization, with a focus on storage applications. I show how to design systems to address the challenges of using the cloud by building two example systems. The first, Cumulus, implements file system backup to a remote cloud storage provider. With Cumulus I consider the constraints imposed by the interface to cloud storage, and how to work within those constraints to minimize the cost. The second system, BlueSky, is a shared network file server which is backed by cloud storage. BlueSky builds on ideas from Cumulus to reduce system cost. It relies on cloud storage for data durability, but provides good performance by caching data locally. I additionally study how file system maintenance tasks can be offloaded to the cloud while protecting the confidentiality and integrity of file system data. Together, these two systems demonstrate that, despite the challenges, we can bring the benefits of the cloud to enterprise storage applications
-------------------------------------

10131955_183 - 0.758656782673 - technology_and_computing
[vehicle, nev, electric]

Prospects for Neighborhood Electric Vehicles
Neighborhood electric vehicles (NEVs) are a promising strategy for easing the growing tension between demands for greater automotive travel and calls for improved environmental quality. By reducing performance and driving range expectations, NEVs overcome the battery problem of larger electric vehicles while still serving the mobility demands of many travelers. The introduction of NEVs is likely to be slowed by a web of road and vehicle rules designed with the standard vehicle of the past in mind and by uniform vehicle size expectations on the part of consumers, government regulators, and highway suppliers. The energy and environmental benefits are potentially so large, however, and the opportunity to create more human-scale communities so promising that it would be irresponsible not to pursue NEVs in a more deliberate fashion.
-------------------------------------

10139092_183 - 0.996534875935 - technology_and_computing
[distribution, method, bound, algorithm, problem, random, unsatisfiability, exponential, r_a, clause]

Unsatisfiability Bounds for Random Constraint Satisfaction Problems from an Energetic Interpolation Method
The interpolation method, originally developed in statistical physics, transforms distributions of random Constraint Satisfaction Problems (CSPs) to distributions of much simpler problems while bounding the change in a number of associated statistical quantities along the transformation path. By now, it is known that, in principle, the method can yield rigorous unsatisfiability results if one ``plugs in an appropriate functional distribution'' to the derived expressions. A drawback of the method is that identifying the appropriate distribution leads to major analytical challenges as the relevant distributions are, in fact, infinite dimensional objects. We develop a variant of the interpolation method for random CSPs on arbitrary sparse degree distributions which trades accuracy for tractability. In particular, our bounds only require the solution of a 1-dimensional optimization problem (which typically turns out to be very easy) and as such can be used to compute explicit rigorous unsatisfiability bounds. We use this new method to analyze the performance of a number of algorithms on random 3-CNF formulas with n variables and m=rn clauses. A long series of papers analyzing so-called ``myopic'' algorithms has provided a sequence of lower bounds for the satisfiability threshold, which is widely believed to be r~4.26. Indeed, for each myopic algorithm A it is known that there exists an algorithm-specific clause-density, r_A, such that if r<r_A, the algorithm finds a satisfying assignment in linear time. For example, r_A is 8/3=2.66... for orderred-dll and 3.003... for generalized unit clause. We prove that for densities well within the provable satisfiable regime, every backtracking extension of either of these algorithms takes exponential time. Specifically, all extensions of orderred-dll take exponential time for r>2.78 and the same is true for generalized unit clause for all r>3.1. Our results imply exponential lower bounds for many other myopic algorithms for densities similarly close to the corresponding r_A.
-------------------------------------

10136450_183 - 0.988801561546 - technology_and_computing
[datum, bayesian, method, model, analysis, mechanism, real, non-ignorable]

Bayesian and Non-parametric Approaches to Missing Data Analysis
Missing data occur frequently in surveys, clinical trials as well as other real data studies. In the analysis of incomplete data, one needs to correctly identify the missing mechanism and then adopt appropriate statistical procedures. Recently, the analysis of missing data has gained more and more attention. People start to investigate the missing data analysis in several different areas. This dissertation concerns two projects. First, we propose a Bayesian solution to data analysis with non-ignorable missingness. The other one is the non-parametric test of missing mechanism for incomplete multivariate data.First, Bayesian methods are proposed to detect non-ignorable missing and eliminate potential bias in estimators when non-ignorable missing presents. Two hierarchical linear models, pattern mixture model and selection model, are applied to a real data example: the National Assessment of Education Progress (NAEP) education survey data. The results show that the Bayesian methods can correctly recognize the missingness mechanism and provide model-based estimators which can eliminate the possible bias due to non-ignorable missing. We also evaluate the goodness-of-fit of these two proposed models using two methods: the comparison of the real data with the predictive posterior distribution and the residual analysis by cross validation. A simulation study compares the performance of the two proposed Bayesian methods with the traditional design-based methods under different missing mechanisms and show the good properties of the Bayesian methods. Further, we discuss the three commonly used model selection criteria: the Bayes factor, the deviance information criterion and the minimum posterior predictive loss approach. Due to the complicated calculation of the Bayes factor and the uncertainty of the DIC, we conduct the last approach, which fails to correctly detect the real model structure for the hierarchical linear model.Second, as an alternative to the fully specified model-based Bayesian method, a novel non-parametric test is proposed to detect the missing mechanism for multivariate missing data. The proposed test does not need any distributional assumptions and is proven to be consistent. A simulation study demonstrates that it has well controlled type I error and satisfactory power against a variety of alternative hypotheses.
-------------------------------------

10134221_183 - 0.999992998167 - technology_and_computing
[url, malicious, system, site, online, contribution, dissertation, detection, large, learning]

Learning to detect malicious URLs
Malicious Web sites are a cornerstone of Internet criminal activities. They host a variety of unwanted content ranging from spam-advertised products, to phishing sites, to dangerous "drive-by'' exploits that infect a visitor's machine with malware. As a result, there has been broad interest in developing systems to prevent the end user from visiting such sites. The most prominent existing approaches to the malicious URL problem are manually- constructed blacklists, as well as client-side systems that analyze the content or behavior of a Web site as it is visited. The premise of this dissertation is that we should be able to construct a lightweight URL classification system that simultaneously overcomes the challenges that face blacklists (which have manual updates that can quickly become obsolete) and client-side systems (which are difficult to deploy on a large scale because of their high overhead). To this end, our contribution is that we develop a highly effective system for malicious URL detection that (in its final form) leverages large numbers of features and online learning to scalably and adaptively construct an accurate classifier. Because our system exploits large amounts of training data and adapts to day-by-day variations, we are able to classify URLs with up to 99% accuracy. As part of pursuing malicious URL detection, this dissertation addresses issues that arise from the use of online learning for this application. Thus, our further contributions include advances in understanding the role of uncertainty in online learning, as well as the benefits of exploiting feature correlations in high-dimensional applications such as URL classification. Overall, the contributions of this dissertation make significant advances in improving malicious URL detection and understanding the role of online learning in this application
-------------------------------------

10137088_183 - 0.973218692715 - technology_and_computing
[battery, solar, electrode, composite, cell, laminate, silicon, li-ion, silver, pattern]

Structural Integration of Silicon Solar Cells and Lithium-ion Batteries Using Printed Electronics
Inkjet printing of electrode using copper nanoparticle ink is presented. Electrode was printed on a flexible glass epoxy composite substrate using drop on demand piezoelectric dispenser and was sintered at 200°C in N2 gas condition. The printed electrodes were made with various widths and thicknesses. Surface morphology of electrode was analyzed using scanning electron microscope (SEM) and atomic force microscope (AFM). Reliable dimensions for printed electronics were found from this study.	Single-crystalline silicon solar cells were tested under four-point bending to find the feasibility of directly integrating them onto a carbon fiber/epoxy composite laminate. These solar cells were not able to withstand 0.2% strain. On the other hand, thin-film amorphous silicon solar cells were subjected to flexural fatigue loadings. The current density-voltage curves were analyzed at different cycles, and there was no noticeable degradation on its performance up to 100 cycles.A multifunctional composite laminate which can harvest and store solar energy was fabricated using printed electrodes. The integrated printed circuit board (PCB) was co-cured with a carbon/epoxy composite laminate by the vacuum bag molding process in an autoclave; an amorphous silicon solar cell and a thin-film solid state lithium-ion (Li-ion) battery were adhesively joined and electrically connected to a thin flexible PCB; and then the passive components such as resistors and diodes were electrically connected to the printed circuit board by silver pasting. Since a thin-film solid state Li-ion battery was not able to withstand tensile strain above 0.4%, thin Li-ion polymer batteries were tested under various mechanical loadings and environmental conditions to find the feasibility of using the polymer batteries for our multifunctional purpose. It was found that the Li-ion polymer batteries were stable under pressure and tensile loading without any noticeable degradation on its charge and discharge performances. Also, the active materials did not decompose at 80°C, and the battery was performing well under low temperature of -27°C. Lastly, the batteries were embedded inside a carbon fiber/epoxy composite laminate to characterize their performance under fatigue loading.Finally, an intense pulsed light (IPL) was used to sinter printed silver nanoink patterns. X-ray diffraction (XRD) was used to find grain size of printed silver nanoink patterns. From these analyses it was confirmed that IPL is able to adequately sinter silver nanoink patterns for printed electronics without degradation of the substrates in less than 30 ms.
-------------------------------------

10168_7 - 0.999400775289 - technology_and_computing
[information, estimator, geometry]

Measurements of generalisation based on information geometry
Neural networks are statistical models and learning rules are estimators. In this paper a theory for measuring generalisation is developed by combining Bayesian decision theory with information geometry. The performance of an estimator is measured by the information divergence between the true distribution and the estimate, averaged over the Bayesian posterior. This unifies the majority of error measures currently in use. The optimal estimators also reveal some intricate interrelationships among information geometry, Banach spaces and sufficient statistics
-------------------------------------

10136909_183 - 0.871376547374 - technology_and_computing
[silicon, porous, optical, palladium, detection, gas, sensor, fabrication, crystal, composite]

Chemical modification of porous silicon for the detection of explosive gases
A thesis describing the construction and evaluation of optical sensors based on porous silicon rugate photonic crystals for the detection of explosive gases such as C₂ hydrocarbons and hydrogen. Useful properties of porous silicon for gas sensor applications include tunable pore size, large surface area, convenient surface chemistries, optical transduction of the sensing signal, and the potential for miniaturization. Chapter One introduces the fundamentals of porous silicon, including fabrication, sensing mechanisms, and sensor challenges. Additionally, the advantages and limitations of current point sensor technologies are briefly discussed. Chapter Two focuses on the evaluation of a carbon/porous silicon composite film for the optical detection of C₂ hydrocarbons. Porous silicon rugate photonic crystals are used as templates for the fabrication of glassy carbon nanofibers. The resulting carbon/porous silicon composites combine the strong adsorption characteristics of carbon and the optical signal transduction of photonic crystals to increase sensitivity. Chapter Three presents a new method for the fabrication of palladium/porous silicon composite films for the optical detection of hydrogen gas. The two-step synthesis involves seeding of palladium by thermal reduction of palladium ions, followed by growth of palladium nanoparticles in an electroless plating solution. Compared to the previous method of immersion plating, this new method is able to deposit palladium nanoparticles deeper into the porous matrix, thereby increasing the sensitivity
-------------------------------------

10135471_183 - 0.999994678294 - technology_and_computing
[channel, network, greater, analyzer, rf, db, channelizer, bandwidth, frequency, high]

Quasi-optical network analyzers and high-reliability RF MEMS switched capacitors
The thesis first presents a 2-port quasi-optical scalar network analyzer consisting of a transmitter and receiver both built in planar technology. The network analyzer is based on a Schottky-diode mixer integrated inside a planar antenna and fed differentially by a CPW transmission line. The antenna is placed on an extended hemispherical high- resistivity silicon substrate lens. The LO signal is swept from 3-5 GHz and high-order harmonic mixing in both up- and down- conversion mode is used to realize the 15-50 GHz RF bandwidth. The network analyzer resulted in a dynamic range of greater than 40 dB and was successfully used to measure a frequency selective surface with a second-order bandpass response. Furthermore, the system was built with circuits and components for easy scaling to millimeter- wave frequencies which is the primary motivation for this work. The application areas for a millimeter and submillimeter-wave network analyzer include material characterization and art diagnostics. The second project presents several RF MEMS switched capacitors designed for high-reliability operation and suitable for tunable filters and reconfigurable networks. The first switched- capacitor resulted in a digital capacitance ratio of 5 and an analog capacitance ratio of 5-9. The analog tuning of the down-state capacitance is enhanced by a positive vertical stress gradient in the beam, making it ideal for applications that require precision tuning. A thick electroplated beam resulted in Q greater than 100 at C to X-band frequencies, and power handling of 0.6-1.1 W. The design also minimized charging in the dielectric, resulting in excellent reliability performance even under hot-switched and high power (1 W) conditions. The second switched-capacitor was designed without any dielectric to minimize charging. The device was hot-switched at 1 W of RF power for greater than 11 billion cycles with virtually no change in the C-V curve. The final project presents a 7 -channel channelizer based on the mammalian cochlea. The cochlea is an amazing channelizing filter, covering three decades of bandwidth with over 3,000 channels in a very small physical space. Using a simplified mechanical cochlear model and its electrical analogue, a design method is demonstrated for RF and microwave channelizers that retains the desirable features of the cochlea including the ability to cascade a large number of channels (for multiple-octave frequency coverage), and a high-order stop-band rejection. A 6-pole response is synthesized in each channel using the top-C coupled topology. A constant absolute 3 dB bandwidth of around 4.3 MHz and an insertion loss of around 3.9 dB is measured in each channel. A high isolation (greater than 35 dB) is achieved between adjacent channels. A reflection loss of greater than 15 dB is measured at the input port over the entire channelizer bandwidth. Application areas for the demonstrated channelizer include wideband, contiguous- channel receivers for signal intelligence or spectral analysis
-------------------------------------

10132488_183 - 0.984230024418 - technology_and_computing
[rail, transit, performance, system, research, impact, us, important]

The Four Dimensions of Rail Transit Performance: How Administration, Finance, Demographics, and Politics Affect Outcomes
The rebirth of rail transit in the US over the past two decades has resulted in a rail transit’s re-emergence as an integral part of both the physical and economic landscapes of many US cities. Currently fifty-four separate rail transit systems are operated in the US (see Appendix A). This re-emergence of rail transit in cities across the US raises an important question. How does society determine if its investment in rail transit is having an impact? More importantly for the current research: how is the impact of rail transit measured across different geographic regions and system types? Performance standards are one way of determining if public investments are reaching established goals. In this research the impact of variables representing four dimensions of transportation performance: administrative, financial, demographic, and political is assessed. Multiple regression analysis is used to assess the impact of important factors representing each of the four dimensions on the performance of all heavy and light rail transit systems in the US.
      This study addresses three important gaps in existing research. First, this study is strictly concerned with the performance of rail transit systems; an area of research which is unique and due to the death of information in the past, absent from current literature. Second, existing research has not ade3quately addressed the impact of specific sources and types of government subsidies on transit system performance. Sources of subsidies include federal, state, and local funding, while types include dedicated and general revenue funding. Finally, existing research has yet to adequately address the impact of local political relationships on transit system performance.
      Results indicate that a significant difference exists between the operation of heavy and light rail transit systems in the US. The main difference is that administrators of heavy rail systems seem to strive to achieve goals more closely associated with standard performance measures, while administrators of light rail systems may target different goals that are not directly associated with or reflected by existing performance measures. The results of this research are extremely useful, not only in terms of determining the impact of important variables on the performance of rail transit systems, but also in helping to focus and redirect performance research.
-------------------------------------

10130213_178 - 0.995747472511 - technology_and_computing
[asc, radar, datum, environment, surface, slam]

X-band radar based SLAM in Singapore's off-shore environment
This paper presents a simultaneous localisation and mapping (SLAM) algorithm implemented on an autonomous sea kayak with a commercial off-the-shelf X-band marine radar mounted. The Autonomous Surface Craft (ASC) was driven in an off-shore test site in Singapore's southern Selat Puah marine environment. Data from the radar, GPS and an inexpensive single-axis gyro data were logged by an on-board processing unit as the ASC traversed the environment, which comprised geographical and surface vessel landmarks. An automated feature extraction routine is presented, based on a probabilistic landmark detector, followed by a clustering and centroid approximation approach. With restrictive feature modeling, and a lack of vehicle control input information, it is demonstrated that via the novel RB-PHD-SLAM Filter, useful results can be obtained, despite an actively rolling and pitching ASC on the sea surface. In addition, the merits of investigating ASC SLAM are demonstrated, particularly with respect to the map estimation, obstacle avoidance and target tracking problems. Despite the presence of GPS and gyro data, heading information on such small ASC's is greatly compromised which induces large sensing error, further accentuate by the large range of the radar sensor. This work is a step towards realising an ASC capable of performing environmental or security surveillance and reporting a real-time active awareness of the above-water scene.
-------------------------------------

10136981_183 - 0.842470362011 - technology_and_computing
[model, gp, process, efficient, spatial]

Bayesian Nonstationary Gaussian Process Models via Treed Process Convolutions
Spatial modeling with stationary Gaussian processes (GPs) has been widely used, but the assumption that the correlation structure is independent of spatial location is invalid in many applications.  Various nonstationary GP models have been developed to solve this problem, however, many of them become impractical when the sample size is large.  To tackle this problem, a more computationally efficient GP model is developed by convolving a smoothing kernel with a latent process.  Nonstationarity in the GP is obtained by partitioning the spatial domain and allowing a separate latent process and kernel for each partition.  Partitioning is achieved using a binary tree generating process.  A Bayesian approach is used to simultaneously guide partitioning and estimate the parameters of the treed model.  Results based on a large real dataset show that this model is fairly computational efficient and has better prediction performance than other competitive models in the literature.  In addition to the treed model, a sequential design for the standard process convolution GP model is also developed based on a method called Particle Learning, which makes on-line inference more efficient than running a batch inference procedure.
-------------------------------------

10138252_183 - 0.999989961783 - technology_and_computing
[datum, center, equipment, air, corrosion, outside-air, measurement, rate, coupon]

Air Corrosivity in U.S. Outdoor-Air-Cooled Data Centers is Similar to That in Conventional Data Centers
There is a concern that environmental-contamination caused corrosion may negatively affect Information Technology (IT) equipment reliability. Nineteen data centers in the United States and two in India were evaluated using Corrosion Classification Coupons (CCC) to assess environmental air quality as it may relate IT equipment reliability. The data centers were of two basic types: closed and outside-air cooled. A closed data center provides cool air to the IT equipment using air conditioning in which only a small percent age of the recirculation air is make-up air continuously supplied from outside to meet human health requirements. An outside-air cooled data center uses outside air directly as the primary source for IT equipment cooling. Corrosion measuring coupons containing copper and silver metal strips were placed in both closed and outside-air cooled data centers. The coupons were placed at each data center (closed and outside-air cooled types) with the location categorized into three groups: 1) Outside - coupons sheltered, located near or at the supply air inlet, but located before any filtering, 2) Supply - starting just after initial air filtering continuing inside the plenums and ducts feeding the data center rooms, and 3) Inside located inside the data center rooms near the IT equipment. Each coupon was exposed for thirty days and then sent to a laboratory for a corrosion rate measurement analysis.
The goal of this research was to investigate whether gaseous contamination is a concern for U.S. data center operators as it relates to the reliability of IT equipment. More specifically, should there be an increased concern if outside air for IT equipment cooling is used To begin to answer this question limited exploratory measurements of corrosion rates in operating data centers in various locations were undertaken. This study sought to answer the following questions:
(1) What is the precision of the measurements
(2) What are the approximate statistical distributions of copper and silver corrosion rates in the sampled data centers
(3) To what extent are copper and silver corrosion measurements related
(4) What is the relationship of corrosion rate measurements between outside-air cooled data centers compared to closed data centers
(5) How do corrosivity measurements relate to IT equipment failure rates
The data from our limited sample size suggests that most United States data center operators should not be concerned with environmental gaseous contamination causing high IT equipment failure rates even when using outside-air cooling.
The research team recommends additional basic research on how environmental conditions, specifically gaseous contamination, affect electronic equipment reliability.
-------------------------------------

10132707_183 - 0.960455530848 - technology_and_computing
[bus, information, access, benefit, vision, auditory, blind, transit, system, impaired]

Removing functional barriers: Public transit and the blind and vision impaired
We surveyed 55 blind and vision impaired bus riders in Santa Barbara, California to analyze their use of the local bus system and identify their frustrations, concerns and desires when using a mass transit system. The most important finding was that they needed better access to INFORMATION. We asked question on what kind of technology they would like to use to better access this information. The study revealed the benefits to be gained by using auditory signage to provide information to identify busses and trains, locate and safely cross streets and find and use terminal resources. We conclude this report with an examination of the benefits of using auditory signs.
-------------------------------------

10134834_183 - 0.96075458646 - technology_and_computing
[design, model, pv, balanced, property]

Characterization of Special Variance Structures for Designs in Model Identification and Discrimination
Models containing the general mean, main effects, and all possible <italic>k</italic> two-factor interaction effects are considered for factorial experiments with <italic>m</italic> factors observed at two levels each. Specifically, fractional factorial designs consisting of <italic>n</italic> runs which permit the identification and discrimination of the models of interest are evaluated and classified. The classifications are dependent on a new property introduced in this dissertation, denoted <italic>P</italic><g</sub><super>V</super>, <italic>g</italic>&ge;1, which relies on the structure of the variance-covariance matrix for the estimates of the model parameters. Designs with the property, <italic>P</italic><g</sub><super>V</super>, <italic>g</italic>&ge;1, permit to divide the models in the class considered into <italic>g</italic> groups so that all models in a group have equal variances for the least squares estimates of the <italic>k</italic> two-factor interaction effects. Ghosh-Tian optimum designs Ghosh and Tian (2006) for <italic>m</italic>=4, <italic>n</italic>=6,...,11, <italic>k</italic>=1, and <italic>m</italic>=5, <italic>n</italic>=7,...,16, <italic>k</italic>=1 are classified with respect to <italic>g</italic> values, <italic>g</italic>&ge;1, in the property <italic>P</italic><g</sub><super>V</super> and presented through illustrative examples. Several characterizations of designs with <italic>P</italic><g</sub><super>V</super>, <italic>g</italic>&ge;1, are provided for the case when <italic>k</italic>=1. Designs such as balanced designs, isomorphic designs and complementary designs with the property <italic>P</italic><g</sub><super>V</super> are proven to have <italic>P</italic><1</sub><super>V</super> for <italic>k</italic>=1. Tables identifying all such balanced designs are provided. It is noted that although D9.2 and D14 are not balanced, these designs in fact take a special form resulting in <italic>P</italic><1</sub><super>V</super>    for <italic>k</italic>=1. These special forms are investigated in depth. In addition, the construction of all designs giving <italic>P</italic><1</sub><super>V</super> for <italic>m</italic>=3, <italic>n</italic>=5, 6, 7, 8,  <italic>k</italic>=1 and <italic>m</italic>=4, <italic>n</italic>=6,...,16, <italic>k</italic>=1 are described. Further, occurrences of <italic>P</italic><1</sub><super>V</super> are presented for fractional factorial designs when <italic>m</italic>=5, <italic>n</italic>=7, 8, 9, <italic>k</italic>=1.Finally, additional characterizations of <italic>P</italic><g</sub><super>V</super>, <italic>g</italic>&ge;1, when <italic>P</italic>>1 are given and illustrated through various examples. Special designs are presented with the property <italic>P</italic><1</sub><super>V</super> for max<italic>k</italic> which is the largest value of <italic>k</italic> in the models of interest that the design has the ability to identify and discriminate. It is shown that balanced designs will have <italic>P</italic><1</sub><super>V</super> for <italic>k</italic>=<italic>m</italic>C2 . Tables identifying all such balanced designs are provided.
-------------------------------------

10132820_183 - 0.999956146899 - technology_and_computing
[vehicle, intelligent, system, highway, initiative]

Roadwise Signaling in the New Millennium
The initiatives known as the Intelligent Vehicle Highway System (IVHS) of the early 1990's, of the Automated Highway System and the Intelligent Transportation System of the middle 90's, and now USDOT's Intelligent Vehicle Initiative of 1998, have all created a climate for rapid advance in ground vehicle communication and control technology. An industry such as that of aerospace lighting, with its advanced technical prowess and experience regarding innovation, materials, sources, its facility with third party regulations and with standards, would be, in my view, in an ideal position to contribute to what appears to be an accelerating effort.
-------------------------------------

10135455_183 - 0.913787639202 - technology_and_computing
[fe65, lrp-ct, aicd, domain, protein, interaction, pid1, idr, modification]

Post-translational modifications in intrinsically disordered regions and their contribution to protein- protein interactions
Intrinsically disordered regions (IDRs) within proteins have recently been established as having vital functions in both protein recognition and signal transduction. An additional level of complexity is introduced by post- translational modifications, which often times fine-tune the function of IDRs. I have focused on two main systems involving post-translational modifications in IDRs. First, I explored the effect of hydroxylation on the stability and function of IκBα. Contrary to the speculation that the hydroxylation has functional consequences, we did not observe changes in "foldedness" (via H/D exchange mass spectrometry), binding to NFkappaB (via surface plasmon resonance), or stability (via proteosome degradation experiments). I also explored the interactions among the IDRs of LRP-CT, AICD, and Fe65, and how post-translational modifications affect these interactions. The intracellular tail of the low-density receptor-related protein (LRP-CT) has been shown to modulate the processing of APP to Abeta, the main component found in Alzheimer's disease brain plaques. The intracellular domain of APP (AICD) and LRP-CT can be linked via the adapter protein Fe65. The PID1 domain of Fe65 can bind LRP-CT and the PID2 domain of Fe65 can bind AICD. The phosphorylation of AICD did not affect the binding of AICD to Fe65, yet the phosphorylation of LRP-CT was necessary for binding to Fe65 and for the trimeric interaction between LRP-CT, Fe65, and AICD to occur. Interestingly, the interaction between the Fe65 PID1 domain and the NPXY motif of LRP-CT is predicted to be phosphorylation-independent because Fe65 PID1 belongs to the Dab-like family of PID domains, in which binding does not depend on phosphorylation. Fe65 PID1 is thus the first Dab-like PID domain identified to bind an NPXY motif in a tyrosine phosphorylation-dependent manner
-------------------------------------

10129845_178 - 0.986012673048 - technology_and_computing
[beam, field, nonlinear]

Analytic fluid theory of beam spiraling in high-intensity cyclotrons
Using a two-dimensional fluid description, we investigate the nonlinear radial-longitudinal dynamics of intense beams in isochronous cyclotrons in the nonrelativistic limit. With a multiscale analysis separating the time scale associated with the betatron motion and the slower time scale associated with space-charge effects, we show that the longitudinal-radial vortex motion can be understood in the frame moving with the charged beam as the nonlinear advection of the beam by the E×B velocity field, where E is the electric field due to the space charge and B is the external magnetic field. This interpretation provides simple explanations for the stability of round beams and for the development of spiral halos in elongated beams. By numerically solving the nonlinear advection equation for the beam density, we find that it is also in quantitative agreement with results obtained in particle-in-cell simulations.
-------------------------------------

10138340_183 - 0.700211034703 - technology_and_computing
[sequencing, method, error]

Mapping single molecule sequencing reads using basic local alignment with successive refinement (BLASR): application and theory
Abstract
				
				
					
						Background
					
					Recent methods have been developed to perform high-throughput sequencing of DNA by Single Molecule Sequencing (SMS). While Next-Generation sequencing methods may produce reads up to several hundred bases long, SMS sequencing produces reads up to tens of kilobases long. Existing alignment methods are either too inefficient for high-throughput datasets, or not sensitive enough to align SMS reads, which have a higher error rate than Next-Generation sequencing.
				
				
					
						Results
					
					We describe the method BLASR (Basic Local Alignment with Successive Refinement) for mapping Single Molecule Sequencing (SMS) reads that are thousands of bases long, with divergence between the read and genome dominated by insertion and deletion error. The method is benchmarked using both simulated reads and reads from a bacterial sequencing project. We also present a combinatorial model of sequencing error that motivates why our approach is effective.
				
				
					
						Conclusions
					
					The results indicate that it is possible to map SMS reads with high accuracy and speed. Furthermore, the inferences made on the mapability of SMS reads using our combinatorial model of sequencing error are in agreement with the mapping accuracy demonstrated on simulated reads.
-------------------------------------

10139231_183 - 0.999272079876 - technology_and_computing
[relay, user, dof, linear]

Degrees of Freedom of the Two-Way Relay MIMO Interference Channel
We investigate the symmetric degrees of freedom (DoF) of the K-pair (2K users) two-way relay Multiple-Input Multiple-Output (MIMO) Gaussian interference channel for K = 2,3 where each user is equipped with M antennas and the relay node is equipped with N antennas. The two users of each pair communicate with each other via the help of the relay only. Expressing the DoF characterization as a function of the ratio γ = M/N, we find that the DoF value per user is piecewise linear depending on M and N alternately. As we will show in this paper, while the DoF achievability only needs linear beamforming transmission and zero-forcing reception, inter-pair signal subspace alignment is essential at the relay node as well as the users. In addition, the DoF converse is first developed based on the linear dimension counting approach, which can be further translated to the information theoretic statement.
-------------------------------------

10134108_183 - 0.999816890309 - technology_and_computing
[system, ufad, energy, air, demand, cooling, ventilation, building, heating, temperature]

A study of time-dependent responses of a mechanical displacement ventilation (DV) system and an underfloor air distribution (UFAD) system : building energy performance of the UFAD system
As alternative systems for saving cooling energy compared to conventional overhead (OH) air-conditioning systems, mechanical displacement ventilation (DV) systems and underfloor air distribution (UFAD) systems have been widely adopted for commercial buildings. In these alternative systems, supply air is discharged from low momentum diffusers located at lower positions close to the floor in the DV system, and the supply air of the UFAD system is distributed by an underfloor plenum and discharged from floor diffusers. To predict transient vertical temperature responses when the heat source or the ventilation flow rate vary in time, we introduce transient two-layer stratification models of the DV and UFAD systems, non-dimensionalize them by competing the filling box time (Baines & Turner 1969) and the replenishment time in which all the air in the enclosure is replaced by supply air and validate them by laboratory experiments using a salt-water analogy. In various scenarios of the heat source and the ventilation flow rate, the models show a good agreement with the laboratory experiments. Building energy simulation, which predicts cooling and heating demands of a building, has been used for building design, environment, economics, and occupant comfort. EnergyPlus, which is a building energy simulation tools developed by the U.S. Department of Energy, has integrated capability to predict cooling and heating demands as well as the HVAC (heating, ventilation and air conditioning) energy consumption. To evaluate performance of energy savings of UFAD, we developed prototype office buildings adopting OH and UFAD, and the UCSD-UFAD model was used to estimate realistic cooling and heating demands by simulating the stratified temperature profile in a room. In Californian climates, annual electricity consumption of UFAD is always lower than that of OH by up to 20 %, since UFAD has more opportunities to utilize the economizer ̀f̀ree cooling" compared to OH. For electricity demand reduction of various Demand Response (DR) activities, increasing room set point temperature is the most effective DR activity and UFAD has higher peak demand reduction compared to OH by approximately 6-10 % when the room set point temperature is higher than 26 Celsius
-------------------------------------

10131850_183 - 0.731155637939 - technology_and_computing
[vehicle, electric]

Electric Vehicle Manufacturing in Southern California: Current Developments, Future Prospects
The report opens with a brief discussion of the role of regional industrial policy in shaping local developmental trajectories.
      An outline of the main varieties of electric vehicle technology is presented. Special attention is paid to the operational characteristics of electric vehicles, the various kinds of components and sub-assemblies that go into the vehicles, and the urban infrastructure requirements needed to support extensive electric vehicle usage. A short summary of electric vehicle developments around the world is appended, with particular emphasis on US, Japanese, and European efforts.
-------------------------------------

10133943_183 - 0.999972031804 - technology_and_computing
[video, stream, bit-rate, user, allocation, method, multiple, quality]

Bit-rate allocation for multiple video streams : dual-frame video coding and competitive equilibrium methods
With the advancement of digital video technology in recent years, there has been an enormous surge in the amount of video data sent across networks. In many cases, a transmission link is shared by more than one video stream. Applications where multiple compressed video streams are transmitted simultaneously through a shared channel include direct broadcast satellite (DBS), cable TV, video- on-demand services, disaster relief response, and video surveillance. Some commercial applications are YouTube and instant video streaming by content providers, such as Netflix, where multiple streams are transmitted simultaneously, and in many cases, these streams share a common transmission channel. Recently, in cognitive radio technology, the secondary or unlicensed users share a pool of bandwidth that is temporarily going unused by the primary or licensed users. In such cases, it has been shown that joint bit-rate allocation schemes for multiple streams can perform better than an equal bit-rate allocation. In this dissertation, we consider the problem of bit-rate allocation for multiple video users sharing a common transmission channel. We consider two separate objectives for bit-rate allocation among multiple video users: (a) improving the video quality averaged across all the users, and (b) improving the video quality of each individual user, compared to the bit-rate allocation for the users when acting independently. We use dual-frame video with high-quality Long-Term Reference (LTR) frames, and propose multiplexing methods to reduce the sum of Mean Squared Error (MSE) for all the users. We make several improvements to dual-frame video coding by selecting the location and quality of LTR frames. An adaptive buffer- constrained rate-control algorithm is devised to accommodate the extra bits of the high-quality LTR frames. Multiplexing of video streams was studied under the constraint of a video encoder delay buffer. The high- quality LTR frames are offset in time among different video streams. This provides the benefit of dual-frame video coding with high-quality LTR frames while still fitting under the constraint of an output delay buffer. The multiplexing methods show considerable improvement over conventional rate control when the video streams are encoded independently, and over multiplexing methods proposed previously in the literature. While the average video quality is improved for multiple video users, such methods often rely on identifying the relative complexity of the video streams. In such methods, not all the videos benefit from the multiplexing process. Typically, the quality of high motion videos is improved at the expense of a reduction in the quality of low motion videos. We use a competitive equilibrium allocation of bit-rate to improve the quality of each individual video stream by finding trades between videos across time. A central controller collects rate-distortion information from each video user and makes a joint bit-rate allocation decision. The proposed method uses information about not only the differing complexity of the different video streams at a given instant of time, but also the differing complexity of each stream over time. Using the competitive equilibrium bit-rate allocation approach for multiple video streams, we show that all the video streams perform better or at least as well as with individual encoding. The centralized bit-rate allocation methods share the video characteristics and involve high computational complexity. In our pricing-based method, we present an informationally decentralized bit-rate allocation for multiple users where a user only needs to inform his demand to an allocator. Each user separately calculates his bit-rate demand based on his video complexity and bit- rate price, where the bit-rate price is announced by the allocator. The allocator adjusts the bit-rate price based on
-------------------------------------

10134505_183 - 0.960210046373 - technology_and_computing
[kernel, gammak, probability, space, embedding, measure, rkh, property, metric, notion]

Reproducing kernel space embeddings and metrics on probability measures
The notion of Hilbert space embedding of probability measures has recently been used in various statistical applications like dimensionality reduction, homogeneity testing, independence testing, etc. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). A pseudometric on the space of probability measures can be defined as the distance between distribution embeddings : we denote this as gammak, indexed by the positive definite (pd) kernel function k that defines the inner product in the RKHS. In this dissertation, various theoretical properties of gammak and the associated RKHS embedding are presented. First, in order for gammak to be useful in practice, it is essential that it is a metric and not just a pseudometric. Therefore, various easily checkable characterizations have been obtained for k so that gammak is a metric (such k are referred to as characteristic kernels), in contrast to the previously published characterizations which are either difficult to check or may apply only in restricted circumstances (e.g., on compact domains). Second, the relation of characteristic kernels to the richness of RKHS--how well an RKHS approximates some target function space--and other common notions of pd kernels like strictly pd (spd), integrally spd, conditionally spd, etc., is studied. Third, the question of the nature of topology induced by gammak is studied wherein it is shown that gammak associated with integrally spd kernels--a stronger notion than a characteristic kernel--metrize the weak* (weak-star) topology on the space of probability measures. Fourth, gammak is compared to integral probability metrics (IPMs) and phi-divergences, wherein it is shown that the empirical estimator of gammak is simple to compute and exhibits fast rate of convergence compared to those of IPMs and phi-divergences. These properties make gammak to be more applicable in practice than these other families of distances. Finally, a novel notion of embedding probability measures into a reproducing kernel Banach space (RKBS) is proposed and its properties are studied. It is shown that the proposed embedding and its properties generalize their RKHS counterparts, thereby resulting in richer distance measures on the space of probabilities
-------------------------------------

10136610_183 - 0.998317434046 - technology_and_computing
[speed, rover, terrain, planetary]

Speed Map for Autonomous Rovers over Rough Terrain
All past NASA planetary rovers have only been able to traverse celestial surfaces at a maximum speed of 0.05-0.09 m/s (0.11-0.20 mph).  There is motivation to operate rovers more autonomously and to increase their speeds upwards to 3m/s on flat hard ground, which is considered fast for planetary rovers.  In this thesis, a novel roughness metric is used to create a speed map and provide planetary rovers with information about the terrain.  The provided information is intrinsic to the terrain regarding its roughness and speed allowing the rover to safely travel over smooth and rough terrain.  The results of this method will benefit path planning algorithms and control operators in improving mission efficiency.
-------------------------------------

10131996_183 - 0.94672346568 - technology_and_computing
[car, parking, space]

Cashing in on Curb Parking
Whether you're driving to work, to a doctor's appointment, or to dinner with a friend, you don't want to reach your destination and then circle the neighborhood for 40 minutes looking for a parking space. You want even less to compete with dozens of other cars looking for that same vacant space, while dodging double-parked cars and listening to honking and cursing.
-------------------------------------

10132749_183 - 0.886740776446 - technology_and_computing
[source, information, operation, trucking, company, traffic]

The Perceived Usefulness of Different Sources of Traffic Information to Trucking Operations
Managers in charge of the California operations of nearly 1,200 trucking companies were asked their opinions regarding how useful various sources of traffic information are to their dispatchers and to their drivers. They were also asked to evaluate the usefulness of improved traveler information systems. Nonlinear canonical correlation analysis was used to link company characteristics and perceptions of the value of the sources. Results showed that evaluations of sources such as Internet traffic information, in-vehicle navigation systems, and area-wide dedicated highway advisory radio are primarily related to the location of operations, whether a trucking operation is private or for-hire, the average length of the company's load moves, and the provision of intermodal services.
-------------------------------------

10133039_183 - 0.800145719394 - technology_and_computing
[activity, datum, collection, execution]

Real-time Tracking of Activity Scheduling/Schedule Execution Within A Unified Data Collection Framework
One of the major foci in transport research is to identify the temporal-spatial decision making structure embedded in activity scheduling and its linkage to actual activity execution. The latter part of the question hasn’t been able to be explored explicitly in real life situation due to the lack of effective data collection means. The paper presents a real-time travel/activity survey system that incorporates the extraction of activity scheduling and execution information within one unified data collection framework. These “revealed” data could be used for explicitly defining the mechanism of how people’s activity schedules dynamically adapt to social-demographic and temporal-spatial constraints and finally leads to the observed activity-travel patterns.
-------------------------------------

10135585_183 - 0.989378640934 - technology_and_computing
[system, student, interface, statics]

A Pen Based Statics Tutoring System
We present an intelligent pen-based tutoring system for Statics - the sub-discipline of engineering mechanicsconcerned with the analysis of mechanical systems in equilibrium under the action of forces. The systemscaffolds students in the construction of free-body diagrams and equilibrium equations for planar devicescomprised of one or more rigid bodies.While there has been extensive research in intelligent tutoring systems, most existing systems relyon traditional WIMP (Windows, Icons, Menus, Pointer) interfaces. With these systems, students typicallyselect the correct problem solution from among a set of predefined choices. With our pen-based interface, bycontrast, students are guided in constructing solutions from scratch, mirroring the way they solve problemsin ordinary practice, which recent research suggests is particularly important for effective instruction.Our system embodies several innovations including a novel instructional technique that focusesstudents' attention on a system boundary as a tool for constructing free body diagrams, a tutorial feedbacksystem based on "buggy rules" that attempt to diagnose and correct problem-solving errors typical of novicestudents, and a hierarchical feedback system which promotes independent problem-solving skills. In winter2010, the tutoring system was used by 100 students in an undergraduate Statics course at our university.Results from pre- and posttests reveal measurable learning gains even after only a short exposure to thesystem. In an attitudinal survey, students reported that, while there is room for improvement, the interface was preferable to a WIMP interface and the methodology implemented in the system was valuable forlearning Statics.
-------------------------------------

10132819_183 - 0.99981581403 - technology_and_computing
[datum, traffic, freeway, information, vehicle]

Fingerprinting Traffic From Static Freeway Sensors
Ask most commuters and they will agree that congestion has reached an intolerable level. To reduce this congestion, engineers need detailed traffic information. Highly detailed information is also prized by traffic scientists, as a prerequisite to improve current traffic theories. Ideally, engineers and scientists would like to obtain from field data the position of each vehicle on a particular facility at every moment in time. The technology to record space-time vehicle trajectories on a massive scale is in its infancy; therefore, analysts must work with much less data. Many freeways are equipped with primitive sensors that can record only anonymous vehicle passages at specific locations with a time series of 0's and 1's. Typically, these detectors are installed on all lanes at sites, called stations, which are spaced about 1 km apart. This article will show that, despite the anonymity and the spatial discreteness of the measurements, a treasure trove of detailed information can be recovered from 0-1 detector data, if one analyzes the data with the right tools. Field data from a 5-lane freeway in Oakland, California (see Figure 1) is used to demonstrate the ideas.
-------------------------------------

10135083_183 - 0.744463609175 - technology_and_computing
[computation, tarragon, performance, communication, latency, model, programming, task, scientific, application]

Tarragon : a programming model for latency-hiding scientific computations
In supercomputing systems, architectural changes that increase computational power are often reflected in the programming model. As a result, in order to realize and sustain the potential performance of such systems, it is necessary in practice to deal with architectural details and explicitly manage the resources to an increasing extent. In particular, programmers are required to develop code that exposes a high degree of parallelism, exhibits high locality, dynamically adapts to the available resources, and hides communication latency. Hiding communication latency is crucial to realize the potential of today's distributed memory machines with highly parallel processing modules, and technological trends indicate that communication latencies will continue to be an issue as the performance gap between computation and communication widens. However, under Bulk Synchronous Parallel models, the predominant paradigm in scientific computing, scheduling is embedded into the application code. All the phases of a computation are defined and laid out as a linear sequence of operations limiting overlap and the program's ability to adapt to communication delays. This thesis proposes an alternative model, called Tarragon, to overcome the limitations of Bulk Synchronous Parallelism. Tarragon, which is based on dataflow, targets latency tolerant scientific computations. Tarragon supports a task-dependency graph abstraction in which tasks, the basic unit of computation, are organized as a graph according to their data dependencies, i.e. task precedence. In addition to the task graph, Tarragon supports metadata abstractions, annotations to the task graph, to express locality information and scheduling policies to improve performance. Tarragon's functionality and underlying programming methodology are demonstrated on three classes of computations used in scientific domains: structured grids, sparse linear algebra, and dynamic programming. In the application studies, Tarragon implementations achieve high performance, in many cases exceeding the performance of equivalent latency-tolerant, hard coded MPI implementations. The results presented in this dissertation demonstrate that data-driven execution, coupled with metadata abstractions, effectively support latency tolerance. In addition, performance metadata enable performance optimization techniques that are decoupled from the algorithmic formulation and the control flow of the application code. By expressing the structure of the computation and its characteristics with metadata, the programmer can focus on the application and rely on Tarragon and its run-time system to automatically overlap communication with computation and optimize the performance
-------------------------------------

10134235_183 - 0.996495386045 - technology_and_computing
[helios, datacenter, switch, bandwidth, scalable, modular]

Simulation to scale of the HELIOS system
Within the space of a few years, IT companies with need for large-scale datacenters are considering alternatives to building traditional central datacenter facility. They have embraced a modular datacenter (also called pod, a smaller mobile datacenter) as the building block for reasons such as efficient cooling, power savings, mobility, easier deployment and administration. However, delivering scalable inter-pod bisection bandwidth remains a challenge. Current solutions that promise full bisection bandwidth between arbitrary pair of hosts can be expensive and power-intensive for interconnecting modular datacenters. Our recent work FPR+10 proposed Helios, a scalable switch architecture for interconnecting modular datacenters using a hybrid core switch array consisting of electrical and optical switches. We show that Helios delivers a significant reduction in cost, cabling complexity, and power while providing scalable on-demand bandwidth to the communicating pair of hosts. In this thesis, we propose a TCP flow-based simulator for the Helios architecture. We describe the design/implementation, and validate the correct behavior of the simulator with the testbed execution of Helios prototype. We then evaluate the performance of Helios system against a variety of parameters - like scale (number of pods), link aggregation, and the composition of the core switch array - for differing communication patterns. The primary goal of this tool is to provide insights into how Helios might scale. Possible applications of this tool include network planning, searching newer optimizations, identifying bad application performance, planning VM migration such that the VMs better utilize Helios infrastructure
-------------------------------------

10134320_183 - 0.99999752957 - technology_and_computing
[channel, feedback, communication, gaussian, information, linear, sum, receiver, upper, code]

Feedback communication systems : fundamental limits and control-theoretic approach
Feedback links from the receivers to the transmitters are natural resources in many real-world communication networks which are utilized to send back information about the decoding process as well as the channel dynamics. However, the theoretical understanding of the role of feedback in communication systems is yet far from being complete. In this thesis, we apply techniques from information theory, estimation and control, and optimization theory to investigate the benefits of feedback in improving fundamental limits on information flow in communication networks. We focus on three network models: Gaussian multiple access channels, Gaussian broadcast channels, and wiretap channels. First, combining the Lagrange duality technique and tools from information theory we derive an upper bound on the sum rate achievable by linear codes for the Gaussian multiple access channel with feedback. This upper bound is further shown to coincide with the known lower bound, hence establishing the linear sum capacity. Next, we study the application of tools from the theory of linear quadratic Gaussian (LQG) control in designing codes for feedback communications. For the Gaussian broadcast channel with feedback, we construct a linear code based on the LQG optimal control and establish the best known lower bound on the sum rate. In addition, depending on the spatial correlation of the noise across different receivers, it is shown that in the high signal-to-noise ratio regime, the sum rate achieved by this code can increase linearly with the number of receivers. Third, we consider the wiretap channel with an eavesdropper and study the benefits of a rate-limited feedback link. We propose a new technique based on which we derive an upper bound on the maximum rate of reliable and secure communication. For the special case in which the eavesdropper's signal is a degraded version of the legitimate receiver's signal, this upper bound matches the known lower bound establishing the secrecy capacity. Finally, we present results for the binary multiplying channel, one of the simplest two-way channels for which the capacity region is not known. We apply tools from stochastic control to establish sufficient conditions for optimality, and use the concept of directed information to analyze the performance of coding schemes
-------------------------------------

10136008_183 - 0.999998176196 - technology_and_computing
[datum, system, methodology, information, high, mining, technique, human, application, frequent]

Behavior-Aware Design, Optimization and Information Mining in Wearable Sensing Systems
Wearable sensing systems are becoming widely used for a variety of applications including sports, entertainment, and the military. These systems have recently enabled a variety of medical monitoring and diagnostic applications. Such sensing systems have high potential to significantly improve the quality of life for large segments of the population and enable conceptually new types of applications. However, various research challenges including energy sensitivity and semantic complexity must be addressed before these devices can reach their full potential in improving our lives. The need for multiple sensors,  high frequency sampling and constant monitoring leads these systems to be power-hungry and expensive with a short operation lifetime. In addition these systems generate a massive amount of data, where designing efficient and effective data  mining algorithms to interpret and analyze the data is very costly due to field experts' involvement. This dissertation presents a methodology that takes advantage of contextual and semantic properties in human physiological behavior to enable efficient design and optimization of such systems from the data and information point of view. The methodology uses combinatorial modeling and simultaneous minimization to reduce the wireless communication and local processing power consumption. The effectiveness of this technique is shown on an insole instrumented with 99 pressure sensors placed in each shoe, which is used for human gait analysis. Further, the proposed methodology is used to design unsupervised data mining techniques to extract information such as frequent or rare events and patterns from collected multi-dimensional time series data of wearable sensing devices, where high level of noise and uncertainty is inherent. This approach transforms multi-dimensional time series data to combinatorial space and constructs behavior models, which is used for frequent and rare event detection and pattern classification. The effectiveness of this method is demonstrated by applying this technique to discovery and classification of  frequent human activities and abnormalities.
-------------------------------------

10135601_183 - 0.999954571694 - technology_and_computing
[system, application]

Scheduling in Multiprocess Systems
Applications and the systems that support them are becoming increasingly morecomplex, more powerful and more ubiquitous. Applications ranging from traffic sensingto online retailers to social networks are part of our every day lives. People have begunto rely on these systems and expect them to perform their operations in a timely fashion.Providing a quick response is critical in some applications and has a significant effect inothers. The complexity of modern of applications is not the only challenge of meeting userexpectations, providing a personalized experience requires the processing of huge amountsof raw data. Although generalized schedulers have shown good results for speeding upmany systems, they are not well suited and cannot provide the most efficient solution inmany situations.In this dissertation, I design, build and evaluate several different specializedscheduling system for speeding up and providing quality of service across a range of differentsystems. I present RG-EDF, Misco and MiscoRT and Cacheflow. Starting with lowlevel system, RG-EDF is an efficient storage scheduler for sensor devices equipped withflash memories. Misco is a distributed data processing system and application frameworkfor mobile smart-phones, MiscoRT is its failure aware, real-time, application scheduler.Finally,Cacheflow is a system for improving client response times in internet scale contentdelivery networks aimed at social networking sites. Through experimental evaluations,I show that my systems provide a significant performance improvement over generalizedscheduling mechanisms and existing approaches.
-------------------------------------

10137573_183 - 0.99997617516 - technology_and_computing
[channel, code, source, block, system, progressive, multimedia, unequal, protection, communication]

Bandwidth and rate allocation tradeoffs of source : channel coding, packetization and modulation in unequally protected multimedia communication systems
A conventional approach to the design of wireless multimedia communications is the layered approach, in which the network layers function independent of each other. This kind of layered approach is inspired partly by Shannon's separation theorem in which the optimization of each block is equivalent to optimization of the overall source-channel coding operation. However, the separation theorem is valid only in a point-to-point communication scenario in the case of asymptotically long block lengths of data and assumes huge amounts of processing power and delay. Therefore, current practical communication systems strive to jointly design building blocks of a multimedia system for better performance. The focus of this dissertation is therefore to present various joint designs for different channel models and systems, although limited by physical constraints such as bandwidth, power and complexity. First, a robust coded scheme for progressive multimedia transmission is considered for an additive white Gaussian noise channel, a Rayleigh fading channel, and a frequency selective channel using in combination different unequal protection methods. We investigate the judicious use of the limited bandwidth through the combination and optimization of a progressive source coder, a rate compatible punctured convolutional code and a hierarchical modulation. Next, we investigate a novel packet formatting scheme for progressive sources using interleavers and various channel codes. The source coder is combined with a concatenated block coding mechanism to produce a robust transmission system for embedded bit streams. The objective is to create embedded codewords such that, for a particular information block, the necessary protection is obtained via multiple channel codings, contrary to the conventional methods which use a single code rate per information block. We show that near capacity performance can be achieved using the proposed scheme in conjunction with low density parity check codes in a binary symmetric channel scenario. We initially focus on coding strategies for multimedia where the channel state information is missing. A generalized Digital Fountain (DF) code is proposed to provide efficient universal forward error correction solution for lossy packet networks with increased unequal error protection and unequal recovery time properties. We propose a progressive source transmission system using this generalized code design. We apply the generalized DF code to a progressive source and show that it has better unequal protection and recovery time properties than other published results
-------------------------------------

10175393_189 - 0.922369259964 - technology_and_computing
[context, usage, mobile, dependency]

Context in Mobile System Design:  Characterization, Theory, and Implications
Context information brings new opportunities for efficient and effective applications and services on mobile devices. Many existing work exploit the context dependency of mobile usage for specific applications, and show significant, quantified, performance gains by utilizing context. In order to be practical, such works often pay careful attention to the energy and processing costs of context awareness while attempting to maintain reasonable accuracy. These works also have to deal with the challenges of multiple sources of context, which can lead to a sparse training data set.
Even with the abundance of such work, quantifying context-dependency and the relationship between context-dependency and performance achievements remains an open problem, and solutions to manage the and challenges of context awareness remain ad-hoc. To this end, this dissertation methodologically quantifies and measures the context dependency of three principal types of mobile usage in a methodological, application agnostic yet practical manner. The three usages are the websites the user visits, the phone numbers they call, and the apps they use, either built-in or obtained by the user from the App Store . While this dissertation measures the context dependency of these three principal types of mobile usage, its methodology can be readily extended to other context-dependent mobile usage and system resources. This dissertation further presents SmartContext, a framework to systematically optimize the energy cost of context awareness by selecting among different context sources, while satisfying the system designer’s cost-accuracy tradeoffs. Finally, this thesis investigates the collective effect of social context on mobile usage, by separating and comparing LiveLab users based on their socioeconomic groups.
The analysis and findings are based on usage and context traces collected in real-life settings from 24 iPhone users over a period of one year. This dissertation presents findings regarding the context dependency of three principal types of mobile usage; visited websites, phone calls, and app usage. The methodology and lessons presented here can be readily extended to other forms of context and context-dependent usage and resources. They guide the development of context aware systems, and highlight the challenges and expectations regarding the context dependency of mobile usage.
-------------------------------------

10129915_178 - 0.961134091502 - technology_and_computing
[center, procedure, design, innovative, technology, catheter-based, integration, percutaneous, chordal, medicine]

Design of a Catheter-Based Device for Performing Percutaneous Chordal- Procedures
Center for Integration of Medicine and Innovative Technology
-------------------------------------

10135887_183 - 0.922165012605 - technology_and_computing
[visibility]

Image-based meteorologic visibility estimation
Awareness and understanding of atmospheric visibility has strong implications for our
daily lives. In addition to being critical for navigation, it acts as an indicator for air quality
and pollution. The devices traditionally used to measure visibility, transmissometers
and nephelometers, are expensive and often require field maintenance and calibration.
Visibility camera systems are increasingly being deployed to measure atmospheric visibility;
however, their use has so far been limited to qualitative analysis. The primary
focus of this study is to develop image analysis techniques to derive quantitative measurements
of visibility from such camera systems. We take advantage of the Beer-Lambert
law, which defines the exponential relation by which light is attenuated when traveling
through a medium. This is used to define a standard visibility model, which then allows
us to frame the problem as a simple log-linear relation. We investigate several numerical
models to estimate visibility, including single and multivariate linear least square
regression, Laplacian-regularized linear least squares regression, and approximation with
the M5' linear regression tree algorithm. We demonstrate the effectiveness of these algorithms
on images and ground truth visibility measurements from the PhoenixVis.net
visibility camera system. The features chosen by the multivariate feature selection process
provide insight into the benefit of edge contrast and color saturation as indicators
for poor visibility. In addition, we investigate Lambertian lighting and a dark channel
prior as cues for salient regions for visibility estimation.
-------------------------------------

10134048_183 - 0.999999035711 - technology_and_computing
[fpga, application]

Emulation of SystemC Applications for Portable FPGA Binaries
As FPGAs become more common in mainstream general-purpose computing platforms, capturing and distributing high-performance implementations of applications on FPGAs will become increasingly important. Even in the presence of C-based synthesis tools for FPGAs, designers continue to implement applications as circuits, due in large part to allow for capture of clever spatial, circuit-level implementation features leading to superior performance and efficiency. We demonstrate the feasibility of a spatial form of FPGA application capture that offers portability advantages for FPGA applications unseen with current FPGA binary formats. We demonstrate the portability of such a distribution by developing a fast on-chip emulation framework that performs transparent optimizations, allowing spatially-captured FPGA applications to immediately run on FPGA platforms without costly and hard-to-use synthesis/mapping tool flows, and sometimes faster than PC-based execution. We develop several dynamic and transparent optimization techniques, including just-in-time compilation, bytecode acceleration, and just-in-time synthesis that take advantage of a platform's available resources, resulting in orders of magnitude performance improvement over normal emulation techniques and PC-based execution.
-------------------------------------

10134377_183 - 0.999998987945 - technology_and_computing
[information, network, wireless, system, fundamental, throughput, channel]

Large wireless networks : fundamental limits and design issues
As information networks grow in magnitude and complexity, new models and frameworks are necessary to understand the nature of information transmission. In this thesis we demonstrate how fundamental questions arising in the design of large wireless networks can be addressed by applying methods from information theory, physics, networking and control. We focus on three examples of emerging systems architecture. First, we investigate the maximum achievable throughput in a wireless ad-hoc network. By combining Maxwell's physics of wave propagation and Shannon's theory of information, and departing from idealistic stochastic channel models for signal propagation, we derive an upper bound to the law that determines the scaling of throughput with the population size of the network, and conclude that the scaling achieved by multi-hop communication is optimal in any constant density wireless network. Second, we study how to aggregate information from uncoordinated nodes by considering a random-access system with multiple nodes transmitting information to a common receiver. We characterize the maximum achievable throughput of channels of practical interest and demonstrate how the performance of current systems can be improved by allowing encoding rate adaptation at the transmitters and joint decoding at the receiver. Finally, we explore the fundamental limits of control over wireless channels and demonstrate the relationship between the degree of instability of a system and the time varying rate of communication in the feedback link
-------------------------------------

10131289_183 - 0.998946694789 - technology_and_computing
[pavement, semi-active, truck, simulation]

Active and Semi-Active Heavy Truck Suspensions to Reduce Pavement Damage
Active and semi-active suspensions have been evaluated for application on tractor/semi-trailer trucks using a pitch plane simulation model called "VESYM". VESYM is a fully non-linear time domain simulation model. This paper reviews the effect of alternative heavy truck suspensions on pavement damage by using the flexible pavement simulation program "VESYS". It is shown that by estimating axle tire force and using this signal to modulate a semi-active shock absorber that significant reduction in pavement degradation can be achieved.
-------------------------------------

10131831_183 - 0.999263705967 - technology_and_computing
[work, payoff, modem, desk, writer, switching, problem, hundred, convenience, science]

Telecommuting: What's the Payoff?
Science fiction writers and high-tech enthusiasts may envision a world without commuting. Already, modern telecommunications technology allows people separated by hundreds of miles to work together as if they had adjacent desks. By simply lifting a phone, or switching on a computer modem, we can do our office work from anywhere - even from home. But the convenience telecommuting offers is not problem free.
-------------------------------------

10136930_183 - 0.996571748714 - technology_and_computing
[heuristic, graph, set, algorithm, selection]

Statistical Heuristic Selection for Graph Coloring
Although a heuristic algorithm's usefulness is grounded in its empirical performance on a set of problem instances, much of recent graph coloring heuristic development neglects statistical methodology in several important ways.  First, heuristic parameters are often set in an ad-hoc, irreproducible fashion.  Second, heuristic parameters are often tuned and evaluated on the same set of instances, causing over-tuning.  Last, the common winner-take-all approach limits a heuristic's application to a very specific set of graph instances.To address the above issues, we employ machine learning techniques to perform instance-based algorithm selection from a set of diverse heuristic algorithms, including multiple parameterizations of the same algorithm.  The implemented strategy improves on a winner-take-all strategy by over a color on average on a set of IID random graphs when allowed a maximum runtime of approximately 15 minutes, and in general achieves near-optimal heuristic selection on unseen graphs.
-------------------------------------

10132195_183 - 0.930078116579 - technology_and_computing
[pnn, incident, detection, framework, universality, algorithm, freeway, requirement, performance, database]

A Neuro-Genetic-Based Universally Transferable Freeway Incident Detection Framework
A universal freeway incident detection framework is a task that remains unfulfilled despite the promising approaches that have been recently explored. The need for an operationally successful incident detection and management system as a vital component of any advanced traffic management system, is well established and recognized. Only recently however, researchers and practitioners have begun to increasingly realize that for an incident detection framework to be universally operational and successful, it needs to fulfill all components of a set of recognized needs. It is the objective of this research to define those universality requirements and produce an incident detection framework that possesses the potential to fulfill them.
      A new potentially universal freeway incident detection framework has been proposed, developed and evaluated. The research effort was started by defining a comprehensive set of requirements that any universal incident detection algorithm or framework should fulfill. Among these requirements, an incident detection algorithm needs to be operationally accurate, automatically transferable, and capable of automatically adapting to changes in the freeway environment. This set of universality requirements was used as a template against which all algorithms within the scope of this study have been evaluated. Three major incident and loop detector databases were heavily utilized, two of which are unprecedented real databases collected from two major freeway sites in California and Minnesota, namely the Alameda County's I-880 freeway database and the Minneapolis' I-35W database. The universality of the most well known existing incident detection algorithms was tested using the above databases. Serious lack of the universality, particularly transferability, was detected in all existing algorithms. Prior to the development of the new universal framework, limits on acceptable performance were elicited from TMC surveys conducted as part of this effort. Preliminary investigation of two promising advanced neural networks, namely the LOGICON and the PNN, was conducted. The PNN was more appealing due to its universality potential. The PNN was modified using a principal components transformation layer that resulted in performance enhancements. This together with its potential universality, led to the choice of the modified PNN for in-depth development. The in-depth development stage was divided into three phases. The first was the extraction of a new and improved input feature set that produced more distinct classes in the input feature space. The new features enhanced the transferability of the PNN and made the framework more compliant with the universality requirements. The second phase was the on-site real time retraining of the PNN after transferability, a phase that produced near optimal classification results and detection performance. The third phase was the development of a post processor output interpreter that linked the isolated 30 second outputs of the PNN and produced a sequentially updated probabilistic measure of existence of an incident in the field. The overall PNN-based framework was found to be fully compliant with the entire set of universality requirements. Finally, a new approach for training a multi-smoothing-parameter version of the PNN was investigated. The approach utilized genetic algorithms for optimizing the selection of the smoothing parameters. Obtained results indicated an improvement in performance over the single smoothing parameter PNN but at the expense of longer training time.
      The superiority and universality of a particular advanced neural network model, namely the PNN, was concluded in this research, as compared to the Logicon and the MLF neural networks, as well as existing conventional freeway incident detection algorithms. Adding the principal components transformation layer to the PNN was found to enhance its performance. Although the genetically optimized version of the PNN showed better transferability, both versions showed equally good performance after retraining. The PNN was concluded to be more practical for TMC implementation due to its instantaneous training capabilities.
-------------------------------------

10136147_183 - 0.999894356677 - technology_and_computing
[smartphone, user, application, powerleash, battery, background, usage, resource]

Automating Personalized Battery Management on Smartphones
The widespread use of smartphones and proliferation of mobile applications are reshaping many other areas ranging from social networking to health care Today's smartphones are much more capable than before, but mobile application are still restricted by limited resources on smartphones. The key hypothesis of this dissertation is that resource management on smartphones can be improved by adapting to usage patterns of users. We extensively studied users in the wild to characterize smartphone usage. We discovered significant diversity in smartphone usage. Along all aspects that we studied, users differ by one or more orders of magnitude. This finding suggests that resource management policies and algorithms on smartphones can become more effective if they learn and adapt to user behavior.We developed the prototype of a system that adaptively manages battery, one of the most strained resources on smarthpones, and evaluated its performance. PowerLeash is a system that gives users control over their smartphones' battery lifetime when running background applications. With PowerLeash a user who is running power consuming background applications on her smartphone can decide how long her battery should last. PowerLeash continuously monitors the phone's battery level, the user's interactions with the phone, and progress of background applications. It builds a personalized model to estimate battery consumption based on usage and background applications progress. Using the on-line model and other information, PowerLeash dynamically adjusts the power consumption of background applications to meet the user's desired battery lifetime. We have designed PowerLeash to be easy to deploy, easy to use, and easy to incorporate in background applications. PowerLeash can run on any Android smartphone as a user level application, and relies only on information that is available to user-level processes. We present the design of PowerLeash and a detailed performance evaluation based on user studies. We use the lessons from deploying PowerLeash on volunteers smartphones to inform future iterations.
-------------------------------------

10137029_183 - 0.999117716212 - technology_and_computing
[beam, gaussian, phase, ray]

On the Passage of Gaussian Beams through Cusps in Ray Paths
Gaussian beams are asymptotic solutions to hyperbolic partial differentiable equations which propagate along null bicharacteristic curves in phase space.  To build gaussian beams, one constructs a phase and an amplitude by using data along a specific null bicharacteristic.  The current construction assumes that the ray path a beam follows in position space is smooth.  In this work, we extend the construction to the case in which the ray path has cusps and deduce the phase shift that occurs when a beam passes through these cusps.  We also present a new formula for the phase of a gaussian beam.
-------------------------------------

10138950_183 - 0.999355907048 - technology_and_computing
[structure, robot]

Kinematics Algorithms for Tensegrity Structures
Tensegrity-based structures hold promise for the field of lightweight, compliant robots. However, most prior efforts to model and plan the shape of these structures have focused on special cases or on static structures. This work attempts to generalize a dynamic-relaxation form-finding method into a form that provides solutions to problems analogous to Forward and Inverse Kinematics problems in serial-chain robots. Details of the implementation of these algorithms into a model tensegrity robot are also presented, as well as suggestions for experimental validation of results.
-------------------------------------

10130329_178 - 0.872275510569 - technology_and_computing
[form, technologic, design, reference, responsive, bibliographical, human, settlement, urban, age]

Ecologically responsive urban forms : the design of human settlements in a technologic age
Includes bibliographical references (p. 186-195).
-------------------------------------

10134305_183 - 0.977596935206 - technology_and_computing
[transport, property, system, impurity, graphene, potential, experimental, density, type, state]

The Theory of Thermal, Thermoelectric and Electrical Transport Properties of Graphene
Motivated by the experimental measurement of transport properties such as electrical and hall conductivity, thermopower and Nernst, I present a study of longitudinal and transverse transport in graphene for the dilute limit of impurities. The temperature and carrier density dependence in this system display a number of anomalous properties that can be related to three effects: 1) emergence of &ldquo chirality &rdquo, 2) vanishing density of states at the chemical potential in the ideal undoped (zero gate voltage) systems and 3) nature of scattering. In an attempt to theoretically understand these anomalous transport properties, I use the theory of quantum transport in a two-dimensional system with Unitary(lattice vacancy) and screened Coulomb(charge impurity in the underlining substrate) scatterers. I show for a system such as graphene, the type of scattering potential has a profound effect on all transport properties, even though both types of potentials induce low energy states that yield a finite density of states at zero energy. My results are compared with experimental data for both types of scatterer and I show for a single set of impurity parameters all transport properties can be reproduced to agree qualitative with the features observed in experimental data. Parts of this work have been submitted to Physical Review B and are currently in the review process.
-------------------------------------

10135516_183 - 0.858570663057 - technology_and_computing
[restoration, image, method, problem, video, total, variation, subproblem, variant, function]

Numerical optimization for image and video restoration
Image restoration is an inverse problem where the goal is to recover an image from a blurry and noisy observation. An image restoration problem can be formulated as a total variation regularized least-squares minimization where the objective function is the l2-norm squares of the residue between the observation and the prediction. Since the total variation norm is not differentiable, existing methods are inefficient. In this dissertation, a fast numerical optimization method is proposed to solve total variation image restoration problems. The method transforms the original unconstrained problem to an equivalent constrained problem and uses an augmented Lagrangian method to handle the constraints. The transformation allows the differentiable and non- differentiable parts of the objective function to be separated into different subproblems where each subproblem may be solved efficiently. An alternating strategy is then used to combine the subproblem solutions. The image restoration method is extended to handle video restoration problems. The proposed method considers a video as a space -time volume, and introduces a three-dimensional total variation regularization function to enhance the spatial and temporal consistency. The new video restoration framework opens a wide range of applications, including video deblurring and denoising, disparity map refinement, and hot-air turbulence removal. Practical image and video restoration methods need to take into account spatially variant blur and blind deconvolution issues. Therefore, spectral properties of the spatially variant convolution matrices are studied. A fast and robust blind deconvolution method for single image spatially variant out-of-focus blur removal is proposed
-------------------------------------

10129784_178 - 0.911232816496 - technology_and_computing
[power, reference, bibliographical, solar-fossil, hybrid, generation, fuel]

Hybrid solar-fossil fuel power generation
Includes bibliographical references (p. 83-92).
-------------------------------------

10135265_183 - 0.999994974732 - technology_and_computing
[vehicle, map, system, datum, lane-level, advanced, digital]

GIS For Mapping of Lane-Level Data and Re-Creation in Real Time For Navigation
Advanced navigation systems for advanced driver assistance systems for safety of vehicle occupants and for autonomous vehicles require high accuracy digital maps. These maps should contain enough attributes and precision to be able to guide the vehicles within their lanes. Along with these digital maps we also require them to process the data in real time. In this Thesis we will design a geodatabase for such a lane-level digital map using a nodal approach. We collect decimeter accuracy data by using a datalog vehicle (Rover). We followed GIS practices for database development and use GIS management tools such as ArcGIS. In the second half of the thesis we implemented a Human Machine Interface for an Advanced Drive Assistance System. This system will query data from the geodatabase and process it to graphically display on screen the vehicle and the lane-level map of the region around it. It will also display a predicted vehicle position to notify drivers of future lane departures. We tested this system along the Interstate 80 in the Donner Lake Region where it was a part of a Snow Plow Guidance Project implemented by UCR for CALTRANS.
-------------------------------------

10136162_183 - 0.999928281474 - technology_and_computing
[model, network, theory, result, learning, texture, classification, cognition, confabulation, investigation]

Towards more biologically-plausible computational models for cognition, texture classification, and network replication
Neuroscience and machine learning often operate at two ends of a spectrum. The former sometimes finds itself entrenched in the details of experimentation, and the latter sometimes finds itself drifting into the expanse of theory. Both fields can mutually coexist, and when they do, have produced invaluable results in computational neuroscience towards more plausible models of biological solutions. This dissertation presents two detailed investigations into the benefits of this interdisciplinary field: a model for cognition and a model for vision. Experiments during these investigations led us to a third result: a new learning approach called neural network tomography. We introduce our universal theory of cognition, Confabulation Theory, and discuss its biological plausibility. Confabulation Theory posits that the cerebral cortex, in conjunction with the thalamus, is implementing a repeated functional architecture of thalamocortical modules, each encoding one attribute which an object in the individual's mental universe may possess. These modules are interconnected with concurrence statistics called knowledge links, are capable of confabulating a state, and are carefully controlled with action commands. We use Confabulation Theory to build a model for natural language processing and present striking results in sentence generation with context. Subsequently, we focus on the task of texture classification, which we argue is a more primitive operation than object recognition, and therefore, appropriate for investigation with the goal of elucidating biology's solution for processing visual stimuli. We develop a hierarchical model for texture classification, carefully informed by neuroscience results, and demonstrate state-of-the-art performance on a challenging texture classification dataset in the context of our human psychophysical experiment. Finally, we survey existing methods in neural network learning and propose a new approach with several valuable theoretical advantages. By rephrasing the task of function approximation as replicating the topology and weights of an existing universal approximator network, we show that several of the drawbacks of classical backpropagation learning can be avoided. We define a new objective function, mean squared curvature (MSC), and demonstrate that minimizing the MSC of the difference between the networks during the replication process produces favorable results and allows networks to be reverse-engineered iteratively
-------------------------------------

10133915_183 - 0.852300148954 - technology_and_computing
[datum, dynamics, numerical, validation, flow, fluid, experimental]

Experimental validation of flow properties in a novel Y- graft for the Fontan surgery
Numerical analysis has become an increasingly popular tool in the scientific community. Specifically, computational fluid dynamics (CFD) has become a standard in the field of fluid dynamics research. Although data generated using this technique generally accepted as being reliable, experimental validation is still highly necessary to add credibility to scientific results. This paper aims to experimentally validate numerical results obtained on the fluid dynamics of internal flow of an extracardiac Y- shaped graft used to treat children with single ventricle congenital heart disease. Through a comparison with PIV analysis, the numerical data is shown to be quite accurate despite an increased complexity of flow in the experimental data, most likely attributable to surface effects not captured in the simulations. Hence, feasibility of performing in vitro validation of simulation data is demonstrated
-------------------------------------

10137787_183 - 0.999976585029 - technology_and_computing
[parameter, method, model, system, biochemical, stochastic, estimation, value, simulation, mle]

Accelerated maximum likelihood parameter
estimation for stochastic biochemical systems
Abstract
				
				
					
						Background
					
					A prerequisite for the mechanistic simulation of a biochemical system is detailed knowledge of its kinetic parameters. Despite recent experimental advances, the estimation of unknown parameter values from observed data is still a bottleneck for obtaining accurate simulation results. Many methods exist for parameter estimation in deterministic biochemical systems; methods for discrete stochastic systems are less well developed. Given the probabilistic nature of stochastic biochemical models, a natural approach is to choose parameter values that maximize the probability of the observed data with respect to the unknown parameters, a.k.a. the maximum likelihood parameter estimates (MLEs). MLE computation for all but the simplest models requires the simulation of many system trajectories that are consistent with experimental data. For models with unknown parameters, this presents a computational challenge, as the generation of consistent trajectories can be an extremely rare occurrence.
				
				
					
						Results
					
					We have developed Monte Carlo Expectation-Maximization with Modified Cross-Entropy Method (MCEM2): an accelerated method for calculating MLEs that combines advances in rare event simulation with a computationally efficient version of the Monte Carlo expectation-maximization (MCEM) algorithm. Our method requires no prior knowledge regarding parameter values, and it automatically provides a multivariate parameter uncertainty estimate. We applied the method to five stochastic systems of increasing complexity, progressing from an analytically tractable pure-birth model to a computationally demanding model of yeast-polarization. Our results demonstrate that MCEM2 substantially accelerates MLE computation on all tested models when compared to a stand-alone version of MCEM. Additionally, we show how our method identifies parameter values for certain classes of models more accurately than two recently proposed computationally efficient methods.
				
				
					
						Conclusions
					
					This work provides a novel, accelerated version of a likelihood-based parameter estimation method that can be readily applied to stochastic biochemical systems. In addition, our results suggest opportunities for added efficiency improvements that will further enhance our ability to mechanistically simulate biological processes.
-------------------------------------

10134118_183 - 0.997828034282 - technology_and_computing
[gate, mobility, finfet, length, effect, drain, p-channel, model, current, high]

Compact modeling of experimental N- and P-channel FinFETs
As the conventional bulk CMOS shrinks towards the deep sub -100 nm regime, the advantages of scaling are seriously limited by a series of adverse effects such as random dopant fluctuation, short-channel effects, and mobility degradation primarily due to the high substrate doping level required in ultra small devices. As a solution to extend the scaling limit further, FinFETs have become an important subject of intensive VLSI research. In this dissertation, the analytic potential model for symmetric double-gate MOSFETs is verified and calibrated with experimental n- and p-channel FinFETs over a wide range of gate lengths. Quantum mechanical effects are incorporated in the model to reproduce the measured Cg-Vgs data of n- and p-channel FinFETs. Finite inversion layer thickness due to quantum mechanical carrier confinement at high gate overdrives becomes non-negligible for very thin oxides. The increase of effective oxide thickness degrades the gate capacitance and the drain current. The long-channel mobility is modeled by including both a phonon scattering term and a Coulomb scattering term with opposite field dependence. They are extracted from the mobility degradations in the low and high field regions respectively. The dependence of normalized drain current on gate length at low drain bias reveals that there is a slight mobility dependence on gate length due to different strain effects in n- and p-channel FinFETs respectively. In order to obtain the intrinsic mobility, Shift-and-Ratio method is applied to separate out the source-drain series resistance effects. A useful coefficient is defined and extracted to quantitatively indicate the change of mobility from its long-channel value. The coefficient indicates that the electron mobility is degraded as the gate length decreases, whereas the hole mobility is enhanced due to relaxation of the tensile strain induced by the metal gate. The short-channel model for symmetric double-gate MOSFETs based on the analytic solution to 2-D Poisson's equation is validated in terms of the measured drain-induced barrier lowering, the threshold voltage roll -off, and the subthreshold current slope of sub-100 nm FinFETs. The difference between the extracted effective channel length and the drawn gate length is nearly the same for n- and p-channel FinFETs. Other high-field effects including the channel length modulation and velocity saturation are also incorporated into the model to reproduce the drain current data at high drain bias
-------------------------------------

10135205_183 - 0.988518103147 - technology_and_computing
[instrument, music, computer, practice, media]

A computer music instrumentarium
This dissertation reviews representative works of the history of electronic and computer music from the point of view of instrument design. It traces the way in which artists conceived of their systems with respect to traditional musical instruments and practices. While making evident the inherent differences between the mechanical media of the past and the new electronic and computer media at their hands, artists have forged a new kind of instrument. This instrument is presented in contrast to the traditional concept of musical instrument as a passive tool; as an object that provides a stable timbre over which pitch and amplitude can be articulated. In contrast, the computer-based instrument has an active role in determining the shape and sound character of a composition. The traditional conception of music, as closed works contained in scores, shifts to open environmental structures that can only be perceived and experienced through interaction. Theories of embodiment in cognitive science are surveyed to understand the nature of this interaction. Concepts from media theory are used to understand the process whereby new instruments imitate older instruments, while at the same time revealing what is unique to them as new media. The musician then finds himself to be a hybrid between composer, luthier, and performer. However, he cannot start from scratch in every new work of music. In the not so brief history, we find archetypical practices that guide us; communities around computer programs that offer a body of knowledge and code that can be analyzed, appropriated and modified. Because of its relational nature, code contains part of the instrument (and the composition); its exchange advances music as a cultural practice. An Instrumentarium suggests a set of available instruments from which the composer chooses. In the practice of making music with computers however, there are instead, sensors and interfaces, techniques for sound analysis, generation and manipulation, recording devices, hardware, software and speakers. An instrument is not contained in any one object, but consists of a series of elements that can be combined to form an open configuration. The computer music instrumentarium lies in all possible configurations
-------------------------------------

10139058_183 - 0.997123339926 - technology_and_computing
[rna, structure, circularization, application, novel, rna-seq]

Novel applications of high-throughput RNA sequencing: mapping RNA structure and discovering circular RNAs
High-throughput RNA sequencing (RNA-Seq), although still novel, has primarily been applied as a method for assessing differential RNA abundance or mapping of primary structure of linear transcripts, e.g. inference of splice junctions. I report on two novel applications of RNA-Seq for which I developed computational pipelines. The first (FragSeq) is a coupling of classic enzymatic RNA structure probing with RNA-Seq in order to obtain high-throughput, single-base-resolution endonuclease accessibility maps of entire transcriptomes, thus yielding RNA structure information. A proof-of-principle application of this method on two mouse nuclear RNA samples showed that ssRNA regions of known nuclear ncRNA structures are accurately mapped. Also, mapping of novel structures was validated by follow-up probing. The second application is my pipeline for discovery of RNA circularization from RNA-Seq reads that I applied to a broad unpublished dataset spanning 21 archaeal species and a bacterium, uncovering evidence that C/D RNA guide transcripts are circularized in hyperthermophiles. My findings agree with published findings of circular C/D RNA in three species (<italic>P. furiosus</italic>, <italic>S. acidocaldarius</italic>, and <italic>S. solfataricus</italic>) and provide high-confidence evidence for broad C/D RNA circularization in at least two new species (<italic>I. hospitalis</italic> and <italic>T. kodakaraensis</italic>), arguing that this circularization is phylogenetically widespread. Interestingly, the crenarchaeal hyperthermophile <italic>P. aerophilum</italic> has circularization of transcripts anti-sense to C/D RNAs. This is currently the broadest study of circularization in any domain of life.
-------------------------------------

10137279_183 - 0.997907484061 - technology_and_computing
[pe, network, ode, processing, element, custom, general, system, speedup, physical]

Synthesis of Custom Networks of Processing Elements Onto Field-Programmable Gate Arrays for Physical System Emulation
Executing a complex physical system model in real-time or faster has numerous applications in cyber-physical system. For instance, if a human lung model can be executed in real-time, the lung model can be used to test a ventilator in real-time. A complex physical system can often be captured with thousands of ordinary differential equations (ODEs). We introduce an approach to map the ODEs of a physical system to a custom network of processing elements on a field-programmable gate array (FPGA). A processing element (PE) is a light-weight processor that solves a subset of the ODEs. The custom interconnection of the processing elements is based on the data dependencies of the ODEs. The processing elements can execute in parallel and communicate with each other. To automate the design process, we developed a compilation tool to find a good mapping between the ODEs and the processing elements, and to generate synthesizable HDL (hardware description language) code for the entire design.    We first investigated a general purpose processing element that can solve any type of ODEs. The network of general PEs achieves 10-20x speedups against a single-threaded Intel I7-950 CPU, and 4x speedups against an Nvidia GTX 460 GPU. The network of general PEs also yields 2x speedups compared to a commercial high-level-synthesis tool. We further optimized our approach by building custom processing elements that can only solve certain type of ODEs. For homogeneous physical systems (contains only one or a few types ODEs), the network of custom PEs yields another 6x speedup compared to the network of general PEs, given comparable size. Finally, we introduced the network of heterogeneous PEs, where the network may contain both general PEs and different types of custom PEs. We developed an allocation and binding heuristic to explore the large design space. The network of heterogeneous PEs achieves 7x/6x speedup against the network of general PE/single-type custom PEs, and was on average 10x faster than the circuits generated by a high-level synthesis tool.
-------------------------------------

10133594_183 - 0.999996109097 - technology_and_computing
[system, ofdm, wireless, receiver, rate, channel]

Performance analysis of multi-antenna OFDM systems with phase noise and imperfect channel estimation
The age of wireless technologies and the associated convenience that portable wireless products and services provide, continues to drive the need for more advanced products and broadband wireless services that demand carefully selected spectrally efficient modulation schemes to support the desired higher data rates. The development of multicarrier systems has paved the way to meeting the system requirements and fulfilling these needs. Orthogonal Frequency Division Multiplexing (OFDM) is a multicarrier modulation technique that has provided effective means for achieving high data rates and spectral efficiency requirements in wireless communication systems, while making use of relatively simple receiver designs. In wireless communications, one of the major implementation challenges is system sensitivity to synchronization issues, which is even more pronounced in multicarrier systems such as OFDM. The use of OFDM with a high number of subcarriers, to achieve the high data rates it provides, makes it more susceptible to these non-idealities such as phase noise. Another critically limiting factor of system performance is the introduction of channel estimation errors as a consequence of imperfect channel estimation at the receiver. Accurate channel estimates are very essential in ensuring correct detection of the transmitted signal at the receiver. A thorough understanding of system operation therefore requires a detailed analysis of the effects of all the impairments. This dissertation addresses these system impairments by investigating the effects analytically on OFDM systems, deriving bit error rate (BER) expressions for systems that employ a single antenna at both the transmitter and receiver. The investigation is extended to also include OFDM systems that employ multi- antenna configurations at the receiver followed by a similar multi-antenna configuration at the transmitter. The impact of these negative effects is further investigated when applied to a practical system, comparing the analytical results with simulations that makes use of system parameters of the IEEE 802.16 standards
-------------------------------------

10137349_183 - 0.999999327377 - technology_and_computing
[antenna, component, waveguide, system, size, performance, metamaterial]

Substrate Integrated Waveguide Based Metamaterial Components and Novel Miniaturized Planar Antennas
Miniaturized microwave components have drawn increased attention due to the size shrinking of the modern communication systems. This dissertation proposes two approaches to address this problem by reducing the size of the RF components and the antennas. First, miniaturized substrate integrated waveguide components are introduced and developed by using the metamaterial concept. Traditional waveguide components have excellent performance but with a bulky size due to the above-cutoff frequency operation. We demonstrate that by loading the metamaterial elements, the rectangular waveguide can be operated well below the cutoff frequency while maintaining good performance. Based on the substrate integrated waveguide technology, their wave propagation characteristics are studied and their practical applications for guided and radiated RF/microwave components, including transmission lines, filters, couplers, diplexers, oscillators, and leaky-wave antennas, are proposed and implemented. These devices are substantially miniaturized with superior performance achieved.Second, various miniaturized planar antennas are developed to meet the industry application requirements. Different techniques, such as the metamaterial resonators, meta-surfaces, multi-layer folded structures, and the shared radiator approach, are adopted to design different antennas which are suitable to be applied in many wireless systems, including the WLAN links, cellular phone systems and ultra-wideband communication systems. Some special antennas, such as dually or circularly polarized antennas, diversity antennas, are also designed for specified applications. These antennas exhibit good radiation performance with a smaller size compared with the conventional planar antennas. Some of them are going to be used in commercial WLAN communication systems.
-------------------------------------

10136915_183 - 0.983329962363 - technology_and_computing
[photocatalyst, application, efficient, metal, mesoporous, understanding, catalyst, stage, nanocatalyst, nanomaterial]

Nanomaterials Engineering and Applications in Catalysis
Catalysis plays an essential role in industrial applications of direct relevance to many aspects in our daily lives, such as petroleum refining, fine chemical and pharmaceutical production, energy conversion and storage, and automotive emissions control.  Design and fabrication of highly active catalysts in an efficient and cost-effective way is thus an important topic.  This dissertation discusses our efforts in the engineering and applications of nanomaterials, which could be divided into three consecutive stages: (1) synthesis, (2) stabilization, and (3) application in catalysis.    In the first stage, by using Ag nanoplates as a model system, we attempt to outline the key components that determine the formation of nanomaterials with desired morphology, clarify the roles of each reagent, provide highly reproducible recipes for synthesis, and therefore take a significant step towards the complete understanding of the mechanism behind the experimental phenomena.  Using this understanding, Ag nanoplates with various aspect ratios and widely tunable SPR bands have been successfully obtained.    One of the major challenges for the use of nanostructured materials as catalysts is their chemical and structural stability.  In the second stage, by embedding nanocatalysts within a mesoporous metal oxide shell, we are able to prepare nanocatalysts with enhanced stability in both gas and aqueous phase reactions.  A general strategy, called the "surface-protected etching" process, has been developed as the major synthetic tool for producing mesoporous shells for the stabilization of noble metal nanocatalysts.  A sandwich-like structure was further proposed, in which multi-functional materials could be incorporated to make recyclable and highly efficient catalysts.    Finally, for practical applications, TiO2-based nanomaterials have been used as the model system to investigate the factors that determine the preparation of efficient photocatalysts.  Mesoporous TiO2 photocatalysts with high surface area and high photocatalytic activity have been prepared through a self-assembly approach.  Based on our improved understanding of photocatalysts, we have designed and synthesized a highly efficient, stable, and cost-effective TiO2-based photocatalyst by combining both non-metal doping and noble metal decoration.  The new photocatalysts show excellent performance in degradation reactions of a number of organic compounds under the irradiation of UV, visible, and direct sunlight.
-------------------------------------

10135531_183 - 0.914054415093 - technology_and_computing
[cnt, thermal, conductivity, interface, polymer, material, design, steady, state, result]

Design of a steady state thermal conductivity measurement device for CNT RET polymer composites
The focus of this research project was to design and build a steady state apparatus that could accurately measure the thermal conductivity of different materials. By modeling the experimental design with ASTM standards and taking care to reduce heat loss terms, a design was built that was able to accurately measure two reference materials (Pyrex and Stainless) to within 1% of their reference values. After verifying the experimental design, the thermal conductivity of the CNT RET polymer was determined and compared with 3-omega results. Both data sets from different thermal conductivity methods yield an increase in thermal conductivity with an increase in volume percent CNT. The results from 3-omega depicted a evident percolation of thermal conductivity as a function of volume percent as compared to a linear relationship from steady state results. These differences in the thermal conductivity could have been caused by differences in testing samples, such as CNT orientation. Variations in the locations of functional groups within the CNT and location of the CNT attachment to the RET polymer chain can vary significantly from sample to sample. As a result, the varying interfaces between CNT to CNT and CNT to polymer matrix can result in significant thermal interface resistances. Future work may include research into vertically aligned CNT thermal interface materials, specifically their interface characterization. A need for accurate steady state thermal conductivity measurements and a understanding of CNT polymer composites and CNT interfaces are crucial in developing future thermal interface materials (TIMs)
-------------------------------------

10130096_178 - 0.944107831164 - technology_and_computing
[potential, packet, reference, gaussian, function, bibliographical, wave, scattering, shrinking, delta]

The scattering and shrinking of a Gaussian wave packet by delta function potentials
Includes bibliographical references (p. 59).
-------------------------------------

10133423_183 - 0.83734552134 - technology_and_computing
[p2p, content, distribution, podcast, internet, network]

Measuring and Modeling Applications for Content Distribution in the Internet
The focus of this dissertation is on measuring, analyzing and modeling emerging appli- cations in the Internet. Specifically, we concentrate on understanding the internals of content distribution paradigms such as Peer-to-Peer (P2P) systems and podcasts. This dissertation consists of three main thrusts which we describe below. P2P streams have been reported to constitute nearly 61% of all upstream traffic. P2P streams are used for disseminating content ranging from video programs to linux images. This everpresent ubiquity of P2P networks has also allowed them to be used for sharing copyrighted material. This has resulted in organizations like the RIAA, taking legal action against file-sharers. As a result P2P users have employed defenses against being monitored by such organizations. We have found that a little caution pays off a lot, since there is a 100% probability of a naive P2P user being monitored when accessing these networks. Further, as a case study, we present a comprehensive study about eDonkey, a popular P2P network. We identify the limitations of current approaches to measure P2P networks. Additionally, we find that P2P flows traverse through the Internet quite differently than http flows. Based on this, we present metrics useful for distinguishing P2P traffic from other forms of traditional content distribution in the Internet. Finally, podcasts, a relatively new content distribution mechanism is expected to garner an audience of nearly 56 million subscribers by 2010. Measuring and modeling podcasts re- mains an open problem despite the significance that has been gained by this application. This form of content distribution is best described as a push based mechanism, which is different from traditional http based content distribution. We measure podcast streams, analyze them and develop a traffic generator, SimPod, for simulation purposes.
-------------------------------------

10130210_178 - 0.999945423799 - technology_and_computing
[glider, robot, vertical]

Vertical Glider Robots for Subsea Equipment Delivery
We have developed an underwater vehicle that offers significant performance improvements over existing sub sea elevators. Our Vertical Glider Robot falls under its own weight to a precise location on the seafloor, employing streamlining, active steering, and basic navigation instrumentation. We examine typical at-sea mission requirements, mention several key governing parameters, and outline our design approach. We then describe a prototype device, and present results from model-scale experiments.
-------------------------------------

10133486_183 - 0.997863045019 - technology_and_computing
[traffic, od, framework, dynamic, estimation, model, step, application, simulation, planning]

Dynamic Demand Input Preparation for Planning Applications
A spectrum of traffic engineering and modern transportation planning problems requires the knowledge of the underlying trip pattern, commonly represented by dynamic Origin- Destination (OD) trip tables. In view of the fact that direct survey of trip pattern is technically problematic and economically infeasible, there have been a great number of methods proposed in the literature for updating the existing OD tables from traffic counts and/or other data sources. Unfortunately, there remain several common theoretical and practical aspects which impact the estimation accuracy and limit the use of these methods from most real-world applications. This dissertation itemizes and examines these critical issues. Then, the dissertation presents the developments, evaluations, and applications of two new frameworks intended to be used with the current and near-future data, respectively.
      The first framework offers a systematic and practical procedure for preparing dynamic demand inputs for microscopic traffic simulation under planning applications with an estimation module based solely on traffic counts. Under this framework, the traditional planning model is augmented with a filter traffic simulation step, which captures important spatial-temporal characteristics of route and traffic patterns within a large surrounding network, to improve the flow estimates entering and leaving the final microscopic simulation network. A new bounded dynamic OD estimation model and a solution algorithm for solving a large problem are also proposed.
      The second framework utilizes additional information from small probe samples collected over multiple days. There are two steps under this framework. The first step includes a suite of empirical and hierarchical Bayesian models used in estimating time dependent travel time distributions, destination fractions, and route fractions from probe data. These models provide multi-level posterior parameters and tend to moderate extreme estimates toward the overall mean with the magnitude depending on their precision, thus overcoming several problems due to non-uniform (over time and space) small sampling rates. The second step involves a construction of initial OD tables, an estimation of route-link fractions via a Monte Carlo simulation, and an updating procedure using a new dynamic OD estimation formulation which can also take into account the stochastic properties of the assignment matrix.
-------------------------------------

10137098_183 - 0.959809283251 - technology_and_computing
[communication, protocol, uv, neighbor, system, discovery]

Development of a UV Communication Network Using a Software Defined Radio
Recently, research on the use of ultraviolet light for communication purposes has gained popularity. Analytical models that predict the manner in which UV light propagates and scatters have begun to be designed and developed. Using this information, a high level network protocol can begin to be created that would be able to take advantage of the unique UV channel properties. This thesis explores different aspects that comprise an outdoor UV communication system. A leadership based neighbor discovery protocol is designed that is able to handle the initial configuration of the communication system. The protocol creates an interference free environment that helps reduce the time required for neighbor discovery. The protocol is based on the sequential discovery of a node's neighbors. At the termination of the protocol, a node is able to know the addresses of the neighbors that surround it and the directions that it can use to communicate with them. A medium access control protocol is proposed to moderate access to the medium making the communication among devices orderly and efficient. Practical UVOC-MAC is a random access based protocol designed for an outdoor ad hoc network.  Practical UVOC-MAC is able to handle deafness and the hidden/exposed node problems that arise with the use of directional antennas. The design of Practical UVOC-MAC is tied to the UV PHY layer properties which allow the use of non-line of sight links for communication. Spatial reuse is achieved by the protocol by allowing it to adaptively choose the direction of the communication.	A test bed is constructed allowing us to analyze the performance of the communication system.  A software defined radio is used construct transceivers that exchange information using the UV channel. A software framework is used to implement the neighbor discovery and MAC protocols. Preliminary results on the performance of the communication system tested bed are analyzed. The bit error rate and path loss are used to evaluate the performance of the communication system.
-------------------------------------

10138409_183 - 0.864127807613 - technology_and_computing
[site, vs30, proxy]

Development of geologic site classes for seismic site amplification for central and eastern North America
The time-averaged shear wave velocity in the upper 30 m of a site (Vs30) is the most common site parameter used in ground motion prediction equations for the evaluation of seismic site response. It is often the case that Vs30 is not available at sites with earthquake recordings; for example in the NGA-East site database only 45 of 1149 sites have measured values of Vs30. Accordingly, estimates of Vs30 are often made on the basis of available proxies that are widely available such as ground slope, geomorphic terrain categories, and surface geology. We compile a database of 1930 measured and inferred Vs30 values in Central and Eastern North America (CENA) to test slope and geomorphology-based proxy methods. The results indicate that these existing proxy methods are biased for sites with Vs30greater than 400 m/s. Based on a careful review of geological conditions in the CENA, we propose nineteen geologic classes based on setting (i.e., glaciated or non-glaciated), age, and depositional environmental that can form the basis for geology-based proxy estimates of Vs30 as well as for simplified stratigraphic columns.
-------------------------------------

10132113_183 - 0.999992865041 - technology_and_computing
[systems, advanced, information]

Integration of GIS with Activity-Based Model in ATIS
Intelligent Vehicle Highway  Systems (IVHS), which aim at the utilization of advanced information processing and communication  technologies for improving  travel efficiency  and safety, have become  an important policy measure in recent years. One of their major  components, Advanced Traveler Information Systems (ATIS), has been developed  specifically to assist drivers in trip planning and decision making  on destination selection,  departure time, route choices and congestion avoidance.
-------------------------------------

10135195_183 - 0.999850626909 - technology_and_computing
[material, device, magnetic, spin, anisotropy, intrinsic, transfer]

High anisotropy materials for magnetic nanotechnologies
The main content of this dissertation will be on novel magnetic materials for spintronics and, more precisely, for spin transfer torque based memory and spin logic devices. While the general concept of spin transfer torque has been clearly demonstrated the vast majority of research has been on "conventional" transition metal alloys (e.g. permalloy (NiFe) and CoFe). These materials are typically soft magnetic materials where the anisotropy is controlled by the device shape. However, is clear that such materials are far from optimal for understanding the underlying physics of spin-torque and its application to spin-transfer devices. The aim of the research will be on developing novel magnetic films, wires and nano-elements for which the intrinsic materials parameters such as intrinsic defects, magnetic anisotropy, damping constant, spin polarization and magnetization can be tuned over a large range, controlled, optimized and utilized for spin- transfer devices. This will enable tests of the basic physics of spin-transfer, including the magnetization dynamics and switching time as well as the threshold currents for current induced excitations, which depend sensitively on the intrinsic materials parameters. This will further allow the materials and device architectures to be optimized for spin-torque applications
-------------------------------------

10138833_183 - 0.99999887281 - technology_and_computing
[system, network, scada]

Networked Loads in the Distribution Grid
Central utility services are increasingly networked systems that use an interconnection of sensors and programmable logic controllers, and feed data to servers and human-machine interfaces. These systems are connected to the Internet so that they can be accessed remotely, and the network in these plants is structured according to the SCADA model. Although the physical systems themselves are generally designed with high degrees of safety in mind, and designers of computer systems are well advised to incorporate computer security principles, a combined framework for supervisory control of the physical and cyber architectures in these systems is still lacking. Often absent are provisions to defend against external and internal attacks, and even operator errors that might bypass currently standalone security measures to cause undesirable consequences. In this paper we examine a prototypical instance of SCADA network in the distribution network that handles central cooling and heating for a set of buildings. The electrical loads are networked through programmable logic controllers (PLCs), electrical meters, and networks that deliver data to and from servers that are part of a SCADA system, which has grown in size and complexity over many years. 
-------------------------------------

10138348_183 - 0.999633687831 - technology_and_computing
[source, grid, open, openadr, dr, protocol, software, smart, development]

OpenADR Open Source Toolkit:  Developing Open Source Software for the Smart Grid
Demand response (DR) is becoming an increasingly important part of power grid planning and operation. The advent of the Smart Grid, which mandates its use, further motivates selection and
development of suitable software protocols to enable DR functionality. The OpenADR protocol has been developed and is being standardized to serve this goal. We believe that the development of a
distributable, open source implementation of OpenADR will benefit this effort and motivate critical evaluation of its capabilities, by the wider community, for providing wide-scale DR services
-------------------------------------

10138107_183 - 0.999797836391 - technology_and_computing
[waveform, eeg]

Locating Spatial Patterns of Waveforms during Sensory Perception in Scalp EEG
The spatio-temporal oscillations in EEG waves are indicative of sensory and cognitive processing. We propose a method to find the spatial amplitude patterns of a time- limited waveform across multiple EEG channels. It consists of a single iteration of multichannel matching pursuit where the base waveform is obtained via the Hilbert transform of a time-limited tone. The vector of extracted amplitudes across channels is used for classification, and we analyze the effect of deviation in temporal alignment of the waveform on classification performance. Results for a previously published dataset of 6 subjects show comparable results versus a more complicated criteria-based method. 
-------------------------------------

10132652_183 - 0.99995630053 - technology_and_computing
[model, pavement, datum, parameter]

Ameliorating congestion by income redistribution
Properly specified pavement deterioration models are an important input for the efficient management of pavements, the allocation of cost responsibilities to various vehicle classes for their use of the highway system, and the design of pavement structures. However, most empirical deterioration progression models developed to date have had limited success. This paper is concerned with the development of an empirical rutting progression model using experimental data. The data used in this paper comprise an unbalanced panel data set with more than 14,000 observations taken from the AASHO Road Test. The salient features of the model specification are (1) the model eschews conventional (predefined) axle load equivalencies and structural numbers in favor of relationships determined entirely by the data itself; (2) a thawing index variable has been incorporated to capture the effects of the environmental factors in the AASHO Road Test; and (3) the model predicts incremental changes in rut depth, which is particularly advantageous in a pavement management context. The specified model is nonlinear in the variables and the parameters and is estimated using both fixed-effects and random-effects specifications to account for unobserved heterogeneity. The estimation results show that the model replicates the pavement behavior well, that the inclusion of an environmental variable is important to avoid biases in other parameters, and that the size of the unobserved heterogeneity is significant. It is also found that interactions between some parameters in the nonlinear specification leads to significant differences between parameter estimates among the two wheel paths rutting models.
-------------------------------------

10136458_183 - 0.999999480765 - technology_and_computing
[channel, error, code, synchronization, decoding, capacity, windowed]

Synchronization error channels & windowed decoding in theory & practice
Right from the birth of communication theory, synchronization errors have been a challenge. In the first part of this dissertation, we will consider a class of synchronization error channels and develop a rigorous information theoretic analysis. We provide analytical bounds on the capacity of channels that introduce deletions or replications. For channels that introduce deletions and replications, we develop methods to approximately estimate the achievable information rates. Following this, we consider specific applications in magnetic recording where synchronization errors play a key role. For these applications, we provide bounds and numerical estimates of the channel capacity as well as the zero-error capacity. In the second part of the dissertation, we will focus on a coding theoretic problem of analyzing a low-complexity decoding scheme for spatially coupled codes over the erasure channel. We describe the operation of windowed decoding, and analytically establish its asymptotic performance limits. For protograph-based LDPC convolutional codes, which are a variant of the spatially coupled codes, we identify characteristics of code ensembles that result in good performance with the windowed decoding algorithm over erasure channels with and without memory
-------------------------------------

10129778_178 - 0.832465930262 - technology_and_computing
[sensor, reference, elastomeric, bibliographical, tactile]

Tactile sensing using elastomeric sensors
Includes bibliographical references (p. 99-111).
-------------------------------------

10134840_183 - 0.99998930304 - technology_and_computing
[rna, cmv, satellite, replication, virus, absence, helper, satrna, host, chapter]

Studies on Replication of Cucumber Mosaic Virus Satellite RNA
Satellite RNA variant-Q associated with cucumber mosaic virus (CMV) is a highly structured single-stranded RNA molecule of 336 nucleotides in length. Satellite RNA is an important subviral pathogen in agriculture because it modulates symptom expression in CMV-infected plants. In the agricultural industry, CMV is economically very important since it exhibits a broad host range comprising over 1200 plant species world-wide. Although satellite RNA has no sequence homology with CMV genome, satellite RNA replication is thought to occur entirely in the cytoplasm and is exclusively dependent upon CMV replicase.  Chapter 1 of this dissertation describes an Agrobacterium-mediated transient gene expression system that allows the expression of individual viral and satellite RNA uncoupled from the virus replication in vivo, and by utilizing this system we demonstrate that CMV coat protein expressed independently of replication can enhance the accumulation of an individual CMV RNA. Virions formed by the CMV coat protein in the absence of virus replication encapsidates some host RNAs, and the satellite RNA can accumulate to a detectable level in the absence of the helper CMV. Chapter 2 describes a novel finding that satRNA expressed in the absence of its helper CMV is amplified by the host.  By using immunostaining with antibody against double strand RNA, and confocal microscopy to identify the subcellular distribution of double stranded satellite RNA in situ, we have found that in the absence of the helper virus, satRNA can amplify in the nucleus. Furthermore, sequence analysis of satRNA oligomers formed in the absence of CMV replication showed that the junction between two monomeric forms have a unique heptanucleotide sequence GGGAAAA, which is not present in the junction of satRNA oligomers formed in the presence of CMV. Finally in chapter 3, by using agroinfiltration system, we demonstrate that the negative-sense strand of satellite RNA is not amplified by the host cell because it is not recruited into the nucleus. However, consistent with the previously published results, the negative-sense satellite RNA is replicated by the helper CMV but only to a low-level. These novel findings will place the viroids, hepatitis delta virus and satellite RNAs in a closer relationship with each other in terms of their replication mechanisms. Implications of these findings are discussed in Conclusion chapter of this dissertation.
-------------------------------------

10139180_183 - 0.939905759298 - technology_and_computing
[cusum, in-control, distribution, density]

Cumulative Sum Algorithms Based on Nonparametric Kernel Density Estimators
Cumulative sum (CUSUM) algorithms are used for monitoring in various applications, including manufacturing, network monitoring, financial markets, biosurveillance, and many more. A popular CUSUM technique for detecting a change in the in-control distribution of an independentdata sequence is based on repeated use of the sequential probability ratio test (SPRT). Some optimality results have been derived for the SPRT-based CUSUM when the in-control and out-of-control distributions are fully known. We introduce an approximation formula for thethreshold value of an SPRT-based CUSUM. Limited research has been performed on CUSUM techniques when the distributions are not fully specified. This research is concerned about how to use the CUSUM when the underlying in-control distribution is arbitrary and unknown, and the out-of-control density is either an additive or a multiplicative transformation of the in-control density. The proposed solution combines an adaptive nonparametric kernel density estimator derived from an in-control sample of observations with a smoothed bootstrap algorithm that enables the CUSUM to work effectively for reasonably sized sets of in-control data.
-------------------------------------

10136441_183 - 0.983317393583 - technology_and_computing
[newt, analytic, system, lineage, disc, replay, architecture, debugging, output, apus]

Newt : an architecture for lineage -based replay and debugging in DISC systems
Data-intensive scalable computing (DISC) systems facilitate large-scale analytics to mine "big data" for useful information. However, understanding and debugging these systems and analytics is a fundamental challenge to their continued use. This thesis presents Newt, a scalable architecture for capturing fine-grain lineage from DISC systems and using this information to analyze and debug analytics. Newt provides a unique instrumentation API, which actively extracts fine-grain lineage across complex, non-relational analytics. Newt combines this API with a scalable architecture for storing lineage to accommodate the high throughputs of DISC systems. This architecture enables efficient dataflow tracing queries across thousands of operators found in modern data analytics. Newt extends tracing with replay, enabling users to perform step-wise debugging or regenerate lost outputs at a fraction of the cost to execute the entire analytics. Newt further facilitates replay for re-executing analytics without bad inputs to produce error-free outputs. Finally, Newt also enables retrospective lineage analysis, which we use to identify errors in the dataflow using outlier detection techniques. We illustrate the flexibility of Newt's capture API by instrumenting two DISC systems: Apache Hadoop and Hyracks. This API incurs 10-51% time overhead and 30-120% space overhead on workloads consisting of relational and non-relational operators, including a Hadoop-based de novo genomic assembler. Newt can also accurately replay selected outputs, which can reduce the time to recreate errors during debugging. We show that it incurs 0.3% of the original runtime when replaying individual outputs in a WordCount workload. Finally, this work shows the effectiveness of Newt's debugging methodology by pinpointing faulty operators in a dataflow
-------------------------------------

10134453_183 - 0.965525684275 - technology_and_computing
[datum, cmr, processing, result, continuous, approach]

Continuous MapReduce : an architecture for large-scale in-situ data processing
This thesis addresses a fundamental data management challenge faced by cloud service providers: analysis of semi-structured log data generated by large-scale compute infrastructure. This analysis is a crucial aspect of a cloud provider's business, creating competitive advantage by mining user behavioral patterns and ensuring efficient use of resources. However, the amount of data produced in this environment is rapidly growing. The current approach brings data to a central location before analysis, incurring a significant cost and delaying results. As scale increases, the time and cost of data migration alone will render this approach infeasible. We present Continuous MapReduce (CMR), an architecture for large- scale in-situ data processing. CMR is designed to be scalable, responsive, and available while processing logs across thousands of data center servers. CMR extends the MapReduce programming model to allow continuous queries over these data streams, building on concepts found in distributed stream processing. The salient architectural features include an in-situ approach, incremental processing with sliding windows, and a relaxed consistency model. We have built a prototype CMR framework using Mortar, a distributed stream processor, and evaluated it against current batch processing systems. Our results indicate that this approach can improve result latency by 30% for batch and continuous queries. In addition, CMR's consistency model enables it to return results quickly in the face of failure, and still maintain high result fidelity. These results indicate CMR is a valuable tool for addressing the scalability issues of next generation data processing environments
-------------------------------------

10136604_183 - 0.999996299555 - technology_and_computing
[communication, channel, uwa, design, acoustic, aspect, efficient, capacity]

Spectrally efficient underwater acoustic communications : channel characterization and design aspects for OFDM
In this dissertation, we consider design aspects of spectrally efficient underwater acoustic (UWA) communications. In particular, we first focus on statistical characterization and capacity evaluation of shallow water acoustic communications channels. Wideband single-carrier and multi-carrier probe signals are employed during the Kauai Acoustic Communications MURI 2008 (KAM08) and 2011 (KAM11) experiments, to measure the time-varying channel response, and to estimate its statistical properties and capacity that play an important role in the design of spectrally efficient communication systems. Besides the capacity analysis for unconstrained inputs, we determine new bounds on the achievable information rate for discrete-time Gaussian channels with inter-symbol interference and independent and uniformly distributed channel input symbols drawn from finite-order modulation alphabets. Specifically, we derived new bounds on the achievable rates for sparse channels with long memory. Furthermore, we explore design aspects of adaptive modulation based on orthogonal frequency division multiplexing (OFDM) for UWA communications, and study its performance using real-time at-sea experiments. Lastly, we investigate a channel estimation (CE) method for improving the spectral efficiency of UWA communications. Specifically, we determine the performance of a selective decision directed (DD) CE method for UWA OFDM-based communications
-------------------------------------

10129924_178 - 0.999501530308 - technology_and_computing
[hybrid, capture, mode, safety, automaton]

Control of Hidden Mode Hybrid Systems: Algorithm termination
We consider the problem of safety control in Hidden Mode Hybrid Systems (HMHS) that arises in the development of a semi-autonomous cooperative active safety system for collision avoidance at an intersection. We utilize the approach of constructing a new hybrid automaton whose discrete state is an estimate of the HMHS mode. A dynamic feedback map can then be designed that guarantees safety on the basis of the current mode estimate and the concept of the capture set. In this work, we relax the conditions for the termination of the algorithm that computes the capture set by constructing an abstraction of the new hybrid automaton. We present a relation to compute the capture set for the abstraction and show that this capture set is equal to the one for the new hybrid automaton.
-------------------------------------

10132009_183 - 0.999775830943 - technology_and_computing
[technology, apts, transit, operator, performance, assistance, system, program]

Improving Transit Performance with Advanced Public Transportation System Technologies
This report identifies opportunities to improve transit performance using Advanced Public Transportation System (APTS) Technologies, assesses transit operator viewpoints on and experiences with APTS technologies, and proposes how current adoption and utilization practices might be improved so that these technologies are used in a more efficient and effective manner.
      The research consisted of three main phases. First, we identified APTS technologies and developed a framework for assessing their potential value in improving transit system performance. We considered three types of APTS technology, automatic vehicle monitoring (AVM), advanced traveler information systems (ATIS), and advanced fare payment systems (AFP). After describing these technologies, we employ the theory of cybernetics to explain their potential performance benefits. From the standpoint of cybernetics, APTS technologies promise to reduce the cost and improve the effectiveness of a set of regulatory processes by which transit operators and users respond to disturbances in their environment. The categories of APTS benefit thus include (I) cost reduction, (II) improved ability to correctly choose regulatory responses and (III) a richer set of regulatory responses from which to choose. Examples of these benefits for a wide range of transit regulatory processes are presented.
      The second phase of the research consisted of a set of seven case studies of individual transit operators chosen to include properties that have adopted APTS technologies as well as those that have not. Through interviews with management, staff, and line personnel, we investigated the properties’ experiences with and attitudes toward APTS technologies. On the basis of these case studies, we generalize about the circumstances leading to active consideration of APTS adoption, the factors influencing the outcome of adoption decisions, and the process of implementing adopted technologies. We also identify opportunities for improving the adoption process, including (I) more explicit identification of the performance goals the technology is expected to further, and consideration of alternative means of achieving these goals, (II) better information about the organizational resources required to successfully implement APTS technologies, and more balanced assistance programs that combine capital assistance for purchasing hardware with other in-kind and financial assistance to facilitate implementation, (III) development of methodologies, perhaps based on the cybernetic paradigm, to systematically identify “information bottlenecks” that hamper transit system performance and that APTS technologies can alleviate.
      The third phase of the study consisted of a transit operator survey, in which 52 transit operators, 37 of which had adopted at least one APTS technology and 15 had not, were included. The main findings of the survey are (I) AVM is rated somewhat lower than electronic farebox (EFB) technology (which we use as a “benchmark for the ratings) with regard to ease of implementation and satisfaction with vendors, while ATIS ratings are not significantly different that those of EFB, (II) many operators that have not adopted a given APTS technology are actively investigating doing so, (III) most transit operators have positive attitudes about APTS technologies, but do not expect that they will pay for themselves in a direct monetary sense, and thus will required government financial assistance in order to acquire them.
      We conclude with recommendations for a program that we believe will lead to more effective and efficient use of APTS technologies. The proposed program, entitled MOTUS, for MOdel Technology USer, involves intensive assistance and collaboration with a small number of operators identified as promising APTS users. The program would consist of identifying a set of areas in which performance improvements are desired, developing integrated strategies, often involving APTS technologies, designed to improve performance in these areas, implementing the programs, and monitoring the results. Transit operators would receive MOTUS assistance in all aspects of these initiatives. By concentrating assistance in this way, MOTUS is expected to substantially increase the capacity of participating operators, and others who will learn from the participants, to use APTS technologies wisely and aggressively.
-------------------------------------

10129807_178 - 0.992575471575 - technology_and_computing
[method, flow, time, reference, level, bibliographical, set, dependent, planning, path]

Path planning in time dependent flows using level set methods
Includes bibliographical references (p. 167-177).
-------------------------------------

10135922_183 - 0.991234949586 - technology_and_computing
[datum, checklist]

Broad-scale citizen science data from checklists: prospects and challenges for macroecology
"Checklists" of organisms --- records of the species seen in a specific area during a relatively short time period --- are routinely collected by hobbyists for some taxa of organisms, most notably birds.  Gathering and curating these checklists creates a data resource that we believe is underutilized by macroecologists and biogeographers.  In this paper, we describe what we perceive to be the strengths of these data as well as caveats for their use.  While our comments apply widely to data of this type, we focus on data from eBird, a program that collects checklist data on birds around the world, although principally in the Western Hemisphere.
-------------------------------------

10135678_183 - 0.748958398048 - technology_and_computing
[magnetic, material, bulk, grain, property, size, faraday]

Tailoring Magnetic Properties in Bulk Nanostructured Solids
Important magnetic properties and behaviors such as coercivity, remanence, susceptibility, energy product, and exchange coupling can be tailored by controlling the grain size, composition, and density of bulk magnetic materials.  At nanometric length scales the grain size plays an increasingly important role since magnetic domain behavior and grain boundary concentration determine bulk magnetic behavior.   This has spurred a significant amount of work devoted to developing magnetic materials with  nanometric features (thickness, grain/crystallite size, inclusions or shells) in 0D (powder), 1D (wires), and 2D (thin films) materials.  Large 3D nanocrystalline materials are more suitable for many applications such as permanent magnets, magneto-optical Faraday isolators etc.  Yet there are relatively few successful demonstrations of 3D magnetic materials with nanoscale influenced properties available in the literature. 	Making dense 3D bulk materials with magnetic nanocrystalline microstructures is a challenge because many traditional densification techniques (HIP, pressureless sintering, etc.) move the microstructure out of the "nano" regime during densification.  This dissertation shows that the Current Activated Pressure Assisted Densification (CAPAD) method, also known as spark plasma sintering,  can be used to create dense, bulk, magnetic, nanocrystalline solids with varied compositions suited to fit many applications.  	The results of my research will first show important implications for the use of CAPAD for the production of exchange-coupled nanocomposite magnets.  Decreases in grain size were shown to have a significant role in increasing the magnitude of exchange bias.  Second, preferentially ordered bulk magnetic materials were produced with highly anisotropic material properties.  The ordered microstructure resulted in changing magnetic property magnitudes (ex. change in coercivity by almost 10x) depending on the relative orientation (0° vs. 90°) of an externally applied magnetic field to the sample.  Third, a dense magneto-optical material (rare earth oxide) was produced that rotates transmitted polarized light under an externally applied magnetic field, called the Faraday Effect.  The magnitude of the rare earth oxide Faraday Effect surpasses that of the current market leader (terbium gallium garnet) in Faraday isolators by ~2.24x.
-------------------------------------

10137225_183 - 0.99875242431 - technology_and_computing
[speed, extremum, optimization, endurance, perturbation, algorithm, aircraft, form, two-dimensional, simulation]

Improving aircraft endurance through extremum seeking
The length of time a jet aircraft is capable of remaining airborne can be maximized by flying at the speed that produces the least amount of drag. This speed may be predicted based on wind tunnel models, but the optimal speed for any aircraft in service differs somewhat from the calculated speed. Identifying the optimal speed has the potential to realize fuel savings and improve endurance. Extremum seeking is a non-model based form of real time nonlinear optimization that is suitable for problems such as this; however, traditional extremum seeking involves adding a small periodic perturbation to the control input. In this application, this would mean perturbing the throttle, which could erase the fuel savings otherwise achieved by the optimization process. To address this problem, a modified form of extremum seeking is developed that uses atmospheric turbulence in place of throttle perturbations. Using stochastic averaging, it is proven analytically that the extremum-seeking controller stabilizes the speed of the aircraft to the minimum-drag speed, with an average offset proportional to the third derivative of the drag curve and the variance of the airspeed. Brief simulation results illustrate the performance of the basic algorithm. Next, a new form of extremum seeking is introduced that extends a recent development in extremum seeking (called Newton method extremum seeking) to systems using stochastic perturbations. This work is parallel to the work on endurance optimization, but is relevant because the gradient estimator developed herein correctly estimates a two-dimensional gradient with perturbations of different amplitudes in the two dimensions. This is used in a refinement of the basic endurance optimization algorithm that involves a two-dimensional dependence; lift and drag are treated as functions of not only angle of attack (as implicitly assumed to this point) but also Mach number. Optimization proceeds along a line of constant lift in this two-dimensional plane. Analysis proves similar convergence properties for the refined algorithm, and the algorithm is tested in a high fidelity simulation lent by local industry. Simulation results show improvement over the nominal loiter speed
-------------------------------------

10175444_189 - 0.947602516684 - technology_and_computing
[group, sparsity, algorithm, datum]

Recovering Data with Group Sparsity by Alternating Direction Methods
Group sparsity reveals underlying sparsity patterns and contains rich structural information in data. Hence, exploiting group sparsity will facilitate more efficient techniques for recovering large and complicated data in applications such as compressive sensing, statistics, signal and image processing, machine learning and computer vision. This thesis develops efficient algorithms for solving a class of optimization problems with group sparse solutions, where arbitrary group configurations are allowed and the mixed L21-regularization is used to promote group sparsity. Such optimization problems can be quite challenging to solve due to the mixed-norm structure and possible grouping irregularities. We derive algorithms based on a variable splitting strategy and the alternating direction methodology. Extensive numerical results are presented to demonstrate the efficiency, stability and robustness of these algorithms, in comparison with the previously known state-of-the-art algorithms. We also extend the existing global convergence theory to allow more generality.
-------------------------------------

10139384_183 - 0.999992968507 - technology_and_computing
[network, capacity, dof, interference, message, cake, graph, alignment, study, solution]

Topological Interference Management through Index Coding
While much recent progress on interference networks has come about under the assumption of abundant channel state information at the transmitters (CSIT), a complementary perspective is sought in this work through the study of interference networks with no CSIT except a coarse knowledge of the topology of the network that only allows a distinction between weak and significant channels and no further knowledge  of the channel coefficients' realizations. Modeled as a degrees-of-freedom (DoF) study of a partially connected interference network with no CSIT, the problem is found to have a counterpart in the capacity analysis of wired networks with arbitrary linear network coding  at intermediate nodes, under the assumption that the sources are aware only of the end to end topology of the network. The network capacity (wired) and DoF (wireless) region, expressed in dimensionless units as multiples of the capacity (wired) and DoF (wireless) of a single link, are found to be bounded above by the capacity of an index coding problem where the antidotes graph is the complement of the interference graph of the original network and the bottleneck link capacity is normalized to unity. The problems are shown to be equivalent under linear solutions. An interference alignment perspective is then used to translate the existing index coding solutions into the wired network capacity and wireless network DoF solutions, as well as to find new  and unified solutions to different classes of all three problems. For networks with $K$ messages, a study of the extremes -- when each message achieves half the cake, and when each message can achieve no more than $1/K$ of the cake, reveals the necessary and sufficient conditions for each, in terms of alignment graphs and demand graphs, respectively. Half the cake per message is achievable if and only if the alignment graph has no internal conflicts. No more than $1/K$ of the cake is achievable if and only if the network can be relaxed into a $K$-unicast setting with an acyclic demand graph. For half-rate-feasible networks, best case capacity (DoF) improvements over the best orthogonal (TDMA) and multicast (CDMA) solutions are explored for multiple groupcast and multiple unicast settings, and shown to be of polynomial order in the number of messages. For intermediate cases where neither half the cake, nor $1/K$ of the cake per message is capacity (DoF) optimal, the interference alignment perspective is used to characterize the symmetric capacity (DoF) of all cases where each  alignment set either does not contain a cycle or does not contain a fork. A study of linear feasible rates shows duality properties that are used to extend the scope of previous results. For wireless networks, extensions to multiple antenna networks are made in symmetric settings where all nodes are equipped with the same number of antennas. The study of certain topologies of interest, motivated by cellular networks reveals interesting aligned frequency reuse patterns.
-------------------------------------

10130064_178 - 0.999788248383 - technology_and_computing
[power, technique, amplifier, reference, efficiency, bibliographical, wave, enhancement, rf, millimeter]

Efficiency enhancement techniques for RF and millimeter wave power amplifiers
Includes bibliographical references (p. 123-126).
-------------------------------------

10135220_183 - 0.999744642913 - technology_and_computing
[efficiency, envelope, broadband, amplifier, ea, signal, system, mhz, power, high]

Envelope amplifier for broadband base-station envelope tracking power amplifier
Envelope tracking (ET), which is a dynamic supply control technology to realize high efficiency power amplifiers, is a promising approach for base-station transmitters of the future. The envelope amplifier (EA) in ET systems provides a non-constant modulated power supply to the RF transistor. It is challenging to design so that it is both broadband and high efficiency, while meeting the stringent linearity requirements for high peak-to-average ratio signals in modern wireless communication systems. This thesis focuses on EA design and implementation for efficiency enhancement of ET systems with broadband envelope input signals. First, the ET system architecture is analyzed, and the efficiencies of RF transistors and envelope amplifiers are described. Secondly, the principles of the EA operation are investigated, and each circuit stage is carefully designed for broadband signals. Then, an EA model, including the RF transistor load, is developed in PSpice, and many simulations are described in order to better analyze and design the broadband EA for high efficiency. After the design, a broadband EA is implemented on a PCB board. The testing with constant resistive loads is carried out to verify the function and measure the efficiencies of 5 MHz WCDMA and 20 MHz LTE-A downlink envelope signals. Finally, tests on the whole ET system are performed, and the overall drain and power added efficiencies are tabulated. For this broadband envelope amplifier, the efficiency for a 5 MHz WCDMA input signal is above 75%, and for 20 MHz LTE-A, it works robustly with an efficiency of 62%
-------------------------------------

10134857_183 - 0.863809755154 - technology_and_computing
[structure, atsm]

Term structure of interest rates and monetary policy
My dissertation solves various difficulties of Affine-Term -Structure Models (ATSM) known or unknown in the literature, by providing an explicit mapping between the reduced-form and ATSM parameters for identification analysis, developing a more reliable, better-behaved and faster estimation procedure, and coming up with a framework to examine how well those models fit the data. I also apply ATSM to assess the effectiveness of unconventional monetary policy under the zero lower bound during the current financial crisis, and develop new measures for contribution of maturity structure of Treasury debt to the term structure of interest rates
-------------------------------------

10132611_183 - 0.998827868571 - technology_and_computing
[mode, toll, fast, slow, consumer, user]

Which consumers benefit from congestion tolls?
Consider a consumer who can choose to travel on a congestible fast mode or on a congestible slow mode. Users who most value time will use the fast mode. A toll on the slow mode can induce some people who initially use that mode to switch to the fast mode. A toll on the slow mode with revenue not returned to users then necessarily reduces the welfare of all users. A toll on the fast mode may raise aggregate consumer surplus.
-------------------------------------

10130190_178 - 0.997259185325 - technology_and_computing
[technology, environmental, monitoring, research, center, sensing, alliance, singapore-mit]

Collaborative multi-vehicle localization and mapping in marine environments
Singapore-MIT Alliance for Research and Technology. Center for Environmental Sensing and Monitoring
-------------------------------------

10139499_183 - 0.997889588997 - technology_and_computing
[eqe, zno, efficiency, measurement, quantum, nanowire]

"High Quantum Efficiency of Band-Edge Emission from ZnO Nanowires"
External quantum efficiency (EQE) of photoluminescence as high as 20percent from isolated ZnO nanowires were measured at room temperature. The EQE was found to be highly dependent on photoexcitation density, which underscores the importance of uniform optical excitation during the EQE measurement. An integrating sphere coupled to a microscopic imaging system was used in this work, which enabled the EQE measurement on isolated ZnO nanowires. The EQE values obtained here are significantly higher than those reported for ZnO materials in forms of bulk, thin films or powders. Additional insight on the radiative extraction factor of one-dimensional nanostructures was gained by measuring the internal quantum efficiency of individual nanowires. Such quantitative EQE measurements provide a sensitive, noninvasive method to characterize the optical properties of low-dimensional nanostructures and allow tuning of synthesis parameters for optimization of nanoscale materials.
-------------------------------------

10134942_183 - 0.999940632934 - technology_and_computing
[channel, source, performance, transmission, throughput, unequal, rate, resource, packet]

Cross layer design for the transmission of multimedia over wireless channels
When we transmit multimedia through wireless channels, we need to protect source bits from channel noise. However, due to the constraints on the channel resources, source and channel bits should share the resources optimally in the sense of distortion or throughput. That is, the problem is how to allocate channel resources such as the bandwidth, diversity, or transmit power to the source and channel under system constraints. In addition, due to the unequal priority of source packets, performance can be improved by assigning unequal channel resources to the packets based on their priority. In this dissertation, we introduce an information-theoretic framework which allows us to analyze the system performance mathematically with unequal allocation of the channel resources with respect to the unequal priority of the source packets. By applying the information theoretic framework, an algorithm to find the throughput-optimal unequal error protection (UEP) is derived. The first example to apply this framework is the progressive image transmission over block fading channels with relay-assisted distributed spatial diversity. Assuming a progressive image coder with a constraint on the transmission bandwidth, we formulate a joint source- channel rate allocation scheme that maximizes the expected source throughput. Specifically, using Gaussian as well as BPSK inputs on at Rayleigh fading channels, we lower bound the average packet error rate by the corresponding mutual information outage probability, and derive the average throughput expression as a function of channel code rates as well as channel signal-to-noise ratio (SNR) for a frequency-division multiplexing-based system both without relaying and with a half-duplex relay using a decode-and- forward protocol. At high SNR, the optimization problem involves a convex function of the channel code rates, and we show that a known recursive algorithm can be used to predict the performance of both systems. The second example is the layered transmission of a Gaussian source over multiple relays using superposition coding. At first, we analyze the outage probability and performance in terms of average throughput and distortion for decodeand-forward (DF) protocols with single-layer and superposed two-layer coding. For the superposition coding approach, we consider different power allocations to the base and enhancement layers. Then, we propose a simple protocol which assigns a pre-determined number of relays to individual layers instead of repeating the superposition coded packet at the relay. We also present numerical results based on the analysis to compare the performance. We then consider a practical application where motion compensated fine granular scalable (MC-FGS) video is transmitted over multi -input multi-output (MIMO) wireless channels and the leaky and partial prediction schemes are applied in the enhancement layer of MC-FGS video to exploit the tradeoff between error propagation and coding efficiency. For reliable transmission, we propose UEP by considering a tradeoff between reliability and data rates, which is controlled by forward error correction (FEC) and MIMO mode selection to minimize the average distortion. In a high Doppler environment where it is hard to get an accurate channel estimate, we investigate the performance of the proposed MC-FGS video transmission scheme with joint control of both the leaky and partial prediction parameters and the UEP. In a slow fading channel where the channel throughput can be estimated at the transmitter, adaptive control of the prediction parameters is considered
-------------------------------------

10135391_183 - 0.99953816677 - technology_and_computing
[system, design, energy, water, type, solar, desalination, pv]

Solar powered desalination system
With the increasing need for fresh water sources, especially in California with its "Water Crisis" coupled with the global "Energy Crisis" there is rising desire for fresh water production through renewable means. A study was conducted to evaluate the most efficient design for a solar powered desalination system. Two basic design types were considered. The first design type is using photoelectrochemical (PEC) cells to produce hydrogen, which would then be used to produce thermal energy to desalinate by distillation. The second design type is using photovoltaics (PV) to produce electrical energy to desalinate by membrane. The study concluded that a PV- reverse osmosis (RO) system would be the most energy and space efficient. An RO system was assembled and tested to show feasibility. Future work includes powering the RO system using PV and calculating the system efficiency. Focusing on emergency drinking water applications, a single element design was proposed. This single element design is meant for a compact, portable solar powered desalination system
-------------------------------------

10134127_183 - 0.986976630185 - technology_and_computing
[user, ranking, function, query, rf, datum, video, training, internet]

Learning Ranking Functions for Video Search on the Web
Videos on the Internet have become widespread. However search engines are still mostly limited to using associated text data to find desired content. In this dissertation, we build ranking functions that can directly analyze image and video content and assign a ranking to a database with respect to user queries.A common approach to building ranking functions is to use a machine learning algorithm to perform a priori training of class concepts and use the trained classifier as the ranking function. However, a priori training of class concepts for retrieval is daunting since users queries can be very diverse. In addition, a priori training cannot capture the subjective component of user queries. For example, if a user were searching for videos of ``nice basketball shots,'' there would be no way to know what the user considers ``nice.'' Relevance feedback (RF) is an interactive search framework that captures user subjectivity and supports on-the-fly learning of target classes.However, RF is limited in its need for large amounts of user feedback when the data being searched are complex (e.g. Internet content). Transfer learning (TL) is a machine learning formulation where existing knowledge about a related ``source'' classification task can be used to improve the generalization performance of a ``target'' task (where training data is scarce). In this dissertation we explore the combination of RF and TL and present a framework which can learn more from the user with less feedback. We show extensive experiments with real-world data taken from the Internet and show improved performance over past RF frameworks.Although our RF and TL framework is effective for a wide range of queries, we acknowledge that there are some highly specific but common queries users could make which would benefit from more dedicated design of a ranking function. For example, finding particular people using face recognition would be an important type of query on the Internet. The problem in this case is well defined and objective. While the problem is specific, it is important enough to warrant the dedicated design of a ranking function. Thus we complete our studies in this dissertation through the exploration of a robust face recognition based ranking function and show strong results in a challenging face identity retrieval task.
-------------------------------------

10134383_183 - 0.99999441163 - technology_and_computing
[system, intelligent, classifier, intelligence, sensor, recognition]

The systems engineering of a network-centric distributed intelligent system of systems for robust human behavior classifications
Automating intelligence within sensor networks for situational awareness and responses is the overall motivational application for this dissertation. Traditionally, intelligence is manually gathered and extracted by intelligence analysts. However, there will never be enough intelligence analysts, intelligent centers, or even bandwidth (for mobile sensors) to manually extract information for intelligence from raw sensor data. Fusing a large number of sensor types and inputs is also required. All of this can be implemented and automated in an artificial intelligent (AI) hierarchy described herein, and therefore not require human power to observe, fuse, and interpret. This objective is fulfilled in this systems dissertation with several independent systems combined together to form an intelligent system of systems (SoS). In order to design and implement an intelligent SoS, there are a number of unique contributions from this author in this dissertation. The first six listed author contributions are systems' developments as Chief Engineer on the intelligent SoS and the last six contributions are novel technological developments. The following are the SoS systems' developments : (1) a Fixed Camera System containing a multi-camera network (thirty-six PoE cameras) and six processing units ; (2) a Kiosk System containing dual Pan/Tilt/Zoom cameras, a microphone network and two processing units ; and (3) a Command and Control System containing a database on a server with dual monitors displaying an (4) interactive executive graphical user interface displaying (5) mustered personnel and (6) abnormal behavior alarms. This SoS was designed and built with novel technologies that the author developed for this SoS : (7) high-level syntactical classifiers for classifying human/object behaviors that are predefined based on sequences of (8) identified combinations of fused (9) object recognitions (e.g. body postures and face recognitions) by low-level classifiers on video data, including a (10) generalized parts-based object recognition low-level classifier. The system uses a (11) high-level syntactical classifier to recover from low- level classification errors. This intelligent SoS was built and implemented as a prototype. Additionally, preliminary transitions are underway for transitioning the prototype to a product system, such as (12) providing a Field Programmable Gate Array (FPGA) architecture for the generalized object recognition low-level classifier
-------------------------------------

10135609_183 - 0.999844484146 - technology_and_computing
[pdf, malicious, reader, malware]

Understanding the role of malicious PDFs in the malware ecosystem
The Portable Document Format (PDF) is a widely used, cross -platform file format for document exchange. Several applications exist for parsing and rendering PDF documents, with Adobe's Acrobat Reader being the most widely used PDF reader. Starting in 2007, several vulnerabilities in Adobe Reader were discovered being exploited in the wild. PDF- based exploits continued to proliferate during 2008 and 2009, and, although recent security reports have noted a decline in the numbers of PDF-based malware in 2011, malicious PDFs are likely to continue to be a significant threat for the next few years, given the ubiquity of the PDF format and the existence of a large base of unpatched Adobe Reader installations. In this work, we try to understand the role played by malicious PDFs in the malware and spam ecosystems. We collect data from the execution of a set of about 11,000 malicious PDFs obtained from various sources. We find a correlation between the age of a vulnerability and the number of PDFs exploiting that vulnerability. We also find differences in behavior depending on the distribution vector used. Looking at the final payload of the malicious PDFs, we find that some known pay-per-install services seem to use malicious PDFs as an infection vector. Finally, we see a considerable overlap in malware-hosting domains contacted by malicious PDFs and spam-advertised domains seen in emails collected by various spam feeds, pointing to the use of both vectors for malware distribution
-------------------------------------

10133990_183 - 0.997911783558 - technology_and_computing
[application, page, web, development, database, view]

Web application creation made easy : a SQL-driven rapid development framework and a Do-It- Yourself platform
Building, installing and evolving a custom web application, even one which comprises only Create, Read, Update and Delete (CRUD) pages accessing a single database, is time consuming and expensive. We present two complementary systems that enable the rapid creation, customization and evolution of such a database-driven web application and its pages thereof: a rapid application development framework called FORWARD, and a Do-It-Yourself (DIY) platform called app2you. FORWARD simplifies the development of AJAX web pages by treating them as rendered views, where the programmer specifies a view using visual units and (minimally extended) SQL. Such a declarative approach leads to significantly less code, as the framework automatically solves performance optimization problems that the programmer would otherwise hand-code. Since the pages are fueled by views, FORWARD leverages years of database research on incremental view maintenance by creating optimization techniques appropriately extended for the need of pages (nesting, variability, ordering), thereby achieving performance comparable to hand-coded applications. app2you builds on FORWARD by empowering non- programmer business process owners to create and customize application pages, without programming or database design in a conventional sense. app2you provides a WYSIWYG design facility where the owner specifies the application by manipulating visual aspects of it, responding to questions posed by wizards and setting configuration options. In response, the design facility infers the necessary FORWARD application, including database schema, data structures and code, and immediately produces a revision of the application for the owner's evaluation. The software development cycle is shortened to literally seconds
-------------------------------------

10135722_183 - 0.989713891529 - technology_and_computing
[soc, model, simulation, polity, early, complex]

Emergence of Fusion/Fission Cycling and Self-Organized Criticality from a Simulation Model of Early Complex Polities
A spatial-temporal model of early complex polities is described in which cycles of consolidation and collapse emerged during simulations. Self-organized criticality (SOC) also was clearly observed. SOC is characteristic of simulations for iterative physical phenomena such as earthquakes and forest fires. Social scientists are interested in SOC as a theoretical framework to understand cyclical human cultural processes. In particular there has been considerable speculation that SOC underlies polity cycling. The current model is an attempt to move beyond speculation by demonstrating that: 1) the model unequivocally exhibits SOC, 2) there is a self-evident correspondence between the model’s structure and actual polities as indicated in the archaeological record, 3)hierarchical settlement patterns emerge during simulations, and 4) simulated population distributions are consistent with empirical rank-size survey data typical of early complex polities.
-------------------------------------

10135645_183 - 0.999977407404 - technology_and_computing
[datum, client, server, map, spatial, structure, tgap]

Methodology Comparisons in Progressive Transfer of Vector-Based Geographic Data
In this research we use the tGAP (Topological Generalized Area Partition) methodology to create and consume a variable-scale topological reactive data structure to progressively transfer vector-based GIS data in client/server architectures. Many researches has been carried out in the past in this subject and different approaches and variations of the tGAP data structure have been proposed, but the problem of how to effectively communicate and transfer the data between clients and servers in client/server architecture has not been the main focus of the previous works. We describe various tGAP data structures and implement an ameliorated version of the tGAP data structure using left-right topology without edge references that minimizes both usage of the network bandwidth and the amount of data stored in the database which in turn minimizes the execution time of attribute and spatial queries.We propose two different architectures and perform methodology comparisons. First, we design a client that progressively polls vector-based spatial data from a server. The client application initiates the requests based on its current map extent and keeps track of the features drawn on the map. Every time the user pans or zooms to a smaller/larger scale in the map the multi-threaded application will asynchronously request for the termination of the previous request (if it is still in progress) and polls the data for the new scale/extent. The advantage of multi-threaded architecture in the client is that the UI thread never gets blocked while the background thread is submitting the request and retrieving the spatial data from the server. On the other hand usage of the tGAP data structures will allow the server to transfer spatial data from higher importance levels down to the lower importance levels based upon the hierarchical information that has been stored for all the faces and the edges, i.e. generalized versions of the faces will be transferred first and the client will progressively update the content of its map view by retrieving lower important neighboring objects to obtain more details.In our second design the client submits a request for the geographical data in a given map extent and server fetches the data from a spatial database. The difference with the previous approach is that this time the server pushes the data to the client and keeps track of the objects submitted to each client (session). A background thread in each client retrieves objects from the server in the same fashion, i.e. objects with higher importance levels come and drawn first etc... Since the client benefits from multi-threaded mechanism users would be able to interact with the map while data are pushing from the server and no blockage in their UI thread will occur.In the end we will analyze and compare both methodologies and investigate the effectiveness of database indexes as well as spatial indexes in their performance and will show how both approaches could benefit from them.
-------------------------------------

10136953_183 - 0.880709870528 - technology_and_computing
[trade, model, network, statistics, space, latent, geography, dead]

Network Statistics and Modeling the Global Trade Economy: Exponential Random Graph Models and Latent Space Models: Is Geography Dead?
Due to advancements in physics and computer science, networks have becoming increasingly applied to study a diverse set of interactions, including P2P, neural mapping, transportation, migration and global trade. Recent literature on the world trade network relies only on descriptive network statistics, and few attempts are made to statistically analyze the trade network using stochastic models. To fill this gap, I specify several models using international trade data and apply network statistics to determine the likelihood that a trade tie between two countries is established. I also use latent space models to test the `geography is dead' thesis. There are two main findings of the paper. First, the "rich club phenomenon" identified in previous works using descriptive statistics no longer holds true when controlling for homophily and transitivity. Second, results from the latent space model refute the `geography is dead' thesis.
-------------------------------------

10134531_183 - 0.999785237571 - technology_and_computing
[timing, parallel, algorithm, graph, approach, analysis, gpu, partitioning, serial, result]

A partitioning approach for GPU accelerated level-based on -chip variation static timing analysis
Technology and design trends have made timing analysis the bottleneck of electronic design automation (EDA) tools. Efficient and accurate timing analysis is a challenge that the EDA industry must overcome in order to move forward. Using LLC-OCV leverages Physical Location, Path Level, and Cell type information to further increase timing accuracy. This model introduces increased data complexity as a result of maintaining delays for each unique path-level. We parallelize this computation for co-processing on a CUDA enabled GPU. We introduce a novel divide-and-conquer partitioning approach for computing the per-level delay data used in the level-based aspect of LLC-OCV. Partitioning the circuit graph halves the inherently serial structure of a topological traversal of the circuit graph with a costly but more parallel merge step that combines the solutions of the two partitions. Using a massively parallel GPU-based approach allows us to absorb the cost of merging by performing it in parallel. Our experimental results on the ISCAS '85 benchmark demonstrate our parallel algorithm scales with timing graph size more efficiently than the serial algorithm. Results also show that our partitioning approach allows us to more fully utilize the massively parallel computational resources of the GPU. Our experiments on artificial test cases demonstrate that the parallel algorithms outperform the serial algorithm on large non-linear graph structures. We also find that LOCV timing analysis is a memory bound computation. We expect our algorithm to perform better on the newer Fermi architecture because of the new cached memory architecture
-------------------------------------

10129930_178 - 0.96719738162 - technology_and_computing
[cooperative, number, system, experimental, semi-autonomous, foundation, national, science, validation, development]

Development and experimental validation of a semi-autonomous cooperative active safety system
National Science Foundation (U.S.) (NSF CAREER Award Number CNS-0642719)
-------------------------------------

10130392_178 - 0.985351499561 - technology_and_computing
[site, reference, sinepuxent, bay, bibliographical, projection, cultivation]

Projections toward the cultivation of a site by the Sinepuxent Bay
Includes bibliographical references (p. 207-210).
-------------------------------------

10132012_183 - 0.999851897502 - technology_and_computing
[information, system]

SmartMaps for Advanced Traveler Information Systems Based on User Characteristics
This project has three emphases: (1) an assessment of the user requirements relevant to the design of effective transit information systems; (2) a reconnaissance of enabling technologies and prototypes for electronic information systems; and (3) a set of conceptual designs for electronic, interactive traveler information systems for public transit.
-------------------------------------

10138314_183 - 0.999989731878 - technology_and_computing
[algorithm, mesh, sub-mesh, connected]

Data-Parallel Mesh Connected Components Labeling and Analysis
We present a data-parallel algorithm for identifying and labeling the connected sub-meshes within a domain-decomposed 3D mesh. The identification task is challenging in a distributed-memory parallel setting because connectivity is transitive and the cells composing each sub-mesh may span many or all processors. Our algorithm employs a multi-stage application of the Union-find algorithm and a spatial partitioning scheme to efficiently merge information across processors
percent{\color{red} (we never define disjoint.  Is this too complex a concept)} merge disjoint sets
and produce a global labeling of connected sub-meshes. Marking each vertex with its corresponding sub-mesh label allows us to isolate mesh features based on topology, enabling new analysis capabilities. We briefly discuss two specific applications of the algorithm and present results from a weak scaling study. We demonstrate the algorithm at concurrency levels up to 2197 cores and analyze meshes containing up to 68 billion cells.
-------------------------------------

10161_7 - 0.998151397885 - technology_and_computing
[method, model, n-tuple]

Two Bayesian treatments of the n-tuple recognition method
Two probabilistic interpretations of the n-tuple recognition method are put forward in order to allow this technique to be analysed with the same Bayesian methods used in connection with other neural network models. Elementary demonstrations are then given of the use of maximum likelihood and maximum entropy methods for tuning the model parameters and assisting their interpretation. One of the models can be used to illustrate the significance of overlapping n-tuple samples with respect to correlations in the patterns
-------------------------------------

10175612_189 - 0.929007042701 - technology_and_computing
[foundation, science, gpu, detector, n-way, mimo, flexible, national, nsf]

Flexible N-Way MIMO Detector on GPU
National Science Foundation (NSF)
-------------------------------------

10130128_178 - 0.998084110503 - technology_and_computing
[imaging, optical, reference, rydberg, bibliographical, atom]

Optical imaging of Rydberg atoms
Includes bibliographical references (p. 109-111).
-------------------------------------

10129671_178 - 0.888703777665 - technology_and_computing
[operation, form-making, basic]

Basic operations of form-making.
Includes bibliographies.
-------------------------------------

10175450_189 - 0.989479030514 - technology_and_computing
[pcm, performance, memory]

Coding for Phase Change Memory Performance Optimization
Over the past several decades, memory technologies have exploited
continual scaling of CMOS to drastically improve performance and
cost. Unfortunately, charge-based memories become unreliable beyond
20 nm feature sizes. A promising alternative is Phase-Change-Memory
(PCM) which leverages scalable resistive thermal mechanisms. To
realize PCM's potential, a number of challenges, including the
limited wear-endurance and costly writes, need to be addressed. This
thesis introduces novel methodologies for encoding data on PCM which exploit asymmetries in read/write performance to minimize memory's wear/energy consumption. First, we map the problem to a
distance-based graph clustering problem and prove it is NP-hard.
Next, we propose two different approaches:  an optimal solution
based on Integer-Linear-Programming, and an approximately-optimal solution based on Dynamic-Programming. Our methods target both single-level and multi-level cell PCM and provide further
optimizations for stochastically-distributed data. We devise a low
overhead hardware architecture for the encoder. Evaluations
demonstrate significant performance gains of our framework.
-------------------------------------

10135539_183 - 0.999993812631 - technology_and_computing
[service, application, cloud, datum, consistent, availability, computing]

Consistent cloud computing storage as the basis for distributed applications
Cloud computing aims to simplify distributed application development by providing location independent access to vast computing resources with services implementing fault tolerance and scalability. As applications and their associated data migrate225}0t̀o the cloud,'' or data center hosted deployments, users increasingly depend on the availability and reliability of these services. Not only does downtime inconvenience users who rely on these services for access to their personal data, but it also results in lost revenue and user confidence. To increase availability, services replicate their systems and data at multiple data centers, typically using some form of eventual consistency, which results in complicated application and system designs and limits the types of applications that can use these services. This dissertation argues that strongly consistent cloud computing storage and infrastructure services can both significantly simplify and enable new classes of distributed applications by providing a basis for scalability, reliability, and availability. We propose techniques to enable cross-data center replication based on fine-grained scalable replicated state machines that offer either strong or weak consistency. Building upon these techniques, this dissertation describes how the limiting factor for availability in strongly consistent wide-area replicated services often is the time to recover a replica after a failure. We present a solution to this limitation based on a new protocol that maintains strong consistency while greatly reducing recovery time for wide- area services. Deploying these techniques and protocols for consistent cloud computing requires message routing services capable of performing arbitrary processing based on large amounts of service state. This dissertation presents a new framework for building scalable, programmable, extensible software message routers and middleboxes using commodity hardware. This framework performs favorably compared to a commercial load-balancing switch while its flexible architecture enables the protocols we propose for implementing wide-area strongly consistent replication
-------------------------------------

10129745_178 - 0.999852321015 - technology_and_computing
[system, expertise, reference, functional, insight, bibliographical, auditory-motor, structural, adaptation, pathology]

Structural & functional adaptations of the Auditory-Motor System : insights from expertise & pathology
Includes bibliographical references (p. 78-85).
-------------------------------------

10136944_183 - 0.999338125966 - technology_and_computing
[traffic, arrival, network, flow, packet, application, access, entropy, schedule, time]

Proactive, Traffic Adaptive, Collision-Free Medium Access
Wireless networks are a fixture of present day computing. We are seeing a simultaneous increase in network density and throughput demand as the clients of these networks grow accustomed to more data hungry applications. Contention-based channel access methods take bigger performance hits and waste more energy as network density and load increases. It is therefore clear that the future of wireless networking will need to exploit some form of schedule based channel access in order to simultaneously solve the problems of energy consumption and maximization of channel utilization.The focus of this work is on leveraging implicit properties of network traffic to benefit the performance of schedule based medium access mechanisms. We focus on one of these properties: the packet arrival behavior of the traffic. We chose to start our work by trying to answer the following question: "If we use predictions of the behaviors of flows in the network, can we decrease the delay in schedule-based medium access control?" The main idea is to use traffic forecasting to anticipate transmission schedules instead of establishing them reactively, i.e., as traffic arrives at the MAC layer. Although not all applications generate forecastable traffic, we contend that many applications do. Examples of predictable network traffic include Voice-over-IP (VoIP) applications such as Skype, iChat, and Google talk. Video streaming applications have lower QoS demands but also contain many predictable patterns. All of these applications are becoming increasingly commonplace in the home networks of today.An experimental method was used to evaluate the benefit that accurate traffic prediction could have on the performance of a schedule based MAC protocol (DYNAMMA). Comparing the performance of DYNAMMA to our modified version of it (DYNAMMA-PRED) in simulations showed that prediction does improve delay performance of the schedule based protocol significantly, particularly at lower network loads.The next step was to address the topic of extracting patterns out of packet arrival times of each flow with more mathematical rigor. We did this by measuring the entropy of packet arrivals in a network flow. Given that entropy is defined as the "measure of information", its value in this context signifies the amount of pattern in the packet arrival times of a flow - the less information each arrival holds, the more pattern there is overall.During our investigation of the entropy of the packet arrival times, our research produced the concept of an "entropy fingerprint" - a plot of the entropy of the packet arrival times of a flow over a range of time scales. Each entropy fingerprint has numerous characteristics that are related to the packet arrival behavior of the flow that generated it. These fingerprints can be used in many ways, such as identifying what application generated the flow or whether the flow's packet arrivals are likely to be regular or irregular at a given time scale. In addition to the entropy fingerprints, the entropy estimator that we developed turned out to be usable as forecaster as well, able to predict the chances of a packet arrival in the next slot.Analyzing the entropy fingerprints of various types of traffic confirmed that there was useful information in the packet arrival times of network flows that could be leveraged in a schedule based MAC protocol. Furthermore, the work presented us with a traffic forecaster that we could use to extract this information for use in such a MAC protocol.Following this work, we designed a medium access control protocol to embody the fusion of traffic forecasting with a schedule based access control mechanism. We called the protocol TRANSFORMA, which stands for TRAffic FORecasting Medium Access. TRANSFORMA was designed using the principle that the MAC layer should detect the properties of each flow transparently and adapt its level of service accordingly. TRANSFORMA attempts to do that by observing an application flow, learning its pattern, and forecasting the flows future behavior based on the observed one. In its current implementation, TRANSFORMAs forecaster examines the packet arrival process of each application flow and determines the corresponding per-flow inter-packet arrival times. It then uses this information to establish the flows medium access schedule. TRANSFORMA operates under the assumption that applications that place more stringent requirements, e.g., higher data rates and delay sensitivity have forecastable network usage patterns. The simulation results show that TRANSFORMA significantly improves on the delay performance of its predecessor, DYNAMMA.The final contribution of this work is a real-system implementation of TRANSFORMA. This fully functional network link enables experimentation with real application traffic, serves to validate our simulation results and demonstrates that the concepts embodied in TRANSFORMA are practical.
-------------------------------------

10131168_183 - 0.835442814494 - technology_and_computing
[model, area, land-use, urban]

Modeling Land Use and Transportation: An Interpretive Review for Growh Areas
Urban growth is taking new forms in recently urbanized or formerly suburban areas, characterized by low density, heavy dependence on automobile transportation, and multiple activity centers. In order to understand better such 'contemporary urban areas', researchers need land-use models that realistically capture the key features of such areas and that can handle detailed data sets.
      We review the literature on large-scale land-use modeling with this objective in mind. Characterizing the known models along several dimensions describing purpose, conceptual basis, mathematical content, and level of detail, we select models that are representative of the range of approaches taken. Six of these are reviewed in detail, and four others are discussed more briefly.
      We find that the existing literature forces one to choose between tractability and suitability for contemporary urban areas. The key omission in the tractable models is economies of agglomeration that would help explain the emergence of subcenters. Most tractable models also lack a dynamic structure suitable for handling rapid disequilibrium growth. Models that contain these two features are suitable for broad-brush computer simulation, but they cannot be calibrated with real disaggregated land-use data. This conclusion leads to some brief suggestions on directions for future work.
-------------------------------------

10137409_183 - 0.999990576388 - technology_and_computing
[memory, device, flash, material, chapter, bandgap, wide]

Simulation and Experiment of Wide Bandgap Material Based Nonvolatile Memory
Flash memory, primarily NAND Flash, is widely used in our daily life like mobile handsets, tablets, and laptabs to enterprise volume data storage. The huge market demand increases the market revenue from 10.6 billion dollars in 2005 to 25.1 billion dollars in 2011 and it is expected to continue the growth. The increasing demand of data transmission and data storage forms tremendous gap between the flash capacity and application demand. In order to catch the market, flash memory device has been scaled down over 18 years. Highly reliable, fast reading, programming and erasing speed, and lower cost are always pursued. In 1998, the feature size of floating gate memory is 0.7um and right now it has been shrunk to 20nm. There are fundamental issues that impede the further scaling down. Physical limitations include photolithography and small size structure formation; electrical limitation includes interference, capacitive coupling ratio, less number of electrons in the cell, and dielectric leakage. To break the limitation from the conventional floating gate memory, a novel wide bandgap material based flash memory was proposed. Device performance including electrostatic analysis, programming and retention was studied by small signal model. GaN based nonvolatile memory was fabricated and device performance was measured. Chapter 1 introduces the conventional floating gate memory and new technology development. The advantages and disadvantages of different technology have been summarized. Motivation of the wide bandgap material based memory has also been addressed. Chapter 2 describes the wide bandgap material based memory, the advantages of wide bandgap materials and their application in this novel memory device, and the potential how this novel memory device can fit with the mainstream CMOS VLSI technology. Chapter 3 is the device simulation part. It gives a detailed explanation of device function. Memory performance like programming and retention has been studied. Chapter 4 is the experiment study of GaN based flash memory. Device fabrication process flow has been introduced and characterization result has been analyzed. Chapter 5 is the summary.
-------------------------------------

10137738_183 - 0.983506448844 - technology_and_computing
[datum]

Ecoinformatics: supporting ecology as a data-intensive science
Ecology is evolving rapidly and increasingly changing into a more open, accountable, interdisciplinary, collaborative and data-intensive science. Discovering, integrating and analyzing massive amounts of heterogeneous data are central to ecology as researchers address complex ques- tions at scales from the gene to the biosphere. Ecoinfor- matics offers tools and approaches for managing ecological data and transforming the data into informa- tion and knowledge. Here, we review the state-of-the-art and recent advances in ecoinformatics that can benefit ecologists and environmental scientists as they tackle increasingly challenging questions that require volumi- nous amounts of data across disciplines and scales of space and time. We also highlight the challenges and opportunities that remain.
-------------------------------------

10135154_183 - 0.949266239676 - technology_and_computing
[query, problem, source, document, component, matching, answering, xml, subsequence, part]

Query Answering in Data Integration Systems
This dissertation focuses on several important problems in query answering over a single or multi-source database. The first part of this work focuses on the challenges faced in querying a distributed set of data sources. An Online Answering Systems for Integrated Sources (OASIS) is presented which considers source coverage, overlap, and cost to order source accesses such that answers are returned as soon as possible. The first component of OASIS is a fast and scalable method for estimating source overlaps. The second component utilizes overlap estimates and two ordering strategies (static and dynamic) to order source accesses. The last component applies several heuristics to select additional overlap statistics to compute with the goal of obtaining a better source ordering. The efficiency and effectiveness of the proposed system are demonstrated through an extensive experimental evaluation. While the first part of the dissertation focuses on multi-source query answering, the second part of the dissertation focuses on single-source query answering. First, a candidate document ordering strategy is presented followed by a query processing approach for XML documents and queries. The first component considers a candidate document ordering strategy to minimize the expected time to the first k matches. The optimization problem is considered for applications which contain document precedence constraints which restrict the order in which candidate documents must be processed. The second component presents a unified method for solving three important problems in XML structural matching, namely, Filtering, Query Processing, and Tuple-Extraction. The queries and XML documents are represented using a sequential encoding, referred to as Node Encoded Tree Sequences (NETS). The unified solution for the three problems is composed of two procedures, subsequence matching and structural matching, which can be executed concurrently or sequentially depending on the problem. The solution for subsequence matching is based on the dynamic programming recurrence relation for the Longest Common Subsequence (LCS) problem. For structural matching, a new necessary and sufficient condition is presented which provides a simple verification procedure. In addition to using a unified framework, (for easier implementation and maintenance), experimental results show that the proposed algorithms outperform state of the art approaches for the three XML processing problems.
-------------------------------------

10133529_183 - 0.999999315025 - technology_and_computing
[robot, mobile, sensor, localization, robotic, environment, approach, problem, large]

Mobile Robot Navigation With Low-Cost Sensors
Mobile robots are becoming ubiquitous and an essential part of our everyday lives.  They are increasingly taking their place in service-oriented applications including domestic and entertainment roles.  They open up many potential opportunities, but they also come with challenges in terms of their limited sensing capability and accuracy and minimal on-board computing resources.  In this dissertation, we address three fundamental problems in mobile robotics and demonstrate our approach to each of the problems with a mobile robot equipped with low-cost and low-end sensors.  The problems we consider are those of mobile robot calibration, mobile robot localization, and simultaneous localization and mapping.Motion and sensor models are crucial components in current algorithms for mobile robot localization and mapping.  We demonstrate how the parameters of both the motion and sensor models can be automatically estimated during normal robot operations via machine learning methods thereby eliminating the necessity of manually tuning these models through a laborious calibration process.  Mobile robot calibration is important especially for robots relying on cheap and less-accurate sensors.  Results from real-world robotic experiments with a robot equipped with wheel encoders and sonar sensors are presented that show the effectiveness of the estimation approach.Monocular vision has long been regarded as an attractive sensor for the localization of a mobile robot.  In this dissertation, we present a particle filtering approach to real-time pose estimation for a small-scale indoor mobile robot equipped with wheel encoders for its odometry and aided by a standard perspective camera.  Vision is used for detecting naturally occurring static three-dimensional point features or landmarks from the  environment and utilizing the information for correcting the pose as suggested by the odometry.  We validate the effectiveness of the particle filter approach extensively with both simulations as well as real-world data and compare its performance against that of the extended Kalman filter.Simultaneous localization and mapping (SLAM) is a well-studied problem in mobile robotics and the majority of the existing techniques rely on the use of accurate and dense measurements provided by laser rangefinders to correctly localize the robot and produce accurate and detailed maps of complex environments.  In this dissertation, we present our approach to SLAM with low-cost but noisy and sparse sonar sensors in large indoor environments involving large loops.  Results from robotic experiments demonstrate that it is possible to produce good maps of large indoor environments with large loops despite the inherent limitations of sonar sensors.
-------------------------------------

10132823_183 - 0.999892979507 - technology_and_computing
[model, design, pavement, datum, cost, aasho, optimal]

Effect of Performance Model Accuracy on Optimal Pavement Design
In the first part of this paper, an analysis of the data collected during the American Association of State Highway Officials (AASHO) Road Test, based on probabilistic duration modeling techniques, is presented. Duration techniques enable the stochastic nature of pavement failure time to be evaluated as well as censored data to be incorporated in the statistical estimation of the model parameters. The second part of this paper presents the use of economic optimization principles for determining the optimal design of flexible pavements. We study the effect of deterioration model accuracy on optimal design and lifecycle costs, by comparing three models. The first is a simple regression model developed by the AASHO, which forms the basis of design standards in use today. The second is a regression model that was developed with the same AASHO data set, but that includes a correction for data censoring. The third model is the probabilistic model developed in the first part of this paper. The results show that the AASHO model, when used as an input to lifecycle cost minimization, produces a pavement structural number that is lower than that produced by using the other two deterioration models. This results in shorter pavement lives and higher costs due to more frequent resurfacing. The savings in lifecycle cost accrued by using optimal structural number are shown to be quite significant, offering a sound basis for revising current design practices.
-------------------------------------

10138823_183 - 0.927728024513 - technology_and_computing
[density, function, behavior, estimator]

ON THE BEHAVIOR OF NONPARAMETRIC DENSITY AND SPECTRAL DENSITY ESTIMATORS AT ZERO POINTS OF THEIR SUPPORT
The asymptotic behavior of nonparametric estimators of the probability density function of an i.i.d. sample and of the spectral density function of a stationary time series have been studied in some detail in the last 50-60 years. Nevertheless, an open problem remains to date, namely the behavior of the estimator when the target function happens to vanish at the point of interest. In the paper at hand we fill this gap, and show that asymptotic normality still holds true but with a super-efficient rate of convergence. We also provide two possible applications where these new results can be found useful in practice.
-------------------------------------

10135627_183 - 0.995094640099 - technology_and_computing
[array, aao, application, nanotube, pattern, oxide, substrate, aluminum, copolymer, s4vp]

Preparations, properties, and applications of periodic nano arrays using anodized aluminum oxide and di-block copolymer
Self-ordered arrangements observed in various materials systems such as anodic aluminum oxide, polystyrene nanoparticles, and block copolymer are of great interest in terms of providing new opportunities in nanofabrication field where lithographic techniques are broadly used in general. Investigations on self-assembled nano arrays to understand how to obtain periodic nano arrays in an efficient yet inexpensive way, and how to realize advanced material and device systems thereof, can lead to significant impacts on science and technology for many forefront device applications. In this thesis, various aspects of periodic nano-arrays have been discussed including novel preparations, properties and applications of anodized aluminum oxide (AAO) and PS-b-P4VP (S4VP) di- block copolymer self-assembly. First, long-range ordered AAO arrays have been demonstrated. Nanoimprint lithography (NIL) process allowed a faithful pattern transfer of the imprint mold pattern onto Al thin film, and interesting self-healing and pattern tripling phenomena were observed, which could be applicable towards fabrication of the NIL master mold having highly dense pattern over large area, useful for fabrication of a large-area substrate for predictable positioning of arrayed devices. Second, S4VP diblock copolymer self-assembly and S4VP directed AAO self -assembly have been demonstrated in the Al thin film on Si substrate. Such a novel combination of two dissimilar self -assembly techniques demonstrated a potential as a versatile tool for nanopatterning formation on a Si substrate, capable of being integrated into Si process technology. As exemplary applications, vertically aligned Ni nanowires have been synthesized into an S4VP-guided AAO membrane on a Si substrate in addition to anti-dot structured Co/Pdn magnetic multilayer using S4VP self assembly. Third, a highly hexagonally ordered, vertically parallel aluminum oxide nanotube array was successfully fabricated via hard anodization technique. The Al₂O₃ nanotube arrays so fabricated exhibit a uniform and reproducible dimension, and a quite high aspect ratio of greater than ̃1,000. Such high-aspect-ratio, mechanically robust, large-surface-area nanotube array structure can be useful for many technical applications. As a potential application in biomedical research, drug storage/ controlled drug release from such AAO nanotubes was investigated, and the advantageous potential of using AAO nanotubes for biological implant surface coatings alternative to TiO₂ nanotubes has been discussed
-------------------------------------

10133538_183 - 0.994071737169 - technology_and_computing
[surface, model, motion, range, scan, alignment, datum, multiple, subject]

Reconstruction of dynamic articulated 3D models from range scans
Our vision is to enable efficient acquisition and synthesis of highly detailed 3D surface models that are also easy to animate in a plausible and realistic way. The state-of-the-art surface acquisition technology is range scanning, which can measure surface geometry with a high degree of accuracy and speed. However, the output is only a partial view of the surface that has much missing data, and there is no tracking of the surface motion in the case of a moving subject. To reconstruct a complete model of the subject, we must align multiple range scans taken from different times and viewpoints to fill in the missing data and track the motion of the surface. At the same time, we would like to fit a reduced deformable model that expresses the surface motion in terms of a few intuitive parameters. In this dissertation, we develop algorithms to process and align multiple range scans of a moving articulated subject. Our algorithms can automatically align multiple scans to a common pose, thus reconstructing the full geometry of an articulated subject along with a model of its motion. Our methods perform this alignment in a completely unsupervised way: without markers, a template, or a user-defined segmentation of the surface. A key contribution is the use of discrete optimization techniques to automatically estimate the articulated structure of the surface based on its motion. First, we describe a method to align a pair of 3D surfaces that is robust to large motions and much missing data. This algorithm samples rigid transformations between the surfaces and performs an alignment by optimizing an assignment of the transformations to the surface. Its robustness to large motions makes it useful for initializing a registration. Next, we present a technique to automatically fit an articulated surface motion model to a pair of range scans. We efficiently solve for the transformations and weights of this model by repeatedly estimating them in alternating fashion. The key benefit of this approach is that the solved model parameters can be used to easily and intuitively edit the pose of the scanned geometry. Finally, we improve and combine these two approaches to automatically reconstruct an articulated 3D model from multiple range scans. We reduce alignment error by simultaneously solving for the alignment of all input scans. We demonstrate that this method can reconstruct a variety of poseable, articulated 3D models from partial surface data acquired by a range scanner
-------------------------------------

10129819_178 - 0.916025132876 - technology_and_computing
[system, environmental, architectural, mechanical, control, curriculum, bibliography, approach]

An approach to environmental control and mechanical systems in an architectural curriculum.
Bibliography: leaves 52-53.
-------------------------------------

10134193_183 - 0.99999623524 - technology_and_computing
[datum, integration, booly, system, analysis]

Booly : a new data integration platform for systems biology
Data integration continues to remain a difficult and escalating problem in bioinformatics. The goal of this thesis is to develop a data integration platform that addresses two recurring issues in current data integration methods: 1) the issue of naming and identity and 2) the barrier of entry for general researchers to contribute and perform analysis of data. We have developed a web tool and warehousing system, Booly, that features a simple yet flexible data model coupled with the ability to perform powerful comparative analysis, including the use of Boolean logic to merge datasets together, and an integrated aliasing system to decipher differing names of the same gene or protein. We applied Booly across heterogeneous data sources and identified genes useful in comparing avian and mammalian brain architecture, which were validated by comprehensive in situ hybridization experiments. The Booly paradigm for data storage and analysis should facilitate integration between disparate biological and medical fields and result in novel discoveries that can then be validated experimentally
-------------------------------------

10134495_183 - 0.999893358563 - technology_and_computing
[fire, detection, system, technology]

Real-time fire detection in low quality video
For over fifty years, simple smoke and heat sensors have been the primary means of automated fire detection. We are now at the point where computer processing power is cheap enough and machine vision technology is sophisticated enough for a new generation of automated fire detection systems : video-based fire detection (VBFD). While current smoke and fire detection technology has proven to be reliable and effective, VBFD technology promises to go where existing systems can't and to detect fires faster than its venerable predecessors ever could. This thesis explores a few methods for achieving real-time video-based fire detection in low quality data. Assuming a stationary source camera, we describe an algorithm that uses a support vector machine to classify short, targeted video sequences as fire/non-fire. The algorithm achieves a classification rate of 96.0% on a holdout set of real world data. Furthermore, the system is robust with respect to the distance from the fire source, works day or night, and only requires the processing power of a common desktop computer
-------------------------------------

10133570_183 - 0.999984811192 - technology_and_computing
[technology, operator, access, network, challenge, user, performance, composition, resource]

Operator and radio resource sharing in multi-carrier environments
A key challenge that the mobile networking world is facing is seamless network composition. In spite of a wide range of available access technologies, the operator agreements prevent users to freely access these networks. With seamless network composition, users can gain better quality of service, and operators can provision less bandwidth by sharing their resources. Al-Fares et al. previously proposed a resource reservation framework that implements this network composition concept. We extended their work by analyzing the benefits of operator cooperation in a realistic environment using their framework. In our scenario, we tried to leverage the difference in burstiness in small timescales to shed the peak usage of one operator onto another. We utilize Swing to reproduce traffic based on the real 3G trace data. The results show that even when the operator's capacities are limited, the cooperation can help maintain quality of service for most sessions. Nevertheless, even if all access technologies can be accessed seamlessly, more challenges appear to be which technology users should use and which combination of access technologies is appropriate to deploy. To provide more insight to these challenges, we investigated the performance delivered by various kinds of wireless technologies. We utilized Swing's abilities to tune the topology parameters to reflect different access technologies. Our results show that WiFi can provide significantly better performance compared to the 3G. On the other hand, we expect the performance of LTE to be comparable to or even better than WiFi
-------------------------------------

10130374_178 - 0.919043848246 - technology_and_computing
[bureaucracy, system, public, reference, bibliographical, service, leaf, change]

System change in a public service bureaucracy
Includes bibliographical references (leaves 285-290).
-------------------------------------

10130331_178 - 0.999717187275 - technology_and_computing
[system, reference, model, bibliographical, three-dimensional, leaf, graphic, interactive]

System for constructing three-dimensional interactive graphic models
Includes bibliographical references (leaves 73-81).
-------------------------------------

10132994_183 - 0.999999542772 - technology_and_computing
[hardware, point, track, feature, fast, software, tracking, system, selection, visual]

Fast Visual Feature Selection and Tracking in a Hybrid Reconfigurable Architecture
We present a fast visual feature tracking system which takes advantage of dedicated hardware to perform the computationally intensive step of selection. A software system uses the output of the hardware selector to develop tracks using filtering and data association techniques, and image-based validation. By using separate, custom hardware to select hundreds of points per frame then associating these points to tracks in software, we achieve greater than 60Hz real-time tracking.
-------------------------------------

10130045_178 - 0.999581455385 - technology_and_computing
[panel, fourier, transform, order, reference, high, fast, method, bibliographical, leaf]

Precorrected Fast Fourier Transform accelerated high order panel method
Includes bibliographical references (leaves 197-201).
-------------------------------------

10138222_183 - 0.998923037724 - technology_and_computing
[lighting, user, level, illuminance, light, service, subject, system, lantern, survey]

Illumination Sufficiency Survey Techniques: In-situ Measurements of Lighting System Performance and a User Preference Survey for Illuminance in an Off-Grid, African Setting
Efforts to promote rechargeable electric lighting as a replacement for fuel-based light sources in developing countries are typically predicated on the notion that lighting service levels can be maintained or improved while reducing the costs and environmental impacts of existing practices.  However, the extremely low incomes of those who depend on fuel-based lighting create a need to balance the hypothetically possible or desirable levels of light with those that are sufficient and affordable. 

In a pilot study of four night vendors in Kenya, we document a field technique we developed to simultaneously measure the effectiveness of lighting service provided by a lighting system and conduct a survey of lighting service demand by end-users. We took gridded illuminance measurements across each vendor's working and selling area, with users indicating the sufficiency of light at each point. User light sources included a mix of kerosene-fueled hurricane lanterns, pressure lamps, and LED lanterns.

We observed illuminance levels ranging from just above zero to 150 lux. The LED systems markedly improved the lighting service levels over those provided by kerosene-fueled hurricane lanterns. Users reported that the minimum acceptable threshold was about 2 lux.  The results also indicated that the LED lamps in use by the subjects did not always provide sufficient illumination over the desired retail areas. Our sample size is much too small, however, to reach any conclusions about requirements in the broader population. Given the small number of subjects and very specific type of user, our results should be regarded as indicative rather than conclusive.  We recommend replicating the method at larger scales and across a variety of user types and contexts.  Policymakers should revisit the subject of recommended illuminance levels regularly as LED technology advances and the price/service balance point evolves.
-------------------------------------

10129912_178 - 0.999715691928 - technology_and_computing
[dc]

Simulation of a DC to DC power conversion module for the all-electric ship
Massachusetts Institute of Technology. Sea Grant College Program (NA10OAR4170086)
-------------------------------------

10133135_183 - 0.997288784614 - technology_and_computing
[speed, link, average, hourly, emission, lane, different]

Examining the Effects of Variability in Average Link Speeds on Estimated Mobile Source Emissions and Air Quality
Running Stabilized Emissions from vehicle exhausts are estimated by combining travel activity quantified as vehicle miles of travel (VMT) or vehicle hours of travel (VHT) with emissions factors which are adjusted for speeds on facility links. These speeds which are used for adjusting emissions factors are averaged for fixed periods (typically for one hour intervals) and across the lanes of multi-lane links. In real world traffic conditions, however, average speeds are variable for higher time resolutions and across lanes. Incorporating the variability in average speeds during calculation will result in different magnitudes of estimated running stabilized emissions for a given period of time and a facility link. Lane volume and occupancy measurements from 1376 single loop detector stations on 830 freeway links in Los Angeles were used to estimate flow rates, hourly average link speeds, 15-minute average link speeds and hourly average lane speeds. Then, these flow rates and average speeds were used to estimate hourly link CO, CO2, NOx, PMx  and TOG emissions and hourly gridded emissions based on different average speed resolutions for the summer of 1997. This study statistically examines the hourly differences in running stabilized emissions when 15-minute link and hourly lane average speeds are used in contrast to hourly link average speeds for running stabilized emissions estimation. Moreover, effects of speed variability on regional air quality are evaluated by examining the differences in estimated gridded ozone (O3) and fine particulate matter (PM25) concentrations based on different gridded emissions inventories (estimated using hourly link, 15-minute link and hourly lane average speeds). The results show that the magnitudes of estimated hourly link running stabilized emissions are different especially when hourly lane average speeds are used in contrast to the hourly link average speeds.
      Estimated O3 and PM25 concentrations also differ, however, the differences in these estimated secondary pollutant concentrations show different patterns when compared to the differences in estimated emissions.
-------------------------------------

10133903_183 - 0.97628178119 - technology_and_computing
[rotor, brake, design, disc, mountain]

Brake rotor design and comparison using finite element analysis : an investigation in topology optimization
Disc brake technology used for mountain bikes, and mountain bike technology in general, has improved significantly as the sport of mountain biking has evolved. Disc brakes on bicycles are relatively new compared to their use on other vehicles. Rotor design is varied for rotors of the same intended use for many companies; some still use the same initial rotor designs that were introduced over a decade ago. Through the finite element analysis and optimization process, understanding the difficulties of designing disc brake rotors and the validity of certain design trends in current disc brake rotors is pursued. Additionally, this investigation uses finite element methods to design and optimize a mountain bike disc brake rotor using topology optimization. More specifically, the goal is to design a lighter rotor that maintains similar structural performance as rotors that are currently commercially available. The new rotor design was compared to two existing rotor geometries. The strength of the new rotor is comparable to existing rotors A and B. Weight improvements of 14.3% and 12.4% over rotor A and Rotor B, respectively, are realized
-------------------------------------

10133946_183 - 0.999701233117 - technology_and_computing
[query, algorithm, problem, tree, datum, node, optimal]

Data Analysis and Query Processing in Wireless Sensor Networks
This work minimizes the cost of answering queries in wireless sensor networks. To answer a query, data generated by the sensors needs to be collected and processed. We optimize the cost by constructing sophisticated query trees. Queries are divided into two categories: queries that need data from all the nodes in the network and queries that need data from a subset of nodes only.For the first type of queries we propose a distributed algorithm to construct a near-optimal balanced communication tree with minimum overhead. Such a tree has inherently minimal number of collisions during query execution, and therefore avoids numerous retransmissions. Our algorithm outperforms previous work both in tree construction overhead and in tree balance.For the second type of queries we present methods for constructing query trees to route and perform in-network processing of data. First, we focus on snapshot queries and show that minimizing the problem is NP-hard. We propose a dynamic programming algorithm to compute the optimal solution for small problem instances. We also propose a low complexity, approximate, heuristic algorithm for solving larger problem instances efficiently. Finally, we adapt the Fermat point problem (1-median problem) for a weighted graph, and propose a centralized solution that is used as heuristic in the above algorithms.Dealing with continuous queries of the second category, we present an optimal distributed algorithm to adapt the placement of a single operator. Our parameter-free algorithm finds the optimal node to host the operator with minimum communication cost overhead. Three ideas, proposed here, make this feature possible: 1) identifying the special, and most frequent case, where no flooding is needed, otherwise 2) limitation of the neighborhood to be flooded and 3) variable speed flooding and eves-dropping. To our knowledge this is the first optimal and distributed algorithm to solve the 1-median (Fermat node) problem. In our experiments we show that for the rest of cases our algorithm saves 30\%-80\% of the energy compared to previously proposed techniques.
-------------------------------------

10139946_183 - 0.816286103318 - technology_and_computing
[window, air, quality, building, comfort, occupant]

Adaptive Comfort in Mixed-Mode Buildings: Research Support Facility, National Renewable Energy Lab
The RSF is performing well for the occupants in terms of indoor environmental quality (IEQ), particularly with respect to air quality. The building scores in the 90th percentile of the buildings in the CBE database for thermal comfort and air quality satisfaction. Nevertheless, less than 80% of the building’s occupants are satisfied with temperature, air movement, and air quality, so there is room for improvement to meet the code standard.
      The new survey methods allowed us to explore how occupants use and view their windows. The windows were rated extremely highly in terms of accessibility, usability, and responsiveness; however, they are not used as often as expected. Only 6% of window users adjust their windows on a daily basis, compared to 38% weekly and 41% monthly. The most frequently cited reason for opening windows is fresh air, followed by wanting to feel cooler, and then the desire to increase air movement.  Other reasons for opening and closing windows include the desire to save energy or increase the connection with the outdoors.  From this it is clear that people operate windows for more than just thermal comfort.
-------------------------------------

10132458_183 - 0.950257124898 - technology_and_computing
[toll, model, jurisdiction, road, cost, tax, local, collection, network]

On Whom the Toll Falls: A Model of Network Financing
This dissertation examines why and how jurisdictions choose to finance their roads. The systematic causes of revenue choice are explored qualitatively by examining the history of turnpikes. The question is approached analytically by employing game theory to model revenue choice on a long road. The road is covered by a series of jurisdictions seeking to maximize local welfare. Jurisdictions are responsible for building and maintaining the local network. Complexity arises because local network users may not be local residents, and local residents may use non-local networks. Key factors posited to explain the choice of revenue mechanism include the length of trips using the road, the size of the governing jurisdiction, the degree of excludability, and the transaction costs of toll collection. These factors dictate the size and scope of the free rider problem. It is hypothesized that smaller jurisdictions and lower collection costs favor tolling policies over taxes.
      The analytical model is operationalized by assuming jurisdictions have two decisions: the strategic decision to tax or toll, and the tactical decision of setting the rate of tax or toll. Models of user demand as a function of trip distance and monetary cost and of network costs as a function of traffic flow and the number of toll collections are specified. The values of the constants and coefficients of the model are developed from recent cost literature and the estimation of a model of collection costs from California Toll Bridge data.
      The model is applied to evaluate the dissertation's hypotheses. The application evaluates the welfare implications of a jurisdiction and its neighbors imposing general tax, cordon toll, odometer tax, or perfect toll policies. Sensitivity tests of the model under alternative behavioral assumptions, and with varying model coefficients are conducted. Finally, policy implications from the analysis are drawn. The general trends which bode well for road pricing (electronic toll collection (ETC), decentralization, advanced infrastructure, privatization, and federal rules) are established. Possible scenarios for three cases are presented: deploying ETC and building new toll roads, and converting free roads.
-------------------------------------

10139530_183 - 0.991866434886 - technology_and_computing
[cloud, performance, application, different, computing, environment, virtualized, result]

I/O Performance of Virtualized Cloud Environments
The scientific community is exploring the suitability of cloud infrastructure to handle High Performance Computing (HPC) applications. The goal of Magellan, a project funded through DOE ASCR, is to investigate the potential role of cloud computing to address the computing needs of the Department of Energy?s Office of Science, especially for mid-range
computing and data-intensive applications which are not served through existing DOE centers today. Prior work has shown that applications with significant communication or
I/O tend to perform poorly in virtualized cloud environments. However, there is a limited understanding of the I/O characteristics in virtualized cloud environments. This paper will present our results in benchmarking the I/O performance over different cloud and HPC platforms to identify the major bottlenecks in existing infrastructure. We compare the I/O performance using IOR benchmark on two cloud platforms - Amazon and Magellan. We analyze the performance of different storage options available, different instance types in multiple availability zones. Finally, we perform large-scale tests in order to analyze the variability in the I/O patterns over time and region. Our results highlight the overhead and variability in I/O performance on both public and private cloud solutions. Our results will
help applications decide between the different storage options enabling applications to make effective choices.
-------------------------------------

10133644_183 - 0.731464937359 - technology_and_computing
[silicon, porous, sensor, chemical, response, surface, method]

Manipulation of surface chemistry and nanostructure in porous silicon-based chemical sensors
An ideal environmental sensor has zero baseline drift, a fast response time, is sensitive and selective to the analyte of interest, and has the ability to be miniaturized. Porous silicon is an attractive material for sensing applications due to its high surface area, readily modified surface chemistry, and optical signal transduction capability. This thesis describes the construction and chemical modification of porous silicon photonic crystals for use in chemical sensing. The specific aims of this work were to develop new methods to maximize sensor stability, remove background signal interference, and to induce chemical specificity into the sensor. This thesis begins with an introduction on porous silicon preparation methods and its sensing mechanisms, as well as different chemicals and biomolecules that have been detected and their detection limit. We show a multitude of chemical modifications of the porous silicon surface that produce long-term stability and induce analyte class specificity to the sensor. Next, a method to remove interfering effects of changing relative humidity from the response of porous silicon is developed. Two porous silicon films are separately etched and chemically modified into silicon, one on top of the other. The response of each film is measured simultaneously. Each film has a characteristic response towards water vapor. The effect of changing humidity can then be accounted for by calculating the weighted difference between the two layer responses. Thereby, building an internal reference into the sensor. A second type of internal spectral reference to eliminate artifacts associated with varying angle of incidence of an optical probing detector relative to a one-dimensional photonic crystal sensor was developed. The chemically non-responsive internal spectral reference was built into a photonic crystal sensor chemically modified to respond specifically to hydrofluoric acid. Lastly, a simple and inexpensive method to etch patterns into porous silicon was developed. A masking layer was imprinted onto a silicon surface through microcontact printing, followed by an electrochemical etch. The imprinted residue reduces the etching rate of the bulk silicon below, while unmasked silicon etches normally. The resulting inhomogeneous etching rate of silicon transfers the pattern into the bulk silicon
-------------------------------------

10137229_183 - 0.990389508436 - technology_and_computing
[architecture, loss]

Beyond Mere Containment:  The Neapolitan Treasury Chapel of San Gennaro and the Matter of Materials
This paper is a consideration of problems encountered in attempting an art historical analysis of the complex baroque forms of architecture in seventeenth- and eighteenth-century Naples, specifically when confronted on the one hand by the rather bald, roughly contemporaneous accounts thereof and, on the other and more especially, by the thrilling experience of entering these buildings today -- experiences that leave one overwhelmed and at a loss, at a loss for words sufficient to them and at a loss in their regard. To look at these buildings today in terms of their affective material productivity, even if they can only be articulated incompletely, is to ask historians to undertake the kind of visual work that they are seldom accustomed to. It means staying the customary hastiness that sees architecture as mere instantiation of idea, and instead – while resisting the temptation to interpret architecture as merely the sum of its parts -- requires a willingness to inquire into the materiality of aspects of architecture and objects which yield ‘nothing’ to see (such as dark areas within sculpture, non-figurative passages within architecture, the shine of silver, illegible letters of unknowable alphabets). Simultaneously we need also to widen our usual scope of vision to restore to architecture its affective elements that make it work. This is to require the mobility of architecture’s affect to engage us fully and temporally, rather than to dissect architecture into a “document”of a “social,” “political,” “cultural,” or “material” history, supposedly capable of embracing it fully, but to which it is, in fact, subordinated.
-------------------------------------

10135510_183 - 0.999991902482 - technology_and_computing
[robot, map, system, value, area]

Multi Robot Cooperation Based on Auction Theory With a Value Map
The goal is to create a robotic search party that autonomously coordinates tasks to efficiently find the target. The induced coordination minimizes the communication, computation, and distance traveled while maximizing area covered. Each robot in our system has a unique combination of sensors and abilities. These include movement speed, localization accuracy, and computational abilities. The Market based algorithm allows the robot to achieve the above goals effectively. Allowing each robot to exchange tasks in an effort to maximize their unique utilities. With the addition of a value map as a representation of what areas have been observed, the robots have a driving force to move to unseen locations due to the increased value. Each robot maintains its own version of the map until it encounters another robot and share value maps. The combination allows each robot to understand what areas have been observed, hence limiting possible locations to move to. When robots meet, the algorithm pushes them away with the second path method. The robots divert themselves in an effort to maximize the area searched. Our system is distributed with no central agent with full knowledge or control of any system. Reducing the effect of robot failure to the system.
-------------------------------------

10135553_183 - 0.998847494664 - technology_and_computing
[webpage, feature, content, model, classification, url, learning]

Detecting malicious Webpages using content based classication
In this thesis, we propose a supervised learning approach to detect malicious Webpages. We use features from the URL, textual content, structural tags, page links and visual appearance of a Webpage. First, we demonstrate an offline classification model using batch learning algorithms and evaluate the benefit of including each of the feature types. Then we illustrate an online classification model using online learning algorithms on a larger dataset. For both these models, we use a live feed of labeled data collected from a large webmail provider. For a base rate of 66%, we achieve 98% accuracy using a combination of URL and Webpage content features for classification. We observe that incorporating Webpage content features in addition to the URL features, reduces the error rate by about 50%
-------------------------------------

10175441_189 - 0.899235316081 - technology_and_computing
[method, computational, soma, user, meshfree]

Sequentially Optimized Meshfree Approximation as a New Computation Fluid Dynamics Method
This thesis presents the Sequentially Optimized Meshfree Approximation (SOMA) method, a new and powerful Computational Fluid Dynamics (CFD) solver. While standard computational methods can be faster and cheaper that physical experimentation, both in cost and work time, these methods do have some time and user interaction overhead which SOMA eliminates. As a meshfree method which could use adaptive domain refinement methods, SOMA avoids the need for user generated and/or analyzed grids, volumes, and meshes. Incremental building of a feed-forward artificial neural network through machine learning to solve the flow problem significantly reduces user interaction and reduces computational cost. This is done by avoiding the creation and inversion of possibly dense block diagonal matrices and by focusing computational work on regions where the flow changes and ignoring regions where no changes occur.
-------------------------------------

10134925_183 - 0.999998449091 - technology_and_computing
[channel, performance, system, uplink, power, noise]

Performance Limitations of Linear Systems over Additive White Noise Channels
This thesis develops a framework to address the performance limits of feedback control systems with communication constraints modeled by additive white noise channels. By searching for the fundamental bounds on the control performance, we explore the relationship between the known limitations caused by the intrinsic properties of linear control systems and the characteristics of the communication channels. We analyze multiple-input multiple-output systems with the channel placed at either the uplink or downlink. We also study the stabilization conditions for single-input single-output systems when both channels are present in the closed loop.  For systems with uplink channels, we derive explicitly the analytical expressions for the necessary and sufficient conditions for stabilization and the best achievable performance under the channel input power constraint. The optimal tracking performance exhibits clear dependence on the power constraint and noise levels of the channel, and additionally on the unstable poles and nonminimum phase zeros of the plant. For systems with downlink channels, we derive a lower bound for the performance that incorporates the plant gain in the entire frequency range. Moreover, we use and optimize scaling as a method of channel compensation to exploit the channel and deal with the white noise. This simple strategy is shown to significantly improve the tracking performance. Also, we attempt to discover the optimal power allocation for each of the uplink parallel channels to achieve the best tracking performance. It is shown that, the optimal strategy is to allocate more power to a more problematic channel, in contrast to the widely-known ``water-filling'' solution, which is to maximize the capacity. Lastly, for first-order systems controlled over both uplink and downlink channels, we analyze the achievable region of the signal-to-noise ratios of the channels for stabilizability.
-------------------------------------

10139187_183 - 0.999995326759 - technology_and_computing
[algorithm, method, simulation, design, gpu, software, circuit, mc, parallel, performance]

Parallel and Statistical Analysis and Modeling of Nanometer VLSI Systems
Electronic design automation (EDA) is an important part of the integrated circuit (IC) industry, and has been evolving together with design and fabrication technologies. This evolution is reflected in both algorithm perspective and software implementation perspective. Innovative algorithms delivers accurate and reliable results in shorter computation time, and thus saves human resource and R&D cost. Smartly designed software can utilize hardware resources efficiently and maximize computing performance.However, EDA is now facing many complicated cases in the current VLSI technology. As integration scales to the sub-90~nm regime, the performance of ICs is becoming less predictable. Different sources of variations are caused from manufacturing process and these variations will finally end up with parametric yield loss. To deal with yield loss, efficient algorithms are required to accurately predict the performance of a circuit at the design stage. Unfortunately, given the high complexity of VLSI systems, software tools with traditional algorithm and implementation suffer from the ``curse of dimensionality'' problem. For instance, Monte Carlo (MC) method, which is the most trustworthy way to capture statistical information of a design, becomes inefficient as a large number of samplings are needed for an accurate analysis of the variations of the circuit response. Also, many verifications require transient simulation or frequency domain simulation of the full-chip design in order to guarantee an optimized product. Typical power grid has a tremendous size of over billion nodes, and takes several days for traditional transient simulation to calculate time domain response. It is a strong trend to use parallel computing such asmulti-threaded CPU and general purpose GPU.The objective of this thesis is to study the tough issues in these aforementioned simulations and discuss our algorithm and software solutions to reduce the computation cost without hurting the accuracy of the results. We also take the benefits of modern multi-core and many-core computer architectures, such as multi-core CPU and general purpose GPU, to gain speed in our simulation. The characteristics of computational independency in MC simulation is exploited and hence we have developed a GPU parallel Monte Carlo analysis based on a symbolic technique. Our parallel MC of circuit transfer functions has been verified using statistics extracted from classical MC, and the proposed method is proved to be effective. We also study an optimization based method to derive the performance bounds as a non-Monte Carlo solution. The bounds from this method is accurate without over-conservativeness. To accelerate linear algebra operations in equation solveing tasks,we apply GPU on fine-grained tasks in GMRES solver and attain impressive speedup over traditional CPU method. The management of different levels of GPU resources, such as thread organization and memory assignment, are discussed so that the data intensive feature of GPU can be fully rendered and its weakness like high latency be well hidden by its superb data throughput. All algorithms and implementations are demonstrated with representative numerical experiments and thorough comparisons among different methods and platforms.
-------------------------------------

10137006_183 - 0.999985628742 - technology_and_computing
[datum, storage, ability]

Management of High-Volume Real-Time Streaming Data in Transient Environments
In an information-driven world, the ability to capture and store data in real time is of the utmost importance.  The scope and intent of such data capture, however, varies widely.  Individuals record television programs for later viewing, governments maintain vast sensor networks to warn against calamity, scientists conduct experiments requiring immense data collection, and automated monitoring tools supervise a host of processes which human hands rarely touch.  All such tasks have the same basic requirements --- guaranteed capture, management, storage, and analysis of streaming real-time data --- but with greatly differing parameters.  Our ability to process and interpret data has grown faster than our ability to store and manage it, a characteristic which now hinders our ability to exploit it.The work presented in this dissertation demonstrates a means of integrating data management with the physical storage layer in order to gain superior performance not otherwise achievable.  By fusing a close understanding of the disk hardware with the necessary components of a high-performance storage system, a unique method of data handling is constructed.  This approach allows for hard performance guarantees and quality of service regulation at near-maximum hardware capabilities in a transient data environment for indefinite periods of time.  The core storage system is made to understand the data as more than just a stream of bytes, uniting indexing and query capabilities into basic operations.  All such gains are fully compatible with the accoutrements of a large-scale storage system, such as reliability and control mechanisms, which results in a fully viable new storage architecture.
-------------------------------------

10136032_183 - 0.999987731109 - technology_and_computing
[fastener, connector, modeling, model]

Simplified modeling methods for mechanically fastened connections in flight structures
Simplified modeling and analyses methods for aerospace connectors have been around for the better part of 30 years, but continue to evolve with technology. For the preliminary design process, the modeling of individual fasteners is time consuming and unforgiving, as the fastener sizes and locations often change during the process, necessitating a new model each iteration. Further simplifying the design process introduces a so called connector pad, which acts as a representative for entire groups of fasteners within a finite element model. Expanding upon previous developments from the last quarter century, this new method creates small groups of solid elements that are able to accurately represent any number of fasteners that exist within that region. Using FEMAP and NX NASTRAN to validate extensive linear displacement test data, this connector pad can be seen as another successful way to model mechanical connectors in flight structures. The success of the model lies strictly in linear displacements, however. Stresses, strengths, and margins of safety are not considered in this Thesis, though it is possible to extract such data. A group of specimens, consisting of varying state of the art connector types such as Hi-Lok fasteners, and ranging in connection density from two to eight fasteners, was tested axially within the linear elastic range. This data was compared to several different computer based fastener modeling techniques, such as using springs, beams, solids, or the new connector pad to represent the fastener. Not only does this study work to further validate the established techniques, but it also gives rise to the possibilities and advantages of a new, more simplified approach to modeling mechanical fasteners in aircraft
-------------------------------------

10129973_178 - 0.999306385538 - technology_and_computing
[problem, network, reference, bibliographical, optimization, connectivity]

Optimization problems in network connectivity
Includes bibliographical references (p. 115-120).
-------------------------------------

10135602_183 - 0.999988165163 - technology_and_computing
[server, system, job, parallel, control, class, traffic, heavy, policy, buffer]

On dynamic scheduling of a parallel server system with certain graph structure
We consider a problem of dynamic scheduling for a parallel server system. This system consists of finitely many infinite capacity buffers (classes) for holding incoming jobs awaiting service and finitely many non-identical servers working in parallel. Jobs within each buffer are served on a first-in-first-out basis. Each job requires a single service before it leaves the system. Each server can work on at most one job at a time, but it may be capable of processing several different classes of jobs over time, and it may suspend service of a job to work on a job of another class. Jobs of a given class incur holding costs at a rate proportional to the number of jobs in the class at each instant of time. The system manager seeks to minimize a cumulative discounted holding cost by dynamically scheduling waiting jobs to available servers. Following a method introduced by Harrison, for this parallel server system in heavy traffic we approximate the scheduling control problem by a Brownian control problem (BCP), which can be reduced to an Equivalent Workload Formulation (EWF). We first prove that the server-buffer graph, consisting of servers and buffers linked by basic activities, is a forest of trees. Then we give sufficient conditions for a least control process to be the optimal solution of the EWF. Under these conditions, we propose a continuous review threshold-type control policy that exploits partial pooling of servers. We conjecture that this policy is asymptotically optimal for the original parallel server system in the heavy traffic limit. To illustrate the solution of the EWF, and our proposed control policy for the original network we give a three buffer, three server example. We prove that this control policy is asymptotically optimal for this example in the heavy traffic limit. This is the first instance of a proof of asymptotic optimality for a parallel server system with partial pooling is the usual heavy traffic regime
-------------------------------------

10135343_183 - 0.914714806919 - technology_and_computing
[system, interaction, chapter, heisenberg, lattice, domain, study, wall, kagome, energy]

Frustrated Magnetism in Low-Dimensional Lattices
In this dissertation we present the results of a theoretical investigation of spin models on two-dimensional and quasi one-dimensional lattices, all unified under the concept of quantum frustrated antiferromagnetism, and all discussing various aspects of the antiferromagnetic Heisenberg model on the kagom&eacute; lattice. In the Introduction (Chapter 1), we discuss at some length such concepts as frustration and superexchange, among others, which are of common relevance in the rest of the chapters. In Chapter 2, we study the effect of Dzyaloshinskii--Moriya (DM) interactions on the zero-temperature magnetic susceptibility of systems whose low energy can be described by short-range valence bond states. Our work shows that this treatment is consistent with the experimentally observed non-vanishing susceptibility -- in the specified temperature limit -- of the spin-1/2 kagom&eacute; antiferromagnetic compound ZnCu<sub>3</sub>(OH)<sub>6</sub>Cl<sub>2</sub>, also known as herbertsmithite. Although the objective of this work is explaining the aforementioned characteristic of the experimental system, our methods are more general and we apply them to the checkerboard and Shastry-Sutherland lattices as well. In Chapter 3, we discuss our findings in the study of ghost-mediated domain wall interactions in the diamondback ladder. These domain walls are the the spin excitations -- the kinks and the antikinks -- separating the ground states along one chain of the ladder. While as individual entities an antikink is energy costly and a kink energy free, our study finds that both interact via the <italic>ghosts</italic> that they produce in the opposite side of the ladder from where they are located. Through the study of these ghosts, we find that domain walls proliferate in the system above a critical value of the system's coupling constants. It is this proliferation that makes their treatment as free, non-interacting particles impossible, so we study here their interactions both quantitatively and qualitatively, in a region where the latter are yet not very strong, namely below the critical point. Based on the calculated two-body interaction potential, domain walls interact attractively (repulsively) when separated at even (odd) distances, with a strength that decays as 1/s<super>p</super>, where s is their separation and p<1. We also consider higher-order interactions. In the last chapter, Chapter 4, we present our study of the spin-1 kagom&eacute; Heisenberg antiferromagnet. Our approach is to first consider an SU(2)-symmetric parent Hamiltonian with known ground states on the S=1 kagom&eacute; lattice, in which nearest-neighbor Heisenberg interactions are already present. We then enhance these interactions by an additional Heisenberg term added perturbatively in order to move the system closer to a pure Heisenberg antiferromagnet. The results of this enhancement is obtaining a description of the system in terms of an effective Hamiltonian, namely a transverse field Ising AF on the triangular lattice. Based on the particular values of this effective Hamiltonian, our system is found to be in the order-by-disorder phase.
-------------------------------------

10136813_183 - 0.999976856204 - technology_and_computing
[encryption, communication, implementation, safety, public, framework, environment, design, platform, waveform]

Design and implementation of an encryption framework for APCO P25 using an open source SDR platform in an OSSIE environment
Secure and reliable communication is one of the most important issues in the public safety domain. For public safety and emergency response organizations such as the Police and Fire departments, reliability and security of their communications is fundamental and requires both authentication of users as well as encryption of voice and data communication. Project 25 (P25) public safety waveform is the waveform of choice for most public safety and emergency response organizations in Northern America and includes features to enhance reliability and security of communications. This thesis describes the design and implementation of an encryption framework for a P25 waveform in a Software Communication Architecture (SCA) environment on an open-source Software Defined Radio (SDR) platform. The design and implementation of the framework which starts with a high level modeling of its state machine using pseudocode, goes through a bit-true intermediate implementation and ends with the final cycle- true and bit-true platform-specific implementation is discussed. This thesis proposes an encryption framework that is feasible for implementing the P25 encryption specifications and can be rapidly prototyped in an SCA environment on a cheap off-the-shelf SDR platform involving multiple processors
-------------------------------------

10138485_183 - 0.99998834909 - technology_and_computing
[measurement, error, system]

Automated suppression of errors in LTP-II slope measurements with x-ray optics
Systematic error and instrumental drift are the major limiting factors of sub-microradian slope metrology with state-of-the-art x-ray optics. Significant suppression of the errors can be achieved by using an optimal measurement strategy suggested in Rev. Sci. Instrum. 80, 115101 (2009). Here, we report on development of an automated, kinematic, rotational system that provides fully controlled flipping, tilting, and shifting of a surface under test. The system is to be integrated into the Advanced Light Source long trace profiler, LTP-II, allowing for complete realization of the advantages of the optimal measurement strategy method. We describe in detail the system?s specification, design operational control and data acquisition. The performance of the system is demonstrated via the results of high precision measurements with a number of super-polished mirrors.
-------------------------------------

10133169_183 - 0.97560290866 - technology_and_computing
[problem, optimization, vehicle, mass, passenger, transport, real-time]

Real-Time Mass Passenger Transport Network Optimization Problems
The aim of Real-Time Mass Transport Vehicle Routing Problem (MTVRP) is to find a solution to route n vehicles in real time to pick up and deliver m passengers. This problem is described in the context of flexible large-scale mass transportation options that use new technologies for communication among passengers and vehicles. The solution of such a problem is relevant to future transportation options involving large scale real-time routing of shared-ride fleet transit vehicles. However, the global optimization of a complex system involving routing and scheduling multiple vehicles and passengers as well as design issues has not been strictly studied in the past. This research proposes a methodology to solve it by using a three level hierarchical optimization approach. Within the optimization process, a Mass Transport Network Design Problem (MTNDP) is solved. This paper introduces MTVRP and presents a scheme to solve it. Then, the associated algorithm to perform the MTNDP optimization is described in detail. An instance for the city of Barcelona, Spain is solved, showing promising results with regard to the applicability of the methodology for large scale transit problems.
-------------------------------------

10132674_183 - 0.941509446534 - technology_and_computing
[travel, systems, behavioral, model, disaggregate, goal, information, geographic]

The Relationship Between Geographic Information Systems and Disaggregate Behavioral Travel Modeling
This paper introduces the theme of the special issue and lays a foundation for arguments concerning the potential usefulness of Object-Oriented Geographic Information Systems (OOGIS) for the development and testing of disaggregate behavioral travel models. It also states goals for Intelligent Transportation Systems (ITS) research and discusses the role of behavioral travel models in pursuing ITS goals and objectives.
-------------------------------------

10134438_183 - 0.984295744822 - technology_and_computing
[model, morphotype, approach, computational, class, pattern, chiral, continuum]

A new computational approach to simulate pattern formation in Paenibacillus dendritiformis bacterial colonies
Under the harsh conditions of limited nutrient and hard growth surface, Paenibacillus dendritiformis in agar plates form two classes of patterns (morphotypes). The first class, called the dendritic morphotype, has radially directed branches. The second class, called the chiral morphotype, exhibits uniform handedness. The dendritic morphotype has been modeled successfully using a continuum model on a regular lattice; however, a suitable computational approach was not known to solve a continuum chiral model. This work details a new computational approach to solving the chiral continuum model of pattern formation in P. dendritiformis. The approach utilizes a random computational lattice and new methods for calculating certain derivative terms found in the model
-------------------------------------

10135672_183 - 0.999125755806 - technology_and_computing
[silicon, vertical, optical, waveguide, dispersion, nonlinear, bandwidth, incident, grating, pulse]

Optical dispersion and nonlinearity in integrated silicon nanophotonic devices
Increasing demands in speed, bandwidth and power efficiency in computing and communications has led to a burgeoning of the field of silicon photonics. Given their compatibility with complementary metal oxide semiconductor technology, optical systems on silicon provide a low cost, scalable solution to meet future data demands. In this dissertation, we address some of the issues necessary for optics to become a viable platform for applications such as data centers and microprocessors. First, using the concepts in coupled mode theory and finite difference time domain modeling, add drop filters are designed using coupled vertical gratings. The filter is further applied for use as a 1 by 4 wavelength division multiplexer offering bandwidths and a free spectral range unparalleled by state of the art alternatives such as ring resonators. Next, the issue of group velocity dispersion engineering in photonic lightwave circuits is addressed. By incorporating linear chirp into a single vertical grating, quadratic phase may be imparted to an incident wave. To overcome the loss limitations associated with the chirped vertical grating operating in reflection, a coupled chirped vertical grating structure is introduced for operation in transmission. The high index contrast in the silicon-on-insulator platform implies that weak coupling and equivalently small bandwidths, are difficult to achieve in channel waveguide geometries. We present the theory and experimental demonstration of a cladding- modulated Bragg grating implemented using periodic placements of cylinders along a silicon waveguide which enable a wide range of coupling strengths to be realized. Next, group velocity dispersion engineering by varying the waveguide geometry is studied in silicon nitride. Nonlinear loss mechanisms such as two photon absorption are shown experimentally to be absent in the fabricated waveguides at high optical intensities. Two-fold broadening of an incident pulse is demonstrated, showing that silicon nitride is a viable nonlinear material. Finally, we present the first demonstration of a nonlinear optical pulse compressor implemented on silicon. Incident pulses undergoing self phase modulation followed by spectral re-phasing using dispersive elements result in temporal compression. The compression factors achieved are the highest demonstrated on a chip to date
-------------------------------------

10135544_183 - 0.981502605154 - technology_and_computing
[search, robot, method, behavior, pso, time, optimization, region, algorithm, system]

Distributed Multi-Robot Collaboration Using Evolutionary Computation
Robots are utilized in activities such as mine detection, search and rescue, etc. Especially where the environment is dangerous or search time can be optimized, etc. As the cost of robots reduced and application of robotic systems grew, researchers started working on distributed robotic systems. However, creating a robust, flexible and scalable system of collaborating mobile robots that can efficiently perform the search task has been a challenging issue for roboticists. Biologists observed that animal interactions in their societies create a collective behavior that helps them in foraging, group defending and other activities. They show that each animal in its society has a simple role but the collection of these behaviors enables them to accomplish a complex task. Moreover, swarm behavior has interesting properties such as robustness, flexibility and scalability. Based on these studies, researchers have introduced new optimization methods. Roboticists utilize these optimization methods to provide important properties of swarm behavior. PSO is one of the successful optimization methods in this area. Initially, PSO performs well in exploring different search regions, however, in some cases the method doesn't exploit well in promising regions which increases the search time. In this study, mobile robots are deployed to find a target in the search space. Robots interact with each other and move around the search space using PSO algorithm. In addition, the MPSO algorithm is introduced to create an efficient balance between exploration and exploitation of the PSO algorithm. Results show that MPSO reduces search time and minimizes region revisiting.
-------------------------------------

10132787_183 - 0.972864775442 - technology_and_computing
[model, activity-travel, pattern, activity-based, behavior, approach, generation]

Modeling Activity Pattern Generation and Execution
Activity-based approaches are perhaps the most promising alternative to the current travel forecasting methodology. This dissertation first presents a pattern generation model that can serve as a link between activity and trip-based methodologies. The model uses a clustering approach to identify groups of similar activity-travel behavior and relates them to household socioeconomic attributes. Minimally, the pattern generation model is then expanded to serve as the core component of a proposed activity-based microsimulation model that constructs complete origin-destination tables using a wholly activity-based approach. The techniques developed provide due diligence to the complex nature of activity-travel behavior in terms of spatial and temporal constraints, household interactions, and the derived nature of such behavior. A successful application of the expanded model is outlined using data from the 1994 Portland activity-travel survey.
-------------------------------------

10138350_183 - 0.872524690521 - technology_and_computing
[model, ffd, cfd, particle]

Validation of a Fast-Fluid-Dynamics Model for Predicting Distribution of Particles with Low Stokes Number
To design a healthy indoor environment, it is important to study airborne particle distribution indoors. As an intermediate model between multizone models and computational fluid dynamics
(CFD), a fast fluid dynamics (FFD) model can be used to provide temporal and spatial information of particle dispersion in real time. This study evaluated the accuracy of the FFD for predicting
transportation of particles with low Stokes number in a duct and in a room with mixed convection. The evaluation was to compare the numerical results calculated by the FFD with the corresponding
experimental data and the results obtained by the CFD. The comparison showed that the FFD could capture major pattern of particle dispersion, which is missed in models with well-mixed assumptions.
Although the FFD was less accurate than the CFD partially due to its simplification in numeric schemes, it was 53 times faster than the CFD.
-------------------------------------

10137301_183 - 0.999990923911 - technology_and_computing
[p2p, application, system, maintenance, issue, dissertation, user, consistency, interactive, incentive]

Peer-to-Peer Support for Large Scale Interactive Applications
User-interactive applications are evolving in both popularity and scale on the Internet, ranging from simple le-sharing to more demanding applications such as collaborate workspace, massive multi-player online games (MMOGs). These applications are traditionally implemented by Client/Server architectures, which suer from signicanttechnical and commercial drawbacks, primarily high-maintenance cost and limited scalability. To overcome these drawbacks, this dissertation presents a Peer-to-Peer (P2P) approach to support large-scale interactive applications.This dissertation addresses two key design issues for P2P systems to achieve scalability and high performance. The rst issue is to provide incentives for users so that P2P systems can aggregate free resources from unreliable users. The second issue is to design consistency maintenance schemes so that P2P systems can provide reliableservices to meet the application requirements by using free resources from unreliable users.For the rst issue, this dissertation starts with providing a budget based incentive search service, called BuSIS 105, for eciently locating service providers in P2P systems. Then, an incentive trading model, called FairTrade 102, is presented for P2P users to exchange service with each other. Personal currency model is employed in FairTrade to simulate users to contribute to the P2P community in exchange of desired services. To cope with highly dynamic nature of P2P systems, an enhanced incentive trading model, called CoBank 101, is presented to reduce maintenance overhead at each user and improve robustness against malicious attacks. Cooperative banking strategy isused in CoBank to further distribute the maintenance workload.For the second issue, this dissertation begins with providing a consistency maintenance framework, called BCoM 104, 100, balancing between consistency strictness, availability and performance for various P2P interactive applications with heterogenousresource constraints. Then, this dissertation presents a real-time consistency maintenance P2P system, called PPAct 103, for interactive applications such as MMOGs. View discovery and update dissemination are decoupled in PPAct to mitigate the hot spot problem and ensure consistency maintenance under stringent latency constraints.
-------------------------------------

10133982_183 - 0.945171633085 - technology_and_computing
[migration, fls, dickkopf-1, joint, synoviocyte, fl, wnt, cell, induction]

Dickkopf-1 induces migration in fibroblast-like synoviocytes
Rheumatoid arthritis (RA) is a chronic autoimmune disease affecting joint tissues. Disease is characterized by synovial hyperproliferation and joint invasion, severe inflammatory cell influx and matrix destruction. Serum levels of the wnt antagonist, Dickkopf-1, are increased in diseased patients and have been described to affect bone homeostasis in affected joints. Here we introduce an additional role for Dickkopf-1, the induction of migration in fibroblast-like synoviocytes (FLS) in vitro. Our results show that Wnt5a and surprisingly, DKK1 induce migration in wound edge C57Bl/6 FLS. Wnt5a has previously been demonstrated to induce migration in a JNK dependent manner whereas DKK1 has been implicated in both induction and inhibition of migratory behavior. Migrating FLS exhibit activation of JNK1 and JNK2, and the absence of either isoform limits the induction of synoviocyte migration, indicating that JNK is required for FLS migration. However, Dickkopf-1 treatment of JNK1 deficient FLS was able to induce migration in wound edge cells, while JNK2 deficient FLS were refractory. These findings suggest that the Wnt signaling pathways contribute to migration in FLS in vitro. Elevated signaling through Wnt- affiliated molecules may account for the aberrant migratory behavior of synovial cells in rheumatoid arthritis patients
-------------------------------------

10131682_183 - 0.998478977595 - technology_and_computing
[rail, high-speed, system, train, service, technology, british]

British Rail's InterCity 125 and 225
The purpose of this report is to provide a technical review of the two high-speed rail systems developed by British Rail -- the InterCity 125 and 225 -- over the past 20 years, so that they can be compared with other systems for possible future use in the development of a high-speed passenger rail system in California. While the report will examine both systems, greater attention will be given to the newer IC-225 technology, which was introduced in 1990 and uses electric traction. The older IC-125 technology, dating from the mid-1970s in revenue service, merits study as the only example of a high-speed passenger train using diesel traction. These two technologies are of additional interest to the CalSpeed project because British Rail has built no dedicated lines for high-speed services. Other than the tilt trains introduced recently introduced in Sweden and Italy, the IC-125/225 trains are the only conventional high-speed services relying solely on the upgrading of existing tracks of a long-established rail network.
-------------------------------------

10129644_178 - 0.9999561884 - technology_and_computing
[reference, architecture, systems, bibliographical, building]

Systems building in architecture
Includes bibliographical references.
-------------------------------------

10133241_183 - 0.948562426071 - technology_and_computing
[pedestrian, network, planning, research, neighbourhood, space, resident, destination, robust, environment]

A Case Study of Pedestrian Space Networks in Two Traditional Urban Neighbourhoods, Copenhagen, Denmark
Most pedestrian environment and behavior research has applied concepts of connectivity and access uniformly at the neighbourhood scale. Actual pedestrian networks rely on a limited number of routes to provide intra- and inter-neighbourhood pedestrian connections, suggesting the need to focus research. Also, much of the literature has proposed improvements to the built environment that have little relation to the planning system’s ability to implement them.
      This research aims to assess the applicability of a network approach to pedestrian planning. It includes two case studies of comparable neighbourhoods in Copenhagen, Denmark. One neighbourhood has a robust pedestrian space network rich with choice, while the other has a fragmented network that limits pedestrian route choice.
      A randomized survey of 600 households collected data on walking behavior and perceptions of the pedestrian environment. Also, 17 in-person interviews were conducted with residents to understand how attitudes and pedestrian opportunities influenced walking behavior.
      This research also explored the role of Copenhagen’s political culture of planning in building and maintaining robust pedestrian space networks. The theme of state-market balance of power and its relevance to pedestrian policy implementation was explored through over 20 interviews with planners, politicians and private developers, as well as a detailed study of planning documents.
      Survey results found that residents living in a robust pedestrian space network walked to a greater number of local destinations and a broader range of destination types. The relationship held for optional trips to social destinations. Residents living in a more robust pedestrian environment had a larger social network, suggesting neighbourhood design can influence social interaction.
      The in-person interviews illustrated how residents chose routes through their neighbourhood, what constituted a barrier to pedestrian movement, and how social barriers affected the desirability of destinations that seemingly meet standard urban design criteria.
      The political culture research demonstrated that economic constraints place stress on any planning system. A planning system that enjoys consistent political and financial support from elected officials, however, was found to have a superior ability to respond to collective challenges and develop innovative solutions. Further, the enhanced ability to implement policy appears associated with a more reflective approach to planning that encourages planners to enhance their skills and knowledge over time.
-------------------------------------

10138012_183 - 0.999974815479 - technology_and_computing
[platform, permission]

How To Ask For Permission
Application platforms provide applications with accessto hardware (e.g., GPS and cameras) and personal data.Modern platforms use permission systems to protect access to these resources. The nature of these permissionsystems vary widely across platforms. Some platformsobtain user consent as part of installation, while othersdisplay runtime consent dialogs. We propose a set ofguidelines to aid platform designers in determining themost appropriate permission-granting mechanism for agiven permission. We apply our proposal to a smartphone platform. A preliminary evaluation indicates thatour model will reduce the number of warnings presentedto users, thereby reducing habituation effects.
-------------------------------------

10131195_183 - 0.868874062169 - technology_and_computing
[traffic, use, transportation, land, growth, facility, economic, war, policy, order]

Regulating Traffic by Controlling Land Use: The Southern California Experience
American attitudes toward transportation planning have recently undergone significant change. For three decades after the end of World War II, public policy emphasized the construction of new highway and transit facilities in order to remove the backlog of needs which resulted from the combined effects of depression, a war economy continued urban growth, and accelerating automobile ownership. For the most part, there was consensus among transportation policymakers that their primary goal was to accommodate growth by constructing facilities which would have adequate capacity to handle future demand. It was understood that land use patterns and economic development were the sources of traffic, yet there was general agreement that transportation policy should aim to accommodate forecast land use and economic growth rather than to regulate them in order to control traffic.
-------------------------------------

10134977_183 - 0.999935832606 - technology_and_computing
[worm, image, system, experiment]

Automated cancer detection and drug discovery : two biomedical vision systems
Statistical methods from machine learning have been key to the progress of computer vision in recent years. Use of machine learning has led to development of many successful vision applications such as face and pedestrian detectors. Along the same principles of using robust statistical methods, in this thesis we build systems for two biomedical imaging domains. The first system detects cancer in prostate pathology images. Recent technological advancements have made it possible to commercially build whole-slide scanning microscopes that generate digital images of whole-slides at magnifications required for an effective clinical diagnosis. The availability of digital images of pathology slides allows development of Computer Aided Diagnostic (CAD) tools that can improve pathologist's accuracy and efficiency in diagnosis. An automated screener can assist pathologists in diagnosing by suggesting suspicious locations. The screening tool can also reduce the bandwidth required for diagnosing remotely by transferring only the suspicious parts. To provide a base on which such CAD tools can be developed, we build a cancer detector for prostate needle core biopsies, which is one of the most frequently diagnosed tissue. The second system analyzes High-Throughput Screening (HTS) images of C. elegans worms to identify their phenotype. HTS is a class of biological experiments where a large number of similar experiments are conducted to identify a small number of drugs or genes relevant to a biological process. Recently, researchers have started conducting HTS experiments using C. elegans in which the experimental output are images. To assist biologists in analyzing the large number of images generated by the HTS experiments on C. elegans, we develop a system that identifies a worm's phenotype. A preferred way of conducting such experiments is to image the worms in agar. The shadows cast by track marks left by the worms in agar appear similar to the worms which complicates segmenting images of worms. To reliably segment worms in such conditions, we develop a novel segmentation technique that uses multiple visual cues such as texture, contrast and shape to segment the worms. After segmenting the worms, our system also analyzes the fluorescent patterns inside the worms to identify their phenotype
-------------------------------------

10135417_183 - 0.996605610762 - technology_and_computing
[device, ratio, current, lr, memory, resistance, state, substrate, high]

Write-Once Read-Many-Times (WORM) Memory based on Zinc Oxide on Silicon
ABSTRACT OF THE THESISWrite-Once Read-Many-Times (WORM) Memory based of Zinc Oxide on SiliconbyQing ZhangMaster of Science, Graduate Program in Electrical EngineeringUniversity of California, Riverside, August 2011Dr. Jianlin Liu, ChairpersonWrite-once read-many-times (WORM) memory feature is found in ZnO films deposited on silicon substrates. The resistance ratio (R ratio) between the high resistance state (HRS or ON state) and low resistance state (LRS or OFF state) is mostly around 104 for all tested devices. The programming power required to switch the memory devices from HRS to LRS can be as low as 10-3 watts when current compliance is set at 50µA. Lowering the current compliance will result in lower R ratio, because the maximum LRS current level will also be limited by the current compliance. To test the devices for long term storage purpose, endurance to reading pulse with width of 2&#956;s and amplitude of 1V were tested for several randomly selected devices with various top contact sizes. Results shown all devices sustained their R ratio with hardly noticeable resistance change throughout 108 reading cycles. Retention was also tested on several devices. The results showed that only R ratio of HRS showed some drop in resistance after 105 seconds (27.78 hours) testing period, while LRS current level was almost constant throughout. R ratio still retained around 103 after being extrapolated to 100 years. Temperature stability of the devices was also tested up to 120ºC. There was current drop in both HRS and LRS under high temperature over time, but the overall R ratio did not change much, still above 103 for all tested devices. Endurance and retention test results under high temperature showed similar trend as room temperature. Au metal contact devices showed almost undetectable HRS current by the equipment because the probe station used has current noise level which is almost the same as the current level of the HRS devices. Devices with Au and Ti metal have almost the same conductivity at LRS, and Au top contact has the potential to provide higher R ratio for WORM devices. Higher memory performance can be clearly observed in devices fabricated on p-Si substrates than that on n-Si substrates. Devices on p-Si substrate showed low rectifying ratio in HRS, and high rectifying ratio in LRS, thus the R ratio of the devices is higher. The high rectifying ratio in both HRS and LRS in devices with n-Si substrate resulted in lower R ratio of around 102. The switching mechanism of all devices is explained by the filament model, in which conducting filaments consisting of oxygen vacancies are formed after applying external electric field to switch the device to ON state. The filament model explained why all devices' LRS conductivity is independent of contact size. As write-once memory, the LRS devices have to sustain their resistivity through any external electric field. Test results showed that all devices do not reset to HRS after repeatedly attempting to reset devices by looping the -5V ~ 5V sweep on them. Although all devices kept their LRS status, a slight reset can be observed in the negative bias region, positive bias region showed no sign of reset.
-------------------------------------

10137619_183 - 0.940597633094 - technology_and_computing
[method, problem, mesh, crack, system]

On Embedded Methods for Crack Propagation, Virtual Surgery, Shattered Objects in Computer Animation, and Elliptic Partial Differential Equations
We present a collection of embedded methods to solve a variety of scientific computing problems in both 2 and 3 dimensions. Embedded methods make use of a structured background mesh which does not conform to the irregular geometry, such as the domain boundary, of the problem. Instead, the irregular geometry is embedded within the structured mesh's elements, providing a framework to solve many problems involving crack propagation, progressive fracturing, dynamic interfaces, and shape optimization.In Part I, we apply the mesh cutting algorithm of Sifakis et al. Sifakis07 to investigate the modeling of cracks, surgical incisions, and shattering. Specifically, we present a geometrically flexible and straightforward crack propagation method which combines the <italic>eXtended Finite Element Method</italic> (XFEM) with Sifakis07 and an innovative integration scheme which makes use of a subordinate quadrature mesh. We also discuss the application of Sifakis07 and other advances in numerical methods to address the challenges of a virtual surgery simulator. We conclude Part I by describing a system to facilitate the modeling of cracked and shattered objects in the context of visual effects and computer animation.In Part II, we present a numerical method utilizing virtual degrees of freedom to efficiently solve elliptic partial differential equations (specifically: Poisson's equation with interfacial jump conditions; and linear elasticity in the nearly incompressible regime) on irregular domains within a regular background Cartesian grid. Our method enforces Dirichlet boundary conditions and interfacial jump conditions weakly, formulating our system as a constrained minimization problem. In this context, we describe an algorithm to generate an associated discrete Lagrange multiplier space that allows one to derive an equivalent symmetric positive definite linear system. We provide a family of multigrid algorithms to solve this linear system with near optimal efficiency. Our method is second order accurate in <italic>L</italic><super>&infin;</super> and possesses a feature set rarely found among the broad class of embedded methods for elliptic problems.
-------------------------------------

10137317_183 - 0.999956400832 - technology_and_computing
[mobility, node, real, model, spatial, density, human]

Characterizing User Mobility in Wireless Networks
n this work we study mobile wireless networks by looking at mobility management and analysis of human mobility, focusing on the main goal of understanding human mobility and applying our findings on developing new realistic mobility models for simulations. In our work, we start by analyzing Wireless Local Area Networks (WLAN) and GPS traces that record mobility in a variety of network environments. We observe that from a macroscopic level, human mobility is symmetric. We also study the direction of movement which also exhibits symmetric behavior in both real-- as well as synthetic mobility. Moreover, motivated by the symmetric behavior identified, we continued our investigation on real mobility characteristics, by focusing on node spatial density in real applications. We show that human mobility exhibits ``persistent'' behavior in terms of the spatial density distribution of the mobile nodes over time. By using real mobility traces, we observe that the original non-homogeneous node spatial density distribution, where some regions may be quite dense while others may be completely deserted, is maintained at different instants of time.We also show that mobility models that select the next node position based on the position of other nodes, a la ``preferential attachment'', do not preserve the original spatial node density distribution and lead to behavior similar to random mobility as exemplified by the Random Waypoint model. Based on these observations, we propose a simple mobility model that preserves the desired spatial density distribution. We found that performance results expressed by a number of network metrics also match closely results obtained under mobility governed by real traces. We also Introduce a modeling framework to analyze spatial node density in mobile networks under "waypoint"-like mobility regimes. The proposed framework is based on a set of first order ordinary differential equations (ODEs) that take as parameters (1) the probability of going from one subregion of the mobility domain to another and (2) the rate at which a node decides to leave a given subregion.Since the social structure usually guides the choice of new destination in real human mobility, we introduce a user mobility modeling framework that accounts for both the users' social structure as well as the geographic diversity of the region of interest. SAGA, or Socially-- and Geography-Aware mobility model, captures social features through the use of communities which cluster users with similar features such as average time in a cell, average speed, and pause time. SAGA accounts for geographic diversity by considering that different communities exhibit different interests for different locales. Besides introducing SAGA, the contributions of this work includes a model calibration approach based on formal statistical procedures to extract social structures and geographical diversity from real traces and set SAGA's parameters.
-------------------------------------

10131981_183 - 0.988480724384 - technology_and_computing
[analysis, dynamic, program, pavement, model, computer, result]

Dynamic Finite-Element Analysis of Jointed Concrete Pavements
A new dynamic finite-element computer program, DYNA-SLAB, for the analysis of jointed concrete pavements subjected to moving transient loads is presented. The dynamic solution is formulated in both the time and the frequency domains. The structural model for the slab system is the one used in the static computer program ILLI-SLAB. The foundation support is represented by either a damped Winkler model with uniformly distributed frequency-dependent springs and dashpots or a system of semi-infinite horizontal layers resting on a rigid base or a semi-infinite half-space. An important contribution from the study is a new analytical method for determining the stiffness and damping coefficients to be used in the Winkler foundation model. The accuracy of DYNA-SLAB has been verified by comparing the results produced by the program with those from theoretical closed-form solutions and from a powerful dynamic soil-structure interaction computer program called SASSI as well as with field data. The analytical results indicate that dynamic analysis is generally not needed for the design of rigid pavements and that it usually leads to decreased pavement response. Thus, it appears that a quasistatic analysis is sufficient and that the results from this type of analysis will generally be conservative, provided that the wheel loads used in the analysis have been adjusted for the effects of vehicle velocity, truck suspension characteristics, and pavement roughness.
-------------------------------------

10137116_183 - 0.999999089084 - technology_and_computing
[network, information, topology, model, process, dynamic, theory, complex]

Spreading Processes on Networks: Theory and Applications
The interactions between people, technology and modern communication paradigms form large and complex human--machine networks. Complex network theory attempts to address the global and local behavior of such network structures. Of particular interest within the area of network theory is understanding the dynamic behavior of spreading processes on complex networks. In this work, we examine a variety of models covering the intersection of spreading processes and complex network theory, and although we study a large range of problem formulations, we find that--surprisingly--a single parameter effectively summarizes the topology. We begin by examining the effect that topology has on spreading processes in dynamic networks. Dynamic networks are becoming more common due to our increased reliance on and the functionality of mobile devices, smartphones, etc. Specifically, we ask, given discrete information spread through a proximity-based communication channel across dynamic network of mobile end-users, what criteria is required such that the information will ultimately die-out; that is, can we determine the tipping point between information survival and die-out? We show analytically that yes, such a threshold exists, yet it is computationally infeasible to calculate. To avoid such computationally intensive methods, we go on to provide two approximation methods for determining the tipping point. Next, we analyze the effect of topology on the propagation of competing information. Using a novel graph structure we refer to as a composite network, we model the intertwined propagation of competing information across a variety of underlying network layers. Through a combination of analytical and empirical methods, we show how the topology affects the competing information, and ultimately, using topology, we predict the winner of competition.Building on the success of the previous analyses, we formulate a model describing the spread of non-categorical information. Unlike our previous models, the information in this system is represented by a continuous value.We determine the phase transitions of the overall system, relate them to the tipping points in our previous models, and show both analytically and empirically how the structure of the network affects those phase transitions.Ultimately, for each of these models, a single topological parameter, the largest eigenvalue of the adjacency matrix, is all that is necessary to characterize the effect of topology on the spreading process.
-------------------------------------

10132731_183 - 0.999277082273 - technology_and_computing
[model, pavement, datum, mix, westrack]

Parking fees and congestion
Payment deterioration models are an important input for the efficient management of pavement systems, the allocation of cost responsibilities to various vehicle classes for their use of the highway system, and the design of pavement structures. This paper is concerned with the development of an empirical rutting progression model using an experimental data set from WesTrack. The data used in this paper consist of an unbalanced panel data set with 860 observations. The salient features of the model specification are: 1) three properties of the mix are sufficient to model the performance of the asphalt concrete pavement accurately, 2) the model captures the effects of high air temperatures at WesTrack, and 3) the model predicts rut depths by adding predicted values of the increment of rut depth for each time period, which is particularly advantageous in a pavement management context. The three mix properties are a gradation index, which is obtained from the aggregate gradation, the voids filled with asphalt obtained for the construction mix in the Superpave gyratory compactor, and the initial in-place air voids. The specified model is non-linear in the variables and the parameters, and is estimated using a random effects specification to account for unobserved heterogeneity. The estimation results and prediction tests show that the model replicates the observed pavement behavior at WesTrack well.
-------------------------------------

10132832_183 - 0.998740828326 - technology_and_computing
[internet, technology, travel, communication, device, activity, home, information, behavior]

travelbehavior.com - Activity Approaches to Modeling the Effects of Information Technology on Personal Travel Behavior
This paper puts forth some ideas for extending travel behavior modeling to account for interactions between travel and telecommunications. Information technology (IT, sometimes referred to as communications and information technology, or CIT) is burgeoning, providing unlimited business opportunities for entrepreneurs to develop and sell IT products and services. While most of these products and services are not specifically designed to affect travel behavior, they do, often in subtle and unexpected ways. The connectivity of the Internet and the proliferation of capable and affordable home computers and communication devices have encouraged flexible work arrangements and made e-commerce the fastest growing sector of most western economies. For many people, the home has become a viable site for the conduct of certain activities that formerly could only be conducted at non-home locations. In addition, cellular telephones and other portable computer and communications devices have redefined our ability to conduct business and dynamically schedule activities while traveling or at locations away from home or workplace. The wave of technological advances that brought us the Internet, mobile phone, and personal digital assistants (PDA's) is not slowing down. The future will bring a next-generation Internet with higher speed, multimedia capability and intelligent agent technology. It will be accessible by both PC's and "Internet appliances" such as television set-top boxes, videogame consoles and smart handheld devices.
-------------------------------------

10135400_183 - 0.999999491584 - technology_and_computing
[datum, speed, bandwidth, network, time, application, scale, efficient, software-based, measurement]

Algorithms for measuring and enhancing distributed systems
The industry-wide movement toward large data centers and cloud computing has brought many economic advantages, but also numerous technical challenges. In this work we describe three software-based contributions towards improving the communication performance of these large- scale distributed systems. While advances to commodity Ethernet in the data center have increased throughput, efficient application design can produce even more significant speedups. We first describe an efficient, new data structure, called "Difference Digests", that can serve as a building block in distributed-application protocols. Difference Digests identify the elements that differ between two sets using communication and computation overheads proportional to population of the difference. This functionality has many promising applications, including recovery from network partitions, data synchronization, and data de-duplication. Beyond more efficient protocols, tuning applications can further improve performance by reducing network congestion. As link speeds increase, measuring bandwidth at fine time scales produces valuable debugging and tuning information, but is complicated by the immense number of packets and the short per-packet processing time. Further, it is unclear in advance which time scales will prove insightful, but storing all measurements incurs significant storage overhead. Thus, we propose two algorithms to summarize streams of fine-grain bandwidth samples and identify bursty traffic behavior. Our implementation records bandwidth information at speeds up to 10 Gbps while decreasing storage costs by orders of magnitude. After the measurement, bandwidth statistics can be generated for arbitrary time scales down to microseconds, allowing users to identify short-lived phenomena affecting their application's performance. While network capacity has increased, maintaining low latency for time-sensitive flows remains challenging. One approach is to minimize in- network buffering by controlling flows with software-based pacing at the end host. However, these mechanisms are unproven at the Gigabit speeds found in data centers. Hence, we demonstrate measurement tools to evaluate the bandwidth and "burstiness" of one such mechanism, the Linux Token Bucket Filter (TBF). We find that TBF struggles to scale to multi-Gigabit speeds due to Linux's inability to reliably service timers with micro-second accuracy. To address this limitation, we describe two enhancements, which improve software-based pacing at speeds up to 10 Gbps while minimizing bursts
-------------------------------------

10135242_183 - 0.928248991593 - technology_and_computing
[model, probabilistic, seismic, analysis, fe, nonlinear, structure, earthquake, period, psda]

Simplified vector-valued probabilistic seismic hazard analysis and probabilistic seismic demand analysis : application to the 13-story NEHRP reinforced concrete frame-wall building design example
A comprehensive and rigorous probabilistic methodology for performance-based earthquake engineering (PBEE) has been under development under the auspice of the Pacific Earthquake Engineering Research (PEER) Center over the past thirteen years. The probabilistic estimation of the seismic demand is an important part of the PBEE methodology and consists of a two-step procedure: probabilistic seismic hazard analysis (PSHA), and probabilistic seismic demand analysis (PSDA). Two shortcomings are identified in past applications of the PBEE methodology : (i) the use of a single, or scalar, ground motion intensity measure (IM) which is typically taken as the 5% damped linear spectral acceleration, Sa(T1), at the fundamental period of the structure; and (ii) the use of testbed applications based on two- dimensional (2-D) finite element (FE) models (e.g., 2-D frame models) of the considered structures, which are three-dimensional in nature and cannot always be reduced to 2-D models. Sa(T₁) represents an inefficient and insufficient predictor of the nonlinear structural response for structures with significant higher mode effects and significantly different fundamental periods in two orthogonal directions. This dissertation addresses both shortcomings. First, a simplified and computationally efficient vector-valued PSHA (VPSHA), making use of USGS scalar probabilistic seismic hazard maps results, is proposed. Second, a PSDA of an advanced 3-D nonlinear FE model of the 13-story National Earthquake Hazards Reduction Program (NEHRP) reinforced-concrete frame-wall building design example is performed based on the simplified VPSHA for a specific site located in Berkeley, California. Nonlinear dynamic time-history analyses (NDTHA) are performed by subjecting the 3-D nonlinear FE model to an ensemble of 90 bi-directional (horizontal) historical earthquake ground motions. The FE model was developed in OpenSees and results of the FE analyses are used to establish a statistical model between the IMs and different EDPs (e.g., roof drift ratio, interstory drift ratios, and floor absolute accelerations). Based on the computational results from NDTHA, it is found that, for the building structure considered, a vector-valued IM consisting of multiple spectral accelerations at different periods of interest is a sufficient and more efficient predictor of the structural response, and therefore provides for more reliable and mode accurate PSDA results
-------------------------------------

10132030_183 - 0.963079470696 - technology_and_computing
[respondent, datum, model, telecommuting, choice, frequency, household]

Modeling the Choice of Telecommuting Frequency in California: An Exploratory Analysis
This study explores the individual's choice of telecommuting frequency as a function of demographic, travel, work and attitudinal factors. To do this, multinomial logit models are estimated using data collected in a recent survey of employees from three public agencies in California. Separate models are estimated, one for data collected from the Franchise Tax Board in Sacramento, one for data from the Public Utilities Commission in San Francisco, and one for data collected from employees of the City of San Diego. The results show that the most important variables in explaining the choice of frequency of telecommuting from home were the presence of small children in the household (irrespective of respondent gender), the number of people in the household, gender of respondent, number of vehicles in the household, whether respondent recently changed departure time for personal reasons, degree of control over scheduling of different job tasks, supervisory status of respondent, the ability to borrow a computer from work if necessary, and a family orientation. The empirical analysis also shows that model results are not transferable among the three organizations studied.
-------------------------------------

10132775_183 - 0.836805746868 - technology_and_computing
[service, transit]

The Future of Transit: Market-oriented Services Will Include Small Vehicles
Transit operators across the nation are looking for ways to improve the quality of service they offer and increase customer satisfaction. One way they are doing this is to match services to markets. For many services, smaller vehicles are worth a try.
-------------------------------------

10133354_183 - 0.999538997499 - technology_and_computing
[map, traffic, real-time, design, research, cartographic, dissertation]

Real-Time Traffic Maps
This dissertation summarizes research investigating the design of real-time traffic maps. As traffic congestion continues to burden our largest cities, and as the Internet continues to grow at a rapid pace, real-time traffic maps have the potential to be among the world’s most popular maps. Furthermore, as mobile devices and in-car-navigationsystems begin to connect to the Internet, millions of drivers will access and read these maps on an array of media. This dissertation reports on research aimed to understand as well as enhance the design of real-time traffic maps. The dissertation includes reviews of previous scientific research, as well as several online traffic maps from around the world. The dissertation also introduces new methods to design and empirically evaluate the performance of real-time traffic maps. The design methods are guided by established cartographic principles, as well as the findings of human subjects studies that reveal more intuitive cartographic strategies. The investigation pays particular attention to the influences of cartographic classification, and cartographic symbolization; the findings suggest that both classification and symbolization significantly influence how map readers perceive and respond to graphical depictions of traffic conditions. With this in mind, the research suggests it is imperative to not only understand the influence of design variables on map perception, but also to, as best as possible, link map design with map-readers’ perceptual preconceptions, and preferences. This idea of cognitive congruence represents a new challenge for all kinds of cartographers.
-------------------------------------

10135106_183 - 0.999997490958 - technology_and_computing
[power, application, network, scheduling, scheme, throughput, multicore, consumption]

Power Efficient Scheduling for Network Applications on Multicore Architecture
Explosive growth of Internet high-traffic applications, such as web browsing, online searching, video streaming, and gaming, requires orders-of-magnitude increase in system throughput. The advent of commodity multicore platforms in the market has opened a new era of computing for network applications due to their superiority in performance, availability and programmability. Along with increased throughput, however, comes significantly increased power consumption. Collectively, millions of servers in the global network consume a great deal of power. And chip manufactures continue to increase both the number of cores and their frequencies, substantially increasing power consumption. With higher power consumption, energy is expected to become more expensive. Higher power consumption also increases core temperature, which exponentially increases the cost of cooling and packaging, as well incurs indirect and life-cycle costs due to reduced system performance, circuit reliability and chip lifetime. Therefore, power efficiency has become and will continue to be a first-order design issue.In this thesis, we focus on power-efficient scheduling for network applications on multicore architectures. Our goal is to improve the performance of network applications in terms of throughput, latency, power, energy and temperature when deployed on multicore servers. More specifically, we first propose a latency and throughput-aware scheduling scheme based on parallel-pipeline topology. Then, we propose a throughput and latency optimization scheme under given power budget for the parallel-pipeline scheduling topology. We also present a power-optimal scheduling algorithm with regard to traffic variation via the use of per-core Dynamic Voltage and Frequency Scaling (DVFS), power gating and power migration. Further more, we explore temperature related issues by proposing a predictive model-based thermal-aware scheduling scheme. We design, implement, and evaluate our novel schemes on real systems (e.g., Intel Xeon E5335 and AMD Opteron 2350) with benchmark applications ranging from micro level (e.g., CRC checksum calculation and switching table look-up) to IP level (e.g., IP forwarding, routing, and flow classification) to application level (e.g., encryption/decryption and URL-based switching). Through extensive experiments, we observe that our schemes outperform existing approaches substantially.
-------------------------------------

10137730_183 - 0.999707349341 - technology_and_computing
[design, energy, system, thermal, guideline, ufad, tool, radiant, underfloor, comfort]

Advanced Design and Commissioning Tools for Energy-Efficient Building Technologies
This multi-year project generated significant new and improved software design tools and commissioning guidelines for underfloor air distribution (UFAD) systems, new performance guidelines for radiant slab cooled buildings, and an updated advanced Berkeley thermal comfort model. This final report presents detailed results in four major task areas as summarized below.
      Recommended commissioning guidelines were developed for the following three key elements affecting UFAD system performance: (1) procedures for measuring, adjusting, and optimizing room air stratification; (2) a new test protocol for determining air leakage from underfloor plenums; and (3) strategies and methods for controlling and managing thermal decay (temperature gain) in underfloor plenums. Emphasis is placed on commissioning procedures that are practical and as simple as possible for use by commissioning agents, and promote energy efficient operation while maintaining thermal comfort. The guidelines were developed through a combination of field and laboratory experiments, fundamental energy simulations, computational fluid dynamics (CFD) modeling, and simplified design tool studies.
      A number of improvements were made to EnergyPlus/UFAD, a version of the publicly available whole-building energy simulation program, EnergyPlus (developed under a previous PIER contract), which greatly enhanced its capabilities for modeling the more complex heat transfer processes found in UFAD systems. The improved version of EnergyPlus was in turn used as a basis for developing a more comprehensive simplified design tool for determining design cooling loads for UFAD systems, the first of its kind.
      Radiant slab cooling systems were found to demonstrate strong energy saving performance and improved occupant satisfaction in dry western U.S. climates based on a combination of occupant satisfaction surveys, two case studies, and whole-building energy simulations.
      The usability of the Berkeley thermal comfort model was improved by developing a user tutorial and demonstrated by conducting a case study of a building with a radiant floor slab.
-------------------------------------

10133665_183 - 0.998391941467 - technology_and_computing
[display, view, multiview, issue, scene, set, lightfield, problem]

Signal processing for 3D videos and displays
The consumer 3D television revolution is all set to take off in the near future. Any 3D technology which promises a natural viewing experience has to reproduce a continuous lightfield akin to the real visual world in a suitable manner for the human eyes to sample. The input to such a goggles-free 3D display technology is a set of discrete views which have been captured using a set of cameras. The 3D display technology then recreates the continuous 3D scene or lightfield from these views. Many issues arise in the whole 3D scene capture and display pipeline, some of which have been analyzed and solved from a signal processing perspective in this dissertation. First, we take a look at the frequency domain analysis of the 3D scene sampling problem for multiview displays. Next, we consider the reconstruction of a lightfield on the display from a set of views. There are two issues which arise here : aliasing due to undersampling (fewer number of views), as well as light leakage due to the physical construction of the display. We analyze these two issues and provide a joint solution, such that the final reconstructed 3D image is free of aliasing artifacts and also looks sharp on the 3D display. For any practical 3D video system, compression is a crucial issue. There have been recent efforts to extend standard 2D H.264 video coding to the 3D scenario. We propose an extension to such a coding system which takes the display properties into account. Low spatial resolution is an issue in multiview displays due to view multiplexing. Also, based on a lightfield signal analysis, it can be shown that objects away from the zero disparity plane appear aliased on the multiview display, which become blurry when subject to antialias prefiltering. This results in a further loss of spatial resolution. We propose two alternative techniques for view resizing which go beyond conventional sampling theorems, by extending content adaptive resizing algorithms to the 3D scenario, as well as making use of the interplay between 2D and 3D visual cues. The advantage of these algorithms is that they retain high (2D spatial) resolution, without compromising too much on the 3D depth perception provided by the multiview displays. Finally, we revisit the 3D scene sampling problem from a display point of view, i.e. the problem of processing a set of camera views to make them suitable for a given multiview 3D display. To make this possible, we address the view interpolation problem with limited geometry information using a multiscale overcomplete operator framework. We also propose a metric for checking the quality of geometry based view interpolation in the case of non-occlusions. We conclude with some comments about the theoretical and practical feasibility of a such a view interpolation metric
-------------------------------------

10134071_183 - 0.999995414521 - technology_and_computing
[system, log, query, state]

Query-based debugging of distributed systems
One of the most challenging aspects of debugging distributed systems is understanding system behavior in the period leading up to a bug. Since traditional debuggers such as gdb are not well suited to distributed system debugging, developers often resort to annotating their code with log statements and then writing one-off scripts that perform ad-hoc searches through the logged data. To improve this cumbersome process, we propose that the state of a distributed system execution should be programmatically and interactively available for postmortem analysis. We observe that the three defining properties of entries in a distributed system's log are "time," "node identifier," and "event type," and treat the log as a logical cube with these dimensions. By exploiting the structure of this state matrix, developers can use a high-level query language to efficiently extract information instead of manually inspecting log files or writing log processing scripts. In this dissertation, we describe the debugging process based on a query-oriented approach. We begin with an introduction of the state matrix abstraction and show how it can capture useful properties of distributed systems' executions. We then present NyQL, an object-oriented query language operating over the contents of the state matrix and describe one possible implementation as a translation to SQL queries executed over a relational database. Next, we present an implementation of a logging system that generates queryable logs in Mace, a source-to-source translator and library for building distributed systems. We present techniques for mitigating the logging overhead by giving NyQL queries to the \mace translator and show that in many cases queries can be resolved in a few seconds. We then demonstrate how using NyQL simplified debugging a handful of bugs in two different distributed systems. Finally, we extend our logging techniques to systems without source-to -source translators by developing two general-purpose libraries &mdash one in C++ and one in Java. We describe the differences between all three systems in terms of functionality and ease of use and then conclude with some future directions for distributed systems debugging
-------------------------------------

10138104_183 - 0.997583693149 - technology_and_computing
[model, design, tab, zone, comfort, system, building, study, temperature, thermal]

Critical Simulation Based Evaluation of Thermally Activated Building Systems (TABS) Design Models
Thermally Activated Building Systems (TABS) is a recognized low-energy HVAC system. Sizing of these systems is complex due to their slow thermal response. Limited cooling capacity of these systems and inadequacy of conventional sizing method, that assumes high factor of safety, is preventing early adoption of these systems. TABS, however, is proven to be energy-efficient and capable of preserving comfort in several commercial buildings of Europe. There is, however no comprehensive case study report on comfort performance of TABS in the US. With this being the background, my dissertation aims to identify and recommend a design method for TABS that balances between accuracy of multivariable complex design models, high computational cost of models requiring an iterative approach and computational ease of simple single to bivariate linear design models. The dissertation work involved: 1) a systematic qualitative review of seven TABS design models from the literature, and 2) a simulation based quantitative comfort performance assessment of three shortlisted design models. I reviewed seven design and control models and characterized them systematically with an aim to investigate their applicability in various design scenarios and at different design stages. All of these models size water supply temperature (WST) as this parameter will be used for selection and sizing of the cooling plant or the condenser unit. The design scenarios include variable internal heat gain, different building thermal mass, varying pump operating hours and varying solar gain due to orientation. Other parameters affecting cooling load and thermal performance of TABS that were held constant in this study included window-to-wall area ratio, zone volume, construction insulation, supply air temperature and volume flow rate of the ventilation system, external shading, location, TABS mass flow rate, pipe layout, active surface configuration and TABS thermal properties. I considered three design stages: feasibility study, early design decisions, and detailed design sizing and the selection criteria are reliability and ease of implementation. Results of the qualitative analysis indicated that based on the above-mentioned criteria, a hybrid model recommended by ISO 11855 is the best candidate for detailed design and sizing of the cooling plant. An outdoor temperature (Toa) compensated model, a zone operative temperature (OT) feedback based model and the hybrid model from ISO 11855 were isolated for transient simulation based quantitative evaluation in terms of a novel comfort exceedance metric. This metric accounts for both duration and severity of discomfort and is weighted by instantaneous occupancy. For comfort analysis in terms of zone OT, zone RH was maintained using humidistats. TABS was the only cooling system in the building. Twelve simulations were carried out in a standard 5 zone small office building for CZ03 in EnergyPlus v7.0 under 2 different heat gains and 2 construction types.  Results of the simulation study indicated that both the Toa compensated model and zone operative temperature feedback based model provided equally good comfort in 14 out of 20 design scenarios including zone orientation. However, the zone OT feedback model responded better to the heat gain and thermal mass conditions as expected, and is therefore recommended as a more robust model for early and detailed design phase implementation.  The hybrid model recommended by ISO 11855 resulted in comfort exceedance of 10% to 48%, while the recommended threshold exceedance for this study was 3-5%. This model also resulted in significantly reduced discomfort using 24 hours hydronic cooling energy of TABS instead of the design day 24 hours cooling energy of convective system.
-------------------------------------

10134241_183 - 0.99629152605 - technology_and_computing
[damper, degradation, bridge, model, device, seismic, response, performance, viscous, numerical]

Seismic bridge response modification due to degradation of viscous dampers performance
The goal of this thesis is to analyze the variation of a seismic response of a bridge in case of degradation of installed viscous fluid damper. The study was conducted with nonlinear time-history analyses of a detailed three- dimensional FE model of the Vincent Thomas Bridge, provided by Caltrans. Three different type of excitations were used (two white noises and a real measured earthquake). Such numerical model, including cables, suspenders, suspended structure, towers, cable bents and anchorages, reflects the state of the structure after the last retrofit phase, when dampers and fuses were installed and towers were stiffened. A preliminary validation of the numerical model of the bridge, aimed to ensuring the reliability of the FE model, has been carried out by comparing the numerical response with recorded signals. The degraded performance of the dampers was simulated through the use of gap, spring and viscous elements and validated against experimental results of real devices. The parametric study was intended to investigate the effects of progressive degradation of the energy dissipators on the bridge structural performance, both under service load conditions and seismic excitations. Results indicated a significant level of relative displacement experienced by the devices during daily loading conditions, potentially resulting in a premature degradation due to wear of the internal components of the units. For this reason an alternative device was proposed at least in conceptual terms, in order to decompose the dampers during service loading and to engage then only in case of seismic event. A solution to the early degradation of the damper devices has been proposed
-------------------------------------

10129970_178 - 0.989592173196 - technology_and_computing
[community, network, attribute, modularity, place-based, membership, social, mobile, phone]

Place-Based Attributes Predict Community Membership in a Mobile Phone Communication Network
Social networks can be organized into communities of closely connected nodes, a property known as modularity. Because diseases, information, and behaviors spread faster within communities than between communities, understanding modularity has broad implications for public policy, epidemiology and the social sciences. Explanations for community formation in social networks often incorporate the attributes of individual people, such as gender, ethnicity or shared activities. High modularity is also a property of large-scale social networks, where each node represents a population of individuals at a location, such as call flow between mobile phone towers. However, whether or not place-based attributes, including land cover and economic activity, can predict community membership for network nodes in large-scale networks remains unknown. We describe the pattern of modularity in a mobile phone communication network in the Dominican Republic, and use a linear discriminant analysis (LDA) to determine whether geographic context can explain community membership. Our results demonstrate that place-based attributes, including sugar cane production, urbanization, distance to the nearest airport, and wealth, correctly predicted community membership for over 70% of mobile phone towers. We observed a strongly positive correlation (r = 0.97) between the modularity score and the predictive ability of the LDA, suggesting that place-based attributes can accurately represent the processes driving modularity. In the absence of social network data, the methods we present can be used to predict community membership over large scales using solely place-based attributes.
-------------------------------------

10135096_183 - 0.999268377088 - technology_and_computing
[ramp, t-craft, wave, system, oscillation, lmsr, air, reduction, control, ship]

Modeling and control for the reduction of wave induced motion of ramp-connected ships
The focus of the research in this thesis is the control of a unique vehicle in development called the T-Craft. The T- Craft is a surface effect ship (SES) capable of functioning like a catamaran or an air cushion vehicle (ACV). The goal is to ultimately stabilize a ramp connecting a large medium speed roll on/roll off (LMSR) vehicle, such as an aircraft carrier, to the T-Craft. The combination of the T-Craft connected to the LMSR is called the sea base. The system was first modeled by treating each vessel as semi-cylinder mono-hulls and simulated using SimMechanics, a rigid body mechanical toolbox available in Matlab. The ships were connected in a bow-to- stern orientation. In this setup, extremum seeking (ES) was used to tune the ramp length and wave heading to minimize the oscillations. Significant reduction of ramp oscillation was observed. Following tests on the SimMechanics model, a more accurate system model was produced that utilized a wave seakeeping program called AEGIR. This program produces hydrodynamic forces based upon the hull geometry and wave information inputs. In this setup, the T-Craft was modeled as a dual hull catamaran with air cushion effects added separately and the LMSR was modeled as a large mono-hull. Passive control was tested on this new system in the form of a spring damper connection between the ramp and each vessel. Additionally, backstepping was used to control the air cushion pressure in order to reduce the heave of the T- Craft in a side by side connected system. In both cases, ramp angle oscillation reduction is observed
-------------------------------------

10137528_183 - 0.999837658615 - technology_and_computing
[model, snapshot, parameter]

Model Reduction and Parameter Estimation in Groundwater Modeling
Water resources systems management often requires complex mathematical models whose use may be computationally infeasible for many advanced analyses. The computational demand of these analyses can be reduced by approximating the model with a simpler reduced model. Proper Orthogonal Decomposition (POD) is an efficient model reduction technique based on the projection of the original model onto a subspace generated by full-model snapshots. In order to implement this method, an appropriate number of snapshots of the full model must be taken at the appropriate times such that the resulting reduced model is as accurate as possible. Since confined aquifers reach steady state in an exponential manner, a simple exponential function can be used to select snapshots for these types of models. This selection method is then employed to determine the optimal snapshot set for aunit, dimensionless model. The optimal snapshot set is found by maximizing the minimum eigenvalue of the snapshot covariance matrix, a criterion similar to those used in experimental design. The resulting snapshot set can then be translated to any complex, real world model based on a simple, approximate relationship between dimensionless and real-world times. This translation is illustrated using a basin scale model of Central Veneto, Italy, where the reduced model runs approximately 1,000 times faster than the full model. Accurate reduced modeling can be significantly beneficial for advanced analyses such as parameter estimation. A new parameter estimation algorithm is proposed that is an extension of the quasilinearization approach where the governing system of differential equations is linearized with respect to the parameters. The resulting inverse problem therefore becomes a quadratic programming problem (QP) for minimizing the sum of squared residuals; the solution becomes an update on the parameter set. This process of linearization and regression is repeated until convergence takes place. POD is applied to reduce the size of the linearized model, thereby reducing the computational burden of solving each QP. In fact, this study shows that the snapshots need only be calculated once at the very beginning of the algorithm, after which no further calculations of the reduced-model subspace are required. The proposed algorithm therefore only requires one linearized full-model run per parameter at the first iteration followed by a series of reduced-order QPs. The method is applied to a groundwater model with about 30,000 computation nodes where as many as 15 zones of hydraulic conductivity are estimated.
-------------------------------------

10138604_183 - 0.999440994038 - technology_and_computing
[comfort, building, datum, method, field, adaptive, study]

Data Collection Methods for Assessing Adaptive Comfort in Mixed-Mode Buildings and Personal Comfort Systems
The Adaptive Comfort Standard in ASRHAE Standard 55 currently is applicable only to naturally ventilated buildings, and no guidance is given on its relevance to buildings that are mixed-mode (combining operable windows with mechanical cooling), or have other forms of personal comfort systems, such as workstation heaters, fans, or ventilators. For these types of buildings or systems, the extent to which people will find the conventional PMV-based or adaptive-based comfort zones acceptable has a lot to do with how the building is designed and how it is operated throughout the year. And this is simply not yet known. There is a need for more field studies in these types of buildings to assess the specific design and operating characteristics that might influence adaptive comfort.  But to date there are no standardized methods for conducting such studies. The objective of this project was to identify a set of data collection protocols that would eventually allow us to investigate and potentially expand the adaptive comfort standards to account for mixed-mode and personal comfort system design strategies. To arrive at a proposal, we polled professionals for their own views on what data is needed, and also reviewed the literature comparing field methods used over the past decade that have expanded upon traditional thermal comfort field studies.  We used two frameworks to organize the data.  One was a “methods matrix”, describing the ways in which data can be collected, organized by background vs. real-time information, and objective building characteristics and environmental measurements vs. different kinds of surveys.  The second framework was a list of six high-level topics that represent unresolved issues that should be addressed by future data collection.  These issues include the need for improved methods to describe the building, document available controls, account for perceived control and thermal expectations, account for utilized control, determine the influence of outdoor conditions, and (re)define “comfortable”.  This second framework is used for a lengthy discussing of the literature to to understand how far research has come in addressing these issues.  This is later mapped onto the methods matrix in our proposal for new field study protocols.
-------------------------------------

10175384_189 - 0.975800836362 - technology_and_computing
[method, level, design, set]

Developing Innovative Designs with Manufacturing Capability Using the Level Set Method
This thesis discusses how to use topology and shape optimization, specifically the level set method, for innovative design. The level set method is a numerical algorithm that simulates the expansion of dynamic implicit surfaces. In this research, the equations for manufacturability are generated and solved through use of the level set method joined with the COMSOL multi-physics package. Specific constraints are added to make the optimization practical for engineering design. The resulting method was applied to design the best underlying support structure, conforming to both curvature and manufacturability constraints, for the longerons used with the International Space Station solar panels.
-------------------------------------

10134830_183 - 0.978227550538 - technology_and_computing
[policy-aware, user, sender, anonymity, guarantee, request, log, privacy, attacker, offline]

Policy-aware sender anonymity in Location-based services
Sender anonymity in Location-based services (LBS) refers to hiding the identity of a mobile device user who sends requests to the LBS provider for services in her proximity (e.g. ̀̀find the nearest gas station etc.''). The goal is to keep the requester's interest private even from attackers who (via hacking or subpoenas) gain access to the LBS request and to the locations of the mobile user and other nearby users at the time of request. In an LBS context, the best-studied privacy guarantee is known as sender k- anonymity. We show that the state-of-the art solutions for sender k-anonymity defend only against the naive attackers who have no knowledge of the anonymization policy that is in use. We strengthen the privacy guarantee to defend against more realistic "policy-aware' attackers'. We describe a polynomial algorithm to obtain an optimum anonymization policy. Our implementation and experiments show that the policy-aware sender k-anonymity has the potential for practical impact, being efficiently enforceable, with limited reduction in utility when compared to policy-unaware guarantees. Next we extend the policy-aware privacy guarantee to a class of attackers who knows historical user locations i.e. trajectory-aware. We call it the trajectory-aware policy-aware sender k- anonymity guarantee. We describe how this novel privacy guarantee is provided in an offline scenario, when a LBS provider logs the LBS requests sent by its users over a period of time and later wants to publish/share these logs. While these logs can be extremely valuable for advertising, data mining research and network management, we show they pose serious threat to anonymity of the LBS users. We describe a method to anonymize these LBS request logs and preserve trajectory-aware policy-aware sender k- anonymity in the offline scenario. We show that finding the optimum offline policy that provides trajectory-aware policy-aware k-anonymity is NP-Hard. Hence, we describe a novel bounded approximation algorithm and empirically show the effectiveness of this approximation algorithm for anonymizing large sizes of data (up to 1 million users)
-------------------------------------

10130093_178 - 0.998866296178 - technology_and_computing
[technology, level-based, cost, reference, analytic, model, schedule, bibliographical, readiness, framework]

Analytic framework for Technology Readiness Level-based cost and schedule models
Includes bibliographical references (p. 142-148).
-------------------------------------

10130111_178 - 0.962129072381 - technology_and_computing
[method]

Using the method of moments and Robin Hood method to solve electromagnetic scattering problems
Includes bibliographical references (p. 67-69).
-------------------------------------

10135702_183 - 0.993958439084 - technology_and_computing
[model, traffic, emission]

Estimating Emissions Using an Integrated Traffic Model
Regulators concerned with traffic related emissions on large networks should consider allowing modelers to use mesoscopic traffic models (such as the MCDKW model) that can adequately represent congestion along with appropriate emissions models. This would simplify regulatory analyses, reduce errors, and cut costs.
-------------------------------------

10137143_183 - 0.998773476328 - technology_and_computing
[variable, component, independent, datum, analysis, technique, decomposition, oval, image, structure]

Array Independent Component Analysis with Application to Remote Sensing
There are three ways to learn about an object: from samples taken directly from the site, from simulation studies based on its known scientific properties, or from remote sensing images 64.  All three are carried out to study Earth and Mars.  Our goal, however, is to learn about the second largest storm on Jupiter, called the White Oval, whose characteristics are unknown to this day 55.  As Jupiter is a gas giant and hundreds of millions of miles away from Earth 13, we can only make inferences about about the planet from retrieval algorithms and remotely sensed images.  Our focus is to find latent variables from the remotely sensed data that best explain its underlying atmospheric structure.  Principal Component Analysis (PCA) is currently the most commonly employed technique to do so.  For a data set with more than two modes, this approach fails to account for all of the variable interactions, especially if the distribution of the variables is not multivariate normal; an assumption that is rarely true of multispectral images.  The thesis presents an overview of PCA along with the most commonly employed decompositions in other fields: Independent Component Analysis, Tucker-3 and CANDECOMP/PARAFAC and discusses their limitations in finding unobserved, independent structures in a data cube.  We motivate the need for a novel dimension reduction technique that generalizes existing decompositions to find latent, statistically independent variables for one side of a multimodal (number of modes greater than two) data set while accounting for the variable interactions with its other modes.  Our method is called Array Independent Component Analysis (AICA).  As the main question of any decomposition is how to select a small number of latent variables that best capture the structure in the data, we extend the heuristic developed by Ceulemans and Kiers in 10 to aid in model selection for the AICA framework.The effectiveness of each dimension reduction technique is determined by the degree of interpretability of the uncovered hidden variables. AICA discovered two temperature gradients of the White Oval that matched the ones above the visible clouds of the Great Red Spot (in the North-South and West-East directions) 26, isolated the small storm to the South-East of the White Oval and identified the Raleigh scatter of gas molecules 52.  The other techniques, including PCA, did not isolate these or any other interpretable components.
-------------------------------------

10129802_178 - 0.998979547503 - technology_and_computing
[use, virtual, social, robot, reference, toy, language, bibliographical, family, preschooler]

Language use between preschoolers, their families, and a social robot while sharing virtual toys
Includes bibliographical references (p. 66-68).
-------------------------------------

10134315_183 - 0.99564663814 - technology_and_computing
[model, design, noc, optimization, accurate, architecture-level, chip, power, estimation, multiprocessor]

Accurate estimators and optimizers for networks-on-chip
Networks-on-chip (NoCs) are emerging as the de facto on- chip interconnection fabric of choice for both general- purpose chip multiprocessors (CMPs) 68, 108, 110 and application-specific multiprocessor systems-on-chip (MPSoCs) 43, 78. When the number of on-chip cores increases, the need for scalable and high-bandwidth communication fabric becomes more evident 43, 78. Another megatrend in advanced technologies is that power has become the most critical design constraint 57, 6. In this thesis, we present integrated research on NoC power, performance and area modeling to enable efficient early- stage design space exploration that improves our understanding and characterization of the NoC power-area- latency design space. The intellectual merit of our proposed approaches stems from their balanced attack on necessary NoC-specific techniques for (1) architecture- level estimation (to provide correct optimization objectives) and (2) architecture-level optimization (to expand the achievable design envelope). In the architecture-level estimation thrust, we develop new architecture-level estimation methods that are accurate and easily portable to different router microarchitectures. Also, our proposed models can accurately capture implementation effects. Specifically, we develop (1) automatic generation of accurate architecture-level estimation models; (2) portable models across different microarchitectures; and (3) accurate modeling of application-specific integrated circuit (ASIC) implementation flow choices and their impacts. In the architecture-level optimization thrust, we develop (1) trace-driven optimizations of NoC configurations for actual traffic behavior and workloads; and (2) simultaneous floorplan optimization of chip multiprocessors across multiple products. The broader impact of this thesis lies in helping NoC intellectual property (IP) and MPSoC designers reduce design turnaround time in addition to product chip area, delay and power metrics. This will enable the design of more complex and functional products within a given cost and power envelope. With our models, we also develop an infrastructure to extract necessary model inputs from several reliable sources (e.g., Liberty 8, SPICE 15, etc.) to ease the updating of models as new technology files become available. Finally, a significant contribution of this thesis lies in providing a publicly available framework for accurate and efficient NoC modeling 12
-------------------------------------

10135929_183 - 0.999878995274 - technology_and_computing
[set, analog, extremal, theory, structure, finite, family, question]

Shadows and intersections
This thesis makes contributions to extremal combinatorics, specifically extremal set theory questions and their analogs in other structures. Extremal set theory studies how large or small a family of subsets of a finite set X can be under various constraints. By replacing the set X with another finite object, one can pose similar questions about families of other structures. Remarkably, a question and its analogs essentially have the same answer, regardless of the object. Despite these similarities, not much is known about analogs because standard techniques do not always apply. Our main results establish analogs of extremal set theory results for structures such as vector spaces and subsums of a finite sum. We also study intersecting families and shadows in their classical context of sets by researching a conjecture of Frankl and Furedi
-------------------------------------

10175475_189 - 0.734510179875 - technology_and_computing
[iss, strategy, space, re-boost, optimal, manifold, efficient, invariant, maneuver, solution]

Trade Study of Decomissioning Strategies for the International Space Station
This thesis evaluates decommissioning strategies for the International Space Station ISS. A permanent solution is attempted by employing energy efficient invariant manifolds that arise in the circular restricted three body problem CRTBP to transport the ISS from its low Earth orbit LEO to a lunar orbit. Although the invariant manifolds provide efficient transport, getting the the ISS onto the manifolds proves quite expensive, and the trajectories take too long to complete. Therefore a more practical, although temporary, solution consisting of an optimal re-boost maneuver with the European Space Agency's automated transfer vehicle ATV is proposed. The optimal re-boost trajectory is found using control parameterization and the sequential quadratic programming SQP algorithm. The model used for optimization takes into account the affects of atmospheric drag and gravity perturbations. The optimal re-boost maneuver produces a satellite lifetime of approximately ninety-five years using a two ATV strategy.
-------------------------------------

10131511_183 - 0.999772045269 - technology_and_computing
[freight, topology, local, area, lan]

Design for local Area Freight Networks
Local area freight networks (LANs) are used to collect and distribute freight within metropolitan regions. Focusing on common carriers, this paper classifies LAN topologies, then shows how the optimal topology depends on demand characteristics. Continuous space approximations are used to analyze topologies as well as to analyze the relationship between cost and number of terminals. Key findings are summarized in Table 1.
-------------------------------------

10134797_183 - 0.99955103882 - technology_and_computing
[source, vehicle, signal, extremum, field, agent, result]

Extremum seeking for mobile robots
The work in this thesis describes theoretical and experimental results of extremum seeking applied to vehicle(s) with the objective of localizing the source of an unknown, nonlinear, signal field. For environments where position information is unavailable, the extremum seeking method is applied to autonomous vehicles as a means of navigating to find the source of some signal which the vehicles can measure locally. The signal is at maximum intensity at the source and decreases with distance away from the source. Although we only assume that the signal field has a maximum in experiments, to prove theoretical stability we use quadratic form a local approximation of the signal field. We explore the idea of dealing with a very slow or drifting sensor and provide stability results for several distinct variations of an extremum seeking scheme for 1D optimization and 2D source localization with point-mass vehicle dynamics. Detailed convergence analysis and simulations for steering-based source seeking with forward velocity regulation applied to nonholonomic vehicles are provided. We develop a deterministic algorithm in a continuum to deploy a group of autonomous vehicles (agents) capable of measuring relative position to neighbors, in a line formation, which has a higher density of agents near the source of a measurable signal and a lower density away from the source in 1D. We also consider stochastic swarming algorithms in 2D that force the net of agents to spread, maintain a formation, and seek a source without position information, whereby each agent is given a local measurement of signal field and the relative distance from neighbors. Experimental results of extremum seeking applied to mobile vehicles to perform localization, tracking, and level-set tracing of a light source are shown. We perform experiments with multiple vehicles using extremum seeking not only to localize the light source but also to avoid objects and each other. Finally, we discuss details of setting up a testbed to produce a characterized smoke plume and the results of plume source seeking experiments
-------------------------------------

10131879_183 - 0.956206274765 - technology_and_computing
[function, density, polycentric]

Modelling Worker Residence Distribution in the Los Angeles Region
This paper examines the spatial pattern of worker residences with three different density functions: monocentric, polycentric, and dispersive. Analysis of the 1980 journey-to-work census data for the Los Angeles region reveals that the polycentric density function statistically explains the actual distribution better than the monocentric density function, but the dispersive density function fits best. These findings confirm a polycentric spatial pattern, and also imply that overall accessibility to employment opportunities is the primary determinant of residential location choices.
-------------------------------------

10136048_183 - 0.99362951867 - technology_and_computing
[gameplay, agent, starcraft, capability, rts, multi-scale, game, expert, reasoning, learning]

Integrating Learning in a Multi-Scale Agent
Video games are complex simulation environments with many real-world properties that need to be addressed in order to build robust intelligence. StarCraft is a real-time strategy (RTS) game that exhibits both cognitive complexity and task environment complexity. Expert StarCraft gameplay involves many cognitive processes including estimation, anticipation, and adaptation. Achieving the objective of destroying all enemy forces requires managing a number of concurrent subtasks while working towards higher-level objectives. Working towards the goal of building expert-level performance for RTS games presents a multi-scale AI problem, which motivates the need for integrative AI systems.This thesis investigates the capabilities necessary to realize expert StarCraft gameplay in an agent. My central claim is that in order to perform at the level of an expert player, a StarCraft agent must utilize heterogeneous reasoning capabilities. This requirement is motivated by the structure of RTS gameplay, which involves both deliberative and reactive decision making, and analysis of professional gameplay, which demonstrates the need for estimation, adaptation, and anticipation reasoning capabilities. Additionally, StarCraft gameplay involves decision making across multiple scales, or levels of coordination. My approach for supporting these capabilities in an agent is to identify the competencies necessary for RTS gameplay, and develop techniques for implementing and integrating these competencies. The resulting agent, EISBot, integrates reactive planning for plan execution and monitoring, machine learning for opponent modeling, and case-based reasoning for goal formulation and strategy learning. EISBot plays StarCraft at the same action and sensing granularity as human players, and is evaluated against AI and human opponents.The contributions of this thesis are idioms for authoring agents for multi-scale AI problems, techniques for learning domain knowledge from gameplay demonstrations, and methods for integrating a variety of learning algorithms in a real-time, multi-scale agent.
-------------------------------------

10136340_183 - 0.999999496545 - technology_and_computing
[device, application, datum, network, driver, remote, side]

The networked device driver architecture : a solution for remote I/O
With rise of both mobile devices and the cloud, we see users frequently turning to remote servers for both data storage and software services, including running applications. However, once applications are no longer co- located with devices, the traditional device driver architecture no longer facilitates communication between them. Frequently, applications must be rewritten in order to receive data from a remote, rather than local, device. The networked device driver architecture is designed to support input/output devices that are separated by the network from the application(s) to which they are supplying data. The introduction of the network between the device and application also introduces issues such as high latency, low bandwidth, and jitter. We wish to compensate for these problems by alling for the processing of the data sent between the device and application. We also want to maintain network transparency, so that applications do not need to be modified in order to use remote devices. The networked device driver is split into two parts, one on each side of the network. At one end is the device and its unmodified device driver, and on the other end is the unmodified application. An I/O stream that is sourced at one end and sinked at the other may be modified by a set of pipelined transformation modules. Each module comes in a pair, one on each side of the network, with one side typically applying some operation and the other side applying a corresponding one, such as encoding and decoding the format of the data or pausing and resuming the sending of messages. We support network transparency with the pairing of modules, guaranteeing that any modification performed on the data stream will be undone before the message reaches the application. We additionally design our system with the goal of supporting ease of customization/extensibility in support of the vastly different needs of various applications and devices that can benefit from remote I/O. In this work, we explore the necessary trade-offs between ease of development and performance, demonstrating that we can leverage many existing mechanisms without creating a limiting amount of overhead
-------------------------------------

10132936_183 - 0.796430965089 - technology_and_computing
[automobile, speed]

Judging the Speed of Pedestrians and Bicycles at Night
Pedestrians and cyclists have little physical protection. Unlike the inhabitants of automobiles, they are not surrounded by rigid frames and such protective devices as airbags. Yet when pedestrians and cyclists share the roadway with automobiles, as they often do, the chance of collision with severe damage is high. In part, this is due to the great disparity of speeds involved. Pedestrians typically move quite slowly compared to the speed of automobiles, and the speed of bicycles is usually intermediate. Similarly, automobiles are much heavier and thus are capable of causing much more damage to the things with which they collide.
-------------------------------------

10134617_183 - 0.999999662369 - technology_and_computing
[communication, wireless, channel, bandwidth, multi-channel, switching, system]

An efficient multi-channel wireless switching system
To cope with the insatiable demand for a higher data rate in today's single channel wireless communications, extending the spectral bandwidth to transition to a multi- channel communication is a natural course of action. This dissertation, we present a wireless switching architecture that allows a self-interference-free asynchronous packet communication in multi-channel wireless switching networks. We propose a system architecture to resolve the self-interference problem, which arises due to the proximity among RF devices in the switch and the large difference in strengths between receiving and transmitting signals. We then present a straightforward solution of separating the frequency spectra used for receiving and transmitting signals and propose a MAC/PHY cross-layer protocol for efficiently managing the channel bandwidth for asynchronous packet-based communication. We show that, when a K-port wireless switch is used with each port providing 20MHz of bidirectional bandwidth, the total communication bandwidth can be increased to 1.4K x 20 MHz, which is about 2K times as high as a wireless access point with 20 MHz per channel. Of course, the actual data rate depends on the modulation schemes used. We also present a low SINR synchronization system as a physical layer solution of improving the immunity to the interference receiving from adjacent channels in a multi-channel communication environment. Finally, we introduce a scheduling scheme with a dynamic load balancing to ensure global fairness for all users. The performance of our algorithm is compared to that of the Least-Loaded-First (LLF) user assignment policy using simulations
-------------------------------------

10135202_183 - 0.99996916062 - technology_and_computing
[parametric, tunable, tuning, source, enhancement, optical, dispersion, mixer]

Agile optical frequency synthesis via parametric processes
Rapid development in emerging applications, such as real- time combustion monitoring and biomedical imaging, has demanded the agility performance of tunable optical sources to an unprecedented level. Contemporary tunable sources, however, possess limited tuning speed and range due to constraints in tuning mechanisms and gain material engineering. In this dissertation, we introduce a new approach to address the need for highly-agile tunable radiation through extra-cavity tuning enhancement. Recognized by the instantaneous response, wide spectral bandwidth and good power efficiency, parametric effects in silica-based optical fibers are chosen to demonstrate the agility enhancement concepts. Two schemes, namely cavity- less source tuning (CAST) and swept-pump parametric oscillation and translation (SPOT), are consequently developed using the coherent wave-mixing and amplification attributes of parametric processes. Dispersion engineering for parametric mixers forms the cornerstone of the enhancement schemes presented. Highly-efficient, wideband mixing enabled by both homogeneous and stage-wise mixer dispersion management has led to order-of-magnitude improvement in tuning characteristics. Conversely, spectrally localized parametric interaction with proper higher-order dispersion control has allowed swiftly- tunable light generation beyond the spectral range of existing tunable laser sources. This dissertation thus covers the design and experimentation of the parametric mixers that demonstrate record performance in agile light synthesis
-------------------------------------

10137643_183 - 0.908872675225 - technology_and_computing
[object, part, scene, detection, video, image, occlusion, cue, flow, algorithm]

Occlusions and Their Role in Object Detection in Video
Occlusions and disocclusions are essential cues for human perception in understanding the layout of a scene. By analyzing how some parts of the scene go out of the sight (occluded) and new parts appear (disoccluded), one can infer the topology of the objects in it. Since the scene geometry and its dynamics induce this phenomena, they are fundamental cues in computer vision and video processing tasks such as visual exploration, object recognition, activity recognition, tracking and video compression. In this thesis, we first introduce three methods to detect occlusions in an image sequence: (1) a motion segmentation algorithm which partitions an oversegmented image into two parts: a region on which optical flow is expressed with a piecewise-constant field and occluded regions where flow is not defined, (2) an optical flow estimation method which additionally detects occlusions modeling them as sparse subset of the image domain, and (3) a saliency detection algorithm which detects the parts of the image domain whose motion is inconsistent with the camera motion. In the second part of the thesis, we show that the problem of object detection in a video can be cast as an unsupervised segmentation scheme using occlusion cues and solved using convex optimization for an unknown number and geometry of objects in the scene. We further extend this approach by incorporating semantic priors for object categories that are learned from object recognition datasets. This enables the detection algorithm to segment and categorize objects jointly.
-------------------------------------

101638_108 - 0.749981235456 - technology_and_computing
[predicate, model, semantic, object, participant, dative, argument, syntactic]

On the Syntax of Ditransitive Constructions
<p>This paper deals with modelling the argument structure of constructions with two internal arguments expressing a beneficiary/recipient and a patient/theme.  It offers an analysis of the dative shift which captures both the alternative grammatical function mappings and the altered semantics of the participants of the related predicates.  The LMT variant used assumes that semantic participants are sets of semantic entailments of the predicate (Dowty 1991, Ackerman &amp; Moore 2001) and that it is the syntactic representation of the predicate’s valency, rather than a hierarchy of thematic roles, that remains constant in the model (Zaenen 1993, Ackerman &amp; Moore 2001).  Specifically, instead of fixing the thematically ordered participants and allowing them to change syntactic pre-specifications (which can lead to violations of monotonicity), the proposed model keeps constant the syntactic argument positions with their fixed pre-specifications and allows the semantic participants to re-align with them.  Such alternative alignments represent changes in the semantics of the predicate which are recognised when the predicate undergoes dative shift or applicative transitivisation.  Since in the proposed model only those objects which are capable of becoming passive subjects are –r (other objects are +o), the model straightforwardly supports the correct prediction of the theory of object asymmetries (Baker 1988, Bresnan &amp; Moshi 1990) that, when an argument can be a passive subject, it can also be expressed as an object marker in the active – but it does not make the incorrect prediction that the reverse will hold, too.  It also concurs with Alsina’s (1996a) account of the distribution of objective properties other than passivisability; this is regulated by additional constraints which are often semantic in nature and have to be determined on a language-by-language basis.  Finally, by unifying analyses of the non-applied dative and benefactive applicatives, the model provides LMT support for the special morphosyntactic status of the dative as the ‘third structural position’.</p>
-------------------------------------

10132757_183 - 0.928312827275 - technology_and_computing
[model, pavement, quality, road, datum, cost, serviceability, deterioration, goods, error]

Modeling Pavement Performance by Combining Field and Experimental Data
The accurate prediction of pavement performance is important for efficient management of the surface transportation infrastructure.  By reducing the error of the pavement deterioration prediction, agencies can obtain significant budget savings through timely intervention and accurate planning.
      The goal of this research was to develop a methodology for developing accurate pavement deterioration models to be used primarily for the management of the road infrastructure. The loss of the riding quality of the pavement was selected as the performance indicator. Two measures of riding quality were used: serviceability (Present Serviceability Index, PSI) and roughness (International Roughness Index, IRI).
      An acceptable riding quality is important for both the road user and the goods being transported. Riding quality affects the comfort of the user for whom the road is provided, and the smoothness with which goods are moved from one point to another. The vehicle operating costs and the costs of transporting goods increase as the road riding quality deteriorates. These costs are often one order of magnitude more important than the cost of maintaining the road to an acceptable level of service.
      The initial incremental models developed in this dissertation predict serviceability as a function of material properties, pavement structural characteristics, traffic axle configuration, axle load, and environmental variables. These models were developed applying nonlinear estimation techniques using an experimental unbalanced panel data set (AASHO Road Test). The unobserved heterogeneity among the pavement sections was accounted for by using the random effects approach.
      The serviceability models were updated using joint estimation with a field panel data set (MnRoad Project). The updated model estimates riding quality in terms of roughness. This was possible by applying a measurement error model to combine both data sources.
      The main contribution of this research is not the development of a deterioration model itself, but rather the demonstration of the feasibility of using joint estimation and its many advantages, such as: (i) identification and quantification of new variables, (ii) efficient parameter estimates, (iii) bias identification and correction, and (iv) use of a measurement error model to combine apparently incompatible data sources.
-------------------------------------

10138415_183 - 0.999986089343 - technology_and_computing
[simulation, parallel]

Acceleration of Radiance for Lighting Simulation by Using Parallel Computing with OpenCL
We report on the acceleration of annual daylighting simulations for fenestration systems in the Radiance ray-tracing program. The algorithm was optimized to reduce both the redundant data input/output operations and the floating-point operations. To further accelerate the simulation speed, the calculation for matrix multiplications was implemented using parallel computing on a graphics processing unit. We used OpenCL, which is a cross-platform parallel programming language. Numerical
experiments show that the combination of the above measures can speed up the annual daylighting simulations 101.7 times or 28.6 times when the sky vector has 146 or 2306 elements, respectively.
-------------------------------------

10135281_183 - 0.999950561032 - technology_and_computing
[throughput, noc, traffic, design, network, algorithm, number]

Throughput-driven design of networks-on-chip
With the increasing number of processing cores in many- core chip-multiprocessors (CMPs) and with the ever-growing number of IPs being integrated in multiprocessor systems- on-chips (MPSoCs), the need for a scalable high-bandwidth communication fabric becomes more evident. Networks-on- Chip (NoCs) have successfully catered to these needs and are fast emerging as the de-facto communication fabric in both the CMP and MPSoC domains. Both throughput and latency are important performance metrics in the design of NoCs. This thesis focuses on a throughput- driven NoC design paradigm where maximizing NoC throughput is the primary design objective. NoCs that can sustain high throughput are necessary to handle high on-chip traffic volumes expected in current and future many-core processors with a large number of processing cores and potentially running a larger number of concurrent threads. For bandwidth-hungry applications, an increase in the sustainable throughput often translates into significant reductions in communication latency during temporary traffic bursts when the network is driven close to saturation. The problem of maximizing throughput in NoCs is tackled using two different approaches. First, novel oblivious and adaptive routing algorithms are proposed for mesh and torus topologies that maximize throughput by load -balancing traffic over all network links. The proposed oblivious routing algorithms guarantee optimal worst-case throughput, which is the throughput sustained under the most adversarial traffic. Providing such guarantees is important for general-purpose CMPs where target applications and resulting traffic patterns are not known at design time. Compared to existing solutions that achieve optimal worst-case throughput, the proposed algorithms achieve significantly lower latency and higher average-case throughput. The adaptive routing algorithm proposed for mesh networks improves the accuracy of congestion estimation over prior solutions by maintaining fine-grained destination- based congestion estimates that provide greater visibility into the global congestion state of the network. This results in better routing decisions and more efficient load- balancing. Next, a new router architecture is proposed that extends NoC throughput by more efficiently multiplexing packets onto network links. It deviates from the input- buffered router architecture traditionally used in NoCs. Instead, it practically emulates an output-buffered router that is known to achieve higher throughput
-------------------------------------

10136840_183 - 0.999871067702 - technology_and_computing
[gate, design, method, modern, standard, cell, power, process]

Implications of Modern Semiconductor Technologies on Gate Sizing
Gate sizing is one of the most flexible and powerful methods available for the timing and power optimization of digital circuits. As such, it has been a very well-studied topic over the past few decades. However, developments in modern semiconductor technologies have changed the context in which gate sizing is performed. The focus has shifted from custom design methods to standard cell based designs, which has been an enabler in the design of modern, large-scale designs. We start by providing benchmarking efforts to show where the state-of-the-art is in standard cell based gate sizing. Next, we develop a framework to assess the impact of the limited precision and range available in the standard cell library on the power-delay tradeoffs.In addition, shrinking dimensions and decreased manufacturing process control has led to variations in the performance and power of the resulting designs. We provide methods for gate sizing with statistical delay, and compute bounds to show that full statistical power optimization is not essential. Lastly, to address the complexities of doing in design in a yet immature process, we provide a method to perform incremental discrete gate sizing to account for both anticipated and unanticipated changes in the manufacturing process parameters.
-------------------------------------

10137107_183 - 0.978719828382 - technology_and_computing
[performance, method, function, gradient, extremum, hessian, kalman, filter, estimate, second]

A Method of Extremum Seeking Control Based on a Time Varying Kalman Filter and its Application to Formation Flight
This dissertation presents a novel extremum seeking control method which combines a time-varying Kalman filter with a Newton Raphson algorithm. The Kalman filter is used to estimate the gradient and Hessian of a performance function. The resulting estimates are used in the Newton Raphson algorithm to guide the system to a local extremum of the performance function.Convergence of the method to a local extremum is proven when the system is subject to noisy measurements. This is accomplished by showing that the output of the algorithm is a supermartingale. It is shown that the system will converge to an area around the extremum with a radius defined, in part, by the error covariance of the Kalman filter estimates. The method is applied to two examples. The first utilizes a single independent parameter performance function. The second applies the method to the problem of formation flight for drag reduction. In the first example, two implementations of the method are examined. The first uses only gradient estimates.  The second uses both gradient and Hessian estimates. Both implementations show good convergence in the presence of noisy measurements.The second example is of formation flight for drag reduction. The problem is described in some detail and includes an aerodynamic development of the drag-reduction phenomenon. The problem is explored with two simulations. The first uses coefficient of induced drag as its performance function and estimates the gradient and Hessian of the performance function. It shows good convergence of the method. The second simulation first uses pitch angle and then aileron deflection as its performance function. It estimates the gradient but not the Hessian of the performance function. It also shows good convergence.
-------------------------------------

10130089_178 - 0.890474515252 - technology_and_computing
[electricity, bibliographical, reference, residential, space-conditioning, demand-driven, occupancy-moderation, pricing]

Zoning and occupancy-moderation for residential space-conditioning under demand-driven electricity pricing
Includes bibliographical references (p. 138-144).
-------------------------------------

10137463_183 - 0.99999851919 - technology_and_computing
[transistor, device, design, iii-v, non-planar, current, multigate, channel, co-integration, substrate]

III-V Multigate Non-Planar Channel Transistor Simulations and Technologies
As the relentless scaling of conventional Si CMOS transistors continues, it becomes more and more challenging to further increase device drive current and reduce leakage current and power consumption. III-V multigate non-planar channel transistors have emerged as a promising contender in the post-Si era due to its high carrier mobility and superior electrostatic control of the non-planar structure. For device design, current Technology Computer Aided Design (TCAD) modeling, however, fails to accurately predict device behaviors in decananometer dimensions. Further, for the analog/RF applications, parasitics engineering plays a determining role in an ultrathin body structure but the conventional symmetric source (S) / drain (D) architecture restricts design and optimization versatility. Moreover, for III-V transistors fabrication, the device-level co-integration capability is crucial but still not mature in current technology.In this work, a systematic methodology is developed to calibrate TCAD hydrodynamic model against Monte Carlo (MC) simulation in the quasi-ballistic regime. Good fits of both IDS-VGS and IDS-VDS curves have been demonstrated at various device dimensions. This methodology facilitates an accurate and time-efficient device simulation. Secondly, we explore a GaAs accumulation mode vertical transistor for asymmetric S/D design and optimization. Separate control of S/D spacer thickness and underlap length can be implemented and their individual impact on analog performance is discussed. Device design guidelines for different analog/RF metrics improvement are presented. Thirdly, we develop a VLSI-compatible top down process with co-integration capability in III-V multigate non-planar channel transistor fabrication. Nanowires are patterned by photolithography and etching of a source substrate and transferred to another receiving substrate by transfer stamping. A VLSI cleanroom tool is used in the transfer process to accurately position nanowires. This technique yields large arrays of aligned GaAs nanowires, and facilitates device-level co-integration of III-V multigate non-planar channel transistors on the same substrate with close proximity and overlay accuracy.
-------------------------------------

10138994_183 - 0.88810042917 - technology_and_computing
[system, idea, kinship, use, nomenclature]

The Recognition of Kinship  Terminologies As Formal Systems
We now know what kinship terminologies are and what their function is in kinship systems, even though this knowledge is not yet widespread.  Every social system consists of a set of organizations built up interactively by the use of specific idea systems: governmental systems are systems of organizations built up by the use of governmental ideas, military systems by the use of military ideas, economic systems by the use of economic ideas, and so on, including kinship systems by the use of kinship ideas.  These social idea systems are not preeminently nomenclatures per se, but are associated with distinctive nomenclatures, just in the way that geometry is not a nomenclature but is associated with a nomenclature.   For kinship, the core of the nomenclature has mainly been encountered and studied under the heading of “kinship terminologies.”   The ideas associated with them are the ideas that make up their definitions.  These are highly systematic and form powerfully generative  conceptual calculi.  This paper describes how this recognition has emerged from earlier, quite different, formulations.
-------------------------------------

10129781_178 - 0.972325914107 - technology_and_computing
[robust, acoustic, time, reference, environment, bibliographical, synchronization, detection, signal, ocean]

Robust acoustic signal detection and synchronization in a time varying ocean environment
Includes bibliographical references (p. 99-100).
-------------------------------------

10135610_183 - 0.993002363536 - technology_and_computing
[robot, design, research, ball, locomotion, wheel]

Design and analysis of an articulated spoke multi-modal robot and design and implementation of object manipulation features
This research encompasses the development of two mobile robots. Both of the robots feature relatively large coaxial wheels on either side of a chassis for locomotion. The first has rimless wheels formed by articulated spokes used to relocate the center of gravity and modify the support structure. The second uses the stabilization of a boom extended vertically for travel. Part 1 considers the locomotion of the actuated spoke rolling/walking/climbing robot. This novel platform provides agility for a robot capable of rolling like a wheel with a varying radius, climbing over obstacles, shimmying within a vertical shaft and creeping at a very compact height. The research focuses on determining the dynamics for a specific design, developing semi-autonomous motion planning and utilizing feedback control for end-effecter positioning and active suspension. The experimental portion focuses on sensor and actuator implementation, motion planning of varied locomotion modes and feedback control performed by an on- board microprocessor. Part 2 considers the design of three novel ping pong ball handling features for an existing rolling and ball flinging robot. Previously, the robot was able to throw balls that were manually loaded. This research in mechanical design yielded greater autonomous capabilities by adding ball-pickup, storage and release mechanisms
-------------------------------------

10136861_183 - 0.999630947008 - technology_and_computing
[memory, gate, property, nc]

Study of Nanocrystal Structures and Their Memory Applications
In the field of floating gate memory, also known as flash memory, silicon (Si) nanocrystals (NC) are one of the leading alternatives to traditional poly-Si floating gate memory because of their improved scalability, speed and simpler fabrication. In our research, Si NCs are grown on top of oxide covered carbon nanotubes (CNTs) by gas source molecular beam epitaxy for the purpose of making floating gate memory with the CNT as the field effect transistor (FET) channel. At certain conditions the NCs align on the apexes of the CNTs. These alignment properties are studied with relation to different growth conditions and sample parameters.  Also FET and memory device characteristics are studied for devices based on this structure as well as the frequency response of devices that exhibit ambipolar properties.  Another memory technology that has been attractive to replace flash due to its low power consumption and its stacking properties is resistive memory. Here ZnO NCs are used as the resistive switching material. The memory characteristics are studied with a conductive atomic force microscope contacting a single NC. Both of these technologies will be shown to have favorable properties compared to the current floating gate technology.
-------------------------------------

10137756_183 - 0.992715509522 - technology_and_computing
[model, datum, effect, kinematic, interaction, ground]

Kinematic soil-structure interaction effects from building and free-field seismic arrays in Japan
Ground motions at the foundation levels of structures differ from those in the free-field as a result of inertial and kinematic interaction effects. Inertial interaction effects tend to produce narrow-banded ground motion modification near the fundamental period of the soil-structure system, whereas kinematic effects are relatively broad-banded and concentrated at high frequencies. Kinematic interaction effects can be predicted using relatively costly finite element analyses with incoherent input or simplified models. The simplified models are semi-empirical in nature and derived from California data. These simplified models are the basis for seismic design guidelines used in the western United States, such as ASCE-41 and a pending report published by NIST. We compile some available data from building and ground instrumentation arrays in Japan for comparison to these two sets of models. We demonstrate that the model predictions for the sites under consideration are very similar to each other for modest foundation sizes (equivalent radii under about 50 m). However, the data show that both approaches overestimate the transfer function ordinates relative to those from Japanese data. This indicates that the semi-empirical models currently in use are conservative relative to these data sets. We speculate as to possible causes for the observed discrepancies.
-------------------------------------

10133503_183 - 0.999998856173 - technology_and_computing
[power, design, amplifier, device, millimeter-wave, system, development]

Design of millimeter-wave power amplifiers using InP heterojunction bipolar transistors
The focus of this dissertation is on the development of high power, monolithically integrated amplifiers for millimeter-wave wireless communication systems utilizing InP DHBT devices. Due to the ever increasing bandwidth requirements of wireless communications systems, the large amount of spectrum available at millimeter-wave frequencies is making these frequency bands increasingly more relevant. This spurs the need for the development of the various circuit and system building blocks required for implementation of reliable communications systems taking advantage of these wide-band channels. The challenges posed in the development of millimeter-wave power amplifier design range from device model development, to circuit design aspects and also compact power combiner design. The large signal nature of power amplifier operation requires device models, which accurately capture the nonlinear and heating effects of the devices used in the power amplifier design. Techniques for the measurement and model extraction of InP DHBT devices for millimeter- wave applications are discussed in detail. As part of the design of a compact millimeter-wave power amplifier, the optimum design of thermal ballasting networks and cascode termination impedances are described. Design tradeoffs in the choice of load resistance for the design are also explained. Using these techniques, a compact power amplifier operating at 72GHz was designed exhibiting 20.6dBm output power and 13.9% PAE. Finally, the design of a novel planar radial power splitter and combiner architecture is described. Transmission lines, vertical transitions and microstrip crossovers required for the implementation of this structure are explained in detail. Using this structure a high power amplifier is designed with a center frequency of 72GHz. This amplifier demonstrates an output power of 24.6dBm along with a PAE of 8.9%
-------------------------------------

10138734_183 - 0.915494650437 - technology_and_computing
[building, ev, storage, commercial, energy, stationary]

Optimal Planning and Operation of Smart Grids with Electric Vehicle Interconnection
Connection of electric storage technologies to smartgrids will have substantial implications for building energy systems. Local storage will enable demand response. When connected to buildings, mobile storage devices such as electric vehicles (EVs) are in competition with conventional stationary sources at the building. EVs can change the financial as well as environmental attractiveness of on-site generation (e.g. PV or fuel cells). In order to examine the impact of EVs on building energy costs and CO2 emissions, a distributed-energy-resources adoption problem is formulated as a mixed-integer linear program with minimization of annual building energy costs or CO2 emissions and solved for 2020 technology assumptions. The mixedinteger linear program is applied to a set of 139 different commercial buildings in California and example results as well as the aggregated economic and environmental benefits are reported. Special constraints for the available PV, solar thermal, and EV parking lots at the commercial buildings are considered. The research shows that EV batteries can be used to reduce utility-related energy costs at the smart grid or commercial building due to arbitrage of energy between buildings with different tariffs. However, putting more emphasis on CO2 emissions makes stationary storage more attractive and stationary storage capacities increase while the attractiveness of EVs decreases. The limited availability of EVs at the commercial building decreases the attractiveness of EVs and if PV is chosen by the optimization, then it is mostly used to charge the stationary
storage at the commercial building and not the EVs connected to the building.
-------------------------------------

10133635_183 - 0.999915020192 - technology_and_computing
[channel, feedback, error, delay, estimation, term, system, receiver, imperfection, multiple]

Transmit beamforming for multiple antenna systems with imperfect feedback
Multiple antennas can effectively minimize the negative impact of multiplicative fading in wireless communication systems by providing spatial diversity. In this thesis we consider a spatial diversity scheme with multiple antennas at the base station. In order to achieve the optimum performance gains, i.e., to achieve both the array gain and the diversity gain, the transmitter needs to know channel information. In frequency division duplexing systems the channel information has to be fed back to the transmitter. This feedback requirement leads to various forms of imperfection. A typical practical system has three main sources of feedback imperfection, namely, channel estimation errors, channel quantization, and feedback delay. In this thesis we comprehensively study the impact of feedback imperfections on the performance of multi-antenna systems. We develop a general framework capturing the three forms of feedback imperfection, i.e., estimation errors, quantization, and delay, for both spatially independent and correlated fading scenarios. In the modeling of imperfect feedback, we show that depending on the beamforming vector construction, the feedback delay error term can be known or unknown at the receiver. On the other hand, channel estimation error term is always unknown at the receiver. In a slow fading context, i.e., in scenarios where channel remains constant for the entire packet, we highlight the fact that both the estimation error term and the delay error term remain constant, with estimation error term unknown at the receiver and delay error term known at the receiver, for the entire packet while the thermal noise changes from symbol-to-symbol. For spatially independent channels, with the help of general framework, we then analytically quantify the effect of the three forms of feedback imperfection on the symbol and bit error probabilities of both M-PSK and M-ary rectangular QAM constellations with Gray code mapping. We also derive an analytical expression for the average packet error probability with BPSK signaling. In addition, with channel estimation errors and feedback delay, for spatially correlated channels, we develop codebook design algorithms specific to the modulation format and ergodic capacity. The new optimum codebooks show an improvement in performance compared to the existing set of codebooks available in the literature. Utilizing high resolution quantization theory and assuming perfect channel estimation at the receiver, we analyze the loss in average symbol error probability for spatially independent and correlated finite-rate feedback transmit beamforming multiple input single output systems with M1xM2-QAM constellation. We also address the issue of minimizing the negative impact of feedback delay. A natural way to combat the effect of feedback delay is channel prediction. We study the role of ergodicity in wireless channel modeling and provide an insight into when statistical channel models that employ ensemble averaging are appropriate for the purpose of channel prediction. Simulation results complement the extensive set of analytical expressions derived in the thesis
-------------------------------------

10135179_183 - 0.99899772648 - technology_and_computing
[packet, implementation, token, flow, netbump, tcp]

Packet pacer : an application over NetBump
Many ideas for adding innovative functionalities to networks require either modifying packets or performing alternative queuing to packets in flight on the data- plane. Modifications to existing network is difficult and often faced with hindrance like ease of deployability and ability to test with production-like traffic. NetBump is a platform for experimenting, evaluating and deploying these ideas with minimal intrusiveness, while leaving the switches and endhosts unmodified. In this thesis, we evaluate TCP packet pacing as an application over NetBump. We propose a modified token bucket implementation which accurately estimates the token requirement for sustained well-paced TCP flow, thus smoothing the bursty behavior. We were able to able to monitor various crucial features of the TCP flow and take informed decisions to pace the out going flow. Finally, we perform micro-benchmarks to see the effect of pacing. In general, our packet pacer implementations reduces the number of buffer overflows. Specifically, the modified token bucket implementation performs the best with zero buffer overflows
-------------------------------------

10133763_183 - 0.999964363546 - technology_and_computing
[datum, center, network, architecture, location]

LDP : location discovery protocol for data center networks
Data center architectures are gaining more and more interest nowadays due to the emerging need for hosting tens of thousands of workstations with significant aggregate bandwidth requirements. The underlying network architecture of a data center typically consists of a tree of routing and switching elements that count to thousands of components for the large-scale data centers that are deployed today. Recent research activity has been focusing on many of the challenges that one is faced with on building such large in scale data centers, such as scalability issues, dealing with the possibly inherent non -uniformity of the bandwidth among data center nodes, minimizing the actual cost of building and deploying these data centers, reducing the cabling complexity involved with interconnecting the routing and switching elements that a data center is composed of and coming up with efficient forwarding and routing mechanisms to be used within the data center. In this context, many solutions to efficient forwarding and routing have been proposed recently that take advantage of a switch' s relevant location in the global topology. This study focuses on a specific challenge regarding data center architectures, which is that of providing a plug and play functionality to a data center's switches so that no such switch will require any kind of human intervention or physical device configuration for figuring out its location in the underlying network architecture. We present a scalable, distributed and efficient Location Discovery Protocol for data center network topologies (LDP). Our solution is generic, allowing for any kind of multi-rooted tree architecture to be used when interconnecting the switching elements that constitute the core of a data center network
-------------------------------------

10135764_183 - 0.999984294016 - technology_and_computing
[system, air, thermal, pec, personal, quality, comfort]

Study of a Personal Environmental Control System Using Opposing Airstreams
The goal of this joint project is to examine the physical performance of a Personal Environmental Control (PEC) or Personal Ventilation (PV) system that uses opposing airstreams in order to control the microclimate around the human head, and its effect on people’s thermal comfort and perceived air quality. Both 4” (10.16 cm) and 2” (5.08 cm) diameter nozzles with respective exit velocity 2.0 m/s and 3.0 m/s (respective Reynolds numbers are 13,000 and 10,000) were studied with particle image velocimetry (PIV), hotwire anemometry and flow visualizations. Parallel human subject tests were conducted to validate their effects on people’s perceptions of thermal comfort and indoor air quality. The radial resultant jets generated from the PEC system are able to penetrate the free convection flow at a manikin’s face, and possess a fluctuation frequency of 2 ~ 4 Hz. The PEC system can improve thermal comfort and perceived air quality, while no dry eye discomfort was detected.
-------------------------------------

10136014_183 - 0.999497553538 - technology_and_computing
[fluid, interface, method, equation, viscosity, velocity, case, stokes, pressure, virtual]

Virtual Node Algorithms for Stokes Interface Problems
We present two numerical methods for the solution of the Stokes equations designed to handle both interfacial discontinuities, geometrically irregular flow domains and discontinuous fluid properties such as viscosity and density. The methods are efficient, easy to implement and yield second order accurate, discretely divergence free velocities. We call these methods Virtual Node Algorithms. The first method handles the case in which the fluid viscosity is continuous across the interfaces, while the second method handles the case in which the fluid viscosity is discontinuous across the interfaces. In both cases, we assume the fluid viscosity to be uniformly constant over the spatial extension of each fluid. We discretize the Stokes equations using an embedded approach on a uniform MAC-grid employing virtual nodes at interfaces and boundaries. Interfaces and boundaries are represented with a hybrid Lagrangian/level set method. For the continuous viscosity case, we rewrite the Stokes equations as three Poisson equations and use the techniques developed in Bedrossian et al. (2010) 1 to impose jump and boundary conditions. We also use a final Poisson equation to enforce a discrete divergence-free condition. All four linear systems involved are symmetric positive definite with three of the four having the standard 5-point Laplace stencil everywhere. Numerical results are presented indicating second order accuracy in L&#8734; for both velocities and pressure. For the discontinuous viscosity case, we presented a method which requires no knowledge of the jumps on the fluid variables and their derivatives along the interface. The degrees of freedom associated with virtual nodes allow for accurate resolution of discontinuities in the fluid stress at the interfaces but require a Lagrange multiplier term to enforce continuity of the fluid velocity. We provide a novel discretization of this term that accurately resolves the constant pressure null modes. The discrete coupled equations for the velocity, pressure and Lagrange multipliers are in the form of a symmetric KKT system. Numerical results are presented indicating second order accuracy for the velocities and first order accuracy for the pressure (in L&#8734;).
-------------------------------------

10132138_183 - 0.911019028099 - technology_and_computing
[model, emission, modal, prediction, datum, algorithm, transportation]

Modal Activity Models for Predicting Carbon Monoxide Emissions from Motor Vehicles
Motor vehicle emission prediction models have been shown to underestimate emissions by factors of 2 to 3 in some cases. There are many reasons why current models do not predict automobile emissions under ‘real-world’ driving conditions accurately. Among them are: non-representative driving activity used to derive emission prediction algorithms; lack of driving activity input variables; statistical shortcomings of models; and non-representativeness of tested vehicles compared to on-road vehicles.
      Given the inaccuracy of current emission prediction models and the need to accurately assess transportation control measures, incremental transportation supply changes, and intelligent transportation technologies, new activity-based emission prediction algorithms are required. Improved algorithms need to be sensitive enough to capture the effects of microscopic flow adjustments, or flow smoothing, that are now commonly considered among transportation and air quality planners.
      This paper first presents some results of second-by-second emissions data collected for a 1991 Toyota Camry in Australia. This data is used to demonstrate the importance of modal activity, and the wisdom of incorporating modal variables into an emissions model. The data was used to develop an ‘elemental’ model for prediction of emissions of CO, HC and NOx; however, only the models for acceleration and deceleration CO emissions are presented here.
      As a way to encapsulate the modal modeling approach into the existing modeling framework, a modal model estimated using traditional ‘bag data’ is introduced. The new model, comprised two linear regression modal models, collectively dubbed DITSEM (Davis Institute of Transportation Studies Emission Model), employs modal explanatory variables such as acceleration, positive kinetic energy and proportion of cycle at idle to predict CO emissions from both ‘high’ and ‘normal’ emitting vehicles.
      To measure the model performance of DITSEM, CO emission prediction algorithms embedded in ‘competing’ models (CALINE4 and EMFAC7F) are used to predict emission test results over a wide range of driving cycles. Measures of model performance compared are mean squared error, mean absolute error, Theil’s U-Statistic and the linear correlation coefficient. Statistical comparisons show that DITSEM CO prediction algorithms are superior and capture the effect of microscopic flow changes well.
      The authors suggest that significant interim improvements can be made using the existing ‘bag’ collected data⎛making them sensitive to microscopic changes in travel behavior. A more sophisticated modeling approach, one based on second by second data and similar to the one presented here, could be used for a more long term model improvement program. This large scale second-by-second effort should only be undertaken with specific measurement and modeling objectives in mind, since a great deal of improvement can already be achieved with the current data.
-------------------------------------

10129921_178 - 0.863721333015 - technology_and_computing
[retroactivity, system, function, transfer, effect]

The effect of retroactivity on the transfer function of a phosphorylation system
It was theoretically shown that impedance-like effects, called retroactivity, in biomolecular circuits can significantly impact the behavior of a system. In this paper, we quantify the effect of retroactivity on the transfer function of a phosphorylation system after linearization of its nonlinear model about a steady state. Our analysis shows that retroactivity shifts the poles of the transfer function toward the low frequency.
-------------------------------------

10135008_183 - 0.99999929658 - technology_and_computing
[optical, mapping, algorithm, gpu, cardiac, heart, attack, implementation]

GPU acceleration of optical mapping algorithm for cardiac electro-physiology
Hundreds of thousands of people die from heart attack in the U.S. every year. Cardiac dysrhythmia is known to be one of the most important causes for heart attack. Optical mapping is a tool for researching cardiac dysrhythmia and preventing heart attack. However, the optical mapping algorithm is computationally intensive and consumes a considerable amount of time. For example, 1 second of data requires 3.7 hours of computation. Therefore, a GPU implementation of the optical mapping algorithm is used in cardiac electrophysiology. The efficiency of different input sizes and memory access are studied to improve the GPU performance. The result shows that GPU implementation of the optical mapping algorithm has a 33X speedup over the equivalent CPU implementation
-------------------------------------

10135364_183 - 0.99785615014 - technology_and_computing
[information, neuron, model, maximum, input, entropy, stimulus, system, feature, computation]

Information theoretic approaches to multidimensional neural computations
Many systems in nature process information by transforming inputs from their environments into observable output states. These systems are often difficult to study because they are performing computations on multidimensional inputs with many degrees of freedom using highly nonlinear functions. The work presented in this dissertation deals with some of the issues involved with characterizing real- world input/output systems and understanding the properties of idealized systems using information theoretic methods. Using the principle of maximum entropy, a family of models are created that are consistent with certain measurable correlations from an input/output dataset but are maximally unbiased in all other respects, thereby eliminating all unjustified assumptions about the computation. In certain cases, including spiking neurons, we show that these models also minimize the mutual information. This property gives one the advantage of being able to identify the relevant input/output statistics by calculating their information content. We argue that these maximum entropy models provide a much needed quantitative framework for characterizing and understanding sensory processing neurons that are selective for multiple stimulus features. To demonstrate their usefulness, these ideas are applied to neural recordings from macaque retina and thalamus. These neurons, which primarily respond to two stimulus features, are shown to be well described using only first and second order statistics, indicating that their firing rates encode information about stimulus correlations. In addition to modeling multi-feature computations in the relevant feature space, we also show that maximum entropy models are capable of discovering the relevant feature space themselves. This technique overcomes the disadvantages of two commonly used dimensionality reduction methods and is explored using several simulated neurons, as well as retinal and thalamic recordings. Finally, we ask how neurons in a network should interact in order to transmit the most information about a stimulus. For uniform or Gaussian inputs the optimal solution is independence, whereas for naturalistic inputs the neurons should couple. The coupling strength is then quantified using a pairwise maximum entropy model
-------------------------------------

10175402_189 - 0.99997311219 - technology_and_computing
[model, cnc, component, programming, performance, language, high, heterogeneous, problem, runtime]

Mapping a Dataflow Programming Model onto Heterogeneous Architectures
This thesis describes and evaluates how extending Intel's Concurrent Collections (CnC) programming model can address the problem of hybrid programming with high performance and low energy consumption, while retaining the ease of use of data-flow
programming. 

The CnC model is a declarative, dynamic light-weight task based parallel programming model and is implicitly deterministic by enforcing the single assignment rule, properties which ensure that problems are modelled in an intuitive way.
CnC offers a separation of concerns by allowing algorithms to be expressed as a two stage process: first by decomposing a problem into components and specifying how components interact with each other, and second by providing an implementation for each component. 
By facilitating the separation between a domain expert, who can provide an accurate problem specification at a high level, and a tuning expert, who can tune the individual components for better performance, we ensure that tuning and future development, such as replacement of a subcomponent with a more efficient algorithm, become straightforward.

A recent trend in mainstream desktop systems is the use of graphics processor units (GPUs) to obtain order-of-magnitude performance improvements relative to general-purpose CPUs. In addition, the use of FPGAs has seen a significant increase for applications that can take advantage of such dedicated hardware. We see that computing is evolving from using many core CPUs to ``co-processing" on the CPU, GPU and FPGA, however hybrid programming models that support the interaction between multiple heterogeneous components are not widely accessible to mainstream programmers and domain experts who have a real need for such resources.

We propose a C-based implementation of the CnC model for enabling parallelism across heterogeneous processor components in a flexible way, with high resource utilization and high programmability. We use the task-parallel HabaneroC language (HC) as the platform for implementing CnC-HabaneroC (CnC-HC), a language also used to implement the computation steps in CnC-HC, for interaction with GPU or FPGA steps and which offers the desired flexibility and extensibility of interacting with any other C based language.

First, we extend the CnC model with tag functions and ranges to enable automatic code generation of high level operations for inter-task communication. This improves programmability and also makes the code more analysable, opening the door for future optimizations.
Secondly, we introduce a way to specify steps that are data parallel and thus are fit to execute on the GPU, and the notion of task affinity, a tuning annotation in the specification language. Affinity is used by the runtime during scheduling and can be fine-tuned based on application needs to achieve better (faster, lower power, etc.) results.
Thirdly, we introduce and develop a novel, data-driven runtime for the CnC model, using HabaneroC (HC) as a base language. In addition, we also create an implementation of the previous runtime approach and conduct a study to compare the performance.
Next, we expand the HabaneroC dynamic work-stealing runtime to allow cross-device stealing based on task affinity. Cross-device dynamic work-stealing is used to achieve load balancing across heterogeneous platforms for improved performance.
Finally, we implement and use a series of benchmarks for testing the model in different scenarios and show that our proposed approach can yield significant performance benefits and low power usage when using a hybrid execution.
-------------------------------------

10130063_178 - 0.999969995561 - technology_and_computing
[system, technique, computation-enabled, processing, reference, bibliographical, optimization, datum, human]

Optimization techniques for human computation-enabled data processing systems
Includes bibliographical references (p. 119-124).
-------------------------------------

10137480_183 - 0.999969192967 - technology_and_computing
[tool, individual, prey, use, otter, population, sea, frequency]

Variation in the frequency of tool use across and within sea otter (Enhydra lutris) populations
Sea otters (Enhydra lutris) are well known and conspicuous tool users, but little is known about what drives the maintenance of this behavior in populations or individuals. I investigated how variation in the frequency of tool use across and within sea otter populations may be influenced by ecological factors such as age class, sex and reproductive status, geographic location, feeding habitat, and prey type. Additionally, I explored whether consistent inter-individual differences in tool use occurred and if the frequency of tool use occurrence was related to learned diet specializations. I used observed foraging and tool use data collected from nine sites across two subspecies of sea otters. Over 500 individuals were observed feeding and over 100,000 feeding dives were recorded between 1985 and 2011. Using binary generalized linear mixed effects models, I found the type of prey consumed the strongest predictor of the frequency of tool use across populations, although all factors contributed to the best fit model.  In Monterey, California, I collected longitudinal data on sixty- three individuals for a minimum of one year. I found that individuals specializing in prey that required tools were more likely to carry-over this behavior to other prey consumed. My results suggest that the frequency of tool use in sea otter populations and individuals is influenced by ecological factors such as the consumption of prey that is difficult to access, and the "behavioral inertia" of individuals learning to use tools for particular prey items.
-------------------------------------

10133031_183 - 0.999997165518 - technology_and_computing
[system, architecture, nims, mobile, node, environmental]

Software Architecture for Environmental Sensing and Actuation in NIMS
The new Networked Infomechanical Systems (NIMS) architecture combines both fixed and mobile sensor nodes to achieve a spatiotemporal environment coverage that is dramatically advanced over that of either system alone. NIMS is applied to challenging environmental monitoring applications regarding primary problems in science and public health. The NIMS system includes a horizontally mobile node, a vertically mobile node, and many environmental sensors, actuators, and wireless network communication among these components. This presents a particularly capable monitoring system, but with associated high complexity that must be managed and presented properly to the system designer. This poster presents how EmStar-based architecture helps to achieve the critical requirements for the embedded system software architecture: (1) Support for autonomous and robust system operation in an unattended field environment; (2) Seamless transition from emulation, through laboratory-scale NIMS systems, to the field deployment; (3) Support for control and management of many complex device drivers that appear in NIMS electromechanical systems.
-------------------------------------

10133561_183 - 0.999959068092 - technology_and_computing
[algorithm, strategy, retrieval, step, trial]

The temporal dynamics of strategy execution in cognitive skill learning
The transition from algorithmic to memory-based performance is a core component of cognitive skill learning. E.g., an arithmetic problem such as 4 x 3 may initially be solved with a repeated addition algorithm (4 + 4 + 4), but with practice the answer will be recalled directly from long-term memory (LTM). There has been debate about the temporal dynamics of strategy execution, with some models assuming a race (i.e., independent, capacity unconstrained parallel processing) between algorithm and retrieval, and others assuming a choice mechanism. This work introduces an original paradigm that permits (for the first time) the objective identification of strategy use on every trial, as well as the latency of each component step of an algorithm. I also introduce a technique for appropriately aggregating data across different learning curves. Results are uniquely consistent with a strategy choice mechanism involving a competition between the retrieval strategy and the 1st step of the algorithm. Some previously undiscovered skill-acquisition phenomena (such as increasing latency for algorithm initiation on trials immediately preceding the first correct direct retrieval for each item) are identified and discussed. Examination of partial-algorithm trials (in which the algorithm is initiated, but abandoned prior to completion in favor of direct retrieval) indicates that for algorithms consisting of multiple retrievals from LTM, the bottleneck extends beyond the 1st step of the algorithm, whereas for simple perceptual-motor algorithms, some parallel performance on later steps is possible. I introduce a theoretical framework that can accommodate the results found for different classes of algorithms. Results highlight the importance of studying partial-algorithm trials (something that has not been possible in previous skill-acquisition paradigms), and also the importance of considering the issue of efficiency in strategy scheduling as a factor that may affect performance over the course of practice
-------------------------------------

10129679_178 - 0.969482002461 - technology_and_computing
[form, reference, bibliographical, programmable]

Programmable forms
Includes bibliographical references.
-------------------------------------

10139398_183 - 0.99999740288 - technology_and_computing
[algorithm, mapping, fasthash, amount, read, datum]

Accelerating Read Mapping with FastHASH
Abstract

With the introduction of next-generation sequencing (NGS) technologies, we are facing an exponential increase in the amount of genomic sequence data. The success of all medical and genetic applications of next-generation sequencing critically depends on the existence of computational techniques that can process and analyze the enormous amount of sequence data quickly and accurately. Unfortunately, the current read mapping algorithms have difficulties in coping with the massive amounts of data generated by NGS.
We propose a new algorithm, FastHASH, which drastically improves the performance of the seed-and-extend type hash table based read mapping algorithms, while maintaining the high sensitivity and comprehensiveness of such methods. FastHASH is a generic algorithm compatible with all seed-and-extend class read mapping algorithms. It introduces two main techniques, namely Adjacency Filtering, and Cheap K-mer Selection.
We implemented FastHASH and merged it into the codebase of the popular read mapping program, mrFAST. Depending on the edit distance cutoffs, we observed up to 19-fold speedup while still maintaining 100% sensitivity and high comprehensiveness.
-------------------------------------

10133866_183 - 0.999989731971 - technology_and_computing
[recording, channel, bit, detector, bpm, scheme, island, track, performance, optimal]

From channel modeling to signal processing for Bit patterned media recording
Bit-patterned media (BPM) recording is one method proposed to overcome the density limitations imposed by the superparamagnetic effect in continuous recording media. Channel modeling, equalization, and detection aspects of BPM recording are studied in this dissertation. In BPM recording, each bit is recorded on a single domain ̀ìsland.'' A read channel model for BPM recording is introduced where the signal contribution from each island is evaluated. Intersymbol interference (ISI) and inter- track interference (ITI) are observed in the model due to the considered head/media geometries. The noise that arises from write/read electronics is modeled by additive white Gaussian noise (AWGN). In the model, the main component of the media noise, which is called ̀ìsland jitter'', is assumed to arise from the location fluctuations of islands. Island position shift in the down - track and cross-track directions is modeled with two independent Gaussian random variables. It has been shown that the jitter-induced readback voltage is non-Gaussian. Therefore, higher order approximation for the jitter- induced readback voltage is more accurate in terms of capturing the statistical properties of this noise source. Schemes that utilize different equalization and detection methods are compared for BPM recording channels. A maximum -likelihood (ML) bit sequence detector using the Viterbi algorithm with the modified branch metric is presented for a special case of a symmetric channel response matrix. Joint-track equalization was introduced in the literature before in the context of a single interfering track. A scheme is proposed which utilizes joint-track equalization followed by a Viterbi detector for BPM recording channels. For certain recording densities, simulation results show that the performance of this scheme is comparable to that of the much more complex schemes utilizing optimal bit detection or optimal symbol sequence detection. The proposed scheme also outperforms another scheme of the same complexity introduced in the literature. A parametric study of ITI for BPM recording channel is presented. A surprising phenomenon is observed in the performance curves of optimal bit detectors : The detector performance improved for a certain range of increasing ITI levels for channels both with and without ISI and in the absence as well as in the presence of track misregistration (TMR). For the no-ISI case, this behavior is explained by means of an exact probability of error analysis for the maximum a posteriori (MAP) bit detector, i.e. optimal bit detector. An error event analysis of a punctured ML joint- track detector is used to understand the observed effects of ITI on system performance for channels with ISI
-------------------------------------

10136132_183 - 0.999970237021 - technology_and_computing
[memory, low-power, error, reliability, device, tolerant, design, mobile, work, operation]

LOW-POWER METHODOLOGY FOR FAULT TOLERANT NANOSCALE MEMORY DESIGN
Millions of mobile devices are being activated and used every single day. For such devices, energy efficient operation is very important; low-power operation enables not only long battery time but also improves energy efficiency of the servers that communicate with the mobile devices. However, reduced noise margin due to low-power operation and process variation due to nano-scale transistor feature sizes increase the number of errors in both mobile and server devices. Thus, low-power issues and reliability are strongly related.This work focuses on reliable, low-power methodologies for SRAM memories. It is the first to consider SRAM cell optimization for power and reliability simultaneously. The main contributions are the following. To guide parametric hard faults, this work addresses energy optimality and yield considering redundant spare rows and columns. This work also describes a method for soft error tolerant low-power memory design using an architectural technique to avoid Multiple Bit Upset (MBU) at low voltages. Then methods using dynamic voltage scaling for soft error tolerant low-power memory designs are investigated.This thesis results in the improvement of memory power consumption and in- creases the reliability of memory arrays. Using cell optimization, redundancy utilization, interleaving techniques, and adaptive dynamic voltage scaling, memory reliability is im- proved and power reduction is reduced by 10%-40% depending on the method applied without sacrificing error tolerance.
-------------------------------------

10131525_183 - 0.995288357422 - technology_and_computing
[transportation, surface, system]

Setting the Stage for National Transportation Policy To the Year 2020: The Surface Transportation and Uniform Relocation Assistance Act of 1987
Not widely know to the public policy community outside transportation is the fact that the Interstate Highway System is almost finished. By 1992, if all goes as planned, the United States will have a completed, fully mature Interstate transportation system. Even less known is the fact that federal gas taxes could be extended, thus revenues would continue to "roll" in.
      So large a "pot of gold" is enormous temptation. It tantalizes other underfunded public services, that may mistakenly believe surface transportation has had its day. In part anticipating a "raid," the surface transportation technical community is developing a coalition, the "2020 Plan," to build a consensus similar to the pre-Interstate era (1955-56). If successful, future surface transportation needs will continue to be funded by gas tax revenues from the highway trust fund.
      On what should the funds be spent?
      - existing surface highway and urban mass transportation system repair, restoration, and minor additions?
      - major new highway/urban mass transportation construction and operation?
      - identifying, designing and building a new transport system, yet to be selected?
      Discussion at this preliminary stage is exploratory and growing. By no means has a consensus been developed, but vast needs have been identified.
      To more fully understand how we got, legislatively, to this point, research focuses on a critical piece of transition legislation, the Surface Transportation and Uniform Relocation Assistance Act of 1987. The statute concludes an era beginning with the Interstate System in 1956 and redirected by the Surface Transportation Assistance Act of 1982. It clearly sets the stage for policy issue debates in 1992 and beyond to 2020. Research compares the three important statutes by selected major factors affecting each period of congressional debate, and relevance to fundamental program goals. A goal framework is suggested for considering future transportation legislation.
-------------------------------------

10135491_183 - 0.963584448156 - technology_and_computing
[substrate, transfer, process, device, flexible, ion-cut, implantation, temperature, carrier, high]

Integration of indium phosphide based devices with flexible substrates
Flexible substrates have many advantages in applications where bendability, space, or weight play important roles or where rigid circuits are undesirable. However, conventional flexible thin film transistors are typically characterized as having low carrier mobility as compared to devices used in the electronics industry. This is in part due to the limited temperature tolerance of plastic flexible substrates, which commonly reduces the highest processing temperature to below 200°C. Common approaches of implementation include low temperature deposition of organic, amorphous, or polycrystalline semiconductors, all of which result in carrier mobility well below 100 cm²V⁻¹s⁻¹. High quality, single crystalline III-V semiconductors such as indium phosphide (InP), on the other hand, have carrier mobility well over 1000 cm²V⁻¹s⁻¹ at room temperature, depending on carrier concentration. Recently, the ion-cut process has been used in conjunction with wafer bonding to integrate thin layers of III-V material onto silicon for optoelectronic applications. This approach has the advantage of high scalability, reusability of the initial III-V substrate, and the ability to tailor the location (depth) of the layer splitting. However, the transferred substrate usually suffers from hydrogen implantation damage. This dissertation demonstrates a new approach to enable integration of InP with various substrates, called the double-flip transfer process. The process combines ion- cutting with adhesive bonding. The problem of hydrogen implantation was overcome by patterned ion-cut transfer. In this type of transfer, areas of interest are shielded from implantation but still transferred by surrounding implanted regions. We found that patterned ion-cut transfer is strongly dependent upon crystal orientation and that using cleavage-plane oriented donors can be beneficial in transferring large areas of high quality semiconductor material. InP-based devices were fabricated to demonstrate the transfer process and test functionality following transfer. Passive devices (photodetectors) as well as active transistors were transferred and fabricated on various substrates. The transferred device layers were either implanted through with a blanket implant or protected with an ion-mask during implantation. Results demonstrate the viability of the double-flip ion-cut process in achieving very high electron mobility (2̃800 cm²V⁻¹s⁻¹) transistors on plastic flexible substrates
-------------------------------------

10136783_183 - 0.999978681265 - technology_and_computing
[camera, network, person, multiple, face, modeling]

Individual 3D Face Modeling and Recognition in a Video Network
Given an uncalibrated network of video cameras, we are tasked with the problem of building a 3D model of every person's face as they move within the network. The process of doing so requires overcoming many challenges including person detection, tracking, cross-camera correspondence (how to determine if a person in one camera is the same person in another), and the final 3D model reconstruction from multiple views in real-time or at near real-time speeds (efficient modeling and data fusion from multiple hardware sources). Towards this goal, a wireless camera network was designed and built from the ground up, a new tracking algorithm and a cross-camera human signature method was developed, and face modeling using multiple cameras in a real-world setting was performed.
-------------------------------------

10134658_183 - 0.957513456059 - technology_and_computing
[spatial, sound, relation, frequency, grouping, auditory, bregman, perceptual, motion]

The Influences of Spatial and Motion Properties on Auditory Grouping
ABSTRACT OF THE DISSERTATION	The Influences of Spatial and Motion Properties on Auditory GroupingbyRyan RobartDoctor of Philosophy, Graduate Program in PsychologyUniversity of California, Riverside, December 2010Dr. Lawrence D. Rosenblum, ChairpersonDespite a plethora of research and theory on spatial hearing and the perceptual organization of sound, the question of the relative importance of spatial and frequency relations in low level auditory grouping remains (Bregman, 1990; Rogers & Bregman, 1993; 1998; Strybel and Neale 1998; Kubovy & VanValkenberg, 2001). Most researchers share the assumption that frequency relations of sounds dominate spatial relations in the perceptual organization of sound. In a natural context, sound sources (e.g. automobiles) create sound that has a broad range of frequency content, but a limited -and often predictable-- range of motion.  It could be that in a natural context, the coherent movement of sound components may be more important in forming auditory groups than frequency similarity among those components.  The research conducted in this project tested a new theory that assumes spatial relations can dominate frequency relations in grouping (dynamic spatial assessment after initial localization DSAIL). Based on the free-field methods used by Rogers & Bregman (1993; 1998), listeners' in these experiments reported the extent of their perceptual grouping of simple tone sequences under various spatial manipulations.  In many cases, the results showed that grouping varied systematically with coherent motion-type opposite trajectory vs shared trajectory and angular separation far (>90°) vs near (<90°) among the tones. The implications of these results are discussed.
-------------------------------------

10138216_183 - 0.972361972344 - technology_and_computing
[model, energy, datum, emission, davis, saa, investment, building, lbnl, service]

Application of the Software as a Service Model to the Control of Complex Building Systems
In an effort to create broad access to its optimization software, Lawrence Berkeley National Laboratory (LBNL), in collaboration with the University of California at Davis (UC Davis) and OSISoft, has recently developed a Software as a Service (SaaS) Model for reducing energy costs, cutting peak power demand, and reducing carbon emissions for multipurpose buildings. UC Davis currently collects and stores energy usage data from buildings on its campus. Researchers at LBNL sought to demonstrate that a SaaS application architecture could be built on top of this data system to optimize the scheduling of electricity and heat delivery in the building. The SaaS interface, known as WebOpt, consists of two major parts: a) the investment & planning and b) the operations module, which builds on the investment & planning module. The operational scheduling and load shifting optimization models within the operations module use data from load prediction and electrical grid emissions models to create an optimal operating schedule for the next week, reducing peak electricity consumption while maintaining quality of energy services. LBNL's application also provides facility managers with suggested energy infrastructure investments for achieving their energy cost and emission goals based on historical data collected with OSISoft's system. This paper describes these models as well as the SaaS architecture employed by LBNL researchers to provide asset scheduling services to UC Davis. The peak demand, emissions, and cost implications of the asset operation schedule and investments suggested by this optimization model are analysed.
-------------------------------------

10134845_183 - 0.999879818551 - technology_and_computing
[datum, multi-channel, transfer, function, exploration, volume, domain, viewpoint, algorithm, selection]

Visual exploration in volume rendering for multi-channel data
Volume rendering has been an important tool to understand scientific 3D data. Traditional volume data contain only one value per voxel, but light microscopy captures multiple protein expressions from different fluorescences. This technology generates multi-channel data where several values are defined at each voxel. Because traditional volume rendering systems have assumed single channel information, i.e., opacity, there exists a significant gap between the technology available for single channel and for multi-channel data, especially in visual exploration methods for better understanding of the data. In this dissertation, we bridge the gap by investigating the characteristics of multi-channel data. The visual exploration in volume rendering has been done in many ways, but two of the most critical and effective approaches for multi-channel data are transfer function design and viewpoint selection. We first propose a new method for multi-channel transfer function design. The challenge for designing multi-dimensional transfer functions is the dimensionality of the domain. Multi-channel data often contain more than three values per voxel, which prevents users from manipulating color and opacity on multi- dimensional domain. Moreover, adding additional attributes, such as gradient, second order derivatives, or textural information, further increases the dimension of the domain. We apply recently-developed nonlinear dimensionality reduction algorithms to reduce the high dimensionality of the domain. In this work, we use Isomap and Locally Linear Embedding as well as Principal Component Analysis. Furthermore, we present a real-time viewpoint selection algorithm for multi- channel data. Because the transfer function dramatically changes the appearance of the multi-channel data, users see different objects in the data depending on the transfer function exploration. This characteristic of visualization of multi -channel data necessitates real-time viewpoint selection. The automatic viewpoint selection in the course of transfer function exploration enables users to quickly understand the data. Our algorithm takes under a second for various volume data sets, which is about 40 to 80 times faster than in previous approaches. This allows the algorithm to be integrated with real-time transfer function exploration
-------------------------------------

10133609_183 - 0.999997475645 - technology_and_computing
[motor, sensor, wireless, fault, health, system, induction, monitoring, diagnosis, classification]

Health Monitoring of Drive Connected Three-Phase Induction Motors ----- From Wired Towards Wireless Sensor Networks
Wireless sensor network (WSN), one of the featured technologies that the U.S. Department of Energy (DOE) has identified to help improve the overall energy efficiency of US industry, provides a potentially low-cost approach for the health monitoring and fault diagnosis of induction motors.  The reduction of machine failures increases plant efficiency and productivity.  Low-cost wireless sensor systems can help the health monitoring of manufacturing equipments by eliminating the cost of installation and increasing the flexibility of system diagnosis.     This research focuses on developing a nonintrusive, condition based health monitory system for drive connected induction motors using the wireless sensor network method. A hierarchical classification system is designed for motor fault diagnosis.  To simulate and analyze a wide range of fault conditions that may arise in induction motors, an experimental test bed is also developed.  Three major branches of induction motor faults are studied, either individually or in combination.  Wired sensors are first used to find optimal features for motor fault classification.  After performing feasibility studies of wireless sensors in electric machinery, two wireless sensor nodes are developed and implemented in the motor health monitoring and fault diagnosis system. The experimental results demonstrate the effectiveness and generalizability of the wireless sensor system for motor health monitoring and fault classification.
-------------------------------------

10132921_183 - 0.969506013993 - technology_and_computing
[access, transit, unlimited, campus]

BruinGo: An Evaluation
Universities and public transit agencies in the United States have together invented an arrangement – called Unlimited Access – that provides fare-free transit service for all students (and, on some campuses, faculty and staff as well). Unlimited Access is not free transit, but is instead a new way to pay for it. The university pays the transit agency for all rides taken by eligible members of the campus community. This paper evaluates the results of UCLA’s Unlimited Access program. Bus ridership for commuting to campus increased by 56 percent during BruinGO’s first year, and solo driving fell by 20 percent. Because these startling results were achieved in a city famous for its addiction to cars, they suggest that Unlimited Access will work almost anywhere.
-------------------------------------

10133679_183 - 0.999943429036 - technology_and_computing
[traffic, system, vision, loop, sensor, technique, surveillance, roadway, datum, localized]

Mobile and Stationary Computer Vision based Traffic Surveillance Techniques for Advanced ITS Applications
During the past decade, new sensing technologies, such as inductive loops, laser range scanners, radar detectors and computer vision sensors have been greatly enhanced and applied to the Intelligent Transportation System (ITS) area. Among all these sensor systems, computer vision-based approaches are one of the most popular and promising techniques used in ITS for traffic evaluation and management, driver assistance, as well as other safety related research. This is primarily due to the advantages of easy maintenance, high flexibility, and low cost for traffic scene monitoring and analysis.  Many stationary vision sensors have been already installed near the roadway, particularly at intersections. In addition, more and more vision sensors are now being installed on mobile vehicles, in order to have real time surrounding traffic information. This dissertation focuses on both mobile and stationary computer vision based traffic surveillance techniques, including the development of a new vision sensor, a survey and development of vision algorithms, as well as their applications in three different aspects of ITS areas with high quantitative requirements. These areas are outlined in further detail below.<bold><italic>Portable Loop Fault Detection</italic></bold>For many years, it has been difficult to quantitatively measure real-time freeway traffic conditions. Numerous research projects have been carried out in traffic surveillance; for example, the Freeway Performance Measurement System (PeMS) operated by Caltrans and UC Berkeley collects, processes, aggregates, and examines traffic data primarily through loop detectors buried beneath the freeway. This type of stationary embedded loop sensor system provides a point measurement for the traffic flow, roadway occupancy and average speed. By aggregating these directly measured traffic data, they can be used to estimate and provide a larger picture of the traffic conditions in certain area. However, the results obtained from embedded loop sensors are not entirely reliable. The embedded loop data delivered to a Traffic Management Center (TMC) may contain errors at one or more sensors, and between the loop detector and the TMC database. As a result, loop fault detection is important. In this dissertation, a stationary-vision based technique has been developed as part of a Portable Loop Fault Detection Tool (PLFDT). This work is complementary to recent research focusing on aggregated faulty loop data at a macroscopic level (the macroscopic level generally considers a large roadway network as consisting of links (roadways) and nodes (e.g., intersections)). The objectives of the PLFDT is to develop a real time, multi-lane, multi-vehicle tracking system for freeways using video cameras as the baseline measurement technique to compare the loop detection signal for direct fault detection for inductive loop system. <bold><italic>Localized Traffic Density Measurement</italic></bold>The embedded loop sensor system provides a direct measurement to traffic flow, roadway occupancy and average speed (only for double-loop detector). This type of sensor network does not directly measure traffic density; instead it can only be estimated. In this dissertation, we have developed systematic techniques to measure traffic conditions by utilizing both on- and off-board computer vision systems. A unique development technique is a combined computer vision and Global Positioning System (GPS) equipped mobile traffic surveillance system to measure localized traffic density. In addition, we correlate the localized density measurement with estimates from embedded loop sensor system using a space-time diagram. Experiments have shown the complementary nature of these sensing techniques. Further, most traffic surveillance computer vision algorithms and techniques are typically based on observing vehicles from stationary rectilinear cameras mounted near roadways. For many applications, some of the key tasks include extracting traffic information such as average traffic speed, flow, and density. However, less research has been carried out in observing and generating the localized traffic map around specific vehicles in the traffic stream. In this dissertation, we have developed a vision-equipped vehicle test bed for traffic surveillance purposes and have experimentally demonstrated the generation of localized traffic density from video processing and synthesizing. In contrast to the off-board surveillance systems (e.g. embedded loop sensor networks and stationary vision monitoring system), this type of on-board surveillance system provides a temporal- and spatial- continuous measurement of the localized traffic density. One of the key components developed is an Orthogonal Omni-directional Vision (OODV) System that has been developed to observe lane-level activity surrounding a vehicle, as well as the ability to observe the surrounding roadway geometry. This vision system uses a special catadioptric mirror providing a 360 degree orthogonal view of the environment. It is different from other catadioptric mirror-based Omni-directional vision systems in that it directly provides an orthogonal image without the need of warping a polar-coordinate based image to a perspective view. Based on this unique OODV, a roadway traffic surveillance system was designed and implemented. It consists of three major components: *	A GPS stamped roadway traffic data collection technique; *	Post video processing that includes automatic vehicle detection/tracking with the ability to correct using interactive tools;*	A traffic parameter (localized density) estimation process.Combined with a GPS receiver that provides approximately 2 - 3 meters spatial resolution, this traffic surveillance system can be applied not only in several traffic applications which require localized traffic density/flow/average speed measurements, but also in some other applications that require detailed roadway geometry acquisition, and vehicle activity analysis. Based on experimentation, it has been shown that the designed mobile surveillance system reports a high detection rate under the dynamic freeway environment in the experiments, with assistance from a human interactive detection module. In order to have a better understanding of dynamic traffic conditions, we have incorporated this localized traffic density measurements into a Dynamic Roadway Network Database (DRND), which has been developed to fuse the roadway traffic data and the probe vehicle data. We believe that with the increasing use of on-board vision sensors, more and more localized traffic information samples can be reported to this type of database. The combinational analysis of temporal-spatial variable density and the embedded loop sensor data will provide a better and more reliable method for traffic condition estimation and prediction. <bold><italic>Bicycle Safety Support System</italic></bold>In addition to these new traffic data collection/analysis techniques and verification process, computer vision techniques are being applied in safety studies as well. In the third part of this dissertation, stationary vision based observations have been made of the timing of bicyclists' intersection crossing maneuvers, to support of efforts in improving traffic signal timing to accommodate the needs of bicyclists. Video recordings were made of bicyclists' crossings and the video images were processed to extract the bicyclists' trajectories. These were synchronized with video images of the traffic signals so that the timing of the bicyclists' maneuvers could be determined relative to the signal phases.  The processed data have yielded cumulative distributions of the crossing speeds of bicyclists who did not have to stop at the intersection and the start-up times and final crossing speeds of the bicyclists who had to cross from a standing start. This study provides a foundation in recommendation of minimal green signal time in terms of safety purpose. The key contributions of this dissertation are:*	A unique OODV System has been designed and developed;*	A vision based systematic approach for developing a mobile traffic surveillance system has been proposed and implemented;*	A time-space diagram based flow calculation method from localized density has been proposed and experimentally verified; *	A stationary-vision based technique has been developed as part of a Portable Loop Fault Detection Tool (PLFDT) to provide baseline data; and*	A stationary vision-based intersection monitoring system has been developed for the quantitative study of bicyclist crossings at signalized intersections.
-------------------------------------

10134516_183 - 0.995139387782 - technology_and_computing
[pressure, measurement, design, casimir]

Measurements of the Casimir Pressure at Low-Temperature
The aim of this dissertation is to perform pioneering precision measurements of the Casimir pressure at temperatures from 4K to 300K and in a distance range from 0.100 - 2 micrometers. To assure the completion of these objectives I have design and constructed a state-of-the-art apparatus especially design to operate under those circumstances. The results will deepen our understanding of a phenomenon that is a key factor in the future of nanotechnology and is the only evidence for the quantum fluctuations of the vacuum.
-------------------------------------

10129798_178 - 0.942747420182 - technology_and_computing
[computational, reference, bibliographical, datum, toolkit, exploration]

Toolkit to democratize the computational exploration of data
Includes bibliographical references (p. 93-95).
-------------------------------------

10139621_183 - 0.999990656171 - technology_and_computing
[asd, network, tsc, connectivity, coherence, functional, graph, non-syndromic, patient]

Brain functional networks in syndromic and non-syndromic autism: a graph theoretical study of EEG connectivity
Abstract
            
            
               
                  Background
               
               Graph theory has been recently introduced to characterize complex brain networks, making it highly suitable to investigate altered connectivity in neurologic disorders. A current model proposes autism spectrum disorder (ASD) as a developmental disconnection syndrome, supported by converging evidence in both non-syndromic and syndromic ASD. However, the effects of abnormal connectivity on network properties have not been well studied, particularly in syndromic ASD. To close this gap, brain functional networks of electroencephalographic (EEG) connectivity were studied through graph measures in patients with Tuberous Sclerosis Complex (TSC), a disorder with a high prevalence of ASD, as well as in patients with non-syndromic ASD.
            
            
               
                  Methods
               
               EEG data were collected from TSC patients with ASD (n = 14) and without ASD (n = 29), from patients with non-syndromic ASD (n = 16), and from controls (n = 46). First, EEG connectivity was characterized by the mean coherence, the ratio of inter- over intra-hemispheric coherence and the ratio of long- over short-range coherence. Next, graph measures of the functional networks were computed and a resilience analysis was conducted. To distinguish effects related to ASD from those related to TSC, a two-way analysis of covariance (ANCOVA) was applied, using age as a covariate.
            
            
               
                  Results
               
               Analysis of network properties revealed differences specific to TSC and ASD, and these differences were very consistent across subgroups. In TSC, both with and without a concurrent diagnosis of ASD, mean coherence, global efficiency, and clustering coefficient were decreased and the average path length was increased. These findings indicate an altered network topology. In ASD, both with and without a concurrent diagnosis of TSC, decreased long- over short-range coherence and markedly increased network resilience were found.
            
            
               
                  Conclusions
               
               The altered network topology in TSC represents a functional correlate of structural abnormalities and may play a role in the pathogenesis of neurological deficits. The increased resilience in ASD may reflect an excessively degenerate network with local overconnection and decreased functional specialization. This joint study of TSC and ASD networks provides a unique window to common neurobiological mechanisms in autism.
-------------------------------------

10136704_183 - 0.999981246327 - technology_and_computing
[algorithm, datum, mining]

Efficient Algorithms for High Dimensional Data Mining
Data mining and knowledge discovery has attracted research interest in the last decade. The size and complexity of real world data is dramatically increasing and although new efficient algorithms to deal with such data are constantly being proposed, the mining of high dimensional data still presents a challenge. In this dissertation, several novel algorithms are proposed to handle such datasets. These algorithms are applied to domains as diverse as electrocardiography (ECG), electroencephalography (EEG), human DNA sequencing, protein sequencing, stock market data, gesture recognition data, motion capture data, accelerometer data, audio data, image data, handwritten manuscripts, etc. This dissertation contributes to the data mining community in three ways:Firstly, we propose a novel algorithm for searching for the nearest neighbor in time series data by using multi-level lower bounding techniques and other speed-up techniques. The proposed algorithm, called UCRSuite, is faster than the previous state-of-the-art by several orders of magnitude. Because search algorithms are primitive and a bottleneck in complex data mining algorithms, this contribution is likely to make a significant impact. Secondly, we propose two approximation algorithms to handle the high dimensional data. A fast shapelet discovery algorithm, called FastShapelet, has been proposed to discover approximate shapelets, which are as accurate as those found by an exact search. In addition, we show an unsupervised algorithm, called DocMotif, which can discover similar figures from given manuscripts. The proposed algorithms are faster than the best known algorithms by two or three orders of magnitude and the discovered results are not measurably different from the exact algorithm. Moreover, in the second work, a detailed mathematical analysis for bounding an error is provided.In my final contribution, we show that in order to create a useful clustering of a single time series, an algorithm must have the freedom to ignore some data. We propose a Minimum Description Length based time series clustering algorithm that has this ability. My results demonstrate that not only is the proposed algorithm parameter-free, but it is also efficient and effective for time series clustering.
-------------------------------------

10133224_183 - 0.950449440744 - technology_and_computing
[recognition, network, neural]

Face Recognition Using a Neural Network Simulating Olfactory Systems
A novel chaotic neural network K-set has been constructed based in research on biological olfactory systems. This non-convergent neural network simulates the capacities of biological brains for signal processing in pattern recognition. Its accuracy and efficiency are demonstrated in this report on an application to human face recognition, with comparisons of performance with conventional pattern recognition algorithms. 
-------------------------------------

10137181_183 - 0.962938713127 - technology_and_computing
[detection, filter]

Multiple-Fault Detection and Isolation Based on Disturbance Attenuation Theory
In this dissertation, a linear estimator for fault detection and isolation called the Game Theoretic Multiple-Fault Detection Filter is derived for both continuous and discrete systems. The detection filter uses a disturbance attenuation formulation to bound the transmission of disturbances to the output, approximately blocking all but one fault from each of a set of projected residuals. However, different from previous approximate methods for single-fault detection filters, the multiple-fault detection filter utilizes a secondary optimization problem to generate a solution for the estimator gain that achieves more advanced detection filter goals. Specifically, the current work examines an optimization that increases sensitivity of each projected residual to its target fault. For the continuous case, it is proven that the new detection filter approximates previous detection filters obtained from geometric and spectral theories and extends them to finite time-varying systems. Further, the detection filter is demonstrated via numerical examples.
-------------------------------------

10132556_183 - 0.880654691812 - technology_and_computing
[preference, datum, vehicle, model, logit, sp, alternative-fuel, attribute, mixed]

Joint mixed logit models of stated and revealed preferences for alternative-fuel vehicles
We compare multinomial logit and mixed logit models for data on California households' revealed and stated preferences for automobiles. The stated preference (SP) data elicited households' preferences among gasoline, electric, methanol, and compressed natural gas vehicles with various attributes. The mixed logit models provide improved fits over logit that are highly significant, and show large heterogeneity in respondents' preferences for alternative-fuel vehicles. The effects of including this heterogeneity are demonstrated in forecasting exercises. The alternative-fuel vehicle models presented here also highlight the advantages of merging SP and revealed preference (RP) data. RP data appear to be critical for obtaining realistic body-type choice and scaling information, but they are plagued by multicollinearity and difficulties with measuring vehicle attributes. SP data are critical for obtaining information about attributes not available in the marketplace, but pure SP models with these data give implausible forecasts.
-------------------------------------

10135473_183 - 0.999940890395 - technology_and_computing
[system, estimation, state, sensor, linear, kalman, measurement, placement, time, strategy]

Estimation techniques for large-scale turbulent fluid systems
Estimation, in general, involves the determination of a probability distribution. This probability distribution describes the likelihood that any particular point in phase space accurately represents the truth state. That is, without knowing the actual state of a system, estimation strategies attempt to represent the probability of any given state using only a time history of noisy observations and, when available, an approximate dynamic model of the system of interest. For low-dimensional linear systems with Gaussian uncertainty in the initial state, state disturbances, and measurement noise the de facto solution to the estimation problem has been the Kalman Filter, which provides a method to propagate the mean and covariance forward in time, making the appropriate updates to both upon the receipt of each new measurement. Although ubiquitous within academia and industry, since many systems of interest are either of very high dimension or cannot be described by linear dynamics with Gaussian uncertainty, the Kalman Filter is inappropriately applied in many applications. The present thesis first reviews extensions of estimation theory to high-dimensional systems and demonstrates the first successful reconstruction of 3D turbulent channel flow (Retau = 100), using wall information only, via the Ensemble Kalman Filter. Then a new hybrid method of estimation is described which improves estimation results for such high-dimensional systems by employing recent machine learning techniques (specifically, the Normal- Hedge algorithm) to consistently combine multiple estimators. Lastly, since the measurement operator critically determines the quality of the estimate, a gradient-base sensor/actuator placement strategy for Linear Time Invariant systems is presented. Using a test system (the Ginzburg-Landau equation) this sensor placement strategy is demonstrated by determining the optimal location for sensors in such a way that minimizes scalar metrics of the covariance matrix. With this theory clearly established, optimal sensor placements are determined for dynamic sensors in a 2D environmental plume estimation problem
-------------------------------------

10131507_183 - 0.865216694394 - technology_and_computing
[probit, model, multinomial]

Estimability in the Multinomial Probit Model
Random utility models often involve terms which represent alternative-specific errors, and the main attractive feature of the multinomial probit (MNP) model is that it allows a rather general covariance structure for these errors. However, since observed choices only reveal information regarding utility differences, and since scale cannot be determined, not all parameters in an arbitrary MNP specification may be identified. This paper examines identification restrictions that arise in the linear-in-parameters multinomial probit framework, and provides discussion and recommendations for estimation and analysis of probit normalizations.
-------------------------------------

10131678_183 - 0.847021669126 - technology_and_computing
[high-speed, train, datum, california, study]

High-Speed Trains For California
This report represents that conclusions of the first year of IURD's study of the potential for a high-speed passenger train service in California. Seven previous studies have each dealt with a specific high-speed train technology; each attempted an evaluation, standardized so far as data permitted, of its technical and economic viability.
      The present report first summarizes and synthesizes these seven studies, attempting a systematic point-by-point comparison. Then it goes on to develop a possible high-speed network for California in the light of known facts about the state's physical and economic geography. It develops physical profiles for such a route, and uses available cost data to produce an estimate of total construction cost. It gives simulations of timings between the major urban areas. These data will be used as basic inputs to the second stage of the work, now under way, which will analyze the market prospects for such a system and the ways in which it may be financed.
-------------------------------------

10136900_183 - 0.964108333367 - technology_and_computing
[par2, model, effect, mouse, airway, beta-arrestin-2, protective, study, cockroach, inflammation]

The Pro-Inflammatory but not Protective Effects of Protease-Activated-Receptor-2 In the Airways are Abolished in Beta-Arrestin-2 Mice in OVA, Cockroach Frass and Alternaria-Induced Models of Allergic Asthma
<bold>ABSTRACT OF THE DISSERTATIONThe Pro-Inflammatory but not Protective Effects of Protease-Activated-Receptor-2 In the Airways are Abolished in Beta-Arrestin-2 Mice in OVA, Cockroach Frass and Alternaria-Induced Models of Allergic AsthmabyHeddie Lynn NicholsDoctor of Philosophy, Graduate Program in Biomedical SciencesUniversity of California, Riverside, December 2012Dr. Kathryn A. DeFeaProteinase Activated Receptor-2 (PAR<sub>2</sub>), a G Protein Coupled Receptor (GPCR) activated by serine-like proteases, is reported to have both protective and pro-inflammatory effects in the airway. Given these dual and apparently opposing actions, both inhibitors and activators of PAR<sub>2</sub> have been proposed as therapeutics for asthma.  PAR<sub>2</sub> can signal through two independent pathways: a G-protein-dependent and a beta--arrestin-2-dependent/G-protein-independent one. The beta-arrestin-dependent pathway promotes leukocyte migration, while bronchiolar smooth muscle relaxation requires G-protein signaling intermediates. These studies address the hypothesis that inflammatory responses to PAR<sub>2</sub> activation are mediated by beta-arrestins, while prostaglandin production and smooth muscle relaxation are not. Our initial studies focused on a mouse ovalbumin model for PAR<sub>2</sub>-modulated airway inflammation to focus specifically on PAR<sub>2</sub> effects.  During the course of our studies, two models of PAR<sub>2</sub> dependent airway inflammation were introduced. They are: <italic>Alternaria alternata</italic>, a fungus that commonly grows in homes and on plant; and Cockroach Frass from <italic>Blatella germanica</italic> a common household pest.  These models induce a more robust inflammatory response and are more physiologically relevant as they used common household allergens.   As determined by flow cytometry, cytospin and immunohistochemistry, PAR<sub>2</sub>-induced overall lung inflammation, mucus production, airway responsiveness and recruitment of eosinophils and CD4+-lymphocytes to the Broncho Alveolar Lavage Fluid (BALF) were abolished in beta-arrestin-2-/-, compared with wild type mice. These results were exacerbated in our AltA and BG models.  In contrast, PAR<sub>2</sub> promoted equivalent bronchial epithelium-dependent tracheal smooth muscle relaxation and production of PGE2 in both wild type and beta-arrestin-2-/- mice. Our data suggest that the PAR<sub>2</sub>-enhanced inflammatory process is beta-arrestin-2-dependent, while the `protective' anti-constrictor effect of bronchial epithelial PAR<sub>2</sub> is beta-arrestin-independent.</bold>
-------------------------------------

10137658_183 - 0.98127026272 - technology_and_computing
[access-time, variation, sram, process, wid, model, var-tx, design, power, yield]

Design and Analysis of Robust Variability-Aware SRAM to Predict Optimum Access-Time to Achieve Yield Enhancement in Future Nano-Scaled CMOS.
Design variability due to inter-die (D2D) and intra-die (WID) process variations has the potential to significantly reduce the maximum operating frequency and the effective yield of high-performance chips in future process technology generations.  This variability manifests itself by increasing the access-time variance and mean of fabricated chips. This thesis proposes a new hybrid analytical-empirical model, called VAR-TX, that exhaustively computes and compares all feasible architectures subject to D2D and WID process variations (PV).  Based on its computation, VAR-TX predicts the optimal architecture that provides minimum access-time and minimum access-time variation for yield enhancement in future 16-nm on-chip conventional six-transistor static random access memories (6T-SRAMs) of given input specifications and given area and power constraints.  The given specifications include SRAM size and shape, number of columns, and word-size. In addition, this thesis reviews 6T-cell design challenges and the main causes for failure.  Also provided are several newly designed or modified circuits that are crucial for SRAM stability, reliability, robustness, speed, and reduced power consumption.  This thesis also compares the impact of D2D and WID variations on access-time for 16-nm SRAM with the 45-nm and 180-nm nodes and demonstrates that the drastic increase in the 1- and 3-sigma of the smaller nodes is mainly due to the increase in the WID variations.  A considerable number of simulation results regarding access-time, leakage current, and dynamic power are presented and analyzed throughout this thesis to help predict the impact of process, operation, and temperature variations on SRAM variability, as well.  Finally, the VAR-TX model argues previously published works that suggest that square SRAM always produces minimum delays and it significantly extends and enhances the older models by adding both an extra dimension of architectural consideration and additional device parameter fluctuation to the analysis, while producing delay estimates within 8\% of Hspice results.
-------------------------------------

10134525_183 - 0.999998483472 - technology_and_computing
[datum, center, network, modular, bandwidth, server, container, architecture, challenge]

A hybrid network architecture for modular data centers
The emergence of the mega data center has resulted in the basic building block of ever larger data centers changing from a rack comprising tens of servers to a self-contained modular shipping container that holds upto a thousand servers. These self-contained modular blocks include networking, power and cooling equipment besides servers. However, provisioning bandwidth between these containers at a large scale is still a significant challenge. Traditional approaches to provisioning bandwidth use electrical packet switches with a scale-up architecture and are often highly oversubscribed. More recent proposals such as those using clos networks promise full bisection bandwidth between servers, albeit at a high cost and power consumption. We present Helios, a hybrid architecture for modular data centers that combines electrical packet switching and optical circuit switching in a single network and dynamically provisions bandwidth between the modular containers on demand. We investigate this design from an architectural standpoint by building a fully functional prototype and explore its implications for data center networks and the challenges that it introduces. Our prototype shows the feasibility of building such a system and achieving high performance at considerably lower cost. Additionally it uncovers several issues that pose new challenges for designing large data center networks and problems that arise when circuits are rapidly reconfigured
-------------------------------------

10134818_183 - 0.820346166838 - technology_and_computing
[hsc, kit, signaling, cell, hematopoietic, shp2, mouse]

Deciphering a Kit-Shp2-Kit axis in regulation of adult hematopoietic stem and progenitor cells
It is now widely accepted that the fine balance between hematopoietic stem cell (HSC) self-renewal and differentiation is required for blood cell homeostasis and is evidently orchestrated by the dynamic interplay between environmental cues and intrinsic genetic setup. Nevertheless, how the niche signal-initiated intracellular signaling cascades are regulated in HSCs is less understood. The Stem cell factor (SCF)/Kit system has served as a classical model in deciphering molecular signaling events in the hematopoietic compartment. It has long been known that the microenvironment, or the so- called niche, of Sl/Sl mouse (steel-Dickie mice) that harbors a mutation in membrane-bound SCF or Kit-ligand, is not capable of supporting normal HSC functions. Study of various white spotting (W) mutant mice revealed an essential role of SCF-Kit signaling in HSC survival and quiescence maintenance. Kit is now a most frequently used marker for HSCs and progenitors. However, it remains to be elucidated how Kit expression is regulated in HSCs. In this dissertation, we report that a cytoplasmic tyrosine phosphatase Shp2, acting downstream of Kit, promotes Kit gene expression, constituting a Kit-Shp2-Kit signaling loop. Inducible ablation of none-receptor protein tyrosine phosphatase number 11 (PTPN11/Shp2) in adult hematopoietic compartment led to severe cytopenia in bone marrow, spleen and peripheral blood in mice. Shp2 removal resulted in elevated HSC death and loss of HSC quiescence, causing suppression of the functional HSCs/progenitors pool. Shp2- deficient HSCs failed to reconstitute lethally irradiated recipients in a cell autonomous manner due to combined defects in homing, self-renewal and survival. We show that Shp2 regulates coordinately multiple signals, including AKT, MAPK and STAT3 pathways, to promote Kit expression via the transcriptional factor Gata2. Therefore, this study reveals a novel signaling mechanism of kinase- phosphatase-kinase cascade in HSCs/progenitors in adult mammals
-------------------------------------

10129789_178 - 0.997902020348 - technology_and_computing
[side, system, bibliographical, reference, power, operation, thermal, cold, energy, plant]

Cold side thermal energy storage system for improved operation of air cooled power plants
Includes bibliographical references (p. 81-82).
-------------------------------------

10135245_183 - 0.999989560944 - technology_and_computing
[sensor, electrode, monitoring, wireless, non-contact, discrete, eeg, ecg, application, response]

Non-contact biopotential sensing
Ubiquitous physiological monitoring will be a key driving force in the upcoming wireless health revolution. Cardiac and brain signals in the form of ECG and EEG are two critical health indicators that directly benefit from long -term monitoring. Despite advancements in wireless technology and electronics miniaturization, however, the use of wireless home ECG/EEG monitoring is still limited by the inconvenience and discomfort of wet, contact electrodes. This research focuses on the development of non-contact electrodes, which do not require direct electrical skin contact as a patient-friendly alternative and begins with a review of the field. Early attempts at building non-contact sensors using off-the-shelf commercial components demonstrated the feasibility of building low-cost, wireless, wearable ECG and EEG monitoring systems. As part of this early work, it was discovered that the interface noise from the insulating medium between body and sensor was often dominant, contributing significant new knowledge in this field. Further research revealed that discrete amplifiers contained many limitations, especially regarding frequency response and noise that were difficult to surmount. Previous implementations known in the literature required extensive manual tuning and calibration in order to boost the input impedance of discrete amplifiers, an imperfect and tedious process. To overcome the challenges with using discrete components, a fully custom analog sensor front- end was developed, achieving input impedances and frequency responses far exceeding than what was previously possible, all completely without the need for manual adjustment. Validation of this sensor in ECG applications show that it easily meets medical grade frequency response specifications and attains closer signal correlation to adhesive wet electrodes. Neural applications of this sensor were also explored and validated within an EEG (stead state visual evoked potential) brain-computer interface and benchmarked against dry and wet sensors. Successful real-time control of a computer, to a degree never before demonstrated with non-contact sensors, was achieved with the electrodes placed on top of hair, completely without gels or skin preparation. Additional sensor applications including EOG eye tracking and low- power integrated, focal-plane video compression are also discussed
-------------------------------------

10132441_183 - 0.999533575115 - technology_and_computing
[object, retrieval, behavior]

Water Retrieval by Norway Rats:  Behavior as Deduction
The origin of behavior consistent with effective ("optimal") policies is an important topic in behavioral biology. In many cases, novel behavior patterns that emerge in unfamiliar situations are based on "trial and error" learning guided by rewards and punishments. The present work shows how an appropriate novel response canemerge full-blown in response to new contingencies if the situation has generic featuresthat can be recognized. This work is concerned with object retrieval, i.e., carriage of valued objects to a place of safety by Norway rats (Rattus norvegicus (Berkenhout); Rodentia: Muridae). Experiment 1 shows that selective retrieval of objects containing water over dry objects of the same material can occur immediately when rats are made thirsty; it is unlikely that this is a specific adaptation, since the opportunity to retrieve water in this way would rarely arise under natural conditions. Experiment 2 shows that without initial exposure to both objects under ad lib conditions (where the retrieval preference was for the dry objects), a process of trial and error is apparent as thirsty rats learn to select the appropriate object. It is argued that if object retrieval behavior is linked to a generic incentive feature and features such as wetness are receded into this general term, then appropriate object retrieval can be generated by a kind of deductive process. This type of generalist strategy would appear to be highly adaptive, in part because the usual tradeoffs between specialist and generalist strategies may not apply.
-------------------------------------

10134989_183 - 0.993936638296 - technology_and_computing
[design, power, distribution, performance, pdn, mobile, platform, network, system, product]

System level design of power distribution network for mobile computing platforms
Providing a reliable power distribution network (PDN) is a critical design challenge for mobile system on chip platforms. A well-designed power distribution network should be robust enough to support chipset performance while avoids eroding product profit margins through excessive design guardbanding. The solution space between these two requirements is small for PDN designs. On one hand, an inadequate PDN design can lead to test failures, missed performance targets, and intermittent functional problems in the field. On the other hand, some of the more direct PDN improvements such as adding on-die regulators, on-package discrete decoupling capacitors, and package layers increase die and package size, and could cost tens to hundreds of millions of dollars per product line. Mobile platform PDN design is challenging due to limited form factor, heterogeneous congested blocks with different design specifications, and adoption of multiple low-power techniques and modes. Therefore, it is important to develop a set of PDN design methodologies and analysis tools that can guide the product development from product inception through test and debug. This dissertation focuses on different aspects of reliable power distribution network design for mobile computing platforms. First, we propose an early-stage power distribution modeling framework to analyze the power distribution during design cycle. We consider the complete closed-loop system from voltage regulation module, printed circuit board, package and silicon die for the co- simulation. Subsequently, an enhanced time domain and frequency domain analysis flow is proposed. For assessing the performance of the power distribution in the presence of multiple functional power modes, we introduce a worst- case current loading generation. The current generation algorithm synthesizes the functional vector load based on anti-resonance (i.e., resonance-aware) and rogue wave to gain more realistic worst-case voltage variation. We investigate the impact of power distribution variation on the performance of mobile processors. We estimate the impact of power integrity considerations on low-power processor performance through pre-silicon simulation and post-silicon measurements. We present a predictive performance model under voltage and temperature variations to guide the designers in the early stages of design. As part of this effort, new emerging technologies for design of power distribution are investigated. A reliability- aware model for 3D stacked chips is developed. The model considers the complete system including Through Silicon Via (TSV), substrate noise and stacked dies from both time - and frequency-domain perspectives. Finally, we discuss the recent efficient direction towards on-die regulations for design and optimization of the Linear Dropout based PDN under worst performance. In summary, the complete framework of this thesis aims to provide the means for designing robust power distribution for current evolving mobile computing platforms
-------------------------------------

10137190_183 - 0.999963405135 - technology_and_computing
[code, manipulation, architecture, specific, universal, city, urban, design]

Code Manipulation: Architecture In-Between Universal and Specific Urban Space
Experiences from both academia and practice demonstrate that the legal instruments that comprise the primary tool for carrying out city planning in the U.S. have grown increasingly complex and abstract. Processing the universal rather than the specific aspects of urbanism, these zoning codes have a limited capacity to adapt to local significance and site-specific characteristics, to which architecture is much more responsive, and thus often constrain design innovation. Although various attempts have been made to improve the interconnection between the universal and the specific, we need a wider array of analytic frameworks within the discipline of architecture for evaluating the broader implications of the codes that regulate the form and use of buildings within the context of contemporary city planning. Taking architecture as an intermediary instrument, this study develops the notion of code manipulation as an analytical framework to be used for stimulating and evaluating designs beyond the constraints of code. Investigation of three housing projects, and the urban contexts from which they arise, expands our understanding of how the manipulation of zoning codes can be used as a generative material for the design of architecture, as well as how this procedure can both advance the production of disciplinary knowledge, and catalyse urban transformation. A loft conversion in Greenwich Village (1967-1970); an accessory dwelling unit in Venice, California (2006-2009); and a real estate development along the High Line in New York City (2005-2011) represent diverse attempts to employ code manipulation to mediate between the universal and the specific aspects of urbanism, while, at the same time, informing the making of cities through innovative design. Comparative analysis reveals how architects can explore solutions in opposition to zoning provision, and how code manipulation can serve to inform policy makers about lucrative potentials and tendencies being repressed by their own rules. Demonstrating a range of outcomes strengthens the argument that conventional zoning controls hamper architectural responses to the shifting premises of urban life. Seeking to reinforce architecture's role in the making of cities, this research explicates the potentials of code manipulation to establish new interconnections between universal and specific urban space.
-------------------------------------

10129976_178 - 0.907066297602 - technology_and_computing
[reference, analog, collective, bibliographical, computation, bio-inspired]

Bio-inspired collective analog computation
Includes bibliographical references (p. 101-102).
-------------------------------------

10139630_183 - 0.914851321987 - technology_and_computing
[datum, framework, source, species, protein, gostruct, annotation]

Combining heterogeneous data sources for accurate functional annotation of proteins
Abstract
            
            Combining heterogeneous sources of data is essential for accurate prediction of protein function. The task is complicated by the fact that while sequence-based features can be readily compared across species, most other data are species-specific. In this paper, we present a multi-view extension to GOstruct, a structured-output framework for function annotation of proteins. The extended framework can learn from disparate data sources, with each data source provided to the framework in the form of a kernel. Our empirical results demonstrate that the multi-view framework is able to utilize all available information, yielding better performance than sequence-based models trained across species and models trained from collections of data within a given species. This version of GOstruct participated in the recent Critical Assessment of Functional Annotations (CAFA) challenge; since then we have significantly improved the natural language processing component of the method, which now provides performance that is on par with that provided by sequence information. The GOstruct framework is available for download at http://strut.sourceforge.net.
-------------------------------------

10135660_183 - 0.97415811593 - technology_and_computing
[spin, transport, slg, graphene, germanium, temperature]

Spin Transport and Relaxation in Graphene and Germanium
In this thesis, I summarize our studies investigating the spin dependent properties of graphene over the last five years and the spin transport in germanium in the last three years.  In the field of graphene spintronics, this thesis includes three major advances in these fields. First, room temperature spin transport in single layer graphene (SLG) is achieved using transparent contacts (Co/SLG) and an electron-hole asymmetry of spin transport in SLG is observed. Second, tunneling spin  injection into SLG is achieved using TiO2 seeded MgO barriers. A large non-local magnetoresistance (MR) of 130 ohms is observed at room  temperature. Third, long spin lifetime in  SLG and bilayer graphene (BLG) are observed. Furthermore, strongly contrasting behavior for SLG and BLG is observed, in which SLG is dominant by Elliot-Yafet (EY) spin relaxation at low temperatures while In BLG is dominant by Dyakonov-Perel spin relaxation at low temperatures. In the field of spin transport in germanium  (Ge), this thesis includes growth of single-crystalline, atomically smooth MgO film on Ge(001) by molecular beam epitaxy, the origin of Fermi level pinning in Ge Schottky junctions using epitaxially grown ultrathin MgO films, and electrical spin injection and transport in germanium using both nonlocal and three terminal spin transport methods.
-------------------------------------

10134388_183 - 0.999998029377 - technology_and_computing
[simulation, cluster, gpu, single, time, cell, system, core, performance]

GPU accelerated cardiac electrophysiology
Numerical simulations of cellular membranes are useful for both basic science and increasingly for clinical diagnostic and therapeutic applications. A common bottleneck in such simulations arises from solving large highly complex stiff systems of ordinary differential equations (ODEs) thousands of times for numerous collocation points (representing cells) throughout a three -dimensional volume. For some electrophysiology simulations, over 98% of the time is spent solving these systems of ODEs when run in serial on a single core. We have reduced the time to simulate a single heartbeat from 4.5 hours on a 48 core Opteron cluster (MPI implementation) to 12.7 minutes on a Desktop workstation equipped with a $500 GPU accelerator that also economizes power consumption. This improvement over cluster performance transforms the simulation workflow, and at this level of performance we can realize larger scale simulations, previously only feasible on a cluster, that are needed in a clinical setting. To achieve this same performance on our Opteron cluster would theoretically require at least a 1020 core system. Thus, the GPU has effectively miniaturized the hardware requirement to perform the simulation. We also demonstrate 23x to 280x speedups across a wide spectrum of cardiac cell models running on the nVidia GTX-295 GPU compared with a multithread implementation on a 4-core Intel i7 processor. Our simulator employs a source-to-source translator that converts a higher level python description of a cell model into highly tuned and optimized CUDA source code, which is then compiled with the CUDA C compiler for execution. Our optimizations include automatic kernel partitioning and a software managed-memory cache. Our translator also removes numerical singularities introduced by using single precision arithmetic
-------------------------------------

10132988_183 - 0.99370535092 - technology_and_computing
[sensor, heterogeneous]

EmTOS: A Development Tool for Heterogeneous Sensor Networks
Recently deployed Wireless Sensor Network systems (WSNs) are increasingly following {\em heterogeneous
-------------------------------------

10138396_183 - 0.996946896931 - technology_and_computing
[datum, information, resource, proteomic, online, arabidopsis, portal, site]

MASCP Gator: an aggregation portal for the visualization of Arabidopsis proteomics data
Proteomics has become a critical tool in the functional understanding of plant processes at the molecular level. Proteomics-based studies have also contributed to the ever-expanding array of data in modern biology, with many generating Web portals and online resources that contain incrementally expanding and updated information. Many of these resources reflect specialist research areas with significant and novel information that is not currently captured by centralized repositories.
The Arabidopsis (Arabidopsis thaliana) community is well served by a number of online proteomics resources that hold an abundance of functional information. These sites can be difficult to locate among a multitude of online resources. Furthermore, they can be difficult to navigate in order to identify specific features of interest without significant technical knowledge. Recently, members of the Arabidopsis proteomics community involved in developing many of these resources decided to develop a
summary aggregation portal that is capable of retrieving proteomics data from a series of online resources on the fly. The Web portal is known as the MASCP Gator and can be accessed at the following address: http://gator.masc-proteomics.org/. Significantly, proteomics data displayed at this site retrieve information from the data repositories upon each request. This means that information is always up to date and displays the latest data sets. The site also provides hyperlinks back to the source
information hosted at each of the curated databases to facilitate more in-depth analysis of the primary data.
-------------------------------------

10134163_183 - 0.99928453347 - technology_and_computing
[system, task, energy, lifetime, assignment, dynamic, algorithm, ilp, consumption, wireless]

Energy management in wireless healthcare systems using dynamic task assignment
Wireless healthcare systems are hierarchical and heterogeneous in nature with components that have different energy and performance capabilities. Ensuring the optimal energy consumption across all these components while meeting performance requirements is a critical issue. In such systems with processing, sensing, and communication tasks, allocation of tasks to devices of the system affects the system battery lifetime and energy consumption. This thesis presents a number of static and dynamic task assignment strategies to save energy and extend system lifetime. The problem of optimal task assignment with objectives related to minimizing the total energy consumption and maximizing the system lifetime are formulated using Integer Linear Program (ILP)-based solutions. The ILP based solutions are able to improve the battery lifetime by up to 1.4 times compared to performing all of the processing tasks on the backend server. Given the dynamic nature of wireless systems, three dynamic algorithms are proposed. These algorithms are computationally efficient and are able to adapt to changing system conditions in real-time unlike ILP based solutions. DynAGreen algorithm is a graph-based task assignment algorithm with the objective of minimizing total system energy consumption. DynALife algorithm is a heuristic task assignment strategy that extends system battery lifetime. DynAGreenLife balances both system energy and system lifetime in wireless healthcare systems. Our dynamic scheduling techniques are able to improve system lifetime by up to 88% and on an average 30% in comparison to the static task assignment given by the ILP in dynamically changing urban conditions that represent real life scenarios
-------------------------------------

10133629_183 - 0.99998935024 - technology_and_computing
[interface, user, student, pentop, system, design, feedback, issue, pen, instructional]

Enabling Instructional Applications on Pentop Computers
While traditional computer interfaces based on the mouse and keyboardare ubiquitous, they are ill suited to many common applicationdomains. This is particularly true in education, where recent researchsuggests that students perform better when instructional interfacesare more similar to work practice.  Thus, the goal of our work is tocreate computational techniques and user interface design principlesto enable natural, pen-based tutoring systems that scaffold studentsin solving problems in the same way they would ordinarily solve themwith paper and pencil. In our work, we have focused on interfacessuitable for a ``pentop computer,'' a writing instrument that is usedwith special dot-patterned paper, and that has an integrated digitizerand embedded processor. A pentop is capable of producing dynamic outputin the form of synthesized speech and recorded sound clips. Accurate shape recognition is an essential foundation for developingpen-based interfaces.  We created a trainable, multi-stroke recognizerthat is insensitive to orientation, non-uniform scaling, and drawingorder.  Symbols are represented internally as attributed relationalgraphs describing both the geometry and topology of the symbols.Symbol recognition is accomplished by finding the definition symbolwhose attributed relational graph best matches that of the unknownsymbol.  We developed five efficient approximate matching techniquesto perform the graph matching.To explore instructional and interface design issues, we createdNewton's Pen, a pentop-based statics tutor. This system, which isintended for undergraduate education, scaffolds students in theconstruction of free body diagrams and equilibrium equations.Newton's Pen employs a finite state machine architecture thateffectively models the student's problem-solving progress, thusserving as a convenient means for providing context-sensitive tutorialfeedback.  User studies suggest that Newton's Pen is an effectiveteaching tool, and that students are satisfied with the interface.A key issue in the design of pentop interfaces is how to provideeffective feedback to the user.  To explore this issue, we developedPaperCAD, a system that enables users to query geometric informationfrom printed CAD drawings. PaperCAD employs two methods of feedback:audio feedback with an adjustable level of conciseness, and a PDA thatprovides a video display of the portion of the drawing near the pentip.  This system also employs a novel technique that uses a hiddenMarkov model to correct interpretation errors in hand-writtenequations.  Results of a user study suggest that users are highlysatisfied with the interface and prefer it to a traditional WIMPinterface.
-------------------------------------

10139173_183 - 0.999999219334 - technology_and_computing
[performance, video, wireless, network, room, node, different, trade-off, tx1, reliability]

Security, Reliability and Performance Issues in Wireless Networks
There are trade-offs between security, reliability and performance. In this dissertation, we consider these aspects of wireless operational networks in different settings. Specifically, with respect to security we look at replay attacks and functional reliability, with respect to reliability and performance we look at video transmission and visible light communications. Packet replay attack is a type of denial-of-service(DOS) attack, wherein an attacker replays overheard packets in the wireless network. Our experiments indicate that even a single attacker can degrade the route throughput by up to 61%. We design a lightweight detection and prevention system, COPS (for Copycat Online Prevention System), that intelligently uses a combination  of digital signatures and Bloom filters to cope with the attack. We implement COPS on real hardware and perform experiments on our 42 node wireless testbed. Our measurements indicate that COPS achieves its objective; it can efficiently contain the effects of replayed packets to a local neighborhood without incurring high resource consumption penalties. Later, we look into the functional reliability (FR) of the nodes in wireless networks. The FR is typically assessed based on evidence collected by nodes with regards to other nodes in the network. However, such evidence is often affected by factors such as channel induced effects and interference. We design a framework for collaborative assessment of the FR of nodes, with respect to different types of functions; our framework accounts for the above factors that influence evidence collection. We also design a module that drastically reduces the overhead at the expense of slightly increased uncertainty in the assessed FR values. We implement our framework on an indoor/outdoor wireless testbed. We show that with our framework, each node is able to determine the FR for every other node in the network with high accuracy. Next, we research on the performance of video transmission in wireless networks. The end-user experience in viewing a video depends on the distortion; however, also of importance is the delay experienced by the packets of the video flow since it impacts the timeliness of the information contained and the playback rate at the receiver. Unfortunately, these performance metrics are in conflict with each other in a wireless network. We investigate this trade-off between distortion and delay for video. We validate our analysis via extensive simulations. Surprisingly, we find that the trade-off depends on the specific features of the video flow: it is better to trade-off high delay for low distortion with fast motion video, but not with slow motion video. Our simulation results further quantify the trade-offs in various scenarios. Finally, we look into a visible light system in two rooms with a door open in between. Two emitters tx1 and tx2 are located in room 1 and room 2 respectively. We use BPPM, vary pulse width within the slot to provide different dimming levels. We propose a modfied ray-tracing algorithm to calculate the channel impulse response. We also provide a BER analysis considering different combinations of system parameters. Our results show that if tx2 is just illuminating, it does not impact the performance of communications in room 1. However, if both tx1 and tx2 are transmitting, the performance in room 1 is degraded to different levels depending on the position of the receivers. To improve the performance of communications in room 1 in this case, we can increase the dimming level of tx1. Moreover, when dimming level is limited, our results show that reducing the bit rate of tx1 improves the performance in room 1 dramatically. For example, when the bit rate of tx2 is 8Mb/s, reducing the bit rate of tx1 from 8Mb/s to 4Mb/s makes the BER drop from 10^-3 to 10^-13 .
-------------------------------------

10135110_183 - 0.998700545127 - technology_and_computing
[gene, image, system, model, different, organism, transcription, cell, algorithm]

Machine learning and the quantitative analysis of confocal microscopy with an application to the embryogenesis of drosophila melanogaster
In a multicellular organism, different genes are active in different cells. These patterns of gene activity control the development and differentiation of cells, ultimately deciding the form as an adult organism. Great progress has been made in the study of developmental genetics over the past 60 years. Much of this research is based on data collected at multi-cell resolution. However, it is believed that the details of gene regulation have to be understood at a molecular, rather than cellular, level. Current imaging techniques allow the measurement of multiple gene transcription events at single-molecule resolution, capturing a 3-D volumetric view of the developing organism. Unfortunately, the resulting 3-D images are too complex to allow for manual analysis. In other words, the data is available, but it is difficult to extract and to translate into a usable form. We develop a system which transforms images of multiple gene activity patterns into discrete models which capture the different combinations of nascent transcription occurring within individual cells. Using simple computer vision algorithms and sophisticated machine learning, we have developed image segmentation and counting algorithms that are adaptive. We employ active learning to engage experimentalists to teach the system their preferences, and we use these preferences to tune the parameters of these simple computer vision algorithms. This system, coupled with the experimental methods of the McGinnis and Bier Labs, is used to transform a large image archive of early stage Drosophila embryos imaged using multiplex Fluorescent in situ Hybridization (FISH) into a quantified model of Drosophila transcriptional repression at the detail of a single-molecule. With this model we study the regulation of the transcriptional state of individual nuclei within regions of developing Drosophila embryos, directly measuring the impact of Snail repressor on transcription of genes short gastrulation (sog), ventral nervous system defective (vnd), and rhomboid rho
-------------------------------------

10129814_178 - 0.999986339606 - technology_and_computing
[human-computer, reference, interface, contribution, bibliographical, three-dimensional, leaf, sound]

Contribution of three-dimensional sound to the human-computer interface
Includes bibliographical references (leaf 50).
-------------------------------------

10131036_183 - 0.99997808879 - technology_and_computing
[code]

STDS06.COD:  Cultural Complexity
This file describes codes for variables v149-v158 of the SCCS data set. These codes are from George P. Murdock and Caterina Provost. 1971. Measurement of Cultural Complexity. Ethnology 12:379-392.
-------------------------------------

10136377_183 - 0.999127020161 - technology_and_computing
[set, quantization, trapping, distribution, decoder, code, message, region]

Absorbing Set Distributions, Quantization and Practical Message Passing Decoders
It is well recognized that low-density parity-check (LDPC) codes can suffer from an error floor when decoded iteratively. This performance degradation is often attributed to the class of objects known as trapping sets. As a subset of the trapping set collection, there exists a class of graphical structures called the absorbing sets. An absorbing set is a combinatorially-defined object; in particular a fully absorbing set is stable under bit-flipping decoding. By construction, there can exist trapping sets that are not stable under such a decoder. As a result, for finite-precision, iterative decoding algorithms used over additive channels, absorbing sets can describe decoding errors more accurately than the broader class of trapping sets. In the first part of this thesis, we compute the normalized logarithmic asymptotic distributions of absorbing sets and fully absorbing sets, including elementary (fully) absorbing sets. We compare distributions of absorbing and trapping sets for representative code parameters of interest, and quantify the (lack of) discrepancies between the two. Good absorbing set properties are implied for known structured LDPC codes, including repeat accumulate codes and protograph-based constructions. Establishing the distribution of fully absorbing sets (especially when the discrepancy with the trapping set distribution is significant) allows one to further refine the estimates of the error rates under bit-flipping and related decoders.To reduce implementation complexity, the messages in a practical message passing decoder are necessarily quantized. Absorbing regions act as "decoding regions" around absorbing sets. In the second part of this thesis, we take a closer look at the interplay between quantization and absorbing regions. We provide a study of a range of quantization choices, the impact of quantization on the candidate absorbing regions, and derive guidelines for practical decoders. We show that, due to the non-linear dynamics of message passing decoders, coarser quantization may in fact perform better than finer quantization. Results of this type of work can be particularly useful in designing high performance decoders for very high-reliability storage systems, such as emerging data storage hard disk and solid state drives.
-------------------------------------

10134018_183 - 0.914635114552 - technology_and_computing
[clustering, datum, similarity, measure, technique, algorithm, protein, large, concept, constraint]

Clustering Techniques for Data Mining and Protein Design Around The Concept of Locality
Advances in technology have expedited the use of acquisition ability in computers to obtain data from diverse sources via sensors or imaging techniques with high throughput. The collected data usually tend to be extremely large, and processing a large volume of data requires computationally intensive resources. Clustering techniques simplify the data by partitioning it into meaningful groups and allow us to analyze a large volume of data in a relatively short period of time with high accuracy.This dissertation introduces several novel approaches that improve the performance of semi-supervised and unsupervised clustering by utilizing the concept of locality.  It makes two specific contributions:1.	<italic>Magnetically Affected Paths</italic>: A novel approach to apply the user-defined constraints through local manipulations in semi-supervised clustering. MAP refines the clustering results by increasing the weight of the edges connecting the objects that are in the neighborhood of a cannot-link constraint, and decreasing the weight of the edges connecting the objects that are in the neighborhood of a must-link constraint. MAPClus framework introduced in this dissertation integrates the MAP concept into the clustering algorithms by applying a three-step algorithm. The efficacy of the algorithm is demonstrated through extensive experimental evaluations on several synthetic and real datasets.2.	<italic>Wavelet-Based Similarity Measures</italic>: A family of similarity measures which exploits the ability of wavelet transformation to analyze the spectral components of the physicochemical properties and suggests a more sensitive way of measuring the similarity of biological molecules. We demonstrate the validity of our wavelet-based similarity measures by employing them in two different protein clustering applications. In the first set of experiments, we use the measures to identify the relationships between mutant proteins that were obtained by alanine scanning. Additionally, we present how accurate our methods are in recognizing the connection between charge density and electrostatic potential in homology models.
-------------------------------------

10134131_183 - 0.819065510956 - technology_and_computing
[algorithm, learning, label, space, version, method, agnostic, active, complexity]

Algorithms for active learning
This dissertation develops and analyzes active learning algorithms for binary classification problems. In passive (non-active) learning, a learner uses a random sample of labeled examples from a fixed distribution to select a hypothesis with low error. In active learning, a learner receives only a sample of unlabeled data, but has the option to query the label of any of these data points. The hope is that the active learner needs to query the labels of just a few, carefully chosen points in order to produce a hypothesis with low error. The first part of this dissertation develops algorithms based on maintaining a version space---the set of hypotheses still in contention to be selected. The version space is specifically designed to tolerate arbitrary label noise and model mismatch in the agnostic learning model. The algorithms maintain the version space using a reduction to a special form of agnostic learning that allows for example-based constraints; this represents a computational improvement over previous methods. The generalization behavior of one of these algorithms is rigorously analyzed using a quantity called the disagreement coefficient. This algorithm is shown to have label complexity that improves over that of previous methods, and matches known label complexity lower bounds in certain cases. The second part of this dissertation develops algorithms based on simpler reductions to agnostic learning that more closely match the standard abstraction of supervised learning procedures. The generalization behavior of these algorithms are also analyzed in the agnostic learning model, and are shown to have label complexity similar to the version space methods. Therefore, these algorithms represent qualitative improvements over version space methods, as strict version space methods can be risky to deploy in practice. The first of these algorithms is based on a relaxation of a version space method, and the second is based on an importance weighting technique. The second algorithm is also shown to automatically adapt to various noise conditions that imply a tighter label complexity analysis. Experiments using this algorithm are also presented to illustrate some of the promise of the method
-------------------------------------

10134379_183 - 0.999996060695 - technology_and_computing
[code, error, quantum, algorithm, measurement]

Codeword Stabilized Quantum Codes and Their Error Correction
Quantum decoherence and errors represent some of the major challenges arising in quantum computations.  Quantum error correcting codes  protect quantum states against these errors and make quantum computing more reliable. One of the main problems of quantum error correction is the design of feasible decoding algorithms that can simplify error-correction for general quantum codes. The dissertation addresses decoding of general Codeword Stabilized (CWS) codes. This class of quantum codes also includes some other important classes such as additive Stabilizer codes and non-additive Union Stabilizer (USt) codes.We first design a generic error-correcting algorithm for CWS codes and analyze the number of decoding measurements and quantum gates. This algorithm performs exhaustive screening of different error patterns, similar to decoding of classical non-linear codes.  For an n-qubit quantum code correcting up to t erroneous qubits, this brute-force approach consecutively tests all correctable error patterns and employs a separate n-qubit measurement in each test.The main result is a new error grouping technique that enables simultaneous testing of large groups of errors in a single measurement.  To achieve this reduction, we first proceed with a new error-correction algorithm for the USt codes. Secondly, we design an algorithm that converts generic non-linear CWS codes into the simpler quasi-linear USt codes. Each decoding measurement can then either locate the actual error in a given group of errors or entirely eliminate this group. This technique yields a much simpler algorithm that exponentially reduces the number of measurements about 3^t  times in any  t-error-correcting CWS code.
-------------------------------------

10134575_183 - 0.985606180175 - technology_and_computing
[power, temperature, thermal, delivery, different, multi-core, design, approach, simulation, on-chip]

Modeling, Characterization and Simulation of On-Chip Power Delivery Networks and Temperature Profile on Multi-Core Microprocessors
Reliable on-chip power delivery is a challenging design task for sub-100nm and below VLSI technologies as voltage IR drops become more and more pronounced. This situation gets worse as technology continues to scale down. And efficient verification of power integrity becomes critical for design closure. In addition, the increasing process-induced variability makes it even worse for reliable power delivery networks. The process induced variations manifest themselves at different levels (wafer level, die-level and within a die) and they are caused by different sources (lithograph, materials, aging, etc.). In this dissertation, for power delivery networks without considering process variations, we propose an efficient simulation approach, called ETBR (Extended Truncated Balanced Realization), which uses MOR (Model Order Reduction) to speed up the simulation. To make ETBR more accuracy, we further introduce an error control mechanism into it. For power delivery networks with considering process variations, we propose varETBR (variational Extended Truncated Balanced Realization), a reduced Monte-Carlo simulation approach, which can handle a large number of variables and different variation distributions. To further speedup the MOR process used in the fast simulation, a hierarchical Krylov subspace projection based MOR approach, hiePrimor, is proposed.Besides the on-chip power delivery, excessive on-chip temperature has also become a first-tier design constraint as CMOS technology scales into the nanometer region. The exponential increase of power density of the high-performance microprocessors leads to the rapid rising of the average chip temperature. Higher temperature has significant adverse impacts on chip package cost, performance, and reliability. Multi-core techniques provide a viable solution to temperature/power problems. However, designing thermal efficient multi-core microprocessors remains a challenging problem as the temperature in each core can be dramatically different and the resulting large temperature gradients can produce mechanical stress and degrade the chip reliability. In this dissertation, we investigate a new architecture-level dynamic thermal characterization problem from a behavioral modeling perspective to address the emerging thermal related analysis and optimization problems for high-performance multi-core microprocessor design. We propose a new approach, called ThermPOF, to build the thermal behavioral models from the measured or simulated thermal and power information at the architecture level. And then we extend ThermPOF into ParThermPOF, a parameterized thermal behavioral modeling approach that can handle different parameters in multi-core microprocessor design and optimization.
-------------------------------------

10136754_183 - 0.997066409908 - technology_and_computing
[link, led, vlc, lighting, communication, technique, multiple, system, characteristic, light]

Physical Layer Characteristics and Techniques for Visible Light Communications
With the rapid development of semiconductor lighting technologies, the light emitting diodes (LEDs) are promising to eventually replace traditional incandescent and fluorescent lamps for their high energy efficiency, environmental friendliness, and long lifetime. Visible light communication (VLC) utilizing lighting LEDs as transmitters has been an emerging research area since its first proposal. Ubiquitous communication coverage will become possible with wide deployment of lighting LEDs.This thesis studies physical layer characteristics of VLC systems based on either indoor LED lighting or outdoor LED traffic signaling infrastructure. Advanced communication techniques are proposed to cope with LED bandwidth limitations and grant multiple accesses. Their performance is comprehensively analyzed in typical lighting and signaling environments. Firstly, communication link issues are studied. A conversion method from photometric parameters for illumination to radiometric parameters for communication is developed. Two typical VLC links, the line-of-sight (LOS) link and non-line-of-sight (NLOS) diffuse link, are characterized both experimentally and numerically. Some optional reverse link provisions are evaluated for a full duplex system. Different noise sources and background interferences are analyzed, and dominant noises are identified under typical application scenarios. With identified signal propagation and noise characteristics, link performance is then evaluated. Secondly, transceiver design techniques to increase the data rate are proposed, including digital pre-equalization techniques and the optical orthogonal frequency division multiplexing (O-OFDM) whose peak to average power ratio (PAPR) issue is investigated. Thirdly, the capacity of the multiple-input multiple-output (MIMO) VLC system exploring inherent multiple LED transmitters and multiple photodetectors is evaluated. The effects of some system parameters involved in non-imaging and imaging transceivers are analyzed. Finally, a combined wavelength-division and code-division multiple access (WD-CDMA) scheme for indoor VLC scenario is proposed. Its performance is analyzed theoretically and then evaluated using the Monte-Carlo simulation.
-------------------------------------

10137516_183 - 0.999974901395 - technology_and_computing
[optimization, high-level, compiler, rtl, synthesis]

A Study On The Impact Of Compiler Optimizations On High-Level Synthesis
High-level synthesis is a design process which takes an un-timed, behavioral description in a high-level language like C and produces register-transfer-level (RTL) code that implements the same behaviour in hardware. In this design flow, the quality of the generated RTL is greatly influenced by high-level description of the language. Hence it follows that both source-level and IR-level compiler optimizations could either improve or hurt the quality of the generated RTL. The problem of ordering compiler optimization passes, also known as the phase-ordering problem, has been an area of active research over the past decade. An optimization has enabling and disabling effects on other optimizations, and such effects are caused by either the nature of the optimization itself, the input program being optimized, or the target platform for which the code is being optimized. A well-known fact in literature is that the standard optimization order chosen by the compiler writer may not be the best order for every input, and hence can end up producing inferior code. All methods mentioned above are targeted towards compilers producing code that will be executed on a processor. In this study, we explore the effects of both source-level and IR optimizations on high-level synthesis. The parameters of the generated RTL are verysensitive to high-level optimizations, in the sense that a right choice can provide significant benefits and a wrong choice can cause significant degradation. We consider three source-level optimizations commonly used in High-level synthesis. We study them in isolation and then propose simple yet effective heuristics to apply them to obtain a reasonable latency-area tradeoff. We also study the phase-ordering problem for IR-level optimizations from a HLS perspective. As many optimizations that are employed in a typical HLS flow were originally developed with a standard compiler in mind, and given the increasing popularity of HLS, we feel that such a study is essential to building high-quality HLS tools. Our initial results show that an input-specific order can achieve significant reduction in the latency of the generated RTL, and opens up this technology for future research.
-------------------------------------

10133958_183 - 0.99362155889 - technology_and_computing
[microphone, array, localization, particle, approach, coordinate, display, tac, time-delay, audio]

Audio localization in the Automatic Cameraman
This dissertation studies the audio localization component of a touchless interactive display located in the CSE building at UC San Diego. The display has been named The Automatic Cameraman (TAC) and consists of four large television displays, a PTZ camera, and a microphone array. In this work, we propose a simple solution to the problem of accurately pointing the PTZ camera at speaking humans who are interacting with TAC. The focus of this dissertation will be on a novel audio localization and tracking algorithm based on what we call the coordinate- free approach. Previous approaches to localization assume a precise known geometry for the microphone array. This is expressed through a coordinate system for the room with an exact position for each microphone element. As a result, arrays are typically built so that microphone positions can be known easily e.g. as linear or planar with fixed spacing. The coordinate-free method we propose requires no such knowledge of such a coordinate system allowing for an ad-hoc placement of microphones. Our coordinate-free localization algorithm employs a statistical approach by learning a mapping from observed time-delays between microphone pairs directly to a pan and tilt directive for the PTZ-camera. In addition, we explicitly utilize the fact that the training set of time-delay vectors lie on a low-dimensional structure, namely a three-dimensional structure governed by the sound source's true spatial location. We explore various regressor models with special attention to those that are known to exploit this intrinsic low dimensionality. We follow this with a study of a particle filtering based tracker of the time-delays between microphones. Our tracker employs a novel approach to the particle filtering problem based on online learning. It introduces a new, practically useful, particle resampling scheme. It is also more robust to model misspecification than traditional particle filters. In the final part of the dissertation, we examine a MEMS digital microphone based array that we recently implemented on an FPGA. We explore how this digital array will alleviate many of the technical deficiencies of the current analog array in TAC
-------------------------------------

10132835_183 - 0.999143148734 - technology_and_computing
[item, solution, mode, transportation, excess, air, technique, capacity, delivery, long-haul]

Deferred Item and Vehicle Routing within Integrated Networks
This paper studies the possible integration of long-haul operations by transportation mode and service level (defined by guaranteed delivery time) for package delivery carriers. Specifically, we consider the allocation of deferred items to excess capacity on alternative modes in ways that allow all transportation modes to be utilized better. Model formulation and solution techniques are discussed. The solution techniques presented produce efficient solutions for large-scale problem instances. Allowing deferred items to travel by air reduces long-haul transportation costs. These savings increase with the amount of excess air capacity.
-------------------------------------

10133040_183 - 0.999937814118 - technology_and_computing
[model, datum, field, parameter, performance, experimental]

Development of Pavement Performance Models by Combining Experimental and Field Data
The objective of this paper is to demonstrate the development of pavement performance models by combining experimental and field data. A two step approach was used. In the first step a riding quality model based on serviceability consideration is developed. The data set of the American Association of State Highways Officials (AASHO) Road Test is used to this effect. Due to the experimental nature of the AASHO Road Test data set, some of the estimated parameters of the model may be biased when the model is to be applied to predict performance in the field. In the second step, the original model parameters are reestimated by applying joint estimation allowed for with the incorporation of field data set. This data set was collected through the Minnesota Road Research Project (MnRoad). The final model is referred to as the joint model, and it can be used to predict the performance of in-service pavement sections. Joint estimation allowed for the full potential of both data sources to be exploited. First, the effect of variables not available in the first data source were identified and quantified. Further, the parameter estimates had lower variance because multiple data sources were pooled, and biases in the parameters of the experimental model were corrected. Finally, different measurements of the same property were incorporated by using a measurement error model. Thus, the methodology proposed in this paper makes optimum use of available data and yields models of improved statistical properties compared with techniques such as ordinary least squares.
-------------------------------------

10134460_183 - 0.994533016298 - technology_and_computing
[hash, function, design, property, application, security, multus, use]

New approaches for the design and analysis of cryptographic hash functions
Cryptographic hash functions deterministically generate a short digest225}0s̀ummary'' of an input message. Their functionality and perceived security properties have contributed to their use in a wide variety of applications. Unfortunately, traditional design approaches for hash functions target only a single application. This gap between use and design has lead to hash functions not providing the security properties required by certain uses, and, in turn, to vulnerabilities in applications. This thesis argues for the construction of multi-property hash functions. Such a function should enjoy strong guarantees that it simultaneously provides multiple, disparate security properties, while remaining efficient and easy to use. That is, these hash functions are built to reflect the diverse needs of applications. Towards this end, we introduce the notion of a multi-property-preserving domain extension transform, which formalizes the goal of multi- property hashing for a key step in hash design. By analyzing existing transforms from the lense of multi- property-preservation, we explain the inability of traditional hash designs to be multi-property. We propose new domain extension transforms, provide new techniques for their formal analysis in modern cryptography's framework of provable security, and use the techniques to show that the proposed constructions provide the multi- property-preservation guarantees needed to build the next generation of hash functions
-------------------------------------

10136621_183 - 0.972849369628 - technology_and_computing
[user, social, network, model, bipartite, website, news, clustering]

Mining User Groups in the Social News Website: Community Detection in Bipartite Networks.
Community clustering is well-studied in the context of social network analysis. However, in web services such as online retailing, music sharing library and social news website, the user data are more naturally modeled by the bipartite networks, where users are one class of the nodes and the product, song or story are the other. In this work, we propose a logistic weight model to transform the bipartite network to a weighted uni-partite network for clustering purpose. We experiment the model on Balatarin.com, a social news website and show the model leads to remarkably better clustering results by identifying and exploiting more informative users from the data.
-------------------------------------

10132734_183 - 0.727101849706 - technology_and_computing
[discrete, choice, transportation]

Discrete Choice Modeling for Transportation
This paper discusses important developments in discrete choice modeling for transportation applications. Since there have been a number of excellent recent surveys of the discrete choice literature aimed at transportation applications (see Bhat, 1997 and 2000a), this paper will concentrate on new developments and areas given less weight in recent surveys. Small and Winston (1999) give an excellent review of the transportation demand literature that includes many examples of how discrete choice models have been used in demand analysis.
-------------------------------------

10140090_183 - 0.994726760318 - technology_and_computing
[utility, reliability, practice, interruption]

A Quantitative Assessment of Utility Reporting Practices for Reporting Electric Power Distribution Events
Metrics for reliability, such as the frequency and duration of power interruptions, have been reported by electric utilities for many years. This study examines current utility practices for collecting and reporting electricity reliability information and discusses challenges that arise in assessing reliability because of differences among these practices. The study is based on reliability information for year 2006 reported by 123 utilities in 37 states representing over 60percent of total U.S. electricity sales. We quantify the effects that inconsistencies among current utility reporting practices have on comparisons of System Average Interruption Duration Index (SAIDI) and System Average Interruption Frequency Index (SAIFI) reported by utilities. We recommend immediate adoption of IEEE Std. 1366-2003 as a consistent method for measuring and reporting reliability statistics.
-------------------------------------

10137086_183 - 0.999957343158 - technology_and_computing
[system, attacker]

Low-level software security : exploiting memory safety vulnerabilities and assumptions
The security of computer systems depends in a fundamental way on the validity of assumptions made by the systems' designers. Assumptions made about attacker capabilities have a tendency to turn out false and many computer systems are insecure as a direct consequence. This is especially true with memory-safety vulnerabilities whereby an attacker is able to violate the memory-safety guarantees of a software system. Here, system designers have assumed that defenses against code injection or certain other forms of data corruption are sufficient to stop a determined attacker. In this dissertation, I will examine several instances where a system's designer incorrectly assumed that an ad hoc defense against attackers was sufficient to defend the system. First, I show how to defeat the Sequoia AVC Advantage voting machine's hardware defense against code injection. To that end, I construct a proof-of-concept, vote-stealing program by extending return-oriented programming to the Z80. Next, I show that several proposed defenses against return- oriented programming attacks are insufficient by demonstrating Turing- complete, return-oriented programming without returns on the x86. Finally, I turn to systems that attempt to prevent a malicious operating system kernel from interfering with the execution of a protected application. To do so, I introduce Iago attacks : attacks a malicious kernel can mount to subvert the execution of the protected program
-------------------------------------

10136150_183 - 0.980804878454 - technology_and_computing
[algorithm, water, satellite, color, phytoplankton, ocean, salinity, crp, mass, objective]

Identifying and tracking evolving water masses in optically complex aquatic environments
Earth's climate is intimately associated with biogeochemical processes of the sea. Biological Oceanography explores mechanisms controlling carbon uptake by phytoplankton, carbon transfer through biogeochemical processes, and energy flow through ecosystems. Satellite Oceanography affords a synoptic view of the sea surface and reveals underlying physical, chemical, and biological processes. Since the advent of ocean color satellites in 1978, ocean color algorithms evolved from quantifying phytoplankton biomass to addressing more complex bio-optical and oceanographic problems: characterizing inherent optical properties of the water column, estimating primary productivity, and detecting water masses. Locating a water mass, tracking its changes, and discriminating its constituents using bio-optical algorithms are the three objectives of this dissertation. The first objective identifies the location of the Columbia River Plume (CRP) by using light absorption by chromophoric dissolved organic matter (<italic>a</italic> <sub>CDOM</sub>) as an optical proxy for salinity. It relates <italic>in situ</italic> measurements of (<italic>a</italic> <sub>CDOM</sub> to salinity using linear regression analysis, then computes "synthetic" salinity using MODIS-Aqua satellite imagery. The algorithm is robust at predicting salinity of the CRP on the Oregon and Washington shelf. The second objective identifies sub-mesoscale features within the CRP and tracks their changes in space and time. It employs k-means clustering and discriminant function analysis to identify water types from bio-optical and environmental input variables using <italic>in situ</italic> and MODIS-Aqua satellite observations. The algorithm is robust at identifying features in satellite and mooring data, consistent with measured and modeled water masses in previous work. The third objective involves development of an optical model (PHYDOTax) that discriminates phytoplankton taxa contained within an algal bloom. A hyperspectral ocean color signature-library for known phytoplankton (dinoflagellates, diatoms, haptophytes, cryptophytes, chlorophytes, cyanophytes, and phycocyanin-containing eukaryotes) was developed and then PHDYOTax decomposed ocean color spectra for culture mixtures and field samples into constituent taxa. PHYDOTax is robust at discriminating phytoplankton taxa and is one of the first algorithms to distinguish dinoflagellates from diatoms in ocean color data. These algorithms are new tools for the oceanographic community to constrain the location of carbon uptake and transfer through space and time in the CRP, and to partition energy flow through different phytoplankton-taxon dominated ecosystems.
-------------------------------------

10133526_183 - 0.983674430584 - technology_and_computing
[model, deformation, datum, postseismic, fault, earthquake, space, time, lithosphere, depth]

Studies of co- and postseismic deformation of the lithosphere from numerical models and space geodetic data
In this dissertation, I study the co- and postseismic deformation of the lithosphere using numerical models of three-dimensional time-dependent deformation and space geodetic data. I derive an original approach to simulate the static deformation due to faulting and volcanic unrest in a heterogeneous half space with vertical and lateral variations in elastic moduli. The method is based on a semi-analytic elastic Green function in the Fourier domain. I extend the model to include time-dependent inelastic properties of the lithosphere. This approach can be used to model time series of poroelastic rebound, viscoelastic flow and fault creep, three important mechanisms thought to participate in postseismic transients. I use kinematic inversions and forward models of deformation to infer the postseismic mechanisms responsible for the transient that followed the 2003 Altai earthquake. I find that synthetic aperture radar (SAR) data are most compatible with afterslip. The absence of an observable viscoelastic relaxation in the three years following the earthquake can be explained by an effective viscosity of the ductile substrate greater than 10¹⁹ Pa s. I use numerical models of coseismic deformation to explain anomalously strained areas in the East California Shear Zone imaged by SAR line-of-sight (LOS) data in the vicinity of the 1992 Landers and 1999 Hector Mine earthquakes. I find that the enhanced strain can be explained by compliant zones (CZs) surrounding long-lived faults in the Mojave desert. The LOS data is best explained by a 50% reduction of rigidity in volumes of the order of 1-2 km thick around historical faults that extend from 5 km depth for the Calico CZ to 9 km depth for the Pinto Mountain CZ. Finally, I use kinematic inversion of GPS data and forward models to identify the location and rheology of the afterslip that followed the 2004 Parkfield earthquake. The time dependence and amplitude of GPS time series can be explained by slip on an asperity centered at 5 km depth and governed by a rate-strengthening friction with parameter (a-b)=7 x 10⁻³, compatible with values obtained from laboratory experiment. The GPS observations show evidence of lateral variations in the frictional properties on the Parkfield segment of the San Andreas fault
-------------------------------------

10129757_178 - 0.999982454252 - technology_and_computing
[pin, design, reference, machine, sorting, bibliographical, electronic, automated]

Design of an automated sorting and orienting machine for electronic pins
Includes bibliographical references (p. 93-94).
-------------------------------------

10133481_183 - 0.999905094354 - technology_and_computing
[vcsoa, inverter, gate, all-optical, flip-flop, logic, xor, nand]

All-optical logic gates based on vertical cavity semiconductor optical amplifiers
This dissertation focuses on the three most significant logic elements which can be formed from Vertical Cavity Semiconductor Optical Amplifier (VCSOA) inverters for the purpose of all-optical communication systems. These logic elements are the Set-Reset (SR) flip-flop, XOR gate, and NAND gate, and together with the basic building block VCSOA inverter, they hold promise for performance improvement in all-optical signal processing. This dissertation investigates the theory and implementation of these gates, and also provides a study of the dynamic properties of the VCSOA inverter in an effort to understand the overall performance improvement that may be provided by VCSOA logic gates. An all-optical Set-Reset (SR) flip-flop based on VCSOAs is demonstrated experimentally and theoretically. It is shown that the flip-flop can be constructed from 2 cross-coupled VCSOA inverters using the principles of cross-gain modulation, polarization anisotropy, and nonlinear gain to achieve flip-flop functionality. The flip-flop is also shown to be cascadable, have low switching power (1̃0muW), have the potential to be integrated on a small footprint (100mum²), and have the potential for single-wavelength operation. An all-optical exclusive-OR (XOR) and Not-And (NAND) gate are demonstrated experimentally and theoretically. Based on a similar platform as the flip- flop, the XOR and NAND also demonstrate the same advantages in terms of cascadability, low switching power, and high-density integration. The demonstration of XOR and NAND functionality, however, proves the versatility of the VCSOA inverter platform, and the similarity to electrical inverters highlights a key advantage of VCSOAs for developing more complex circuits. Finally, the dynamic behavior of the VCSOA inverter is studied experimentally and theoretically. The similarity between VCSOAs and VCSELs indicate that the response time of both are limited by the same factors, and thus may have the same possible solutions. Large signal analysis indicates a discrepancy in the speed of positive (rising edge) versus negative (falling edge) logic transitions which can be attributed to differences in the carrier recombination time that result from nonlinearity in the intensity-dependent gain. Small signal measurements indicate that the modulation bandwidth of the inverter approximates that of electrically modulated VCSELs, which supports the conclusion that carrier recombination time is the primary limiting factor
-------------------------------------

10138369_183 - 0.995161814329 - technology_and_computing
[model, response, building, mb, motion, component, kinematic, foundation, embedment, interaction]

Assessment of soil-structure interaction modeling strategies for response history analysis of buildings
A complete model of a soil-foundation-structure system for use in response history analysis requires modification of input motions relative to those in the free-field to account for kinematic interaction effects, foundation springs and dashpots to represent foundation-soil impedance, and a structural model. The recently completed ATC-83 project developed consistent guidelines for evaluation of kinematic interaction effects and foundation impedance for realistic conditions. We implement those procedures in seismic response history analyses for two instrumented buildings in California, one a 13-story concrete-moment frame building with two levels of basement and the other a 10-story concrete shear wall core building without embedment. We develop three-dimensional baseline models (MB) of the building and foundation systems (including SSI components) that are calibrated to reproduce observed responses from recorded earthquakes. SSI components considered in the MB model include horizontal and vertical springs and dashpots that represent the horizontal translation and rotational impedance, kinematic ground motion variations from embedment and base slab averaging, and ground motion variations over the embedment depth of basements. We then remove selected components of the MB models one at a time to evaluate their impact on engineering demand parameters (EDPs) such as inter-story drifts, story shear distributions, and floor accelerations. We find that a “bathtub” model that retains all features of the MB approach except for depth-variable motions provides for generally good above-ground superstructure responses, but biased demand assessments in subterranean levels. Other common approaches using a fixed-based representation can produce poor results.
-------------------------------------

10133898_183 - 0.999931496103 - technology_and_computing
[motion, algorithm, image, vector, patch, application, video, robust, estimation, tsv]

Priors and learning based methods for super-resolution
In this dissertation we propose priors and learning based methods for super-resolution and other video processing applications. We also propose efficient algorithms for global motion estimation. We propose total subset variation (TSV), a convexity preserving generalization of total variation (TV) prior, for higher order clique MRF. A proposed differentiable approximation of the TSV prior makes it amenable for use in large images (e.g. 1080p). A generalization to vector valued data enables use of the TSV prior for color images and motion field. A convex relaxation of sub-exponential distribution is proposed as a criterion to determine parameters of the optimization problem resulting from the TSV prior. For super-resolution application, experiments show reconstruction error improvement in terms of PSNR as well as Structural Similarity (SSIM) with respect to TV and other methods. We also propose an image up-scaling algorithm based on &nu; support vector regression. Working in the pixel domain, spatial neighborhood in the form of rectangular patches are used to determine the high resolution pixels at the center of the patch. Since, interpolation involves matching the test patch against a descriptive subset of training patches (support vectors) to find similar training patches which then have higher influence on the result of interpolation, the approach is inherently adaptive to local image content. We also investigate nu support vector regression for compression artifact reduction application. For global motion estimation application, we propose a fast and robust 2D affine global motion estimation algorithm based on phase-correlation in Fourier-Mellin domain and robust least square model fitting of sparse motion vector field. Rotation-scale- translation (RST) approximation of affine parameters is obtained at coarsest level of image pyramid, as opposed to only initial translation estimate, thus ensuring convergence for much larger range of motions. Despite working at coarsest resolution level, use of subpixel- accurate phase correlation provides sufficiently accurate coarse estimates for subsequent refinement stage of the algorithm. Refinement stage consists of RANSAC based robust least-square model fitting to sparse motion vector field, estimated using block-based subpixel-accurate phase correlation at randomly selected high activity regions in finest level of image pyramid. Resulting algorithm is very robust to outliers like foreground objects and flat regions. Experimental results show proposed algorithm is capable of estimating larger range of motions as compared to MPEG-4 verification model, while achieving a speed-up of 200. A combination of priors for statistics of single frames of natural video and motion estimation between different frames of video is essential for good performance of any general video processing application
-------------------------------------

10130057_178 - 0.999983609924 - technology_and_computing
[power, reference, monitoring, microimplant, acquisition, bibliographical, low, datum, tremor, biometric]

Low power data acquisition for microImplant biometric monitoring of tremors
Includes bibliographical references (p. 97-100).
-------------------------------------

10139763_183 - 0.999763204823 - technology_and_computing
[workflow, model, classification]

A Multi-Dimensional Classification Model for Scientific Workflow Characteristics
Workflows have been used to model repeatable tasks or operations in manufacturing, business process, and software. In recent years, workflows are increasingly used for orchestration of science discovery tasks that use distributed resources and web services environments through resource models such as grid and cloud computing. Workflows have disparate re uirements and constraints that affects how they might be
managed in distributed environments. In this paper, we present a multi-dimensional classification model illustrated by workflow examples obtained through a survey of scientists from different domains including bioinformatics and biomedical, weather and ocean modeling, astronomy detailing their data and computational requirements. The survey results and classification model contribute to the high level understanding
of scientific workflows.
-------------------------------------

10130443_178 - 0.999623666538 - technology_and_computing
[navigation, monitoring, alliance, auv, environmental, maneuvering, cooperative, surface, technology, singapore-mit]

Cooperative AUV Navigation using a Single Maneuvering Surface Craft
Singapore-MIT Alliance for Research and Technology. Center for Environmental Sensing and Monitoring
-------------------------------------

10133941_183 - 0.999466766229 - technology_and_computing
[receiver, mixer, tx, signal, passive, iip]

Highly linear SAW-less receiver design techniques for CDMA
The proliferation of wireless communication systems has resulted in ever demanding low-cost handsets. The wide adoption of the Homodyne receiver system is a consequence of industrial requirements. Yet, the inter-stage high-Q filter, rejecting the leaked transmit (Tx) signal between the LNA and mixer, is prevalent in frequency division duplexed (FDD)receivers, so that the amplified Tx signal does not corrupt overall receiver performance. This research is focused on the method of eliminating the inter -stage high-Q filter by adopting new circuit topologies. The main consideration is given to mixer design, since the mixer needs to be either highly linear or to reject the Tx signal by itself. First, a highly linear passive mixer with simple degeneration, in order to improve noise figure (NF) and IIP₂ performance, is researched. A receiver using a proposed passive mixer with degeneration is fabricated in a 0.18Mum Si CMOS process. The operating frequency is from 1.55 to 2.3 GHz. The measured performance shows less than 9.5dB double-sideband (DSB) NF, more than 22dB voltage gain, better than +50dBm uncalibrated IIP2, and higher than +7dBm of IIP₃, while consuming only 10mW from a 2 V supply. In the second approach, an embedded filtering passive (EFP) mixer is researched. Unlike a normal passive mixer, the EFP mixer performs narrow band downconversion, which provides filtering for the Tx signal. A complete receiver, consisting of LNA, EFP mixer, transimpedance amplifier (TIA), and local oscillator (LO) path, is fabricated in a 0.18Mum Si CMOS process. The receiver IC exhibits more than +60dBm of Rx IIP₂, 2.4dB Rx noise figure, and +77dB of triple beat (TB) with 45 MHz offset transmit leakage at 900 MHz Rx frequency while consuming only 18mA from a 2.1 V supply. The proposed receiver IC shows an additional 15dB Tx rejection compared to a conventional receiver. The additional Tx rejection improved the IIP₂ by 10dB and the TB by 30dB
-------------------------------------

10129900_178 - 0.993229001885 - technology_and_computing
[direct, grant, fund, fourier, small, national, quantitative, bilayer, luxembourg, infrared]

Direct and quantitative broadband absorptance spectroscopy on small objects using Fourier transform infrared spectrometer and bilayer cantilever probes
National Research Fund Luxembourg (Grant No. 893874)
-------------------------------------

10131661_183 - 0.999951246908 - technology_and_computing
[system, freeway, non-recurring, traffic, management, congestion, support, real-time, expert, decision]

A Real-Time Expert System Approach To Freeway Incident Management
Fundamental to the operation of most Intelligent Vehicle-Highway System (IVHS) projects are advanced systems for surveillance, control and management of integrated freeway and arterial networks. A Major concern in the development of such Smart Roads, and the focus of this paper, is the provision of decision support for traffic management center personnel, particularly for addressing non-recurring congestion in large or complex networks. Decision support for control room staff is necessary to effectively detect, verify and develop response strategies for traffic incidents. These are events that disrupt the orderly flow of traffic, and cause non-recurring congestion and motorist delay. non-recurring congestion can be caused by accidents, spilled loads, stalled or broken down vehicles, maintenance and construction activities, signal and detector malfunctions, and special and unusual events. The ultimate objective of our research is to implement a novel artificial intelligence-based solution approach to the problem of providing operator decision support in integrated freeway and arterial traffic management systems, as part of a more general IVHS. In this paper, we present and discuss the development of FRED (Freeway Real-Time Expert System Demonstration), a component prototype real-time expert system for managing non-recurring congestion on urban freeways in Southern California. The application of FRED to a section of the Riverside Freeway (SR-91) in Orange County is presented as a case study, and illustrates the current capabilities of the system.
-------------------------------------

10136760_183 - 0.999932420838 - technology_and_computing
[parameter, model, datum]

Algorithmic Parameter Space Reduction of a Systems Biology Model: A Case Study
Ordinary differential equation (ODE) models are often used to quantitatively describe and predict the dynamic responses of biological and other systems.  Models with many parameters, limited measurement data and in need of quantification are typically unidentifiable from available input/output data.  Even models that are structurally identifiable can be difficult to quantify in practice from limited data.  For overparameterized models (OPMs), it is often helpful to simplify the model, by rationally reducing the dimensionality of the parameter space.  This is done by finding a set of "key parameters" to estimate, a subset that best represents the dominant model dynamic responses.  OPMs are often characterized by pairwise parameter correlations close to 1 in magnitude and at least some unacceptably large parameter estimation variances.  The goal is to get the best fit possible with a smaller number of parameters, each with acceptable variances.  Several published methods for selecting the key parameter subset are based on parameter sensitivity analysis and/or analysis of the parameter covariance matrix estimated from the input/output data. We apply a combination of these methods to an overparameterized candidate model of tumor suppressor protein p53.  The model comprises of 4 ODEs, 23 unknown parameters, and noisy output measurements of the 4 state variables and the input.  Three least sensitive and highly correlated parameters were isolated from the analysis and fixed to nominal values.  This  reduced the parameter search space and yielded substantially improved numerical identifiability properties for the resulting simplified model which fitted the data equally well, using both global and local search algorithms.
-------------------------------------

10136736_183 - 0.999998004665 - technology_and_computing
[socr, datum, tool, variable, linear, visualization, statistics, activity, regression, time]

Technology-Enhanced Statistics Education with SOCR
There is an ongoing need for clear and accessible statistics teaching tools for bothlearners and instructors. Applications, step by step tutorials, and visualizations are ex-tremely useful tools for teaching students to think scienti_cally, properly analyze thedata, use proper techniques, and identify common errors. In this paper we will demon-strate technology-enhanced approaches for introductory statistics courses. Speci_callywe develop two di_erent activities, using SOCR (Statistics Online Computational Re-source) data, tools and resources. The _rst activity introduces multiple linear regressionusing appropriate SOCR tools. In general, linear regression is used to describe a rela-tionship between one variable to one or several other variables. Linear regression is used extensively in practical applications such as prediction and measuring the strength of relationships between variables. Proper linear regression techniques will be demonstrated, and appropriate methods for the analysis of regression results will be discussed. The second activity demonstrates the interactive power of the SOCR Motion Charts tool. SOCR Motion Charts allow the visualization of multivariate and high-dimensional data that has time and location dimensions. Used correctly, data visualization and statistical graphics are useful in presenting data in clear, intuitive, and engaging ways. Proper data visualization can reveal patterns and relationships that would have been hidden in other data structures, such as tables. The SOCR Motion Charts tool allows us to represent variables based on their size, time, and location attributes. With this technology we can detect patterns across time, as well as analyze the relationships of variables in terms of their magnitudes and locations. These activities and tutorials are implemented as interactive hands-on learning materials and are openly accessible on the web through the SOCR site www.socr.ucla.edu/.
-------------------------------------

10134410_183 - 0.999468085225 - technology_and_computing
[hmgr, ssd, degradation]

The role of the sterol sensing domain in HMG-CoA reductase regulation in Saccharomyces cerevisiae
Sterol sensing domain (SSD) containing proteins are required for lipid regulation, and are conserved among different organisms. 3-hydroxy-3-methyglutaryl-CoA reductase (HMGR) is a key enzyme for sterol synthesis that contains an SSD. In both mammals and yeast, HMGR undergoes regulated degradation in response to feedback of the mevalonate pathway. The configuration of HMGR changes when regulated by mevalonate molecules, which targets the protein for regulated degradation in the endoplasmic reticulum. The N-terminus of HMGR is necessary and sufficient for regulated degradation. The SSD of HMGR ranges from 5 out of 8 transmembrane spans of the N- terminus, and the SSD of HMGR is conserved between many organisms. In this work we investigated the role of the SSD in regulation of HMGR in Saccharomyces cerevisiae. We made 30 mutations in highly conserved residues of the SSD of budding yeast HMGR, and examined their phenotypes with respect to HMGR regulated degradation. To do this we used flow cytometry to measure HMGR levels in response to pharmacological manipulations of the mevalonate pathway that either increase or decrease degradation signals. We found that the SSD is involved in sensing FPP-derived molecule and oxysterol molecules. The SSD also contributed to the destabilization of HMGR during regulated degradation of the protein
-------------------------------------

10135533_183 - 0.999707759664 - technology_and_computing
[vehicle, traffic, method, sensor, feature, information, state, system, view]

Robust Vehicle State Estimation for Improved Traffic Sensing and Management
As traffic congestion continues to increase, it is critical to better monitor and manage traffic to maximize throughput while maintaining a high degree of safety. In terms of traffic sensing on our roadways, many areas in the United States now have infrastructure-based sensing systems that collect traffic information from embedded loop sensors (e.g. the Caltrans Performance Monitoring System (PeMS)); however, these sensors are expensive to install and are spatially sparse, limiting their use to estimating macroscopic traffic parameters such as average speed, density, and flow at relatively long time intervals (e.g., 5 minutes). In contrast, many Intelligent Transportation System (ITS) applications can benefit from temporally high-resolution (i.e., second-by-second) individual vehicle state estimation, allowing for better traffic management and vehicle safety applications. This dissertation focuses on new methods that have been developed to estimate high-resolution vehicle state information from both stationary, infrastructure-based sensors, as well as on-board state estimation. In terms of infrastructure-based sensing, an increasing number of roadside cameras are now being utilized by Traffic Management Centers (TMCs) to monitor traffic conditions. Video from these cameras can be used to determine high-resolution vehicle state information. As part of this dissertation, new methods have been developed to detect and track vehicles from a stationary monocular video to determine not only velocity trajectory information, but also vehicle pose and structure. As a result, it is possible to track segmented vehicles from stationary cameras that have low vantage points where occlusion is commonly occurs. A two stage approach is used: the first stage consists of vehicle segmentation which extracts information necessary to initialize a vehicle tracking method. A modified meta-heuristic algorithm based on the genetic algorithm is utilized which not only finds the initial vehicle's pose, but also extracts an abstract 3D structure and metric dimension of the vehicle which can provide important information to classify and/or identify vehicles. The second stage is centered around a vehicle tracking method which tracks the vehicle and reports its state through time. In this method, each vehicle is represented as a rectangular cuboid and a particle filter is utilized to estimate the vehicle's state based on features extracted from the initial vehicle's pose and structure.In addition to this off-board infrastructure approach, estimating real-time vehicle state information on-board a vehicle (e.g., vehicle localization) is valuable to a variety of ITS applications. Traditional on-board localization methods that rely only on the Global Navigation Satellite System (GNSS) or on sensor fusion solutions derived from satellite vehicle measurements aiding an Inertial Navigation System (INS) often do not work well in crowded urban environments where buildings and trees can block the line-of-sight to satellites along the vehicle's lateral direction (i.e., creating an "urban canyon"). A new robust localization method is proposed which performs sensor fusion between computer vision measurements, pseudo-range and Doppler measurements from GNSS, as well as measurements from an Inertial Measurement Unit (IMU). This method uses traffic light location data (i.e., mapped landmarks measured through a priori observations) taking advantage of existing infrastructure that is abundant within suburban/urban environments, and is easily detected by color vision sensors in both day and night conditions. A tightly coupled estimation process is employed to use observables from satellite signals as well as known feature observables from a camera to correct an INS which is formulated as an Extended Kalman Filter (EKF). A traffic light detection method is also outlined where the projected feature uncertainty ellipse is utilized to perform data association between a predicted feature and a set of detected features. An important component in this localization method depends on mapped visual features (landmarks) such as traffic lights, traffic signs, and features on large structures. An offline surveying method is proposed using an integrated camera, carrier-phase DGPS, and INS to survey landmarks. This method returns not only the positions of the visual features but also the uncertainty of the features' positions in the form of a covariance matrix. This method is verified on visual features that are surveyed using carrier-phase DGPS. Together, these approaches satisfy positioning requirements of high accuracy, availability, and continuity at low-cost;Finally, a new imaging sensor that combines lenses and mirrors (catadioptric sensors) with a common optical axis to perceive different information around a vehicle is also considered in this dissertation which can be advantageous to probe vehicles as well as for vehicle safety applications. The imaging sensor consists of a central view catadioptric system that returns a panoramic view of the surrounding area using a hyperbolic mirror and a non-central catadioptric system that returns a wide perspective view of the ground plane. Together, these catadioptric sensors provide over 60% of the spherical view, with two image views around the vehicle. By configuring the sensors to be co-axial with a displacement along the optical axis, the sensors retain their field of view with minimal occlusion. This new imaging sensor has applications in driver assistance systems and automated vehicles where the vision information can be used for vehicle classification, lane detection, as well as depth recovery.
-------------------------------------

10134474_183 - 0.999656503571 - technology_and_computing
[media, discrete, track]

Nano-tribology of discrete track recording media
Experimental and numerical methods were used to investigate the flyability of magnetic recording sliders on discrete track recording media. Experiment techniques were developed to study the hysteresis of "touch-down" and "take-off" of magnetic recording sliders on discrete track recording media. The slider dynamics on discrete track recording media was compared with that on conventional smooth (flat) media. Various experimental methods were used to study the contact behavior, wear characteristics and tribological characteristics of discrete recording media. Advantages and disadvantages of discrete track recording media were investigated and compared with conventional smooth media. To improve flyability and mechanical properties of discrete track recording media, planarization of discrete track media was employed
-------------------------------------

10135435_183 - 0.927571417347 - technology_and_computing
[utility, function, non-standard, economics]

Essays in behavioral economics
Neoclassical economics maintains the assumption of a rational decision maker maximizing his utility subject to a budget constraint. Behavioral economics seeks to explain behavior more accurately by using non-standard utility functions, either by introducing non-standard elements of payoffs or non-standard functional forms, or by positing bounded rationality on the part of the decision maker, so that utility may not be maximizes. Each of the papers in this dissertation fills one of these categories. "The Good News-Bad News Effect'' supposes that beliefs, or self- image, enter into an individual's utility function directly, and therefore affect the reaction to and acquisition of new information. "Staying Ahead and Getting Even'' maintains a utility function that is solely a function of monetary rewards, but finds evidence of behavior that is inconsistent with maximization of expected utility of total wealth. Finally, "Hypobolic Discounting and Willingness-to-Wait'' shows individuals responding on a choice task using a heuristic that generates choices intransitives, and is therefore incompatible with maximization of any utility function
-------------------------------------

10137214_183 - 0.989734844444 - technology_and_computing
[space, time, valaida, progressive, music]

Keeping Time, Performing Place: Jazz Heterotopia in Candace Allen’s Valaida
Candace Allen’s 2004 novel Valaida illustrates the migratory patterns of early twentieth-century jazz music and musicians, positing the art form and its performers as “heterotopians”: simultaneously in and outside of the power relations of hegemonic time-space compression, traveling in an alternate and progressive space, signified by the music. Through a reading of heterotopic spaces in Valaida, this article seeks to complicate the notion of heterotopias as purely progressive spaces for reversal and liberation. It does so by emphasizing the double nature of heterotopias as both progressive and reactionary and suggests that the way time is employed in a heterotopic space determines its progressive potential. Spaces of cumulative, static, or frozen time refuse to yield any utopian promise, whereas fluid, dynamic, and ephemeral time offers moments of agency. In the case of Valaida, music and performance offer an alternate space, where the radical potential lies in the moment of communication and community, constituting a diasporic practice and heterUtopian spaces of sound and time.
-------------------------------------

10130514_183 - 0.842769048428 - technology_and_computing
[design, report]

Design of an Underwater Radiance Photometer
U.S. Navy Bureau of Ships Report No. 3-2. This report describes the design features of an instrument for measuring directional luminance in the submarine environment.
-------------------------------------

10137972_183 - 0.945784450845 - technology_and_computing
[mechanism, task, simple, rural]

Comprehension and Risk Elicitation in the Field: Evidence from Rural Senegal
In the past decade, it has become increasingly common to use simple laboratorygames and decision tasks as a device for measuring both the preferences and understanding ofrural populations in the developing world. In this paper, we report the results observed with threedistinct risk elicitation mechanisms, using samples drawn from the rural population in Senegal,West Africa. We test the understanding of and the level of meaningful responses to the typicalHolt-Laury task, to an adaptation of a simple binary mechanism pioneered by Gneezy andPotters in 1997, and to a non-incentivized willingness-to-risk scale. We find a low level ofunderstanding with the Holt-Laury task and an unlikely-to-be-accurate pattern with thewillingness-to-risk question. Our analysis indicates that the simple binary mechanism hassubstantially more predictive power than does the Holt-Laury mechanism. Our study is acautionary note regarding utilizing either relatively sophisticated risk-elicitation mechanisms ornon-incentivized questions in the rural developing world.
-------------------------------------

10135026_183 - 0.999740396073 - technology_and_computing
[cloud, satellite, index, california, image]

Estimating solar irradiance using a geostationary satellite
A method to estimate global horizontal irradiance (GHI) at the surface using a geostationary satellite is presented. The spatial variation of ground and cloud albedo of California is characterized in the 0.55-0.75 mum wavelength spectral region by analyzing a series of images from the visible channel on the GOES West satellite. Using these two characteristic albedo maps a cloud index is generated for each pixel in subsequent images as an estimate of the cloud fraction in that pixel. The cloud index is converted into a clear sky index which is then coupled with the modeled GHI under a clear sky to generate estimates of the GHI under the current cloud conditions. The model was applied to 914 images over 65 days and validated with ground truth measurements in California. The mean bias difference from 119 stations in the California Irrigation Management Information System was 6.7 W/m² (3.2%) and the root mean square difference was 78.6 W/m² (21.9%) which is consistent with previously reported results for satellite algorithms. A Matlab-Java code to execute the model is documented
-------------------------------------

10134794_183 - 0.999682691578 - technology_and_computing
[coupling, phonon, electronic, magneto-optical, temperature, pnictide, in-plane, material, hund, josephson]

Correlations as causation : optical properties of strongly correlated matter
This dissertation presents infrared and magneto-optical spectroscopy experiments that probe emergent phenomena in various correlated and complex electronic materials. By examining the frequency dependent optical constants as a function of temperature, magnetic field, and chemical doping, we draw conclusions about the nature of the physical processes that govern the materials. We predominately explore the vast phase-space of the two currently known families of high-critical temperature superconductors, the cuprates and the pnictides, before turning to a magneto-optical study of Landau levels in a newly discovered three dimensional topological insulator. We present detailed magneto-optical results in the superconducting state of La₂-xSrxCuO₄ and demonstrate that in-plane magnetic order, present only in the underdoped samples, quenches interplane Josephson coupling. We then generalize these results and show that, for spin ordered cuprates, the universal Josephson relation breaks down. Turning to the 122-family of pnictides, we demonstrate that electronic correlations in BaFe₂-xCoxAs₂ are doping independent and lead to spectral weight transfer as a function of temperature over an energy scale defined by Hund's rule coupling. By closely monitoring the infrared- active phonon modes across the structural phase transition, we show how Hund's coupling is evident in another observable: the low-temperature phonon oscillator strength enhancement. This previously anomalous phonon behavior results from reduced electronic screening driven by anisotropic in-plane conductivity as a consequence of the Pauli exclusion principle
-------------------------------------

10133932_183 - 0.99995878782 - technology_and_computing
[simulation, wsn, performance, simulator, approach, technique, overhead, causality]

Improving the performance of distributed simulations of wireless sensor networks
Simulations are key to the design, implementation, and evaluation of wireless sensor networks (WSNs) and their applications. To meet the demands for high simulation fidelity and speed, distributed simulation techniques are increasingly being used in WSN simulators. However, existing distributed WSN simulators only provide limited speedup and scalability because of the large overheads in preserving the causality of the interactions of wireless sensor nodes during distributed simulations. In this dissertation, we examine methods to improve the performance of distributed WSN simulators by controlling the overheads related to distributed simulations and parallelizing simulations. When building distributed simulators, "conservative" and "optimistic" are the two basic approaches for preserving causality. The former ensures causality violations never occur whereas the latter features mechanisms to recover from causality violations. These two approaches incur different overheads and their relative performances vary over different WSNs or simulation hardware. Given that all existing distributed WSN simulators are based on the conservative approach, we study, in the first part of this dissertation, how to improve the performance of the conservative approach in simulating WSNs. We first develop three novel techniques that reduce simulation overheads by exploiting the parallelism in the physical radios, communication protocols and WSN applications. Then we propose a lazy synchronization scheme that further improves simulation performance by identifying and eliminating unnecessary synchronizations during simulations. With these techniques, we implement a fully functional distributed WSN simulator. In the second part of this dissertation, we study the performance of the optimistic approach in simulating WSNs. Our focus is on understanding the relative performance of the two approaches so appropriate simulation strategies can be devised for a WSN. Since events are handled fundamentally differently across these two classes of simulators, it is difficult to compare the approaches for a specific WSN. We address this challenge by developing a novel trace-based performance evaluation technique that separates simulation overheads from actual simulation algorithms or implementations. This allows one to use the same traces to prototype and evaluate any simulation techniques on virtual platforms with arbitrary hardware. We implement this technique in an evaluation framework
-------------------------------------

10136647_183 - 0.999974583495 - technology_and_computing
[algorithm, relay, model, thermal, modeling, multiple, tracking, wireless, method, mimo]

Wireless Relay Transceiver Design, Thermal Modeling and Fast Subspace Tracking
Wireless relays are such methods that can be rapidly deployed to enhance the coverage, reliability and throughput of a wireless network subject to power and spectral constraints. When equipped with multiple antennas, Multiple Input Multiple Output (MIMO) relays, are particularly useful for scattering rich and non-line-of-sight environment. The first part of this dissertation considers a system where two users exchange information via a non-regenerative half-duplex two-way MIMO relay. We study the transceiver design including both source covariance matrices at the two users and beamforming matrix at the relay to maximize the achievable weighted sum rate of the system. We compare the convergence behaviors of the proposed algorithms and demonstrate their advantages over prior algorithms. We also show an optimal structure of the relay matrix, which is useful to reduce the search complexity.As advanced architectures and high-performance hardware are required to implement more powerful but complicated algorithms such as those in a two-way relay system, multiple cores are often integrated on a chip of shrinking size. However, the corresponding dramatically increased power density may lead to significant adverse effects. Dynamic thermal management is widely used to mitigate this problem, where thermal modeling and temperature prediction play the key roles. Unlike conventional bottom-up approaches, a Linear Time-Invariant (LTI) Multiple-Input-Multiple-Output (MIMO) black-box model is adopted and Least Square (LS) based model averaging algorithm with model screening is developed with less temperature prediction errors than the traditional LS algorithm based on model order selection.For VLSI thermal modeling, Model Order Reduction (MOR) is an efficient technique to reduce the modeling complexity where subspace-based methods could be successfully applied. In the third part, motivated by MOR for thermal modeling, subspace tracking is investigated as one of the key procedures for subspace-based methods. We explain the reason to enlarge the actual tracking dimension and equip bi-iteration SVD algorithm with multiple inner iterations and Ritz acceleration. Our proposed algorithms are demonstrated to have much improved performance of both convergence rate and tracking accuracy compared to existing algorithms while still keeping linear complexity without many additional computational consumptions.
-------------------------------------

10133736_183 - 0.991018323441 - technology_and_computing
[complex, magnetic, larger, packing, crystal, triangle, formulum, oxime, magnet, cl]

Single-molecule magnets assembled from oxime stabilized Mn₃III triangles
A series of progressively larger molecules assembled from oxo-centered Mn₃III triangles is presented. All reported complexes have been characterized by bulk magnetic susceptibility and X-ray diffraction. A series of three "single decker" trigonal bipyrimidal Mn₂IIMn₃III⁺¹³ single-molecule magnets (SMMs) is reported. The complexes have the general formula of NEt₄₃Mn₅(R-salox)₃O(N₃)₆X₂ where R = H, Me, or Et; and X⁻ = Cl⁻ or Br⁻. The complexes consist of an oxime stabilized mu₃-O²⁻ Mn³III triangular core capped with two azide bridged MnII ions. Complexes were synthesized using oxime ligands with increasingly larger alkyl groups pendent to the oximate carbon. Crystal packing of these alkyl groups induced structural distortions which have marked effects on crystal packing, molecular symmetry, and magnetic exchange interactions Structural distortions were induced by crystal packing effects in two "double decker" complexes of the formula X₂Mn₈(salox)₆O₂(N₃)₆(MeOH)₂Cl₂ · 2S, where X⁺ is either NPr₄+С or NBu₄+С and S is either MeOH or CHCl₃, which resulted in changes in magnetic behavior. A third compound that is an antiferromagnetic one dimensional chain magnet constructed from Mn₈ units bridged by N³⁻ ions is described. Two larger compounds are also reported, one comprised of four interconnected oxo-centered Mn₃III triangles. The other is a magnetic 3D metal-organic framework (MOF) synthesized using a modified salicylaldoxime. The complex has the formula Mn₁₃Na₉O₄(EtO-salox)₁₂(N₃)₆(MeOH)₆(H₂O)₁₀Cl₃alpha where EtO-salox is 3-ethoxysalicylaldoxime. The MOF exhibited gas adsorption behavior in addition to magnetic data suggestive of a negative D. Finally, a salicylaldehyde functionalized fullerene has been synthesized and characterized through NMR and mass spectrometry
-------------------------------------

10134296_183 - 0.999289433731 - technology_and_computing
[performance, channel, delay, error, time, measurement, zzb, localization, estimation, frequency]

Performance of Time Delay Estimation and Range-Based Localization in Wireless Channels
This thesis studies the performance of time delay estimation and the range-based localization schemes in wireless multipath channels. The research focuses on the localization schemes based on time-of-arrival and time-difference-of-arrival measurements. In multipath environments, time delay measurements suffer from the errors due to weak line-of-sight and rich non-line-of-sight (NLOS) signal paths. Instead of proposing range measurement algorithms, the thesis is devoted to develop theoretical performance lower bounds used as benchmarks to guide algorithm design and provide insight into the behavior of time delay estimation (TDE).The author develops Ziv-Zakai bounds (ZZBs) on Bayesian estimation of time delay, for known pulsed signal and frequency hopping waveforms that propagate through unknown random multipath channels following Rayleigh/Rician distribution, with a uniform prior on the delay.  The bounds do not assume channel knowledge at receivers, providing more realistic and tighter performance limits than the average bound that assumes channel knowledge. The ZZBs also present good performance prediction for maximum a posteriori estimator, tracking a wide range of signal-to-noise ratios.  The ZZB for wideband frequency hopping waveforms reveals the performance benefit for TDE from frequency diversity over frequency-selective fading channels. To evaluate the ZZB, the author proposes a moment generating function approach. The closed-form expressions for independent flat-fading channels enable easy study of the effects of SNR, frequency diversity, and channel statistics on TDE.The TDE errors lead to time-based ranging errors that in turn cause positioning errors and deteriorated localization performance. The thesis models the NLOS range measurement error as a deterministic or random positive bias, following widely adopted distributions for time delay over multipath channels. The error analysis for typical estimators shows that the MSE and bias performance is determined by the statistics of measurement bias and noise, the beacon array geometry and the estimator type.
-------------------------------------

10136149_183 - 0.948799002521 - technology_and_computing
[function, jacobi, n-point, vertex, 1-point, operator]

Vertex Operator Algebras and Jacobi Forms
This thesis develops a theory relating the Jacobi group with <italic>n</italic>-point functions associated with strongly regular vertex operator algebras. The <italic>n</italic>-point functions considered here have additional complex variables and generalize <italic>n</italic>-point functions studied in other works in the mathematics and physics literature. Recursion formulas are discussed which reduce the study of <italic>n</italic>-point functions to the study of 1-point and 0-point functions.We consider the space of 1-point functions associated to inequivalent irreducible admissible modules for a strongly regular vertex operator algebra. We develop transformation laws for this space of functions under the Jacobi group. With additional assumptions, we show that 1-point functions are sums of products of 1-point functions of modules for the commutant subVOA of the vertex operator algebra together with a type of Jacobi theta series. Conditions will be given where these functions are vector-valued weak Jacobi forms. A number of corollaries to these results are developed, including a sharper result in the case of holomorphic vertex operator algebras.Other results contained in this thesis include transformation laws for Jacobi theta functions with spherical harmonics, and a generalization of a result of Miyamoto to include zero modes of elements.
-------------------------------------

10130351_178 - 0.999920775068 - technology_and_computing
[translucent, system, daylighting, architecture, natural, low, floor, bibliographical, rise, reference]

Translucent floor systems as a means of achieving natural daylighting in mid and low rise architecture
Includes bibliographical references.
-------------------------------------

10135899_183 - 0.815199868785 - technology_and_computing
[phase, flow, channel, project, high, october, restoration, water, mark, reach]

Post-Project Performance Assessment of a Multi-Phase Urban Stream Restoration Project on Lower Codornices Creek
In Fall 2010, a partnership between the University of California-Berkeley and the cities of Albany and Berkeley completed the third of four restoration phases planned for a 0.6-mile stretch of Codornices Creek in Alameda County, California, between the San Pablo Avenue and UPRR crossings. Originally initiated in the mid-1990s to improve a straightened and channelized ditch, the project objectives were to convey the 100-year flood, improve user access to the creek, and establish an ecologically valuable riparian corridor dominated by native species (reducing invasive non-natives).  We assessed the performance of the third phase of the project during a high flow of 136 cfs on October 5, 2011. We obtained relevant data and information from project designers, and on October 22, 2011, while evidence of the high flow was still fresh, we conducted a detailed topographic survey of the channel, surveyed high water marks, documented conditions with photographs, and mapped site conditions. In addition, we surveyed cross sections and high water marks in the downstream reaches (Phases 1 and 2 of the overall restoration project).  High water marks show floodplain inundation was inconsistent throughout the three reaches, with the October 5 storm flow largely staying within the constructed banks in Phase 3, and overbank flow occurring in Phases 1 and 2.   Our longitudinal profile shows Phase 3 incised up to 2 ft below the design grade in the upstream portions of the reach, and aggraded up to 2 ft at the downstream end. Survey results also confirm that additional vertical channel adjustment occurred during the October 5 flow. This, along with the presence of an active headcut, suggests that the channel is still in the process of finding geomorphic equilibrium. Cross-section monitoring in Phase 3 should proceed into the future to determine whether channel adjustments continue, and as a basis to assess whether more complexity should be introduced to promote aggradation, channel complexity, floodplain inundation, and more ecologically valuable habitat. 
-------------------------------------

10135426_183 - 0.999983231859 - technology_and_computing
[design, interconnection, technology, performance, optimization, global]

Design and optimization of high-performance low-power CMOS VLSI interconnects
As semiconductor technology advances in the ultra deep sub -micron era, on-chip global interconnections have been an ever-greater barrier to achieving high-performance and low -power for the increasingly larger system-on-chip (SoC) designs. Various on-chip interconnection schemes are proposed to tackle the scaling issue of global wires by manipulating the wire operation regions, changing signaling methods, and applying different equalization techniques. Optimization frameworks are also proposed to aid the transmitter-wire-receiver co-design based on user- defined constraints. For the six representative global interconnection schemes, we investigate their performance metrics with technology scaling by performing optimizations using the proposed SQP-based framework. A set of simple models is also developed to enable early- stage system-level analysis. Performance of different interconnection schemes are predicted and compared over several technology nodes. We further perform studies on the pipelined $RC$ interconnection by exploring its performance metrics with voltage and technology scaling based on different design objectives. A performance evaluation flow is developed to generate the optimal designs for given objectives. Also, impacts of pipelining depth, voltage and technology scaling are illustrated. Finally, we propose an energy-efficient high-speed on-chip global interconnection by employing continuous-time active equalization. Modeling and design of transmitter and receiver circuits are discussed. Analytical formula of received eye-opening is derived for system-level design planning. We further perform transmitter-receiver co- design through an optimization framework and explore the design space to generate design based on best energy- throughput tradeoff
-------------------------------------

10137534_183 - 0.828827208828 - technology_and_computing
[ghz, phase, db, gain, shifter, error, power, dbm, array, phased]

W-band phased array systems using silicon integrated circuits
This thesis demonstrates the silicon-based on-chip W-band phased array systems. An improved wideband I/Q network to minimize the capacitive loading problem is presented, and its implementation in a 60-80 GHz active phase shifter using 0.13 mum SiGe BiCMOS process is demonstrated. In addition, a 67-78 GHz 4-bit passive phase shifter using low-pass pi-network and 0.13 mum CMOS switches is demonstrated. By adding amplifiers to the passive phase shifter with the architecture of alternating amplifiers and phase shifter cells, a low-power BiCMOS 4-element phased array receiver for 76-84 GHz applications are presented. Lastly, a 76-84 GHz 16-element phased array receiver, designed differentially in order to reduce the sensitivity to packaging effect such as ground inductance, is presented. This thesis presents the silicon-based on- chip W-band phased array systems. An improved quadrature all-pass filter (QAF) and its implementation in 60-80 GHz active phase shifter using 0.13 mum SiGe BiCMOS technology is presented. It is demonstrated that with the inclusion of an Rs/R in the high Q branches of C and L, the sensitivity to the loading capacitance, therefore the I/Q phase and amplitude errors are minimized. This technique is especially suited for wideband millimeter- wave circuits where the loading capacitance (CL) is comparable to the filter capacitance (C). A prototype 60- 80 GHz active phased shifter using the improved QAF is demonstrated. The overall chip size is 1.15 x 0.92 mm² with the power consumption of 108 mW. The measured S₁1 and S₂2 are < -10 dB at 60-80 GHz and 60-73 GHz, respectively. The measured average power gain is 11.0-14.7 dB at 60-79 GHz with the rms gain error of < 1.3 dB at 60-78 GHz for 4 -bit phase states. And the rms phase error is < 9.1° at 60-78.5 GHz showing wideband 4-bit performance. The measured NF is 9-11.6 dB at 63-75 GHz and the measured P₁dB is -27 dBm at 70 GHz. In another project, a 67-78 GHz 4-bit passive phase shifter using 0.13 um CMOS switches is demonstrated. The phase shifter is based on a low-pass pi- network. The chip size is 0.45 x 0.3 mm² without pads and consumes virtually no power. The measured S₁1 and S₂2 is < -10 dB at 67-81 GHz for all 16 phase states. The measured gain of 4-bit phase shifter is -19.2 +/- 3.7 dB at 77 GHz with the rms gain error of < 11.25° at 67-78 GHz. And the measured rms phase error is < 2.5 dB at 67-78 GHz. The measured P1dB is > 8 dBm and the simulated IIP3 is > 22 dBm. A low-power 76--84 GHz 4-element phased array receiver using the designed passive phase shifter is presented. The power consumption is minimized by using a single-ended design and alternating the amplifiers and phase shifter cells to result in a low noise figure at a low power consumption. A variable gain amplifier and the 11 degree phase shifter are used to correct for the rms gain and phase errors at different operating frequencies. The overall chip size is 2.0 x 2.7 mm² with the current consumption of 18 mA/channel with 1.8 V supply voltage. The measured S₁1 and S²2 is < -10 dB at 70-88 GHz and 74- 88 GHz, respectively. The measured average power gain is 10.1-18.9 dB at 76-84 GHz with the rms gain error of < 0.6 dB at 76-77 GHz, < 0.8 dB at 79-81 GHz and < 1.1 dB at 81- 84 GHz. The measured rms phase error is < 3.9° at 76-77 GHz, < 7.2° at 79-81 GHz and < 10.4° at 81-84 GHz. The measured NF is 10.5 +/- 0.5 dB at 80 GHz and the measured input P1dB is -26.7 dBm to -23 dBm at 77-80 GHz depending on the gain setting. The on-chip coupling is < -30 dB between adjacent channels. Finally, a 76-84 GHz 16-element phased array receiver in a 0.13 mum SiGe BiCMOS process is presented. All circuits are designed differentially to result in less sensitivity to packaging effect and high channel-to-channel isolation. The overall chip size is 5.0 x 5.8 mm  with the power consumption of 500-600 mA from 2 V supply voltage. The measured Sv(1)1 and Sv(2)2 for all 16 phase states is < -9 dB at 72-88 GHz and 73-86 GHz, respectively. And the measured average power gain (Sv(2)v(1)) is > 10 dB for 76.4-90 GHz with the rms gain error of < 1 dB for 74-84.2 GHz. The measured rms phase error is < 11  for 73.6-83.6 GHz. In order to optimize the rms gain and phase errors, the VGA and 11  phase shifter are used. The measured reverse isolation (Sv(1)v(2)) is > -45 dB. The measured NF is 11.2-13 dB at 77-87 GHz at the maximum gain state. And the measured input P1dB is -20 dBm at 77 GHz and -25.8 dBm at the 83 GHz. The measured coupling between channels is < -48 dB because of the relatively high substrate resistance and the long distance between channels
-------------------------------------

10130498_183 - 0.962129072381 - technology_and_computing
[method]

The K-method of determining the path function
SIO Reference 58-39. The purpose of this note is to introduce a third method of the determination of the path function N*. The two methods already extant are the direct method and the operational method.
-------------------------------------

10137321_183 - 0.999595526021 - technology_and_computing
[software, decision-making, development, problem]

Quantitative Decision-making in Software Engineering
Our thesis is that <italic>software repositories contain latent information that can be mined to enable quantitative decision making</italic>. The decision-making process in software development and maintenance is mostly dependent on software practitioner's experience and intuition. For example, developers use their experiences  when prioritizing bug fixes, managers allocate development and testing resources based on their intuition and so on. Often these human driven decisions lead to wasted resources and increased cost of building and maintaining large complex software systems. The fundamental problem that motivates this dissertation is the lack of techniques that can automate decision-making process in software engineering. As data mining techniques became more mature, mining software repositories has emerged as a novel methodology to analyze the massive amounts of data created during software development process. Significant, repeatable patterns and behaviours in software development can be identified as a result of this mining which are often used for predicting various aspects of software development and maintenance, such aspredicting defect-prone releases, software quality, or bug fix time. In this dissertation we show techniques to effectively mine software repositories, identify significant patterns during software development and maintenance, and recommend actionable aspects that can automate the decision-making process in software engineering.We demonstrate efficient techniques to use the information stored in software repositories and produce results to guide software practitioners so that they can depend less on their intuition and experience and more on actual data. To this end, in this dissertation, we make several contributions.First, we perform several empirical studies to characterize information needs of software developers and managers in the context of decision  making during software development and maintenance. For example, we study what kinds of decision-making problems are important to software practitioners on a daily basis. Second, to facilitate analysis of various types of decision-making problems using a common platform, we design a generic mixed-graph model to capture associations of different software elements. We illustrate how we can build different types of hyper-edges on this mixed-graph to quantify amorphous behaviour and dependencies among various software elements. Third, to demonstrate the effectiveness of our framework, we formalize a set of four important decision-making problems that are challenging to address with the state-of-the-art. We show that our framework can achieve high-levels of prediction accuracies for different types of decision-making problems when tested on large, widely-used, real-world, long-lived software projects.
-------------------------------------

10135518_183 - 0.797503481529 - technology_and_computing
[porous, optical, material, silicon, multifunctional, separation, surface, detection, signal, biomolecule]

Separation, process and detection of biomolecules using silicon-based optical nanostructures
Advancements in the field of optical sensors have resulted in an innovative class of microoptical sensors exhibiting detection capability comparable to those sophisticated analytical laboratory instrumentations. An emerging trend is to integrate these optical sensors/detecting methods into analytical tools with the ability to perform multifunctional tasks (i.e sample filtration, detection, and signal processing etc.) all in one platform. Porous silicon possesses many fascinating features making it an attractive candidate as a spectrally encoded material that is suitable as an identifier/barcode for multi-analyte bioassays and a spatially controlled structure that is applicable as a chromatography matrix for biomolecule separation. Its high surface to volume ratio and readily tailored surface chemistry also provide additional control for enhancing selectivity. Combining its optical and physical properties together with tailored surface moieties, porous silicon material can be treated as a multifunctional material allowing simultaneous separation and detection, capable of the multiplexed, low-level biodetection necessary to accommodate complex biological mixtures such as urine, whole blood, or serum used for disease diagnosis. This thesis begins with an overview on current separation techniques and progress in the field of optical sensors and nanomaterials. The second half of the introduction discusses recent development in porous silicon material with the focus on biosensing and molecular filtration applications. The objective of this thesis is to explore porous silicon as a multifunctional material with the ability to separate, process, and detect biomolecules at low concentration and in real-time with minimal sample preparation. Interrogation of porous silicon material as a multifunctional nanostructure involved three major aspects: 1) manipulation of its optical and spectral information for encoding and signal processing applications, 2) examination of the effect of its physical properties on molecular transport within its porous structure, 3) investigation of analyte-pore surface interaction for enhanced selectivity or better separation based on analyte surface moieties. The last chapter of this thesis provides an example of exploiting porous silicon as a multifunctional matrix that is capable of capturing and concentrating analyte while processing the signal, providing a new strategy for bioanalytics
-------------------------------------

10135174_183 - 0.999944530679 - technology_and_computing
[network, hybrid, datacenter, challenge]

Fundamental challenges for hybrid electrical/optical datacenter networks
Recent research proposals on building hybrid electrical/ optical networks as a way of interconnecting the core layer of a modular datacenter promise reduction in the cost, deployment complexity as well as energy requirement of large scale datacenter networks. We summarize our two years of experience in dealing with such hybrid interconnects. We first show the feasibility of building hybrid networks under homogeneous environments where one application dominates the workload through building a completely functional prototype of Helios/c-Through systems. Then we uncover a number of fundamental challenges and problems, some quite unexpected, which must be addressed for wide adoption of these proposals in industry settings under heterogeneous traffic pattern with multi-tenancy. We then propose a significantly more flexible, fine-grained and responsive way of controlling such hybrid networks under Observe-Analyze-Act framework which is based on OpenFlow. Although the complete picture of how to build these networks in particular the best circuit scheduling algorithm to use is still unclear, but we show that this framework enables a variety of future solutions to the remaining challenges
-------------------------------------

10135352_183 - 0.999998972559 - technology_and_computing
[system, linear, observability, reconstructibility, control, condition, stochastic, nonlinear, controllability, reachability]

Stochastic observability, reconstructibility, controllability, and reachability
This thesis formulates versions of observability, reconstructibility, controllability, and reachability for stochastic linear and nonlinear systems. The concepts of observability and reconstructibility concern whether the measurements of a system suffice to construct a complete characterization of the system behavior while the concepts of controllability and reachability concern whether the actuation of the system suffices to cause the system to behave according to various user specifications. Thus, these concepts are fundamental to the design of control algorithms. In deterministic linear systems, linear algebraic conditions specify whether an unknown state can be exactly reconstructed from the measurements over a finite time interval, and whether there exists an input sequence over a finite time interval which can steer the state to a desired endpoint; thus, the concepts of observability, reconstructibility, controllability, and reachability are straightforwardly defined. The extension to linear stochastic systems is not obvious. While the deterministic matrix conditions have significance in applications such as Kalman filtering and linear-quadratic -optimal control, the presence of noise generally prevents exact reconstruction of the state via the measurements and exact placement of the state via the control inputs. This ambiguity in interpreting the matrix conditions has lead to a multitude of extensions in the literature. Nonlinear behavior introduces further difficulties; even in nonlinear deterministic systems, the generalization of the linear conditions is not immediate; for instance, whereas observability and reconstructibility does not depend on the control input in linear systems, this separation of control and estimation questions need not hold for nonlinear systems. Our purpose is to make precise the stochastic versions of observability, reconstructibility, controllability, and reachability; in the process, we obtain the expected matrix conditions for stochastic linear systems, which arise both in deterministic linear systems analysis and in Kalman filtering theory and linear -quadratic-optimal-control theory. Perhaps unexpectedly, we also obtain an analogous rank condition for the finite- state hidden Markov model. We show important roles of reconstructibility: in linear systems, it corresponds to minimality of the Kalman filter; in nonlinear systems, it is necessary for performance improvement via output feedback over open-loop control. The role of observability in the stability of optimal filters is discussed. Additionally, we demonstrate a connection between stochastic controllability/reachability and Granger causality and its generalizations from the statistics and econometrics literature. The ideas are explored via simulation of a finite-state hidden Markov model for the network congestion control problem
-------------------------------------

10139573_183 - 0.999994840291 - technology_and_computing
[datum, model, library, digital]

All Teh Metadatas Re-Revisited
At last year’s Code4Lib conference Declan Fleming presented ALL TEH METADATAS and reviewed the UC San Diego Library Digital Asset Management system and RDF data model. You may be shocked to hear that all that metadata wasn't quite enough to handle increasingly complex digital library and research data in an elegant way. Our ad-hoc, 8-year-old data model has also been added to in inconsistent ways and our librarians and developers have not always been perfectly in sync in understanding how the data model has evolved over time.
      In this presentation we review our process of locking a team of librarians and developers in a room to figure out a new data model, from domain definition through building and testing an OWL ontology. We also cover the challenges we ran into, including the review of existing controlled vocabularies and ontologies, or lack thereof, and the decisions made to cover the gaps. Finally, we discuss how we engaged the digital library community for feedback and what we have to do next. We all know that Things Fall Apart; this is our attempt at Doing Better This Time.
-------------------------------------

10130425_178 - 0.936459935323 - technology_and_computing
[megan2, grant, model, version, atm-0929282, aerosol, foundation, national, biogenic, science]

The Model of Emissions of Gases and Aerosols from Nature version 2.1 (MEGAN2.1): an extended and updated framework for modeling biogenic emissions
National Science Foundation (U.S.) (Grant ATM-0929282)
-------------------------------------

10133923_183 - 0.999987466218 - technology_and_computing
[problem, device, protocol, networking, black, voip, approach, box]

Enhancing networking protocols in widely deployed devices
Standardization of networking protocols enables interoperability and reduces cost, but is generally a slow process. As a result, there are many cases in which a networking protocol and the devices that implement it are not able to satisfy emerging user requirements. In such cases, the networking protocol falls short of current user requirements but has been too widely deployed to make a redesign sufficiently timely or cost-effective. In this dissertation, we advocate an approach to solving this problem that leaves deployed devices unmodified, by treating them as a black box. Rather, we show that for certain network devices, it may be sufficient to change the behavior of the device by "externally controlling" it, e.g., from software or another device, such that it solves the problem at hand. We demonstrate the effectiveness of such a black box approach for several challenging problems from two different areas: two problems from 802.11 wireless networking and one problem from inter-domain routing. In the first problem, we discuss the increasingly popular use of VoIP applications in the 802.11 protocol. Our experimental results show that 802.11 is ill-designed to carry VoIP traffic : an 802.11 hotspot faced with even a moderate number of VoIP calls in combination with data traffic may severely degrade the performance of both. The second problem, also from the area of 802.11, addresses unfairness of 802.11 in the face of emerging and potentially selfish upload-oriented applications. The third problem is from the area of inter-domain routing. We address a modern ISP's requirement for detailed control over routing to meet traffic engineering and security goals and to support applications such as VoIP. We show that the current inter-domain routing protocol, BGP, lacks the flexibility needed to implement the required degree of route control. We have implemented solutions to these problems that follow a black box approach and evaluated their effectiveness in combination with commercial networking devices. Our evaluation demonstrates that enhancing deployed networking protocols using a black box approach can be effective and efficient, yet portable and backward compatible with legacy devices
-------------------------------------

10136656_183 - 0.875044138394 - technology_and_computing
[design, game]

Mechanizing Exploratory Game Design
Game design is an art form that deals with inherently interactive artifacts. Game designers craft games (assembled from rule systems and content), but they really seek to manipulate play: the interaction between games and players. When developing new games that are similar to past games, a designer may rely on previous experience with related designs and relatively easy access to players familiar with conventional design choices. When exploratorily venturing into uncharted territory, uncovering games that afford novel modes of play, there is a need for practical and technological interventions that improve a designer's access to feedback from the unfamiliar design scenario. In the interdisciplinary space between game design, design studies, computational creativity, and symbolic artificial intelligence (AI), my program of mechanizing exploratory game design aims to amplify human creativity in game design; enable new game designs through deep, play-time design automation; and demonstrate novel tools that respect the concerns of design problems.This dissertation advances a practice of modeling design spaces as logic programs in the answer set programming (ASP) paradigm. Answer set programs can concisely encode the key conditions of artifact appropriateness, and, paired with state of the art algorithms for combinatorial search and optimization, they yield efficient and expressively sculptable artifact generators. I present three major applications of ASP-encoded design spaces to exploratory game design: a powerful machine playtesting tool for analyzing designer-specified games, a novel game prototype employing machine-generated rulesets bound by iteratively discovered constraints, and a suite of level design automation tools that offer unprecedented control over the aesthetic and pedagogical properties of puzzles for a widely deployed educational game. This practice is further developed in a series of introductory programming and advanced modeling tutorials. By formally modeling design spaces as concise and authorable logic programs (and without the need for original algorithm design and maintenance), designer-programmers are broadly empowered to quickly build and evolve the in-house automation required by their own exploratory game design projects. These claims are backed by a spreading adoption of this practice by others and deployed applications to at-scale game design projects.Taken as a whole, this dissertation offers new insight into the nature of game design in relation to other design disciplines, a new design research practice for understanding how design thinking can and should be mechanized, a novel model of transformational creativity suitable for discussing both humans and machines, and a set of new applications for symbolic AI that simultaneously broaden the applicability and tax the limits of this automated reasoning infrastructure.
-------------------------------------

10134868_183 - 0.996286098386 - technology_and_computing
[device, chip-scale, technique, bragg, grating, reflector, miniaturization, waveguide, area]

Miniaturization of chip-scale photonic circuits
Chip-scale photonic circuits promise to alleviate some fundamental physical barriers encountered in many areas of the life sciences and information technologies. This work investigates routes to miniaturization of chip-scale optical devices. Two new techniques and devices based thereon are introduced for the first time. One technique makes use of integrated metallic mirrors to construct reflectors which are by an order of magnitude smaller than their counterparts. Another technique is based on folding of chip-scale devices to fit long structures into small area on a chip. Although both techniques are demonstrated on some specific examples, the developed toolkit is applicable to a wide range of chip-scale devices including modulators, filters, channel add-drop multiplexers, detectors, and others. The major part of this Thesis focuses on miniaturization of waveguide reflectors and the devices based thereon. Fitting long waveguide Bragg gratings into a small area on a chip is demonstrated based on curved waveguide Bragg gratings; theory and analytical model of such structures is developed. In the second part of the Thesis, integrated metallic mirrors are proposed as reflectors with properties complementary to Bragg gratings - low polarization sensitivity, high reflectivity for different transverse modes, and good manufacturability. The feasibility of the proposed ideas is tested in both simulations and experiments. The demonstrated devices including biochemical sensors, micro-resonators, and inline filters are promising for applications in the life sciences and information technologies
-------------------------------------

10136075_183 - 0.999996963505 - technology_and_computing
[system, block-oriented, nonlinear, identification, rank, minimization, problem]

Block-oriented nonlinear system identification using semidenite programming
Identification of block-oriented nonlinear systems has been an active research area for the last several decades. A block-oriented nonlinear system represents a nonlinear dynamical system as a combination of linear dynamic systems and static nonlinear blocks. In block-oriented nonlinear systems, each block (linear dynamic systems and static nonlinearity) can be connected in many different ways (series, parallel, feedback) and this flexibility provides the block-oriented modeling approach with an ability to capture a large class of nonlinear systems. However, intermediate signals in such block-oriented systems are not measurable and the inaccessibility of such measurements is the main difficulty in block-oriented nonlinear system identification. Recently a system identification method using rank minimization has been introduced for linear system identification. Finding the simplest model within a feasible model set restricted by convex constraints can often be formulated as a rank minimization problem. In this research, the rank minimization approach is extended to block-oriented nonlinear system identification. The system parameter estimation problem is formulated as a rank minimization problem or the combination of prediction error and rank minimization problems by constraining a finite dimensional time dependency of a linear dynamic system and by using the monotonicity of static nonlinearity. This allows us to reconstruct non-measurable intermediate signals and once the intermediate signals have been reconstructed, the identification of each block can be solved with the standard Prediction Error method or Least Squares method. The research work presented in this dissertation proposes a new approach for block-oriented system identification by tackling the inaccessibility of measurement of intermediate signals in block-oriented nonlinear systems via rank minimization. Since the rank minimization problem is non-convex, the rank minimization problem is relaxed to a semidefinite programming problem by minimizing the nuclear norm instead of the rank. The research contributes to advances in block-oriented nonlinear system identification
-------------------------------------

10134001_183 - 0.998651857258 - technology_and_computing
[transmission, source, progressive, mimo, different, rate, high, approach, component, uep]

Joint optimization of physical and application layers for wireless multimedia communications
One of the challenges in the next generation wireless communication systems is to provide high quality multimedia services. To address this issue, the approach we take in this dissertation is cross-layer design of communication systems. In the first part of the dissertation, we address transmission of progressive images or scalable video using hierarchical modulation. Thanks to the progressive features, progressive bitstreams enable each user in a multi-user network to decode the source at the rate that their channel allows. These progressive sources have the feature that they have gradual differences of importance in the bitstreams. One would like to have gradual differences in unequal error protection (UEP) to correspond to the gradual differences in importance. However, hierarchical modulation, which is often used for UEP and is currently employed in the Digital Video Broadcasting (DVB) standard, provides only a limited number of UEP levels. By multiplexing hierarchical modulation, we propose a high performance multilevel UEP system for the transmission of progressive sources. In the second part, we consider the transmission of a layered source in a multiple-input multiple-output (MIMO) system for broadcast scenarios. We first analyze the tradeoff between two different MIMO approaches, Alamouti coding and spatial multiplexing, having the same transmission rate. For analytical tractability, we consider high SNR approximate (minimum distance) bit error rates (BER) for both MIMO approaches. Based on this, we propose superposition MIMO coding, where two different MIMO approaches are hierarchically combined such that low-rate high priority components of the source are Alamouti coded, high-rate low priority components are spatially multiplexed, and the two different components are superposed. It is demonstrated that in broadcast scenarios, the proposed MIMO coding maximizes the performance of a layered source which has different data rates for its components. In the third part of the dissertation, we analyze the performance of n-channel symmetric FEC-based multiple description coding for a progressive mode of transmission. Multiple description source coding has recently emerged as an attractive framework for robust multimedia transmission over packet erasure channels. In the analysis, we consider transmission over orthogonal frequency division multiplexing (OFDM) networks in a frequency-selective, slowly-varying, Rayleigh faded environment
-------------------------------------

10137483_183 - 0.991519795196 - technology_and_computing
[quantum, qubit, state, error, measurement, qed, energy, procedure]

Quantum State Protection and Transfer Using Superconducting Qubits
This dissertation presents a theoretical analysis of protocols relevant for quantum information processing with superconducting qubits. The purpose of these protocols is decoherence suppression and quantum information transfer. Our analysis makes use of the standard density matrix formalism and Kraus operator (operator-sum) representation of quantum operations. We also use the mathematical trick of unravelling continuous evolution into discrete scenarios with corresponding probabilities.We show that decoherence due to zero-temperature energy relaxation can be almost completely suppressed, probabilistically, by weak measurement reversal (also known as quantum uncollapsing). To protect a qubit, a weak (partial) quantum measurement moves it towards the ground state, where it is kept during the storage period, while the second partial measurement restores the initial state. This procedure preferentially selects the cases without energy decay events. Stronger decoherence suppression requires smaller selection probability; a desired point in this trade-off can be chosen by varying the measurement strength.We also analyze several simple quantum error correction (QEC) and quantum error detection (QED) protocols, relevant to superconducting qubits. We show that for energy relaxation the repetitive N-qubit quantum codes cannot be used for QEC, but can be used for QED (also known as probabilistic error correction). Moreover, the repetitive code with only two qubits is sufficient for QED. We also present several other two-qubit algorithms realizable with the current technology of superconducting phase qubits; these algorithms can demonstrate QEC for intentional errors and QED for real decoherence.We also analyze a procedure designed to transfer the state of a microwave qubit from one superconducting resonator to another resonator via a transmission line; the emission and capture of the microwave energy is achieved using tunable couplers. The procedure is shown to be robust against experimental imperfections of required pulse shapes. Our results also indicate that a successful state transfer requires nearly equal resonator frequencies for the entire duration of the procedure.
-------------------------------------

10136256_183 - 0.999186832213 - technology_and_computing
[model, time, datum, partner, subject, behavior, cluster, longitudinal, dpm, feature]

Hierarchical and semi-parametric Bayesian models for the study of longitudinal HIV behavior and tuberculosis incidence data
We propose and discuss two distinct and separate innovative Bayesian models.  In the first model, we propose a replacement for standard statistical methodologies for longitudinal sexual behavior data.  HIV intervention trials generally collect sexual behavior data repeatedly over time and involve multiple outcomes including the number of partners which are nested in subjects and the number of protected and unprotected sex acts with each partner which are inherently nested within partners.  The data is further complicated by characteristics of both partners and acts.   Partners can be HIV$^+$ or HIV$^-$ while sex acts can be protected or unprotected.  Properly modeling these outcomes and distinguishing these characteristics is critical.  Here we use a multilevel multivariate Bayesian model for modeling sexual behavior outcomes.  The proposed model accounts for the full complexity of sexual behavior allowing for simultaneous modeling of the number of partners and the number of sex acts with each partner, differentiation of behavior by partner serostatus, accounting for study eligibility criterions associated with the outcome of interest, and correlations between observations with the same subject, observations with the same partner, and observations across time.  We further show that the proposed model can be used to quantify and draw inference on seroadaptive behaviors.  Seroadaptive behaviors describe behaviors that vary based on the HIV status of partners with the goal of reducing the risk of transmission.  The model is used to analyze data from the Healthy Living Project.  In the second half of this thesis, we explore a novel extension to the Dirichlet process mixture (DPM) model to accommodate longitudinal data.  Longitudinal data is characterized by two features.  First, the data are a function of time implying dependence between sampling densities across time.  Second, the \emph{same} subjects are repeatedly measured over time.  The standard DPM model is a nonparametric Bayesian model that naturally clusters similar observations together and assigns a single value to each cluster.  It can be used to model an unknown density but addresses neither of these two features in longitudinal data.  A number of current extensions of the DPM model can accommodate dependent distributions which could be used to model the sampling distributions at each time point addressing the first feature.  However, assumptions in these extensions imply these models do not take advantage of the second feature of longitudinal data where the \emph{same} subjects are followed over time.  To account for both features, we propose the cluster memory Dirichlet process mixture (cmDPM) model extending the DPM model to properly accommodate longitudinal data.  In the cmDPM model, subjects are modeled as a DPM model at baseline.  Cluster assignments at future time points depend on where the subject was previously clustered.  Each subject may retain their cluster from the previous time point with some nonzero probability.  This implies that at later times, subjects are no longer exchangeable and their observed values depend on their previous clustering history.  Clusters that are retained over time evolve through a time dependent process.  The cmDPM model extends the DPM to use both the information of where the subject was previously clustered and the value assigned to that cluster to model subject data at the current time point.
-------------------------------------

10133002_183 - 0.999999474708 - technology_and_computing
[application, location, node, sensor, system, follow-me, logical, deployment]

Infrastructureless Location Aware Configuration for Sensor Networks and the Follow-Me Application
The Follow-me application is a sensor network based active visitor guidance system. It is easy-to-deploy and self-configurable. In the system, sensor nodes with LEDs and buttons are deployed throughout a building, one on the wall at each office doorway. They blink lights to indicate a path through the building, guiding a visitor with a "breadcrumb trail" to the destination. The most important aspect of the follow-me application is location-aware configuration. GPS and similar systems determine location today, but they require substantial infrastructure in the environment or on sensor nodes to locate nodes in a physical coordinate system. For many applications, logical location - the relationship of nodes with each other and their environment - can be more important than physical location. For example, distance along a road and presence of intersections may be more relevant than Euclidean coordinates for applications that track or guide drivers. We developed deployment order, a new algorithm to configure logical location in a sensor network. Deployment order exploits node deployment patterns and simple user interactions to define logical topologies in a completely distributed manner with little human input.
-------------------------------------

10136973_183 - 0.999978178208 - technology_and_computing
[code, ldpc, scb, matrix, ccm, memory, dissertation]

Absorbing Set Analysis of LDPC Codes and Read-Channel Quantization in Flash Memory
High-capacity NAND flash memories achieve high-density by storing more than one bit per cell. Storage systems require extremely low block-error-rates, making powerful error-correcting codes with low-error floors necessary. Low-density parity-check (LDPC) codes are well known to approach the capacity of the additive white Gaussian noise (AWGN) channel, but they often suffer from error floors and require soft information to achieve better performance. This dissertation tackles these two problems.The first part of this dissertation introduces the cycle consistency matrix (CCM) as  a powerful analytical tool for characterizing and avoiding absorbing sets in separable circulant-based (SCB) LDPC codes.   Each potential absorbing set in an SCB LDPC code has a CCM, and an absorbing set can be present in an SCB LDPC code only if the associated CCM is not full column-rank. Using this novel observation, a new code construction approach selects rows and columns from the SCB mother matrix to systematically and provably eliminate dominant absorbing sets by forcing the associated CCMs to be full column-rank.  Simulation results both in software and in hardware demonstrate new codes that have steeper error-floor slopes and provide at least one order of magnitude of improvement in the low FER region.   This dissertation also shows how identifying absorbing-set-spectrum equivalence classes within the family of SCB codes with a specified circulant matrix significantly reduces the search space of code matrices with distinct absorbing set spectra.  For a specified circulant matrix, SCB codes all share a common mother matrix and thereby retain standard properties of quasi-cyclic LDPC codes such as girth, code structure, and compatibility with existing high-throughput hardware implementations. SCB codes include a wide variety of LDPC codes such as array-based LDPC codes as well as many common quasi-cyclic codes.  Hence the CCM approach should find wide application.The second part of this dissertation focuses on coding for flash memory. Traditional flash memories employ simple algebraic codes, such as BCH codes, that can correct a fixed, specified number of errors. This dissertation investigates the application to flash memory of low-density parity-check (LDPC) codes which are well known for their ability to approach capacity in the AWGN channel. We obtain soft information for the LDPC decoder by performing multiple cell reads with distinct word-line voltages.  The values of the word-line voltages (also called reference voltages) are optimized by maximizing the mutual information between the input and output of the multiple-read channel. Our results show that using this soft information in the LDPC decoder provides a significant benefit and enables the LDPC code to outperform a BCH code with comparable rate and block length over a range of block error rates. Using the maximum mutual-information (MMI) quantization in the LDPC decoder provides an effective and efficient estimate of the word-line voltages compared to other existing quantization techniques.
-------------------------------------

10129805_178 - 0.977576504759 - technology_and_computing
[design, medical, reference, precision, bibliographical, efficient, robotic]

Efficient design of precision medical robotics
Includes bibliographical references (p. 106-114).
-------------------------------------

10137394_183 - 0.979808658843 - technology_and_computing
[cdi, outage, algorithm, csi, network, channel, link, matrix, power, mimo]

Using statistical information to improve communication in MIMO networks
This dissertation discusses the use of statistical information about channels, or Channel Distribution Information (CDI), in multiuser MIMO networks. There are three main topics covered here. The first is on how to efficiently quantize CDI for feedback in a multiuser environment. The second is to develop a new outage framework for multiuser MIMO networks, and to provide a near-optimal solution for beamforming and power control in the network. The third is to add limited CSI feedback to the CDI scheme to reduce outage, perform better power control, and provide a more reliable network. In the first part of this work, we develop techniques to efficiently quantize channel covariance matrices in MIMO Rayleigh fading environments. While these covariance matrices change less frequently than the channel matrices themselves, this information needs to be updated when it does change. Furthermore, these covariance matrices have significantly more parameters to quantize than their channel matrix counterparts. Since many applications focus on utilizing the strongest eigenmodes of the channel covariance matrix and since these matrices tend to be low- rank, we focus on efficiently quantizing the dominant eigenvectors of these matrices. We develop Lloyd-type algorithms based on training data from the environment to develop our codebooks. We also develop an algorithm based on the reduced-parameter Kronecker and Weichselberger models to generate codebooks with reduced real-time codeword search. In the second part of this work, we examine single user and multiuser MIMO beamforming networks with CDI. Since CDI changes infrequently compared to CSI, algorithms based on CDI can achieve significant savings in feedback compared to algorithms based on CSI. With CDI, we can only guarantee quality of service for a specified outage probability in the network. Assuming correlated Rayleigh fading on all the links, we derive a closed-form expression for the outage probability. Then, using this expression, we derive algorithms for joint transmit/receive beamforming and power control to minimize the weighted sum power in the network while guaranteeing these outage probabilities. For both single-user and multiuser MIMO scenarios, we present optimal algorithms under the Kronecker model assumption, and we present near- optimal algorithms assuming general correlation structures on the links. We then show that using these algorithms based on CDI, if we are willing to accept given outages on the links, we can achieve comparable power usage in the network relative to algorithms based on CSI. We then extend this framework to the broadcast channel with multiple receivers and transmitters. In the third part of this work, we extend the previous outage framework based on CDI to utilize CSI of the direct links of the channels. The CDI framework considered here looks at minimum Signal- to-Interference-plus-Noise (SINR) requirements from each user that is met with a specified outage probability. This outage on the links is undesirable. However, in many standards, CSI is measured at the receiver via training data from its corresponding transmitter. The CSI on the interfering links is typically unavailable. If this CSI can be measured, this information can be fed back to the transmitters so they will not transmit when a given link is in outage. In addition, CSI feedback can be used for power reduction and thus potentially enable links that were previously in outage to transmit. Algorithms discussing these ideas are developed and discussed. Furthermore, an SINR quantizer is developed to minimize power usage, and combined CDI and CSI results are also discussed for the case of the broadcast channel with multiple transmitters and receivers
-------------------------------------

10136949_183 - 0.999514052672 - technology_and_computing
[power, efficiency, voltage]

CMOS RF Power Amplifier Design for Wireless Communications
In recent years, there has been great technology improvement in wireless communication systems. Novel transceiver architectures and circuits have enabled faster data transfer rate over larger area while burning even less power. CMOS technology, by its unique advantages in cost and integration, has enabled an unprecedented level of integration in modern low-cost, small form-factor and low-power wireless devices. The two aspects above put together has made our phones, mp3 players, etc., smarter and smaller, and most importantly, has made our lives more pleasant.There have been great efforts contributed to integrating the whole transceiver into one single silicon die. However, Radio Frequency (RF) Power Amplifiers (PAs) are still among those few remaining modules that have yet to be successfully integrated. The reduced supply voltage, high on-chip passive loss, and the low breakdown voltage of the thin gate oxide CMOS devices are among the most difficult challenges, which have forced high output power CMOS PAs to operate under large-current and low-impedance levels, where they are vulnerable to parasitic losses. On the other hand, efficiency is a big concern in PA designs. Due to the inevitable power back-off in modern communication systems, it is of great importance to improve the efficiency of a PA when it does not work at its maximum rated power. This work targets two major issues: power combining and average efficiency enhancement. By utilizing on-chip transformer networks and adaptive load techniques, we observe improved efficiency over a wide input range in our proposed PA prototype. And to the author's knowledge, the prototype has the most efficiency recovery points among today's reported fully integrated CMOS PAs.The proposed PA is designed in a 0.18-um CMOS technology with 1.8-V supply voltage. The obtained linear output power and power gain are 22-dBm and 8.5-dB, respectively, with Power Added Efficiency (PAE) of 12% at 2.4-GHz. The maximum achievable efficiency recovery points during input power back-off are 7. Verification results prove our design methodologies and efficiency enhancement algorisms to be effective. Some relevant research results including Nano-particle Magnetic-cored Inductors (NMIs) and NMI/graphene inductor Voltage Controlled Oscillators (VCOs) are reported at the end of this thesis.
-------------------------------------

10135924_183 - 0.999990055468 - technology_and_computing
[wire, ignition, model, electrical]

Ignition and spread of electrical wire fires
Ignition of electrical wires by external heating is investigated in order to gain a better understanding of the initiation of electrical-wire fires. An ignition-to- spread model is developed to systematically explain ignition and the following transition to spread. The model predicts that for a higher-conductance wire it is more difficult to achieve ignition and the weak flame may extinguish during the transition phase because of a large conductive heat loss along the wire core. Wires with two metal-core materials, nichrome and copper having three different diameters, with polyethylene coatings of three different thicknesses are employed and a coil heater was adopted as the ignition source in the experimental study. Experiments show that additional heating times after flash are required in order to fully pass the transition and achieve a spreading flame, agreeing with model predictions. In addition, the effects of different heating lengths, ambient pressures and oxygen concentrations on wire ignition are discussed. Steady flame spread horizontally along thin electrical wires in normal and oxygen-enriched oxygen/nitrogen atmospheres is also investigated both theoretically and experimentally to gain a better understanding of the development of in electrical fires occurring in normal and modified environments. A simplified flame-spread model is developed in an effort to identify the most important effects of the wire thermal conductivity and diameter, the thickness of the insulation, and the oxygen concentration. The experimental results agree qualitatively with the model predictions in a number of respects, while no qualitative disagreements were found. This study may be useful for upgrading the design and standards of future fire-safe wires
-------------------------------------

10130019_178 - 0.982613124327 - technology_and_computing
[system, social, boston, dynamic, reference, bibliographical, characteristic, ecological, description, area]

Intra-metropolitan migration and town characteristics: a description of the Boston Metropolitan area as a dynamic social ecological system.
Includes bibliographical references.
-------------------------------------

10133419_183 - 0.99999874749 - technology_and_computing
[fpga, software, expression, regular, ids, hardware, pcre, system, hdl, tool]

Acceleration of Streaming Applications on FPGAs from High Level Constructs
Field Programmable Gate Arrays (FPGA) based Reconfigurable Computing hardware are programmed with specialized Hardware Description Language (HDL).  FPGAs are increasingly being made available as co-processors on high-performance computation systems.The generation of HDL from high-level software languages is way too complex for a human developer to handle in a reasonable amount of time due to incompatibilities in the execution paradigm between a traditional CPU and on an FPGA. This error prone process manifests itself as the main impediment to a wider use of reconfigurable platforms in high-performance computing. Compilation frameworks are thus a valuable tool for translating traditional high-level  software constructs to HDL for implementation on FPGAs. This dissertation details how we leverage FPGAs for accelerating PERL Compatible Regular Expressions  (PCRE), SNORT  Intrusion Detection System (IDS), Common Processing Functions, and XML Filtering, by compiling high-level software language to HDL.In this dissertation, we detail the implementation of a tool that translatesPCRE code into hardware that is mapped to an FPGA.  Our compiler generates VHDL code   corresponding to the opcodes generated from  regular expressions. We have tuned our hardware implementation to  utilize an NFA based regular expression engine using greedy  quantifiers in much the same way as the software based PCRE  engine does.     The SNORT IDS  system, incorporates the software based PCRE engine   for regular expression matching on the payload.     We benchmark the performance of regular expression based rules  from SNORT IDS using  software only execution on a multi-processor system.     We demonstrate the case when 30% or more number of network packets trigger regular expression matching, the software based IDS cannot  maintain 10 Gbps throughout, and thus requires hardware acceleration.  Using our PCRE to HDL compilation system, we implement regular expressions from  the SNORT ruleset on to the FPGA. These rulesets are organized into one of 16 banks  on the FPGA and all operate in parallel.  We have implemented more than two hundred PCRE  engines based on a plethora of SNORT IDS regular expression rules. These were mapped to the  Xilinx Virtex-4 LX200 FPGA on the SGI RASC RC 100 Blade connected to  the SGI ALTIX 4700 supercomputing system as a testbed. We obtain an  interface throughput of {12.9 GBits/s} and a speedup of  {353X} over software based PCRE execution. We also show that it is  possible to scale down the processing related power consumption of  an IDS by {two orders of magnitude} using an FPGA .    In this dissertation we describe software tools as well as an IDS architecture that leverages reprogrammability of FPGA hardware.Our software tools for Configurable System on a Chip (CSoCs)  generates the communication interface between the software running on the CPU and a tightly coupled IP core based co-processing system. Our tool generates hardware wrappers for the IP Cores that makes them look like a C function invocation in the source code. We also use our tool to support partial reconfiguration: the same wrapper is used for a multitude of IP Cores and the user selects the core to be invoked in the program.      We also demonstrate an adaptable regular expression based IDS  using Virtex-4 LX 200 FPGAs that have been floor-planned for partial reconfiguration. Our novel design allows partial reprogramming across  16 banks of regular expression rule-sets. We implement 448 different regular expressions on two   FPGAs and perform   multiple partial and full reconfigurations. We measure the throughout of the integrated Field Programmable Gate Array (FPGA) and  multiprocessor   SGI Altix system   with varying number of  reconfigurations per minute. The adaptive IDS can provide  better than 10 Gbps throughput  even with 32 partial reconfigurations per minute.In this dissertation we demonstrate  a four step approach that converts user profiles expressed as XPath queries into HDL, suitable for implementation on FPGA.  We convert XPaths to PCRE, cluster them by their common prefixes, compile the PCRE to HDL and finally synthesize and implement them on FPGA. This hardware is usable for XML filtering in pub-sub applications. Our benchmarks reveal orders of magnitude improvement in running time while running XML filtering on FPGA, when compared to the state of the art software based XML filtering systems.Finally, in this dissertation we demonstrate a FPGA based implementation of Prufer sequence generation hardware for streaming XML document. We match the stream with several  Prufer sequence blocks obtained from  twig queries.
-------------------------------------

10133573_183 - 0.999998182773 - technology_and_computing
[camera, network, control, video, tracking, algorithm, activity]

Scene Analysis, Control and Communication in Distributed Camera Networks
As networks of video cameras are being installed in many applications, modeling and inference strategies in video networks have captured more and more interest. There are many challenge problems, such as (i) traditional computer vision challenges in tracking and recognition, robustness to pose, illumination, occlusion, clutter; recognition of objects and activities; (ii) aggregating local information to obtain stable, long-term tracks of objects; (iii) cooperative camera control algorithms for multi-resolution target acquisition; (iv) distributed processing and scene analysis and (v) communication in a distributed manner. The overall aim of this thesis is to study the core issues in network-centric processing, control and communication in a multi-camera network, including frameworks for tracking people in video through changes of activities, tracking in a non-overlapping camera network, decentralized control and tracking in a camera network, and distributed video compression.In our work, we address the problem of tracking in camera networks by dividing into two parts: tracking people through changes in activities and tracking multiple targets in a network of non-overlapping cameras. We present a novel framework for tracking a long sequence of human activities, including the time instances of change from one activity to the next, using a non-linear switching dynamical feedback system. Meanwhile, a multi-objective optimization framework by combining short term feature correspondences across the cameras with long-term feature dependency models is proposed to take care of tracking in a non-overlapping network.We also deal with the problem of decentralized, cooperative control of a camera network and distributed multi-target tracking in such a network of self-configuring pan-tilt-zoom cameras. This control cannot be based on separate analysis of the sensed video in each camera. They must act collaboratively to be able to acquire multiple targets at different resolutions. Our research focuses on developing accurate and efficient camera control algorithms in such scenarios using game theory. For tracking the targets as they move through the area covered by the cameras, we propose a special application of the distributed estimation algorithm known as Kalman-Consensus filter through which each camera comes to a consensus with its neighboring cameras about the actual state of the target.Finally, we present a framework for multi-terminal video compression (MTVC) that exploits the geometric constraints between cameras with overlapping fields of view, and then uses distributed source coding on corresponding points in two or more views. Our  proposed method is composed of two parts - a Distributed Motion Estimation (DME) algorithm and a Distributed Source Coding (DSC) algorithm.
-------------------------------------

10130075_178 - 0.999987381226 - technology_and_computing
[system, reference, multicore, bibliographical, mechanism, self-aware, adaptive]

Adaptive mechanisms for self-aware multicore systems
Includes bibliographical references (p. 90-92).
-------------------------------------

10175405_189 - 0.999996342068 - technology_and_computing
[method, optimization, satellite, real-time, local, computational, trajectory, burden]

Discrete Search Optimization for Real-Time Path Planning in Satellites
This study develops a discrete search-based optimization method for path planning in a highly nonlinear dynamical system. The method enables real-time trajectory improvement and singular configuration avoidance in satellite rotation using Control Moment Gyroscopes. By streamlining a legacy optimization method and combining it with a local singularity management scheme, this optimization method reduces the computational burden and advances the capability of satellites to make autonomous look-ahead decisions in real-time. Current optimization methods plan offline before uploading to the satellite and experience high sensitivity to disturbances. Local methods confer autonomy to the satellite but use only blind decision-making to avoid singularities. This thesis' method seeks near-optimal trajectories which balance between the optimal trajectories found using computationally intensive offline solvers and the minimal computational burden of non-optimal local solvers.  The new method enables autonomous guidance capability for satellites using discretization and stage division to minimize the computational burden of real-time optimization.
-------------------------------------

10135980_183 - 0.999989080445 - technology_and_computing
[fpu, floating-point, design, network, pipelined, processor, operation]

Floating-point Network Node for Multi-University Research Network
Floating-point operations are complex, they are both power and area intensive. The performance of floating-point operations can have a significant impact on the overall performance of a processor. This thesis explores the design, verification and testing of a synchronous pipelined out-of-order IEEE-754 compliant floating-point unit (FPU) for the Santa Cruz Out of Order RISC Engine (SCOORE).SCOORE is a superscalar out-of-order SparcV8 processor and is currently under development in the micro-architecture Santa Cruz (MASC) Laboratory. The processor is planned to be taped out in the 28nm GLOBALFOUNDRIES process as a part of the Multi UniversityResearch Network. Our FPU provides hardware support for floating-point addition, subtraction, multiplication, division, square-root arithmetic operations for single as well as double precision.The design is modeled using the Verilog-2001 HDL and implemented using the STM 90nm process.The three major design units described in this work are a traditional Flop-based pipelined FPU; a latch-based pipelined FPU utilizing the retry mechanism; and the FPU ring network node consisting of the FPU, the programmable built-in self test (BIST) and the demultiplexer (DMUX). The design achieves the operating frequency goal of 1.4 GHz on the TSMC 90nm process.
-------------------------------------

10133621_183 - 0.999986887962 - technology_and_computing
[user, scheduler, utility, job, estimate, scheduling, contribution, algorithm, workload]

On the user-scheduler relationship in high-performance computing
To effectively manage High-Performance Computing (HPC) resources, it is essential to maximize return on the substantial infrastructure investment they entail. One prerequisite to success is the ability of the scheduler and user to productively interact. This work develops criteria for measuring productivity, analyzes several aspects of the user-scheduler relationship via user studies, and develops solutions to some vexing barriers between users and schedulers. The five main contributions of this work are as follows. First, this work quantifies the desires of the user population and represents them as a utility function. This contribution is in four parts: a survey-based study collecting utility data from users of a supercomputer system, augmentation of the Standard Workload Format to enable scheduler research using utility functions, and a model for synthetically generating utility function-augmented workloads. Second, a number of the classic scheduling disciplines are evaluated by their ability to maximize aggregate utility of all users, using the synthetic utility functions. These evaluations show the performance impact of inaccurate runtime estimates, contradicting an oft quoted prior result 55 that inaccuracy of estimates leads to better scheduling. Third, a scheduler optimizing the aggregate utility of all users, using a genetic algorithm heuristic, is demonstrated. This contribution includes two software artifacts: an implementation of the genetic algorithm (GA) scheduler, and a modular, extensible scheduler simulation framework that simulates several classic scheduling disciplines and is interoperable with the Standard Workload Format. Fourth, the ability of users to productively interact with this scheduler by providing an accurate estimate of their resource (run time) needs is examined. This contribution consists of formalizing a frequent casual assertion from the scheduling literature, that users typically "pad" runtime estimates, into an explicit Padding Hypothesis, and then falsifying the hypothesis via a survey-based study of users of a supercomputer system. Specifically, absent an incentive to pad-and including incentives to be accurate-the inaccuracy of runtime estimates only improved from an average of 61% inaccurate to an average of 57% inaccurate. This contribution has implications not only for the proposed genetic algorithm scheduler, but for any scheduler that asks users for an estimate, which currently includes virtually all parallel job schedulers both in production use and proposed in the literature. Fifth, a survey of users of a supercomputer system and associated simulations explore the feasibility of removing one of the defining constraints of the parallel job scheduling problem-the non-preemptability of running jobs. An investigation of users' current checkpointing habits produced a workload labeled with per-job checkpoint information, enabling simulation of a checkpoint-aware GA scheduler that may preempt running jobs as it optimizes aggregate utility. Lifting the non-preemptability constraint improves performance of the GA scheduler by 16% (and 23% compared to classic EASY algorithm), including overhead penalties for job termination and restart
-------------------------------------

10135109_183 - 0.935667738647 - technology_and_computing
[step, problem, vehicle, sensor]

Adaptive observation with vehicle dynamics
Technological advances in unmanned sensor vehicles allows the possibility to solving the Adaptive Observation problem with moving sensors. This problem is addressed in this thesis. Toward applying the solution in real-world problems, the derivation of the solution is broken down into three incremental steps. The first step involves solving a simpler problem at infinite time with stationary sensors. The second step take the theory establish in step one and augment it with vehicle dynamics in finite time. The last step takes a look at implementation issues when implement the theory develop in step two, and make appropriate modifications in order to derive a feasible and practical algorithm
-------------------------------------

10132956_183 - 0.914007050399 - technology_and_computing
[motorization, area, world]

The Developing World's Motorization Challenge
Motorization is transforming cities and even rural areas of the developing world. The economic and social benefits are enormous. It provides individual flexible transportation in urban areas and reduced manual labor and improved market access in rural areas. In the longer term, however, motorization may stifle local development, increase pollution, and create unprecedented safety hazards. Without careful attention to the motorization process, disaster looms for cities of the developing world-- disaster from which the industrialized countries cannot be isolated.
-------------------------------------

10175470_189 - 0.999951435054 - technology_and_computing
[network, scheduling, class, rate, sum, maximal, node]

Maximal Clique Scheduling: A Simple Algorithm to Bound Maximal Independent Graph Scheduling
In this paper, we consider interference networks where the connectivity is known globally while the channel gains are known up to a particular distance from each node. In this setting, we provide a new achievability, called Maximal Clique Scheduling (MCS), which  is a special case of Maximal Independent Graph Scheduling (MIG Scheduling) proposed earlier. The strategy is evaluated using the notion of normalized sum rate which is a metric to evaluate performance of networks with mismatched knowledge.  The achievable normalized sum rate of the proposed MCS strategy is easier to analyze for certain classes of networks and can be used to bound the normalized sum rate of MIG Scheduling. We investigate the normalized sum rate achieved by MCS for two classes of networks. The first class is formed by interference networks where each link is connected with probability $p$. The second class is derived from Wyner 1-D model of placements of base stations and mobile nodes. We find that increasing knowledge about the network leads to increasing normalized sum-rate. However, in a random network, the increase is slower as compared to Wyner network because most nodes are far away from a node and hence learning more helps less until the whole network is known.
-------------------------------------

10138518_183 - 0.998398000095 - technology_and_computing
[building, comfen, user, design, energy, graphic, interface, fundamental, tool, effect]

COMFEN 3.0 - Evolution of an Early Design Tool for Commercial Facades and Fenestration Systems
Achieving a net-zero energy building cannot be done solely by improving the efficiency of the engineering systems. It also requires consideration of the essential nature of the building including factors such as architectural form, massing, orientation and enclosure. Making informed decisions about the fundamental character of a building requires assessment of the effects of the complex interaction of these factors on the resulting performance of the building. The complexity of these interactions necessitates the use of modeling and simulation tools to dynamically analyze the effects of the relationships, yet decisions about the building fundamentals are often made in the earliest stages of design, before a `building? exists to model. 

To address these issues, Lawrence Berkeley National Laboratory (LBNL) has developed an early-design energy modeling tool (COMFEN) specifically to help make informed decisions about building facade fundamentals by considering the design of the building envelope, orientation and massing on building performance. COMFEN focuses on the concept of a ?space? or ?room? and uses the EnergyPlus, and RadianceTM engines and a simple, graphic user interface to allow the user to explore the effects of changing key early-design input variables on energy consumption, peak energy demand, and thermal and visual comfort. Comparative results are rapidly presented in a variety of graphic and tabular formats to help users move toward optimal facade and fenestration design choices. 
While COMFEN 1.0 utilized an ExcelTM-based user interface, COMFEN 3.0 has been reworked to include a simple, more intuitive, yet powerful Graphic User Interface (GUI), a broader range of libraries for associated system and component choices and deliver a wider range of graphic outputs and options. 

This paper (and presentation) outlines the objectives in developing and further refining COMFEN, the mechanics of the program, and plans for future development.
-------------------------------------

10135639_183 - 0.999543243778 - technology_and_computing
[datum, specification, process, business, verification, property, arithmetic, class, dependency, temporal]

Verification of business process specifications with arithmetic and data dependencies
Recent years have witnessed the evolution of business process specification frameworks from the traditional process-centric approach towards data-awareness. Process- centric formalisms focus on control flow while under- specifying the underlying data and its manipulations by the process tasks, often abstracting them away completely. In contrast, data-aware formalisms treat data as first- class citizens. The presence of data implies an increase expressiveness of business process specification, including often data dependencies and arithmetic. This thesis studies the verification problem of temporal properties on data-aware business specifications with data dependencies and arithmetic. In our context, data implies infinite-state systems, for which verification is notoriously difficult. Unlike previous work, we focus on verification that is a) automatic (i.e. no expert user is required to help the process of verification as for theorem provers), b) sound and complete, and c) does not abstract away the data portion, retaining the ability to check the effects of data values on the behavior of the process (e.g. in a prototypical e-commerce business process, abstracting the data would make it impossible to check if the payment received for a product matches the price reported on the bill). We identify a practically significant class of business process specifications with data dependencies and arithmetic, for which verification of temporal properties is decidable. Besides decidability, in the context of commonly occurring classes of specifications, we develop verification techniques with upper bounds palatable to implementation, e.g. PSPACE for a common class of specifications with unary keys and fixed -arity databases with acyclic foreign keys. We implement a verifier prototype based on our theoretical results and measure the running times of the verification of temporal properties on a wide range of business process specifications of different complexities. Our random generation is based on patterns and frequencies found in real-world business process specifications and properties. The average running times measured range from seconds to minutes for the more complex specifications. We argue that the work in this thesis proves the feasibility of automatic verification of temporal properties on highly expressive business process specifications, that is both sound and complete
-------------------------------------

10131859_183 - 0.975634248739 - technology_and_computing
[driver, information, path, ati, traffic, dynamic, strategy, simulation, network]

Simulation of Advanced Traveller Information Systems (ATIS) Strategies to Reduce Non-Recurring Congestion from Special Events
The design and implementation of Advanced Traveller Information Systems (ATIS) providing real-time enroute information to drivers should follow insightful analyses into the dynamics of driver decisions and the resulting traffic flow under information to prevent counter-intuitive and counter-productive results. An important yet often neglected aspect of this problem is the distribution of benefits both over the driver population and for different origins and destinations in the network. This paper presents modifications to and an application of DYNASMART (DYnamic Network Assignment Simulation Model for Advanced Road Telematics) for this problem. DYNASMART is a simulation framework for ATIS experiments which incorporates: 1) real-time traffic flow and control simulation, 2) dynamic network path processing, and 3) microscopic consideration of driver response to information. A boundedly-rational behavioral model is assumed for driver route-choice under non-prescriptive route information. The information strategies are based on multiple paths rather than a single shortest path. Initial paths of drivers were generated from dynamic equilibrium assignments using the CONTRAM program and used as input to DYNASMART. ATIS-equipped drivers change their paths based on a behavioral model (with stochastically assigned parameters) and provided information, while unequipped drivers change routes based on self-observation of traffic conditions. The application presented involves the evaluation of ATIS strategies to alleviate traffic congestion due to spectators leaving a major sports event at Anaheim Stadium. A dynamic traffic demand matrix was estimated from partial link-counts. Interesting insights are derived regarding the higher benefits from ATIS to drivers on congested parts of the network. Robustness of the benefits under various information supply strategies and behavioral scenarios are also discussed.
-------------------------------------

10134270_183 - 0.999994927871 - technology_and_computing
[building, monitoring, datum, real-time, coral, application, real, open, ci, decision]

An integrated cyberinfrastructure for real-time data acquisition and decision making in smart buildings and coral reef monitoring
Buildings and environmental monitoring systems sense their surrounding conditions using various types of sensors. They process information and react to their environments in real time. The integrated cyberinfrastructure (CI) for the real-time data acquisition, real-time data processing and decision making is proposed, analyzed, implemented, and tested in this thesis. The open architecture solution is presented and implemented with open source software packages. Then, the CI is applied in two settings, the simulation and the real world. The smart building application uses the simulation environment, and the coral reef monitoring system uses the sensors deployed in the Pacific Ocean. This research is based on studies performed in Moorea. The use case scenarios and the configuration for detecting and responding to such scenarios are described for both applications
-------------------------------------

10130262_178 - 0.993430209891 - technology_and_computing
[system, development, reference, bibliographical, planning]

The development of a planning system in the U.S.
Includes bibliographical references.
-------------------------------------

10134068_183 - 0.999600114889 - technology_and_computing
[model, parameter, material, bsdf, appearance]

A physically-based BSDF for modeling the appearance of paper
In this thesis a novel physically based appearance model which utilizes both microfacet and diffusion theory is introduced. Although the model could potentially be used for a large variety of complex materials that interact with light that scatters through their surface and sub- surface a more needed and complex application was decided, paper. Paper, which is simply assumed to have diffuse characteristics by many, is actually a rough, complex material consisting of thin layers of cellulose pulp and smooth finishes. These layers give paper a unique front and back surface with each side varying in roughness and levels of gloss. Through the measured data, paper was found to exhibit specular reflection, retroreflection, sheen at grazing angles, and subsurface scattering, all of which is accounted for in the novel appearance model. The Paper BSDF successfully combines, in a physically correct manner, both microfacet theory and as well as scattering and diffusion theory. All of the underlying modified models used as building blocks, share the same parameters and light's interaction with the material is naturally predicted. The model's accuracy was verified through BSDF measurements acquired using the UCSD Hemispherical Gantry with the aid of a non-linear constrained SQP algorithm. All of the parameters of the model have physical meaning and can be easily altered to change the appearance of the material modeled in a predictable manner which provides a significant advantage over other fitting methods such as Lafortune lobes or Wavelets. Although the model proposed is a BSDF it has a direct correlation with a Bidirectional Sub-surface Scattering Distribution Function (BSSDF) which can be used with the same parameters to render the desired material. This essentially provides an efficient way to measure materials that exhibit sub-surface scattering properties as a BSDF and then with the extracted parameters the BSSDF can be used for rendering. The results show that the model can efficiently account for various paper types, with just changes in parameters, varying from matte to photographic glossy coated paper. Finally it is explained how the physically based nature of the model can be used to successfully reduce the search space of it physically meaningful parameters, in order to obtain faster convergence of the fitting algorithm
-------------------------------------

10129991_178 - 0.97119622732 - technology_and_computing
[use, site, flat, continuity, multifaceted, institute, technology, arch, massachusetts, architecture]

Use continuity on a multifaceted flat urban site.
Thesis. 1975. M.Arch.--Massachusetts Institute of Technology. Dept. of Architecture.
-------------------------------------

10134010_183 - 0.997949818884 - technology_and_computing
[vector, memory, hopfield, neural]

An Exploration of Varying Conditions in a Hopfield Neural Network and Applications to a DNA Implementation
A Hopfield Neural Network is a content addressable memory with elements consisting of the correlations between elements of memory vectors.  Recall of a complete memory vector is possible via the introduction of a "corrupted" vector, which is a memory vector with some components altered.  It may also be possible to correctly recall memories with the use of a partial vector.  It may be possible to create such an information storage and retrieval system using DNA as a working substance.  Herein I present some computational results for properties of Hopfield Neural Networks, as well as a theoretical framework for the operation of such a system, including possible limitations in the working substance.
-------------------------------------

10137566_183 - 0.999944366611 - technology_and_computing
[datum, application, participatory, analysis, asthma, health, management, statistical, mobile, user]

Statistical Directions for the Analysis of Participatory Mobile Health Asthma Management Data
Mobile technology has become increasingly popular in the past decade through the combination of device portability and advances in network and internet technology.  Smart phones, in particular, are at the forefront of these technologies, enabling users to remotely track and further involve themselves in the management of personal health through participatory mobile health applications.  Thus far, the majority of participatory applications currently provide users with data visualizations displaying information germane to the user's medical condition, however, there is still a need for in-depth exploratory and inference-based data analysis using advanced statistical methods to maximize the discernment of potential implications carried in these data streams.  This paper provides an overview of the structure of participatory data using an asthma management application as an initial platform and discusses several directions for statistical analysis motivated by three usage cases: individuals using the application, creators of the application, and the scientific community.  Methods include functional and semi-parametric data analysis, mixed modeling, and clustering methods to model variables representing asthma wellness as a function of subject-specific, population level, and latent spatiotemporal factors.  Societal implications are also discussed.
-------------------------------------

10134181_183 - 0.987705453708 - technology_and_computing
[material, object, x-ray, energy, approach, detection, high, system, method]

Performance enhancement approaches for a dual energy x-ray
Dual energy imaging is a technique whereby an object is scanned with X-rays of two levels of energies to extract information about the object's atomic composition (Z). This technique is based on the fact that the X-ray absorption coefficient decreases with X-ray energy for low -Z materials, but begins to increase for high-Z materials due to the onset of pair production. Methods using the ratio of the attenuations for high-energy to low-energy images as an indicator of Z value have been proposed by several people. However, the statistical errors associated with the systems make those indicators unreliable. This thesis will discuss the problems associated with using a dual-energy system for high-atomic-number material (also known as high Z material) detection. We will identify the sources of noise that hinder system performance and propose solutions for noise reduction. Later chapters will deal with methods to automate the high Z detection process. We use a method called adaptive masking to identify possible high Z objects and reduce the false alarms. For objects shielded by materials common in a cargo container, we propose a layer separation approach to estimate the ratio of the high-and low-energy attenuations of the shielded objects. The approaches provided in this thesis are able to enhance the detection rate and reduce the false alarms significantly
-------------------------------------

10136630_183 - 0.999995481279 - technology_and_computing
[interference, channel, network, wireless, system, computing, smartphone, solution, orthogonal, multiple]

Resource Allocation Techniques to Improve the Performance of Wireless Networks
Wireless networks are increasingly becoming important in enabling convenient Internet accessfor users. Many users have WiFi networks in their homes and have access to cellular networks(e.g., 3G, 4G) when they are on the go. Unfortunately, wireless networks do not offer performanceguarantees. The download and upload rates achieved with these networks fluctuate and may oftendegrade to the point where they become practically unusable. One of themajor factors that causesthis performance degradation is interference, which can be defined as the unwanted noise generatedby other devices in the wireless spectrum.The root cause of interference lies within the fact that the open nature of wireless communicationsenables multiple stations to access the spectrum simultaneously. To eliminate interference,wireless communication systems offer multiple orthogonal channels. If multiple stations accessthe spectrum using these orthogonal channels, interference is very likely to be avoided (assumingthat the signal "leak" between orthogonal channels is negligible), as opposed to the case wherestations use the same channel. Therefore, one needs to determine the specific access patterns (i.e.,which channel is used by which transmitter at what point in time) to systematically coordinate thetransmissions of multiple wireless stations.While a large body of previous work exists to coordinate the use of orthogonal channels betweenIEEE 802.11 a/b/g stations, we show that they are not able to address the interference problem forIEEE 802.11n devices due to the use of wide channels in 802.11n systems. Thus, we proposea WLAN management system - called ACORN - that not only allocates orthogonal channels to802.11n access points but also makes intelligent user association decisions to significantly improveaggregate WLAN throughput. Later, we study the interference in cellular networks, in particularOFDMA femtocell systems. We experimentally characterize the interference between OFDMAfemtocells and propose design guidelines that help in building resource management solutions toalleviate interference. We observe that resource management solutions for OFDMA femtocells needto implement unique functionalities due to the fundamental differences between OFDMA accesstechnology and 802.11 WiFi systems. Leveraging the guidelines revealed by our study, we laterpropose FERMI - a resource management solution that mitigates interference between OFDMAfemtocells and at the same time leverages spatial reuse opportunities. We implement FERMI ona WiMAX femtocell testbed and show that it provides throughput benefits as much as 7x over abaseline scheme and it is closer to the optimal solution than its simpler theoretical alternatives.In addition to the multi-cell interference setting addressed by our above-mentioned contributions,we study beamforming in a single cell scenario. Beamforming is a technique that allowsfocusing the energy of transmissions in a particular direction to improve overall signal quality.Since OFDMA schedules multiple users in the same frame (and this is in contrast toWiFi), designingdata scheduling solutions and at the same time benefiting from beamforming is challenging. Wepropose iBUS - a system that addresses uplink data scheduling with beamforming. We show thatiBUS is provably efficient in striking the right tradeoffs inherent in this setting. Evaluations fromboth a prototype system implementation and simulations yield that iBUS improves the throughputby more than 40% compared to an omni-directional scheduling scheme.Finally, we study distributed computing on smartphones. We identify that there are a largenumber of phones that are left idle (often when being charged overnight) for long periods of time.We envision that an enterprise could use such idle smartphones (e.g., phones of its employees) for enabling parallel computing on them. We believe that smartphones offer precious computingresources that have not previously been tapped into from a distributed computing perspective. Inaddition to the computing capacity being offered, our cost analysis and projections show that smartphonesare also an energy-efficient alternative to using dedicated servers for computing purposes.However, enabling distributed computing on a large number of phones is complicated due to theunique set of challenges that smartphones present, such as heterogeneity in CPU speed and variabilityin network bandwidth. We design and implement CWC - a distributed computing infrastructureusing smartphones. Our extensive evaluations demonstrate that CWC can schedule tasks for 1.6xfaster completion that alternative mechanisms.
-------------------------------------

10136819_183 - 0.999982513784 - technology_and_computing
[network, wireless, forensic, discovery, neighbor, evidence, survivability, ghz, band, site]

Survivability Considerations in Wireless Networks
Survivability is usually defined as the capability of a network to deliver data successfully in a timely manner, even in the presence of attacks. Security as well as reliability are included in survivability considerations. Due to the lossy and open nature of wireless links, design of solutions to make the network survive against attacks as well as wireless idiosyncrasies is difficult. In this dissertation, we approach several issues that have to be considered in order to ensure survivability in various contexts. Firstly, we need to understand the causes for why the network is failing or does not deliver data efficiently as supposed. Network forensics could help with this matter. However, provisioning forensic evidence could impact the network performance in terms of throughput and delay. Secondly, we need to make the network reliable when switching from commonly used bands to another, because conventional network protocols may be affected by the characteristics of that particular frequency. Finally, we need to make sure there is security for the communication relying on the online social networks against active seekers of information. For the first issue, we examine the problem of monitoring packet level transmissions and provisioning forensic evidence. We provide an analytical framework that computes the likelihood that forensic evidence exists with respects to packet transmissions. We validate our analytical framework via simulations and real-world experiments on two different wireless testbeds. Then we use the analytical framework as a basis for a protocol within a forensic analyzer to determine the likelihood that nodes drop packets. Our assessments are shown to be close to the ground truth, with an average deviation of 2.3%. Furthermore,  we quantify the trade-offs between provisioning forensic evidence and achieving high performance in wireless networks. We find that the performance remains unaffected up to a certain evidence availability requirement. Beyond this, the throughput could degrade and the delay could increase by as much as 30%.Then, this dissertation presents our work in evaluating neighbor discovery in 60 GHz band. Neighbor discovery is a fundamental process in the self-configuration of ad-hoc wireless networks. However, the unique physical layer characteristics of 60 GHz make neighbor discovery different from that in traditional 2.4 and 5 GHz bands. In particular, neighbors can be discovered via both direct and reflected beams. We analyze two neighbor discovery approaches in 60 GHz band. We examine the impact of system parameters on the discovery efficiency.Finally, this dissertation includes our efforts in doing covert communication on public photo-sharing sites. Steganography can be used to embed secret messages in shared images. However, the image processing performed by photo sharing sites may destroy the embedded messages. We provide an in-depth measurement study of the feasibility of hiding data on four popular photo sharing sites and identify the challenges in communicating covertly on these sites. We propose an approach of embedding secret messages while ensuring their integrity is preserved. We also present an approach for bootstrapping the communication without an out-of-band channel.
-------------------------------------

10135091_183 - 0.998409695265 - technology_and_computing
[wave, model, motion, turbine, design, wind, analysis]

Dynamic analysis of a 5 megawatt offshore floating wind turbine
Offshore wind is a valuable source of renewable energy, as it is typically strong and steady. Turbines have been utilized offshore in parts of Europe and Asia, however only at shallow depths. Floating wind turbines must be implemented in deeper areas to be economical, but this technology is relatively new and untested. This paper describes a numerical analysis model that can be used to investigate the motion of a 5 MW floating turbine subjected to ocean conditions. Prototype designs for a spar buoy and barge platform are studied. The stiffness and damping effects brought about by the mooring lines are evaluated using a dynamic cable model. A boundary element model is used to calculate added mass and damping effects, as well as the forces on the structure caused by the wave- body interaction. The governing equations of motion include all the added mass, damping and stiffness components in the frequency domain. Response of the structure is found by solving the governing equation combined with a wave spectrum to represent actual ocean wave fields. Approximate bending moments at the base of each design are found by inputting the predicted base motion into a linear modal analysis model created in SAP2000. Based on the results found in this paper, incoming waves cause much greater motion of the barge design, especially in the pitching direction
-------------------------------------

10175487_189 - 0.962903797006 - technology_and_computing
[map]

William Marsh Rice estate in Dunellen, NJ, map of grounds
Color image of a hand-drawn map of the William Marsh Rice estate in Dunellen, New Jersey. The upper portion of the map is fragmented, with several sections missing. The following buildings are drawn and labeled: smokehouse, dwelling houses for the help, outhouse, servants quarters, kitchen, and the main residence. The scale is 1” to 16’. “B. Fosgate, Architect, Plainfield, N.J.” is stamped on the left hand side of the map. There is a fragment of illegible writing on the lower portion of the map.
-------------------------------------

10130508_183 - 0.971938798384 - technology_and_computing
[distribution, visibility, transmittance]

The contract transmittance distribution as a possible tool in visibility calculations
SIO Reference 58-68. The purpose of this note is to introduce the concept of the contrast transmittance distribution and to point out its potentially useful application to the general visibility problems in natural aerosols and hydrosols.
-------------------------------------

10129893_178 - 0.999963834378 - technology_and_computing
[exotica, elusive, system, emergent, promising, semiconductor, theoretical, quantum, mode, fantasy]

Quantum physics: Majorana modes materialize
Elusive theoretical fantasies known as Majorana modes have been observed in a hybrid semiconductor–superconductor system. These emergent exotica open up promising prospects for quantum computation.
-------------------------------------

10135263_183 - 0.99887856127 - technology_and_computing
[performance, processor, thread, program, core, multi-core]

Single thread performance in the multi-core era
The era of multi-core processors has begun. These multi- core processors represent a significant shift in processor design. This shift is a change in the design focus from reducing individual program (thread) latency to improving overall workload throughput. For over three decades, programs automatically ran faster on each new generation of processor because of improvements to processor performance. However, in this last decade, many of the techniques for improving processor performance reached their end. As a result, individual core performance has become stagnant, causing diminished performance gains for programs which are single-threaded. This dissertation focuses on improving single-thread performance on parallel hardware. To that end, I first introduce modifications to a new form of parallel memory hardware, Transactional Memory, which can improve the viability of Speculative Multithreading - a technique for using idle cores to improve singlethreaded execution time. These modifications to Transactional Memory improve Speculative Multithreading effectiveness by a factor of three. I further improve the performance of Speculative Multithreading by addressing a primary source of performance loss - the loss of thread state due to frequent thread migrations between cores. By predicting the cache working-set at the point of migration, we can improve overall program performance by nine percent. Recognizing the demand for transistors to be dedicated to shared or parallel resources (more cores, better interconnect, larger shared caches), I next propose a method of improving branch prediction accuracy for smaller branch predictors. I demonstrate that there are regions of program execution where long histories hurt prediction accuracy. I provide effective heuristics for predicting these regions - in some cases enabling comparable accuracies from predictors of half the size. I then address the problem of contention among coscheduled threads for shared multi-core resources. To reduce resource contention, I propose a new technique for thread scheduling on multi-core processors with shared last level caches which improves the overall throughput, energy efficiency, and fairness of the coschedule
-------------------------------------

10134557_183 - 0.983084516055 - technology_and_computing
[element, error, adaptive, indicator, basis, function, bank, finite, hp-adaptive]

p-adaptive and automatic hp-adaptive finite element methods for elliptic partial differential equations
In this dissertation, we formulate and implement p- adaptive and hp-adaptive finite element methods to solve elliptic partial differential equations. The main idea of the work is to use elements of high degrees solely (p- adaptive) or in combination with elements of small size (hp-adaptive) to better capture the behavior of the solution. In implementing the idea, we deal with different aspects of building an adaptive finite element method, such as defining basis functions, developing algorithms for adaptive meshing procedure and formulating a posteriori error estimates and error indicators. The basis functions used in this work are regular nodal basis functions and special basis functions defined for elements with one or more edges of higher degree transition elements). It is proved that with our construction of these basis functions, the finite element space is well- defined and C⁰. Several algorithms are developed for different scenarios of the adaptive meshing procedure, namely, p-refinement, p-unrefinement and hp-refinement. They all follow the 1-irregular rule and 2-neighbor rule motivated by Bank and Sherman, 1983 - MR751598. These rules help to limit the number of special cases and maintain the sparsity of the stiffness matrix, and thus to simplify the implementation and reduce the cost of calculation. The work of formulating a posteriori error estimates and error indicators is the core of this dissertation. Our error estimates and error indicators are based on the derivative recovery technique proposed by Bank and Xu, 2003 - MR2034616, MR2034617 and Bank et al., 2007 - MR2346369. Using the information in formulating the error indicators, we define a hp-refinement indicator which can be utilized to decide whether a given element should be refined in h or in p. Numerical results show that the combination of the two indicators helps automatic hp-refinement to create optimal meshes that demonstrate exponential rate of convergence. In this dissertation, we also consider hp-adaptive and domain decomposition when they are combined using the parallel adaptive meshing paradigm developed by Bank and Holst, 2000 - MR1797889. Numerical experiments demonstrate that the paradigm scales up to at least 256 processors (maximum size of our experiments) and with nearly 200 millions degrees of freedom
-------------------------------------

10136573_183 - 0.830965035507 - technology_and_computing
[datum, diversity, project, scale]

call for data: PREDICTS: Projecting Responses of Ecological Diversity in Changing Terrestrial Systems
The PREDICTS project (www.predicts.org.uk) is a three-year NERC-funded project to model and predict at a global scale how local terrestrial diversity responds to human pressures such as land use, land cover, pollution, invasive species and infrastructure. PREDICTS is a collaboration between Imperial College London, the UNEP World Conservation Monitoring Centre, Microsoft Research Cambridge, UCL and the University of Sussex. In order to meet its aims, the project relies on extensive data describing the diversity and composition of biological communities at a local scale. Such data are collected on a vast scale through the committed efforts of field ecologists. If you have appropriate data that you would be willing to share with us, please get in touch (enquiries@predicts.org.uk). All contributions will be acknowledged appropriately and all data contributors will be included as co-authors on an open-access paper describing the database.
-------------------------------------

10133549_183 - 0.999947900067 - technology_and_computing
[system, vibrissa, motor, cortex, sensory, representation, behavior, datum, model, rhythmic]

Biomechanics and cortical representation of whisking in the rat vibrissa system
Sensory organs are not stationary in the world, and thus sensation can reflect motion of the organism equally as well as stimuli from the environment. Nervous systems must integrate signals of external and internal origin to disentangle a sense of self out of the total sensory input. To examine the representation of self-motion in the nervous system, we use rhythmic whisking behavior in the rat vibrissa system as a model for active sensing. First, we characterize the motor plant underlying this behavior through recordings of behavior and muscle activity. These data inform a biomechanical model that establishes the motor plant for whisking and its physical constraints. Having thus defined the control problem of whisking behavior, we investigate its representation in the vibrissa region of primary motor cortex. We found that single units recorded in the behaving animal accurately represent the rhythmic component of the whisk cycle and that small populations of units accurately encode the amplitude and offset position of individual sweeps of the vibrissae. Finally, we compared this data to recordings in the vibrissa region of primary somatosensory cortex. We find that sensory cortex more reliably encodes the rhythmic component of whisking while motor cortex more reliably encodes slower kinetic parameters. These data are consistent with a model that sensory cortex encodes vibrissa position in a normalized coordinate system with the transformation into absolute coordinates requiring a motor copy signal. More generally, our results bear on what may likely be a more general segregation of representation within the nervous system
-------------------------------------


10129817_178 1
10134418_183 1
10133633_183 1
10133536_183 1
10134612_183 1
10138233_183 1
10135170_183 1
10132992_183 1
10138039_183 1
10134941_183 1
10130367_178 1
10135393_183 1
10135490_183 1
10133313_183 1
10136099_183 1
10134659_183 1
10136363_183 1
10138688_183 1
10135949_183 1
10129932_178 1
10139550_183 1
10132005_183 1
10133624_183 1
10133795_183 1
10136714_183 1
10139115_183 1
10175328_189 1
10136275_183 1
10136683_183 1
10136498_183 1
10138242_183 1
10137508_183 1
10134932_183 1
10134493_183 1
10130171_178 1
10134348_183 1
10131851_183 1
10130446_178 1
10137644_183 1
10138970_183 1
10130083_178 1
10133581_183 1
10132830_183 1
10135143_183 1
10132690_183 1
10133128_183 1
10131391_183 1
10139737_183 1
10135287_183 1
10135656_183 1
10136122_183 1
10175394_189 1
10132706_183 1
10134533_183 1
10133554_183 1
10135055_183 1
10132703_183 1
101649_108 1
10133011_183 1
10136266_183 1
10131539_183 1
10136723_183 1
10135113_183 1
10134979_183 1
10136867_183 1
10139483_183 1
10132132_183 1
10137293_183 1
10137641_183 1
10133729_183 1
10134143_183 1
10136468_183 1
10175482_189 1
10130989_183 1
10139090_183 1
10130507_183 1
10137465_183 1
10130211_178 1
10139392_183 1
10137720_183 1
10133895_183 1
10138702_183 1
10134137_183 1
10136110_183 1
10133120_183 1
10131821_183 1
10135783_183 1
10134002_183 1
10175413_189 1
10137758_183 1
10132642_183 1
10129768_178 1
10137284_183 1
10136412_183 1
10133422_183 1
10136442_183 1
10136805_183 1
10136287_183 1
10134615_183 1
10175373_189 1
10133509_183 1
10133703_183 1
10132225_183 1
10132868_183 1
10134841_183 1
10131706_183 1
10133765_183 1
10135134_183 1
10133905_183 1
10135979_183 1
10130407_178 1
10135551_183 1
10129862_178 1
10139940_183 1
10130065_178 1
10137354_183 1
10133589_183 1
10175543_189 1
10136459_183 1
10139112_183 1
10134269_183 1
10134282_183 1
10129923_178 1
10129808_178 1
10134484_183 1
10132768_183 1
10134097_183 1
10137224_183 1
10135436_183 1
10134082_183 1
10129841_178 1
10137505_183 1
10135475_183 1
10136864_183 1
10131854_183 1
10132833_183 1
10132977_183 1
10134113_183 1
10137336_183 1
10133618_183 1
10134593_183 1
10138820_183 1
10134586_183 1
10136711_183 1
10175419_189 1
10137755_183 1
10175370_189 1
10139961_183 1
10134917_183 1
10136785_183 1
10133724_183 1
10135732_183 1
10136312_183 1
10134568_183 1
10136614_183 1
10136125_183 1
10133686_183 1
10133859_183 1
10139166_183 1
10137113_183 1
10131908_183 1
10131690_183 1
10137300_183 1
10137920_183 1
10137444_183 1
10136321_183 1
10138139_183 1
10136296_183 1
10136911_183 1
10136997_183 1
10129974_178 1
10139263_183 1
10137717_183 1
10130413_178 1
10139650_183 1
10134275_183 1
10139208_183 1
10136039_183 1
10135182_183 1
10135671_183 1
10130258_178 1
10136366_183 1
10133325_183 1
10135070_183 1
10137705_183 1
10131192_183 1
10139520_183 1
10129771_178 1
10135700_183 1
10138381_183 1
10130129_178 1
10139467_183 1
10137142_183 1
10139315_183 1
10129695_178 1
10134216_183 1
10136082_183 1
10135595_183 1
10129759_178 1
10129928_178 1
10136969_183 1
10133860_183 1
10136073_183 1
10133753_183 1
10135372_183 1
10135097_183 1
10130255_178 1
101650_108 1
10137160_183 1
10134472_183 1
10130102_178 1
10135418_183 1
10135926_183 1
10135131_183 1
10137679_183 1
10135776_183 1
10134466_183 1
10137206_183 1
10135303_183 1
10171_7 1
10134187_183 1
10133994_183 1
10134294_183 1
10136649_183 1
10137685_183 1
10136762_183 1
10133930_183 1
10133645_183 1
10137653_183 1
10137025_183 1
10129876_178 1
10134396_183 1
10135424_183 1
10137290_183 1
10136515_183 1
10133919_183 1
10175485_189 1
10134527_183 1
10130051_178 1
10131085_183 1
10130060_178 1
10132739_183 1
10134285_183 1
10139099_183 1
10137612_183 1
10135261_183 1
10134983_183 1
10135191_183 1
10133590_183 1
10129920_178 1
10134053_183 1
10129765_178 1
10134498_183 1
10139984_183 1
10133962_183 1
10175387_189 1
10136617_183 1
10136143_183 1
10139699_183 1
10131718_183 1
10137432_183 1
10134272_183 1
10135386_183 1
10130198_178 1
10134434_183 1
10134337_183 1
10129977_178 1
10131668_183 1
10133214_183 1
10134360_183 1
10136782_183 1
10138875_183 1
10135377_183 1
10138958_183 1
10134898_183 1
10133367_183 1
10131310_183 1
10138037_183 1
10134346_183 1
10135066_183 1
10137011_183 1
10134123_183 1
10133687_183 1
10133867_183 1
10136097_183 1
10138893_183 1
10135575_183 1
10130189_178 1
10130225_178 1
10137475_183 1
10130317_178 1
10138309_183 1
10137130_183 1
10140009_183 1
10134565_183 1
10133885_183 1
10129741_178 1
10137549_183 1
10136410_183 1
101379_108 1
10136764_183 1
10137533_183 1
10175396_189 1
10130041_178 1
10130444_178 1
10134790_183 1
10130286_178 1
101661_108 1
10134491_183 1
10139275_183 1
10138397_183 1
10135658_183 1
10133640_183 1
10134763_183 1
10139349_183 1
10130142_178 1
10129943_178 1
10131037_183 1
10135987_183 1
10138296_183 1
10134921_183 1
10131002_183 1
10138256_183 1
10133897_183 1
10134209_183 1
10137722_183 1
10136663_183 1
10136172_183 1
10137133_183 1
10130352_178 1
10130918_183 1
10137037_183 1
10133165_183 1
10135249_183 1
10136557_183 1
10134956_183 1
10129706_178 1
10133012_183 1
10137286_183 1
10136922_183 1
10136510_183 1
10136246_183 1
10133485_183 1
10131949_183 1
10139787_183 1
10134580_183 1
10135712_183 1
10137615_183 1
10133101_183 1
10134188_183 1
10136983_183 1
10137686_183 1
10134811_183 1
10136076_183 1
10134288_183 1
10134473_183 1
10137213_183 1
10130018_178 1
10133750_183 1
10130124_178 1
10136957_183 1
10134930_183 1
10136198_183 1
10135036_183 1
10132296_183 1
10138100_183 1
10134873_183 1
10134141_183 1
10138573_183 1
10130261_178 1
10135206_183 1
10137393_183 1
10135908_183 1
10136329_183 1
10136948_183 1
10130133_178 1
10134556_183 1
10130076_178 1
10134254_183 1
10137450_183 1
10136803_183 1
10135523_183 1
10136501_183 1
10133933_183 1
10135736_183 1
10133778_183 1
10138247_183 1
10133605_183 1
10133648_183 1
10139540_183 1
10129836_178 1
10132753_183 1
10175488_189 1
10129821_178 1
10137339_183 1
10138949_183 1
10134201_183 1
10134683_183 1
10135072_183 1
10134739_183 1
10130062_178 1
10134032_183 1
10133447_183 1
10132840_183 1
10129931_178 1
10136107_183 1
10135788_183 1
10133062_183 1
10134496_183 1
10135250_183 1
10136871_183 1
10133625_183 1
10139767_183 1
10137899_183 1
10175460_189 1
10133551_183 1
10138250_183 1
10133438_183 1
10137082_183 1
10133643_183 1
10130145_178 1
10130219_178 1
10139488_183 1
10133465_183 1
10135626_183 1
10134953_183 1
10135146_183 1
10133501_183 1
10136634_183 1
10135955_183 1
10130341_178 1
10134446_183 1
10137242_183 1
10137588_183 1
10137129_183 1
10131390_183 1
10133871_183 1
10134437_183 1
10134633_183 1
10138025_183 1
10132831_183 1
10129785_178 1
10175424_189 1
10132673_183 1
10131023_183 1
10138081_183 1
10129972_178 1
10137201_183 1
10135644_183 1
10139148_183 1
10131852_183 1
10134277_183 1
10135543_183 1
10137832_183 1
10134334_183 1
10135114_183 1
10134538_183 1
10134805_183 1
101652_108 1
10139904_183 1
10129791_178 1
10134233_183 1
10133030_183 1
10134651_183 1
10133479_183 1
10137464_183 1
10133700_183 1
10175385_189 1
10175474_189 1
10136308_183 1
10137375_183 1
10134464_183 1
10139163_183 1
10135193_183 1
10136139_183 1
10135031_183 1
10136815_183 1
10130110_178 1
10135267_183 1
10133968_183 1
10134331_183 1
10136080_183 1
10134903_183 1
10135095_183 1
10138342_183 1
10136577_183 1
10133888_183 1
10134366_183 1
10136160_183 1
10131991_183 1
10138961_183 1
10158_7 1
10136488_183 1
10138276_183 1
10133773_183 1
10137340_183 1
10135463_183 1
10131529_183 1
10137227_183 1
10134944_183 1
10136202_183 1
10131389_183 1
10133879_183 1
10137064_183 1
10130266_178 1
10134798_183 1
10136732_183 1
10136602_183 1
10137538_183 1
10129963_178 1
10135063_183 1
10133462_183 1
10130079_178 1
10175401_189 1
10135578_183 1
10137268_183 1
10130192_178 1
10175459_189 1
10136545_183 1
10135235_183 1
10129865_178 1
10133634_183 1
10134985_183 1
10135007_183 1
10131846_183 1
10129752_178 1
10134450_183 1
10139935_183 1
10139185_183 1
10130070_178 1
10130209_178 1
10134918_183 1
10137889_183 1
10129761_178 1
10135573_183 1
10138974_183 1
10134024_183 1
10136117_183 1
10133277_183 1
10132408_183 1
10139167_183 1
10133694_183 1
10131535_183 1
10137763_183 1
10134265_183 1
10129646_178 1
10134344_183 1
10135332_183 1
10135253_183 1
10133365_183 1
10134488_183 1
10131398_183 1
10130052_178 1
10136205_183 1
10133036_183 1
10133550_183 1
10133939_183 1
10133009_183 1
10138895_183 1
10136038_183 1
10130284_178 1
10138267_183 1
10135915_183 1
10129716_178 1
10137903_183 1
10133743_183 1
10131679_183 1
10135402_183 1
10133869_183 1
10137772_183 1
10134963_183 1
10133027_183 1
10132688_183 1
10133426_183 1
10133799_183 1
10132676_183 1
10136065_183 1
10137171_183 1
10133628_183 1
10131680_183 1
10137504_183 1
10129743_178 1
10175438_189 1
10137346_183 1
10134691_183 1
10133984_183 1
10130500_183 1
10137188_183 1
10132145_183 1
10133755_183 1
10137531_183 1
10130201_178 1
10136244_183 1
10134449_183 1
10135951_183 1
10136003_183 1
10136968_183 1
10135371_183 1
10136074_183 1
10133737_183 1
10137609_183 1
10131386_183 1
10135177_183 1
10133514_183 1
10134822_183 1
10132036_183 1
10134411_183 1
10135534_183 1
10133969_183 1
10133520_183 1
10134063_183 1
10134467_183 1
10136068_183 1
10138986_183 1
10134939_183 1
10136827_183 1
10136657_183 1
10132142_183 1
10131779_183 1
10134875_183 1
10135247_183 1
10134954_183 1
10139689_183 1
10133265_183 1
10130999_183 1
10139216_183 1
10129804_178 1
10135030_183 1
10133289_183 1
10132957_183 1
10175430_189 1
10139681_183 1
10134984_183 1
10134713_183 1
10132136_183 1
10131834_183 1
10129910_178 1
10134186_183 1
10138847_183 1
10137026_183 1
10133691_183 1
10136570_183 1
101642_108 1
10130293_178 1
10130061_178 1
10138188_183 1
10136731_183 1
10132166_183 1
10133018_183 1
10132151_183 1
10131702_183 1
10134054_183 1
10135664_183 1
10136818_183 1
10137367_183 1
10175417_189 1
10135479_183 1
10133785_183 1
10137440_183 1
10136929_183 1
10136179_183 1
10130055_178 1
10131676_183 1
10137030_183 1
10137158_183 1
10134987_183 1
10132148_183 1
10135065_183 1
10139515_183 1
10138572_183 1
10134203_183 1
10134602_183 1
10138815_183 1
10134030_183 1
10138162_183 1
10137690_183 1
10129719_178 1
10133873_183 1
10136060_183 1
10133972_183 1
10134960_183 1
10134494_183 1
10130152_178 1
10137992_183 1
10130125_178 1
10136197_183 1
10133623_183 1
10129969_178 1
10134825_183 1
10131955_183 1
10139092_183 1
10136450_183 1
10134221_183 1
10137088_183 1
10168_7 1
10136909_183 1
10135471_183 1
10132488_183 1
10130213_178 1
10136981_183 1
10138252_183 1
10132707_183 1
10134834_183 1
10132820_183 1
10135455_183 1
10129845_178 1
10138340_183 1
10139231_183 1
10134108_183 1
10131850_183 1
10133943_183 1
10134505_183 1
10136610_183 1
10131996_183 1
10132749_183 1
10133039_183 1
10135585_183 1
10132819_183 1
10135083_183 1
10134235_183 1
10134320_183 1
10136008_183 1
10135601_183 1
10137573_183 1
10175393_189 1
10129915_178 1
10135887_183 1
10134048_183 1
10134377_183 1
10131289_183 1
10131831_183 1
10136930_183 1
10132195_183 1
10136147_183 1
10137029_183 1
10138950_183 1
10130329_178 1
10134305_183 1
10135516_183 1
10129784_178 1
10135265_183 1
10136162_183 1
10133915_183 1
10137787_183 1
10134118_183 1
10135205_183 1
10139058_183 1
10137279_183 1
10133594_183 1
10137349_183 1
10136915_183 1
10135531_183 1
10130096_178 1
10133423_183 1
10130210_178 1
10133486_183 1
10137098_183 1
10138409_183 1
10132113_183 1
10135195_183 1
10138833_183 1
10138348_183 1
10138107_183 1
10132652_183 1
10136458_183 1
10129778_178 1
10134840_183 1
10139180_183 1
10136441_183 1
10134453_183 1
10136604_183 1
10129924_178 1
10132009_183 1
10129807_178 1
10135922_183 1
10135678_183 1
10137225_183 1
10175444_189 1
10139384_183 1
10130064_178 1
10135220_183 1
10134857_183 1
10132611_183 1
10130190_178 1
10139499_183 1
10134942_183 1
10135391_183 1
10134127_183 1
10134383_183 1
10135609_183 1
10133990_183 1
10135722_183 1
10135645_183 1
10136953_183 1
10134531_183 1
10129930_178 1
10130392_178 1
10132012_183 1
10138314_183 1
10161_7 1
10175612_189 1
10130128_178 1
10129671_178 1
10175450_189 1
10135539_183 1
10129745_178 1
10136944_183 1
10131168_183 1
10137409_183 1
10137738_183 1
10135154_183 1
10133529_183 1
10132823_183 1
10138823_183 1
10135627_183 1
10133538_183 1
10129819_178 1
10134193_183 1
10134495_183 1
10133570_183 1
10130374_178 1
10130331_178 1
10132994_183 1
10130045_178 1
10138222_183 1
10129912_178 1
10133135_183 1
10133903_183 1
10133946_183 1
10139946_183 1
10132458_183 1
10139530_183 1
10133644_183 1
10137229_183 1
10135510_183 1
10135553_183 1
10175441_189 1
10134925_183 1
10139187_183 1
10137006_183 1
10136032_183 1
10129973_178 1
10135602_183 1
10135343_183 1
10136813_183 1
10138485_183 1
10133169_183 1
10132674_183 1
10134438_183 1
10135672_183 1
10135544_183 1
10132787_183 1
10138350_183 1
10137301_183 1
10133982_183 1
10131682_183 1
10129644_178 1
10133241_183 1
10138012_183 1
10131195_183 1
10134977_183 1
10135417_183 1
10137619_183 1
10137317_183 1
10131981_183 1
10137116_183 1
10132731_183 1
10132832_183 1
10135400_183 1
10135242_183 1
10132030_183 1
10132775_183 1
10133354_183 1
10135106_183 1
10137730_183 1
10133665_183 1
10134071_183 1
10138104_183 1
10134241_183 1
10129970_178 1
10135096_183 1
10137528_183 1
10138604_183 1
10175384_189 1
10134830_183 1
10130093_178 1
10130111_178 1
10135702_183 1
10137143_183 1
10129802_178 1
10134315_183 1
10135929_183 1
10175475_189 1
10131511_183 1
10134797_183 1
10131879_183 1
10136048_183 1
10136340_183 1
10132936_183 1
10134617_183 1
10135202_183 1
10137643_183 1
101638_108 1
10132757_183 1
10138415_183 1
10135281_183 1
10136840_183 1
10137107_183 1
10130089_178 1
10137463_183 1
10138994_183 1
10129781_178 1
10135610_183 1
10136861_183 1
10137756_183 1
10133503_183 1
10138734_183 1
10133635_183 1
10135179_183 1
10133763_183 1
10135764_183 1
10136014_183 1
10132138_183 1
10129921_178 1
10135008_183 1
10135364_183 1
10175402_189 1
10130063_178 1
10137480_183 1
10133031_183 1
10133561_183 1
10129679_178 1
10139398_183 1
10133866_183 1
10136132_183 1
10131525_183 1
10135491_183 1
10136783_183 1
10134658_183 1
10138216_183 1
10134845_183 1
10133609_183 1
10132921_183 1
10133679_183 1
10134516_183 1
10129798_178 1
10139621_183 1
10136704_183 1
10133224_183 1
10137181_183 1
10132556_183 1
10135473_183 1
10131507_183 1
10131678_183 1
10136900_183 1
10137658_183 1
10134525_183 1
10134818_183 1
10129789_178 1
10135245_183 1
10132441_183 1
10134989_183 1
10137190_183 1
10129976_178 1
10139630_183 1
10135660_183 1
10134388_183 1
10132988_183 1
10138396_183 1
10134163_183 1
10133629_183 1
10139173_183 1
10135110_183 1
10129814_178 1
10131036_183 1
10136377_183 1
10134018_183 1
10134131_183 1
10134379_183 1
10134575_183 1
10136754_183 1
10137516_183 1
10133958_183 1
10132835_183 1
10133040_183 1
10134460_183 1
10136621_183 1
10132734_183 1
10140090_183 1
10137086_183 1
10136150_183 1
10133526_183 1
10129757_178 1
10133481_183 1
10138369_183 1
10133898_183 1
10130057_178 1
10139763_183 1
10130443_178 1
10133941_183 1
10129900_178 1
10131661_183 1
10136760_183 1
10136736_183 1
10134410_183 1
10135533_183 1
10134474_183 1
10135435_183 1
10137214_183 1
10130514_183 1
10137972_183 1
10135026_183 1
10134794_183 1
10133932_183 1
10136647_183 1
10133736_183 1
10134296_183 1
10136149_183 1
10130351_178 1
10135899_183 1
10135426_183 1
10137534_183 1
10130498_183 1
10137321_183 1
10135518_183 1
10135174_183 1
10135352_183 1
10139573_183 1
10130425_178 1
10133923_183 1
10136656_183 1
10134868_183 1
10136075_183 1
10134001_183 1
10137483_183 1
10136256_183 1
10133002_183 1
10136973_183 1
10129805_178 1
10137394_183 1
10136949_183 1
10135924_183 1
10130019_178 1
10133419_183 1
10133573_183 1
10130075_178 1
10175405_189 1
10135980_183 1
10133621_183 1
10135109_183 1
10132956_183 1
10175470_189 1
10138518_183 1
10135639_183 1
10131859_183 1
10134270_183 1
10130262_178 1
10134068_183 1
10129991_178 1
10134010_183 1
10137566_183 1
10134181_183 1
10136630_183 1
10136819_183 1
10135091_183 1
10175487_189 1
10130508_183 1
10129893_178 1
10135263_183 1
10134557_183 1
10136573_183 1
10133549_183 1

